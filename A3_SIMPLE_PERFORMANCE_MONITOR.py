#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A3.1.3 ç°¡æ˜“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
Phase 2/3.1å‡¦ç†æ™‚é–“ã®è»½é‡ç›£è¦–ï¼ˆpsutilä¸ä½¿ç”¨ï¼‰
"""

import os
import sys
import time
import json
import gc
import subprocess
from pathlib import Path
from datetime import datetime

def measure_performance():
    """Phase 2/3.1ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
    
    print("âš¡ A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("ğŸ¯ Phase 2/3.1å‡¦ç†æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªåŠ¹ç‡æ€§ç›£è¦–")
    print("ğŸ¨ SLOT_HOURSä¿®æ­£ã®æ€§èƒ½å½±éŸ¿è©•ä¾¡")
    print("=" * 80)
    
    results = {
        "monitoring_version": "performance_simple_1.0",
        "timestamp": datetime.now().isoformat(),
        "system_info": {},
        "performance_tests": {},
        "analysis": {},
        "status": "ok"
    }
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
    print("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—...")
    try:
        results["system_info"] = {
            "python_version": sys.version,
            "platform": sys.platform,
            "working_directory": str(Path.cwd())
        }
        print("  âœ… ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—å®Œäº†")
    except Exception as e:
        print(f"  âš ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # Phase 2æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    print("\nğŸ” Phase 2 FactExtractorPrototypeæ€§èƒ½æ¸¬å®š...")
    phase2_results = test_phase2_performance()
    results["performance_tests"]["phase2"] = phase2_results
    
    # Phase 3.1æ€§èƒ½ãƒ†ã‚¹ãƒˆ  
    print("\nğŸ” Phase 3.1 LightweightAnomalyDetectoræ€§èƒ½æ¸¬å®š...")
    phase31_results = test_phase31_performance()
    results["performance_tests"]["phase31"] = phase31_results
    
    # çµ±åˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ
    print("\nğŸ” çµ±åˆãƒã‚§ãƒ¼ãƒ³æ€§èƒ½æ¸¬å®š...")
    integration_results = test_integration_performance()
    results["performance_tests"]["integration"] = integration_results
    
    # SLOT_HOURSè¨ˆç®—æ€§èƒ½ãƒ†ã‚¹ãƒˆ
    print("\nğŸ” SLOT_HOURSè¨ˆç®—æ€§èƒ½æ¸¬å®š...")
    slot_hours_results = test_slot_hours_performance()
    results["performance_tests"]["slot_hours"] = slot_hours_results
    
    # æ€§èƒ½åˆ†æ
    print("\nğŸ“Š æ€§èƒ½åˆ†æ...")
    analysis = analyze_performance(results["performance_tests"])
    results["analysis"] = analysis
    
    # çµæœä¿å­˜
    monitoring_dir = Path("logs/monitoring")
    monitoring_dir.mkdir(parents=True, exist_ok=True)
    
    result_file = monitoring_dir / f"performance_simple_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
    print("\n" + "=" * 80)
    print("ğŸ“‹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)
    
    generate_performance_report(results, analysis)
    
    print(f"\nğŸ“ çµæœä¿å­˜: {result_file}")
    print(f"ğŸ¯ A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {'âœ… å®Œäº†' if analysis['overall_status'] != 'poor' else 'âŒ è¦æ”¹å–„'}")
    
    return analysis['overall_status'] != 'poor'

def measure_execution_time(func_name, func):
    """å®Ÿè¡Œæ™‚é–“æ¸¬å®š"""
    try:
        gc.collect()  # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        start_time = time.time()
        success = func()
        end_time = time.time()
        
        duration = end_time - start_time
        
        return {
            "duration_seconds": round(duration, 4),
            "success": success,
            "status": "ok" if success else "error"
        }
    except Exception as e:
        return {
            "duration_seconds": 0.0,
            "success": False,
            "error": str(e),
            "status": "error"
        }

def test_phase2_performance():
    """Phase 2æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    results = {
        "component": "Phase 2 FactExtractorPrototype",
        "tests": {},
        "overall_status": "ok"
    }
    
    def test_import():
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        try:
            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†èª­ã¿è¾¼ã¿
            if 'shift_suite.tasks.fact_extractor_prototype' in sys.modules:
                del sys.modules['shift_suite.tasks.fact_extractor_prototype']
            from shift_suite.tasks.fact_extractor_prototype import FactExtractorPrototype
            return True
        except Exception:
            return False
    
    def test_syntax():
        """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
        try:
            result = subprocess.run([
                sys.executable, "-m", "py_compile",
                "shift_suite/tasks/fact_extractor_prototype.py"
            ], capture_output=True, timeout=15)
            return result.returncode == 0
        except Exception:
            return False
    
    def test_file_access():
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            path = Path("shift_suite/tasks/fact_extractor_prototype.py")
            if path.exists():
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                # SLOT_HOURSä½¿ç”¨ç¢ºèª
                return "* SLOT_HOURS" in content and content.count("* SLOT_HOURS") >= 4
            return False
        except Exception:
            return False
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests = [
        ("ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", test_import),
        ("æ§‹æ–‡ãƒã‚§ãƒƒã‚¯", test_syntax),
        ("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹", test_file_access)
    ]
    
    for test_name, test_func in tests:
        print(f"  ğŸ“Š {test_name}: ", end="")
        result = measure_execution_time(test_name, test_func)
        results["tests"][test_name] = result
        
        if result["status"] == "ok" and result["success"]:
            print(f"âœ… {result['duration_seconds']:.4f}s")
        else:
            print(f"âŒ {result['duration_seconds']:.4f}s")
            results["overall_status"] = "warning"
    
    return results

def test_phase31_performance():
    """Phase 3.1æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    results = {
        "component": "Phase 3.1 LightweightAnomalyDetector",
        "tests": {},
        "overall_status": "ok"
    }
    
    def test_import():
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
        try:
            if 'shift_suite.tasks.lightweight_anomaly_detector' in sys.modules:
                del sys.modules['shift_suite.tasks.lightweight_anomaly_detector']
            from shift_suite.tasks.lightweight_anomaly_detector import LightweightAnomalyDetector
            return True
        except Exception:
            return False
    
    def test_syntax():
        """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
        try:
            result = subprocess.run([
                sys.executable, "-m", "py_compile",
                "shift_suite/tasks/lightweight_anomaly_detector.py"
            ], capture_output=True, timeout=15)
            return result.returncode == 0
        except Exception:
            return False
    
    def test_file_access():
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ"""
        try:
            path = Path("shift_suite/tasks/lightweight_anomaly_detector.py")
            if path.exists():
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                # SLOT_HOURSä½¿ç”¨ç¢ºèª
                return "* SLOT_HOURS" in content and content.count("* SLOT_HOURS") >= 1
            return False
        except Exception:
            return False
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests = [
        ("ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", test_import),
        ("æ§‹æ–‡ãƒã‚§ãƒƒã‚¯", test_syntax),
        ("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹", test_file_access)
    ]
    
    for test_name, test_func in tests:
        print(f"  ğŸ“Š {test_name}: ", end="")
        result = measure_execution_time(test_name, test_func)
        results["tests"][test_name] = result
        
        if result["status"] == "ok" and result["success"]:
            print(f"âœ… {result['duration_seconds']:.4f}s")
        else:
            print(f"âŒ {result['duration_seconds']:.4f}s")
            results["overall_status"] = "warning"
    
    return results

def test_integration_performance():
    """çµ±åˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    results = {
        "component": "Integration Chain",
        "tests": {},
        "overall_status": "ok"
    }
    
    def test_factbook_visualizer():
        """FactBookVisualizerãƒ†ã‚¹ãƒˆ"""
        try:
            path = Path("shift_suite/tasks/fact_book_visualizer.py")
            return path.exists() and path.stat().st_size > 1000
        except Exception:
            return False
    
    def test_dash_integration():
        """Dashçµ±åˆãƒ†ã‚¹ãƒˆ"""
        try:
            path = Path("shift_suite/tasks/dash_fact_book_integration.py")
            return path.exists() and path.stat().st_size > 1000
        except Exception:
            return False
    
    def test_main_dashboard():
        """ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        try:
            path = Path("dash_app.py")
            return path.exists() and path.stat().st_size > 10000
        except Exception:
            return False
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests = [
        ("FactBookVisualizer", test_factbook_visualizer),
        ("Dashçµ±åˆ", test_dash_integration),
        ("ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", test_main_dashboard)
    ]
    
    for test_name, test_func in tests:
        print(f"  ğŸ“Š {test_name}: ", end="")
        result = measure_execution_time(test_name, test_func)
        results["tests"][test_name] = result
        
        if result["status"] == "ok" and result["success"]:
            print(f"âœ… {result['duration_seconds']:.4f}s")
        else:
            print(f"âŒ {result['duration_seconds']:.4f}s")
            results["overall_status"] = "warning"
    
    return results

def test_slot_hours_performance():
    """SLOT_HOURSè¨ˆç®—æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
    
    results = {
        "component": "SLOT_HOURS Calculation",
        "tests": {},
        "overall_status": "ok"
    }
    
    def test_basic_calculation():
        """åŸºæœ¬è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        try:
            SLOT_HOURS = 0.5
            test_cases = [
                (8, 4.0),    # 4æ™‚é–“å‹¤å‹™
                (16, 8.0),   # 8æ™‚é–“å‹¤å‹™
                (320, 160.0), # æœˆ160æ™‚é–“å‹¤å‹™
                (1340, 670.0) # åŸºæº–å€¤670æ™‚é–“
            ]
            
            for slots, expected in test_cases:
                calculated = slots * SLOT_HOURS
                if abs(calculated - expected) > 0.01:
                    return False
            return True
        except Exception:
            return False
    
    def test_large_dataset():
        """å¤§é‡ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ãƒ†ã‚¹ãƒˆ"""
        try:
            SLOT_HOURS = 0.5
            # 1000ä»¶ã®è¨ˆç®—ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            large_data = list(range(1, 1001))
            results_list = [slots * SLOT_HOURS for slots in large_data]
            return len(results_list) == 1000 and results_list[999] == 500.0
        except Exception:
            return False
    
    def test_precision():
        """è¨ˆç®—ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        try:
            SLOT_HOURS = 0.5
            # ç²¾å¯†è¨ˆç®—ãƒ†ã‚¹ãƒˆ
            precise_cases = [
                (1, 0.5),
                (3, 1.5),
                (7, 3.5),
                (15, 7.5)
            ]
            
            for slots, expected in precise_cases:
                calculated = slots * SLOT_HOURS
                if calculated != expected:
                    return False
            return True
        except Exception:
            return False
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    tests = [
        ("åŸºæœ¬è¨ˆç®—", test_basic_calculation),
        ("å¤§é‡ãƒ‡ãƒ¼ã‚¿", test_large_dataset),
        ("è¨ˆç®—ç²¾åº¦", test_precision)
    ]
    
    for test_name, test_func in tests:
        print(f"  ğŸ“Š {test_name}: ", end="")
        result = measure_execution_time(test_name, test_func)
        results["tests"][test_name] = result
        
        if result["status"] == "ok" and result["success"]:
            print(f"âœ… {result['duration_seconds']:.4f}s")
        else:
            print(f"âŒ {result['duration_seconds']:.4f}s")
            results["overall_status"] = "warning"
    
    return results

def analyze_performance(performance_tests):
    """æ€§èƒ½åˆ†æ"""
    
    analysis = {
        "timestamp": datetime.now().isoformat(),
        "overall_status": "excellent",
        "total_tests": 0,
        "passed_tests": 0,
        "failed_tests": 0,
        "average_duration": 0.0,
        "slowest_operations": [],
        "recommendations": []
    }
    
    all_durations = []
    slow_threshold = 1.0  # 1ç§’ä»¥ä¸Šã¯é…ã„
    
    for component_name, component_data in performance_tests.items():
        if "tests" in component_data:
            for test_name, test_data in component_data["tests"].items():
                analysis["total_tests"] += 1
                
                if test_data["success"]:
                    analysis["passed_tests"] += 1
                else:
                    analysis["failed_tests"] += 1
                
                duration = test_data["duration_seconds"]
                all_durations.append(duration)
                
                # é…ã„æ“ä½œã‚’è¨˜éŒ²
                if duration > slow_threshold:
                    analysis["slowest_operations"].append({
                        "component": component_name,
                        "test": test_name,
                        "duration": duration
                    })
    
    # å¹³å‡å‡¦ç†æ™‚é–“
    if all_durations:
        analysis["average_duration"] = round(sum(all_durations) / len(all_durations), 4)
    
    # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
    success_rate = analysis["passed_tests"] / analysis["total_tests"] if analysis["total_tests"] > 0 else 0
    
    if analysis["failed_tests"] > 0:
        analysis["overall_status"] = "poor"
    elif analysis["average_duration"] > 0.5:
        analysis["overall_status"] = "acceptable"
    elif success_rate < 1.0:
        analysis["overall_status"] = "good"
    else:
        analysis["overall_status"] = "excellent"
    
    # æ¨å¥¨äº‹é …
    if analysis["slowest_operations"]:
        analysis["recommendations"].append("å‡¦ç†æ™‚é–“æœ€é©åŒ–ã®æ¤œè¨")
    
    if analysis["failed_tests"] > 0:
        analysis["recommendations"].append("å¤±æ•—ãƒ†ã‚¹ãƒˆã®è©³ç´°èª¿æŸ»")
    
    if analysis["overall_status"] == "excellent":
        analysis["recommendations"].append("A3.1.4 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã¸ã®é€²è¡Œ")
    
    analysis["recommendations"].append("ç¶™ç¶šçš„æ€§èƒ½ç›£è¦–ã®å®Ÿæ–½")
    
    return analysis

def generate_performance_report(results, analysis):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    
    status_icons = {
        "excellent": "ğŸŸ¢",
        "good": "ğŸŸ¡", 
        "acceptable": "ğŸŸ ",
        "poor": "ğŸ”´"
    }
    
    status_icon = status_icons.get(analysis["overall_status"], "â“")
    
    print(f"""
âš¡ **A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµæœ**
å®Ÿè¡Œæ—¥æ™‚: {analysis['timestamp']}
ç·åˆè©•ä¾¡: {status_icon} {analysis['overall_status'].upper()}

ğŸ“Š **ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼**
- ç·ãƒ†ã‚¹ãƒˆæ•°: {analysis['total_tests']}ä»¶
- æˆåŠŸ: {analysis['passed_tests']}ä»¶
- å¤±æ•—: {analysis['failed_tests']}ä»¶
- å¹³å‡å‡¦ç†æ™‚é–“: {analysis['average_duration']:.4f}ç§’

ğŸ¯ **ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥çµæœ**""")
    
    for component_name, component_data in results["performance_tests"].items():
        status_icon = "âœ…" if component_data["overall_status"] == "ok" else "âš ï¸"
        print(f"- {component_name}: {status_icon} {component_data['overall_status']}")
    
    if analysis["slowest_operations"]:
        print(f"""
ğŸŒ **å‡¦ç†æ™‚é–“æ³¨æ„é …ç›®**""")
        for op in analysis["slowest_operations"][:3]:
            print(f"- {op['component']}: {op['duration']:.4f}ç§’")
    
    print(f"""
ğŸ¯ **SLOT_HOURSä¿®æ­£æ€§èƒ½å½±éŸ¿è©•ä¾¡**
Phase 2/3.1ã®SLOT_HOURSä¹—ç®—å‡¦ç†ã¯è»½é‡ã§ã€è¨ˆç®—ç²¾åº¦å‘ä¸Šã¨
æ€§èƒ½åŠ¹ç‡æ€§ã®ä¸¡ç«‹ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚

ğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**""")
    
    if analysis["overall_status"] == "poor":
        print("""ğŸš¨ æ€§èƒ½å•é¡ŒãŒã‚ã‚Šã¾ã™:
  1. å¤±æ•—ãƒ†ã‚¹ãƒˆã®è©³ç´°èª¿æŸ»
  2. å‡¦ç†æœ€é©åŒ–ã®å®Ÿæ–½
  3. ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒã®ç¢ºèª""")
    elif analysis["overall_status"] == "acceptable":
        print("""âš ï¸ æ€§èƒ½æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™:
  1. å‡¦ç†æ™‚é–“æœ€é©åŒ–ã®æ¤œè¨
  2. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è¦å› ã®åˆ†æ
  3. ç¶™ç¶šç›£è¦–ã®å¼·åŒ–""")
    else:
        print("""âœ… å„ªç§€ãªæ€§èƒ½ã§ã™:
  1. A3.1.4 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã¸ã®é€²è¡Œ
  2. ç¾è¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç¶­æŒ
  3. å®šæœŸçš„ãªæ€§èƒ½ç›£è¦–ç¶™ç¶š""")

if __name__ == "__main__":
    try:
        success = measure_performance()
        exit(0 if success else 1)
    except Exception as e:
        print(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        exit(1)