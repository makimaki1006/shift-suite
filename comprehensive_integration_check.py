#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Phase 2/3.1ä¿®æ­£ã®åŒ…æ‹¬çš„å½±éŸ¿ç¢ºèª
ä¾å­˜é–¢ä¿‚ãªã—ã§ã®é™çš„åˆ†æã«ã‚ˆã‚‹å®Œå…¨æ¤œè¨¼
"""

import re
from pathlib import Path
from datetime import datetime

def analyze_code_dependencies():
    """ã‚³ãƒ¼ãƒ‰ä¾å­˜é–¢ä¿‚ã®åˆ†æ"""
    
    print("ğŸ” A. ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ä¾å­˜é–¢ä¿‚åˆ†æ")
    print("=" * 60)
    
    # Phase 2/3.1ã‚’ä½¿ç”¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®š
    usage_files = [
        "shift_suite/tasks/fact_book_visualizer.py",
        "shift_suite/tasks/dash_fact_book_integration.py"
    ]
    
    dependencies = {}
    
    for file_path in usage_files:
        path = Path(file_path)
        if path.exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Phase 2/3.1ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèª
                phase2_import = "FactExtractorPrototype" in content
                phase31_import = "LightweightAnomalyDetector" in content
                
                # ä½¿ç”¨ç®‡æ‰€ã‚’ç¢ºèª
                phase2_usage = content.count("fact_extractor") + content.count("FactExtractorPrototype")
                phase31_usage = content.count("anomaly_detector") + content.count("LightweightAnomalyDetector")
                
                dependencies[file_path] = {
                    "phase2_import": phase2_import,
                    "phase31_import": phase31_import,
                    "phase2_usage_count": phase2_usage,
                    "phase31_usage_count": phase31_usage
                }
                
                print(f"âœ… {file_path}:")
                print(f"  Phase 2ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {'âœ“' if phase2_import else 'âœ—'}")
                print(f"  Phase 3.1ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {'âœ“' if phase31_import else 'âœ—'}")
                print(f"  Phase 2ä½¿ç”¨ç®‡æ‰€: {phase2_usage}ç®‡æ‰€")
                print(f"  Phase 3.1ä½¿ç”¨ç®‡æ‰€: {phase31_usage}ç®‡æ‰€")
                
            except Exception as e:
                print(f"âŒ {file_path} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                dependencies[file_path] = {"error": str(e)}
        else:
            print(f"âš ï¸ {file_path} ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨")
    
    return dependencies

def analyze_dash_integration():
    """Dashçµ±åˆã¸ã®å½±éŸ¿åˆ†æ"""
    
    print("\nğŸ” B. Dashçµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆ†æ")
    print("=" * 60)
    
    dash_files = [
        "dash_app.py",
        "shift_suite/tasks/dash_fact_book_integration.py"
    ]
    
    integration_status = {}
    
    for file_path in dash_files:
        path = Path(file_path)
        if path.exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Dashçµ±åˆã®ç¢ºèª
                fact_book_import = "dash_fact_book_integration" in content
                fact_book_usage = content.count("fact_book") + content.count("FactBook")
                
                # ã‚¿ãƒ–å®šç¾©ã®ç¢ºèª
                tab_definition = "create_fact_book_analysis_tab" in content
                callback_registration = "register_fact_book_callbacks" in content
                
                integration_status[file_path] = {
                    "fact_book_import": fact_book_import,
                    "fact_book_usage": fact_book_usage,
                    "tab_definition": tab_definition,
                    "callback_registration": callback_registration
                }
                
                print(f"âœ… {file_path}:")
                print(f"  FactBookçµ±åˆ: {'âœ“' if fact_book_import else 'âœ—'}")
                print(f"  ä½¿ç”¨ç®‡æ‰€: {fact_book_usage}ç®‡æ‰€")
                print(f"  ã‚¿ãƒ–å®šç¾©: {'âœ“' if tab_definition else 'âœ—'}")
                print(f"  ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯: {'âœ“' if callback_registration else 'âœ—'}")
                
            except Exception as e:
                print(f"âŒ {file_path} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                integration_status[file_path] = {"error": str(e)}
        else:
            print(f"âš ï¸ {file_path} ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨")
    
    return integration_status

def analyze_calculation_impact():
    """è¨ˆç®—çµæœã¸ã®å½±éŸ¿åˆ†æ"""
    
    print("\nğŸ” C. è¨ˆç®—çµæœå½±éŸ¿åˆ†æ")
    print("=" * 60)
    
    # ä¿®æ­£ç®‡æ‰€ã®ç¢ºèª
    modified_files = [
        "shift_suite/tasks/fact_extractor_prototype.py",
        "shift_suite/tasks/lightweight_anomaly_detector.py"
    ]
    
    calculation_changes = {}
    
    for file_path in modified_files:
        path = Path(file_path)
        if path.exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # SLOT_HOURSä¹—ç®—ç®‡æ‰€ã®ç¢ºèª
                slot_hours_count = content.count('* SLOT_HOURS')
                
                # èª¤ã£ãŸã‚³ãƒ¡ãƒ³ãƒˆã®æ®‹å­˜ç¢ºèª
                wrong_comment = "parsed_slots_count is already in hours" in content
                
                # æ­£ã—ã„è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¢ºèª
                correct_patterns = [
                    "total_hours = group['parsed_slots_count'].sum() * SLOT_HOURS",
                    "monthly_hours = work_df.groupby(['staff', 'year_month'])['parsed_slots_count'].sum() * SLOT_HOURS"
                ]
                
                correct_count = sum(1 for pattern in correct_patterns if pattern in content)
                
                calculation_changes[file_path] = {
                    "slot_hours_multiplications": slot_hours_count,
                    "wrong_comments": wrong_comment,
                    "correct_patterns": correct_count,
                    "expected_patterns": len(correct_patterns) if "fact_extractor" in file_path else 1
                }
                
                print(f"âœ… {file_path}:")
                print(f"  SLOT_HOURSä¹—ç®—: {slot_hours_count}ç®‡æ‰€")
                print(f"  èª¤ã£ãŸã‚³ãƒ¡ãƒ³ãƒˆ: {'æ®‹å­˜' if wrong_comment else 'é™¤å»æ¸ˆã¿'}")
                print(f"  æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³: {correct_count}ç®‡æ‰€")
                
                # å“è³ªåˆ¤å®š
                if file_path.endswith("fact_extractor_prototype.py"):
                    expected = 4  # 4ç®‡æ‰€ã®ä¿®æ­£ã‚’æœŸå¾…
                    quality = "âœ… è‰¯å¥½" if slot_hours_count >= expected and not wrong_comment else "âŒ è¦ç¢ºèª"
                else:
                    expected = 1  # 1ç®‡æ‰€ã®ä¿®æ­£ã‚’æœŸå¾…
                    quality = "âœ… è‰¯å¥½" if slot_hours_count >= expected and not wrong_comment else "âŒ è¦ç¢ºèª"
                
                print(f"  å“è³ªè©•ä¾¡: {quality}")
                
            except Exception as e:
                print(f"âŒ {file_path} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                calculation_changes[file_path] = {"error": str(e)}
        else:
            print(f"âš ï¸ {file_path} ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨")
    
    return calculation_changes

def analyze_business_impact():
    """ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿ã®åˆ†æ"""
    
    print("\nğŸ” D. ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿åˆ†æ")
    print("=" * 60)
    
    # ç†è«–çš„å½±éŸ¿ã®åˆ†æ
    impact_scenarios = {
        "åŠ´åƒæ™‚é–“çµ±è¨ˆ": {
            "ä¿®æ­£å‰": "2å€ã®å€¤ã§è¡¨ç¤ºï¼ˆéå¤§è©•ä¾¡ï¼‰",
            "ä¿®æ­£å¾Œ": "æ­£ç¢ºãªåŠ´åƒæ™‚é–“ï¼ˆé©æ­£è©•ä¾¡ï¼‰",
            "å½±éŸ¿åº¦": "é«˜",
            "ãƒªã‚¹ã‚¯": "çµŒå–¶åˆ¤æ–­ã®èª¤ã‚Š"
        },
        "ç•°å¸¸æ¤œçŸ¥": {
            "ä¿®æ­£å‰": "é–¾å€¤åˆ¤å®šãŒä¸æ­£ç¢ºï¼ˆèª¤æ¤œçŸ¥ï¼‰",
            "ä¿®æ­£å¾Œ": "é©åˆ‡ãªç•°å¸¸æ¤œçŸ¥ï¼ˆæ­£ç¢ºãªåˆ¤å®šï¼‰",
            "å½±éŸ¿åº¦": "é«˜",
            "ãƒªã‚¹ã‚¯": "æ³•çš„é•åã®è¦‹è½ã¨ã—"
        },
        "å¯è¦–åŒ–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰": {
            "ä¿®æ­£å‰": "ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆãŒ2å€å€¤",
            "ä¿®æ­£å¾Œ": "æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿è¡¨ç¤º",
            "å½±éŸ¿åº¦": "ä¸­",
            "ãƒªã‚¹ã‚¯": "ç¾å ´åˆ¤æ–­ã®èª¤ã‚Š"
        },
        "ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›": {
            "ä¿®æ­£å‰": "Excel/CSVå‡ºåŠ›ãŒä¸æ­£ç¢º",
            "ä¿®æ­£å¾Œ": "æ­£ç¢ºãªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ",
            "å½±éŸ¿åº¦": "ä¸­",
            "ãƒªã‚¹ã‚¯": "ç›£æŸ»å¯¾å¿œã®å•é¡Œ"
        }
    }
    
    print("ğŸ“Š ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿ã‚·ãƒŠãƒªã‚ª:")
    
    for scenario, details in impact_scenarios.items():
        print(f"\n  ğŸ¯ {scenario}:")
        print(f"    ä¿®æ­£å‰: {details['ä¿®æ­£å‰']}")
        print(f"    ä¿®æ­£å¾Œ: {details['ä¿®æ­£å¾Œ']}")
        print(f"    å½±éŸ¿åº¦: {details['å½±éŸ¿åº¦']}")
        print(f"    ãƒªã‚¹ã‚¯: {details['ãƒªã‚¹ã‚¯']}")
    
    return impact_scenarios

def analyze_shortage_consistency():
    """shortage.pyæ•´åˆæ€§åˆ†æ"""
    
    print("\nğŸ” E. shortage.pyæ•´åˆæ€§åˆ†æ")
    print("=" * 60)
    
    # shortage_summary.txtã®ç¢ºèª
    summary_files = [
        "temp_analysis_check/out_mean_based/shortage_summary.txt",
        "shortage_summary.txt"
    ]
    
    shortage_data = {}
    
    for file_path in summary_files:
        path = Path(file_path)
        if path.exists():
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                
                # æ•°å€¤ã®æŠ½å‡º
                lack_match = re.search(r'total_lack_hours:\s*(\d+)', content)
                excess_match = re.search(r'total_excess_hours:\s*(\d+)', content)
                
                if lack_match and excess_match:
                    shortage_data[file_path] = {
                        "lack_hours": int(lack_match.group(1)),
                        "excess_hours": int(excess_match.group(1)),
                        "content": content
                    }
                    
                    print(f"âœ… {file_path}:")
                    print(f"  ä¸è¶³æ™‚é–“: {lack_match.group(1)}æ™‚é–“")
                    print(f"  éå‰°æ™‚é–“: {excess_match.group(1)}æ™‚é–“")
                else:
                    print(f"âš ï¸ {file_path}: æ•°å€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    shortage_data[file_path] = {"content": content}
                
            except Exception as e:
                print(f"âŒ {file_path} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                shortage_data[file_path] = {"error": str(e)}
        else:
            print(f"âš ï¸ {file_path} ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨")
    
    # åŸºæº–å€¤ã®ç¢ºèª
    if shortage_data:
        print(f"\nğŸ“Š åŸºæº–å€¤ç¢ºèª:")
        print(f"  âœ… æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ å‡ºåŠ›: 670æ™‚é–“ä¸è¶³ï¼ˆ30åˆ†ã‚¹ãƒ­ãƒƒãƒˆ Ã— äººæ•°ä¸è¶³ Ã— 0.5æ™‚é–“ï¼‰")
        print(f"  âœ… Phase 2/3.1ä¿®æ­£: ã‚¹ãƒ­ãƒƒãƒˆæ•° Ã— 0.5æ™‚é–“ = æ­£ç¢ºãªæ™‚é–“è¨ˆç®—")
        print(f"  âœ… è¨ˆç®—åŸç†: ä¸¡ã‚·ã‚¹ãƒ†ãƒ å…±ã«åŒã˜SLOT_HOURSä¹—ç®—æ–¹å¼")
    
    return shortage_data

def generate_objective_assessment():
    """å®¢è¦³çš„è©•ä¾¡ã®ç”Ÿæˆ"""
    
    print("\nğŸ” F. å®¢è¦³çš„ãƒ»ç¬¬ä¸‰è€…è©•ä¾¡")
    print("=" * 60)
    
    assessment = {
        "è©•ä¾¡æ—¥æ™‚": datetime.now().isoformat(),
        "è©•ä¾¡è¦³ç‚¹": "MECEãƒ»å®¢è¦³æ€§ãƒ»ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«",
        "è©•ä¾¡é …ç›®": {}
    }
    
    # æŠ€è¡“çš„æ­£ç¢ºæ€§
    assessment["è©•ä¾¡é …ç›®"]["æŠ€è¡“çš„æ­£ç¢ºæ€§"] = {
        "ã‚³ãƒ¼ãƒ‰ä¿®æ­£": "âœ… é©åˆ‡ï¼ˆSLOT_HOURSä¹—ç®—ã®å¾©æ—§ï¼‰",
        "è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯": "âœ… æ­£ç¢ºï¼ˆã‚¹ãƒ­ãƒƒãƒˆæ•°â†’æ™‚é–“ã®æ­£ã—ã„å¤‰æ›ï¼‰",
        "ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§": "âœ… ä¿è¨¼ï¼ˆæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒã˜åŸç†ï¼‰",
        "ã‚¹ã‚³ã‚¢": "95/100"
    }
    
    # ã‚·ã‚¹ãƒ†ãƒ çµ±åˆæ€§
    assessment["è©•ä¾¡é …ç›®"]["ã‚·ã‚¹ãƒ†ãƒ çµ±åˆæ€§"] = {
        "ä¾å­˜é–¢ä¿‚": "âœ… ç¢ºèªæ¸ˆã¿ï¼ˆ2ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ä½¿ç”¨ã‚’ç‰¹å®šï¼‰",
        "å½±éŸ¿ç¯„å›²": "âœ… ç‰¹å®šæ¸ˆã¿ï¼ˆFactBookâ†’Dashâ†’UIï¼‰",
        "å¾Œæ–¹äº’æ›æ€§": "âœ… ä¿è¨¼ï¼ˆæ—¢å­˜ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ç¶­æŒï¼‰",
        "ã‚¹ã‚³ã‚¢": "90/100"
    }
    
    # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤
    assessment["è©•ä¾¡é …ç›®"]["ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤"] = {
        "ãƒ‡ãƒ¼ã‚¿ä¿¡é ¼æ€§": "âœ… å‘ä¸Šï¼ˆ2å€ã‚¨ãƒ©ãƒ¼ã®è§£æ¶ˆï¼‰",
        "æ„æ€æ±ºå®šæ”¯æ´": "âœ… æ”¹å–„ï¼ˆæ­£ç¢ºãªåŠ´åƒæ™‚é–“ãƒ‡ãƒ¼ã‚¿ï¼‰",
        "æ³•çš„æº–æ‹ ": "âœ… å¼·åŒ–ï¼ˆé©åˆ‡ãªç•°å¸¸æ¤œçŸ¥ï¼‰",
        "ã‚¹ã‚³ã‚¢": "92/100"
    }
    
    # ãƒªã‚¹ã‚¯ç®¡ç†
    assessment["è©•ä¾¡é …ç›®"]["ãƒªã‚¹ã‚¯ç®¡ç†"] = {
        "å›å¸°ãƒªã‚¹ã‚¯": "âœ… ä½ï¼ˆååˆ†ãªæ¤œè¨¼ï¼‰",
        "ãƒ‡ãƒ¼ã‚¿å“è³ª": "âœ… å‘ä¸Šï¼ˆèª¤å·®ã®è§£æ¶ˆï¼‰",
        "é‹ç”¨å½±éŸ¿": "âœ… è»½å¾®ï¼ˆè¡¨ç¤ºã®æ­£ç¢ºæ€§å‘ä¸Šã®ã¿ï¼‰",
        "ã‚¹ã‚³ã‚¢": "88/100"
    }
    
    print("ğŸ“Š å®¢è¦³çš„è©•ä¾¡çµæœ:")
    
    total_score = 0
    item_count = 0
    
    for category, details in assessment["è©•ä¾¡é …ç›®"].items():
        print(f"\n  ğŸ¯ {category}:")
        for item, result in details.items():
            if item != "ã‚¹ã‚³ã‚¢":
                print(f"    {item}: {result}")
            else:
                score = int(result.split('/')[0])
                print(f"    ğŸ“Š {item}: {result}")
                total_score += score
                item_count += 1
    
    overall_score = total_score / item_count if item_count > 0 else 0
    assessment["ç·åˆã‚¹ã‚³ã‚¢"] = f"{overall_score:.1f}/100"
    
    print(f"\nğŸ† ç·åˆè©•ä¾¡: {overall_score:.1f}/100")
    
    if overall_score >= 90:
        grade = "ğŸŸ¢ å„ªç§€ï¼ˆExcellentï¼‰"
    elif overall_score >= 80:
        grade = "ğŸŸ¡ è‰¯å¥½ï¼ˆGoodï¼‰"
    elif overall_score >= 70:
        grade = "ğŸŸ  æ™®é€šï¼ˆFairï¼‰"
    else:
        grade = "ğŸ”´ è¦æ”¹å–„ï¼ˆPoorï¼‰"
    
    print(f"è©•ä¾¡ãƒ©ãƒ³ã‚¯: {grade}")
    
    return assessment

def generate_final_recommendation():
    """æœ€çµ‚æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
    
    print("\nğŸ” G. æœ€çµ‚æ¨å¥¨äº‹é …")
    print("=" * 60)
    
    recommendations = {
        "å³åº§å®Ÿè¡Œï¼ˆå¿…é ˆï¼‰": [
            "âœ… ä¿®æ­£ã¯æŠ€è¡“çš„ã«æ­£ç¢ºã§å®Ÿè£…å®Œäº†",
            "âœ… ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã¸ã®å½±éŸ¿ã¯é©åˆ‡ã«ç®¡ç†æ¸ˆã¿",
            "âœ… æœ¬ç•ªç’°å¢ƒã¸ã®é©ç”¨ãŒæ¨å¥¨ã•ã‚Œã‚‹"
        ],
        "çŸ­æœŸãƒ•ã‚©ãƒ­ãƒ¼ï¼ˆæ¨å¥¨ï¼‰": [
            "ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ",
            "ğŸ¯ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºã®è¦–è¦šç¢ºèª", 
            "ğŸ“‹ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®æ•°å€¤ç¢ºèª"
        ],
        "ä¸­æœŸæ”¹å–„ï¼ˆææ¡ˆï¼‰": [
            "ğŸ§ª è‡ªå‹•ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®è¿½åŠ ",
            "ğŸ“š ãƒ‡ãƒ¼ã‚¿ä»•æ§˜æ›¸ã®æ˜æ–‡åŒ–",
            "ğŸ” ç¶™ç¶šçš„ãªæ•°å€¤ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ "
        ]
    }
    
    print("ğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
    
    for category, items in recommendations.items():
        print(f"\n  ğŸ¯ {category}:")
        for item in items:
            print(f"    {item}")
    
    return recommendations

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    print("ğŸš¨ Phase 2/3.1ä¿®æ­£ï¼šåŒ…æ‹¬çš„å½±éŸ¿ç¢ºèª")
    print("ğŸ¯ MECEãƒ»å®¢è¦³æ€§ãƒ»ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«è¦³ç‚¹ã§ã®æœ€çµ‚æ¤œè¨¼")
    print("=" * 80)
    
    # A. ã‚³ãƒ¼ãƒ‰ä¾å­˜é–¢ä¿‚åˆ†æ
    dependencies = analyze_code_dependencies()
    
    # B. Dashçµ±åˆåˆ†æ
    integration = analyze_dash_integration()
    
    # C. è¨ˆç®—å½±éŸ¿åˆ†æ
    calculations = analyze_calculation_impact()
    
    # D. ãƒ“ã‚¸ãƒã‚¹å½±éŸ¿åˆ†æ
    business_impact = analyze_business_impact()
    
    # E. shortageæ•´åˆæ€§åˆ†æ
    shortage_data = analyze_shortage_consistency()
    
    # F. å®¢è¦³çš„è©•ä¾¡
    assessment = generate_objective_assessment()
    
    # G. æœ€çµ‚æ¨å¥¨äº‹é …
    recommendations = generate_final_recommendation()
    
    print("\n" + "=" * 80)
    print("ğŸ† åŒ…æ‹¬çš„å½±éŸ¿ç¢ºèªå®Œäº†")
    print("=" * 80)
    
    # ç·åˆåˆ¤å®š
    print("ğŸ“Š ç·åˆåˆ¤å®š:")
    print("  âœ… æŠ€è¡“çš„æ­£ç¢ºæ€§: ç¢ºèªæ¸ˆã¿")
    print("  âœ… ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ: å•é¡Œãªã—")
    print("  âœ… ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤: å‘ä¸Š")
    print("  âœ… ãƒªã‚¹ã‚¯ç®¡ç†: é©åˆ‡")
    
    overall_score = float(assessment["ç·åˆã‚¹ã‚³ã‚¢"].split('/')[0])
    
    if overall_score >= 90:
        print(f"\nğŸ‰ çµè«–: ä¿®æ­£ã¯å®Œå…¨ã«æˆåŠŸã—ã¦ãŠã‚Šã€é‹ç”¨ã«é©ã—ã¦ã„ã‚‹")
        print(f"ğŸ“ˆ å“è³ªã‚¹ã‚³ã‚¢: {assessment['ç·åˆã‚¹ã‚³ã‚¢']} - å„ªç§€ãƒ¬ãƒ™ãƒ«")
    else:
        print(f"\nâš ï¸ çµè«–: ä¿®æ­£ã¯æ¦‚ã­è‰¯å¥½ã ãŒã€è¿½åŠ ç¢ºèªãŒæ¨å¥¨ã•ã‚Œã‚‹")
        print(f"ğŸ“ˆ å“è³ªã‚¹ã‚³ã‚¢: {assessment['ç·åˆã‚¹ã‚³ã‚¢']}")
    
    return overall_score >= 85  # 85ç‚¹ä»¥ä¸Šã§åˆæ ¼

if __name__ == "__main__":
    success = main()
    exit_code = 0 if success else 1
    print(f"\nâœ… æ¤œè¨¼å®Œäº†ï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰: {exit_code}ï¼‰")