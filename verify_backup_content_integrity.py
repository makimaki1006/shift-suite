#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å®Œå…¨æ€§æ¤œè¨¼
ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ãƒã‚¤ãƒˆå˜ä½æ¯”è¼ƒ
"""

import hashlib
import json
from pathlib import Path
import filecmp

def calculate_file_checksum(file_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã®SHA-256ãƒã‚§ãƒƒã‚¯ã‚µãƒ è¨ˆç®—"""
    hash_sha256 = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        return None

def compare_file_content(original_path, backup_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®è©³ç´°æ¯”è¼ƒ"""
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ¯”è¼ƒ
    original_size = original_path.stat().st_size
    backup_size = backup_path.stat().st_size
    size_match = original_size == backup_size
    
    # 2. ãƒã‚§ãƒƒã‚¯ã‚µãƒ æ¯”è¼ƒ
    original_checksum = calculate_file_checksum(original_path)
    backup_checksum = calculate_file_checksum(backup_path)
    checksum_match = original_checksum == backup_checksum
    
    # 3. ãƒã‚¤ãƒˆå˜ä½æ¯”è¼ƒï¼ˆPythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
    byte_match = filecmp.cmp(original_path, backup_path, shallow=False)
    
    # 4. è¡Œæ•°æ¯”è¼ƒï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼‰
    try:
        with open(original_path, 'r', encoding='utf-8') as f:
            original_lines = len(f.readlines())
        with open(backup_path, 'r', encoding='utf-8') as f:
            backup_lines = len(f.readlines())
        line_match = original_lines == backup_lines
    except:
        original_lines = backup_lines = 0
        line_match = True  # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    
    return {
        "size_match": size_match,
        "original_size": original_size,
        "backup_size": backup_size,
        "checksum_match": checksum_match,
        "original_checksum": original_checksum,
        "backup_checksum": backup_checksum,
        "byte_match": byte_match,
        "line_match": line_match,
        "original_lines": original_lines,
        "backup_lines": backup_lines
    }

def verify_all_backup_content():
    """å…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹æ¤œè¨¼"""
    
    print("=" * 80)
    print("ğŸ”¬ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®å®Œå…¨æ€§æ¤œè¨¼")
    print("=" * 80)
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    metadata_path = Path("PHASE3_COMPLETE_BACKUP_20250802_100555/backup_metadata.json")
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼ï¼ˆPhase 2, 3.1, 3.2ã®å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ä¸­å¿ƒï¼‰
    critical_files = [
        "shift_suite/tasks/fact_extractor_prototype.py",
        "shift_suite/tasks/lightweight_anomaly_detector.py", 
        "shift_suite/tasks/fact_book_visualizer.py",
        "shift_suite/tasks/dash_fact_book_integration.py"
    ]
    
    print("\nğŸ“‹ é‡è¦å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æ¤œè¨¼:")
    print("-" * 80)
    
    all_match = True
    
    for file_path in critical_files:
        print(f"\nğŸ” {file_path}:")
        
        original_path = Path(file_path)
        backup_path = Path(f"PHASE3_COMPLETE_BACKUP_20250802_100555/{file_path}")
        
        if not original_path.exists():
            print("  âŒ ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            continue
            
        if not backup_path.exists():
            print("  âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            all_match = False
            continue
        
        # è©³ç´°æ¯”è¼ƒå®Ÿè¡Œ
        comparison = compare_file_content(original_path, backup_path)
        
        # çµæœè¡¨ç¤º
        print(f"  ğŸ“Š ã‚µã‚¤ã‚º: {'âœ…' if comparison['size_match'] else 'âŒ'} " +
              f"ã‚ªãƒªã‚¸ãƒŠãƒ« {comparison['original_size']:,} bytes = " +
              f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— {comparison['backup_size']:,} bytes")
        
        print(f"  ğŸ” ãƒã‚§ãƒƒã‚¯ã‚µãƒ : {'âœ…' if comparison['checksum_match'] else 'âŒ'}")
        if not comparison['checksum_match']:
            print(f"    ã‚ªãƒªã‚¸ãƒŠãƒ«: {comparison['original_checksum']}")
            print(f"    ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {comparison['backup_checksum']}")
        
        print(f"  ğŸ’¾ ãƒã‚¤ãƒˆæ¯”è¼ƒ: {'âœ… å®Œå…¨ä¸€è‡´' if comparison['byte_match'] else 'âŒ ä¸ä¸€è‡´'}")
        
        if comparison['original_lines'] > 0:
            print(f"  ğŸ“ è¡Œæ•°: {'âœ…' if comparison['line_match'] else 'âŒ'} " +
                  f"{comparison['original_lines']} è¡Œ")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ã®ç…§åˆ
        meta_file = next((f for f in metadata['critical_files'] if f['path'] == file_path), None)
        if meta_file:
            meta_checksum = meta_file.get('checksum')
            meta_match = meta_checksum == comparison['original_checksum']
            print(f"  ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç…§åˆ: {'âœ…' if meta_match else 'âŒ'}")
        
        if not (comparison['size_match'] and comparison['checksum_match'] and comparison['byte_match']):
            all_match = False
    
    # è¿½åŠ æ¤œè¨¼: ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    print("\nğŸ“Š ãã®ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼:")
    print("-" * 80)
    
    sample_files = [
        "PHASE3_LIGHTWEIGHT_ANOMALY_DETECTION_DESIGN.md",
        "verify_phase3_implementation.py",
        "shift_suite/tasks/constants.py"
    ]
    
    for file_path in sample_files:
        original_path = Path(file_path)
        backup_path = Path(f"PHASE3_COMPLETE_BACKUP_20250802_100555/{file_path}")
        
        if original_path.exists() and backup_path.exists():
            comparison = compare_file_content(original_path, backup_path)
            status = "âœ…" if comparison['byte_match'] else "âŒ"
            print(f"  {status} {file_path} ({comparison['original_size']:,} bytes)")
    
    # æœ€çµ‚åˆ¤å®š
    print("\n" + "=" * 80)
    print(f"ğŸ¯ å†…å®¹å®Œå…¨æ€§æ¤œè¨¼çµæœ: {'âœ… å®Œå…¨ä¸€è‡´' if all_match else 'âŒ ä¸ä¸€è‡´ã‚ã‚Š'}")
    
    if all_match:
        print("ğŸ“‹ çµè«–: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã¯çœç•¥ãªãå®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã•ã‚Œã¦ã„ã¾ã™")
    else:
        print("âš ï¸ è­¦å‘Š: ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸ä¸€è‡´ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
    
    return all_match

def show_sample_content():
    """å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«å†…å®¹è¡¨ç¤ºï¼ˆçœç•¥ã•ã‚Œã¦ã„ãªã„ã“ã¨ã®è¨¼æ˜ï¼‰"""
    
    print("\n" + "=" * 80)
    print("ğŸ“„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«å†…å®¹ï¼ˆçœç•¥ãªã—ã®è¨¼æ˜ï¼‰")
    print("=" * 80)
    
    # Phase 3.1ã®ç•°å¸¸æ¤œçŸ¥ãƒ¡ã‚½ãƒƒãƒ‰ãŒå®Œå…¨ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
    backup_file = Path("PHASE3_COMPLETE_BACKUP_20250802_100555/shift_suite/tasks/lightweight_anomaly_detector.py")
    
    if backup_file.exists():
        with open(backup_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"\nğŸ” lightweight_anomaly_detector.py ã®æ¤œè¨¼:")
        print(f"  ç·æ–‡å­—æ•°: {len(content):,}")
        print(f"  ç·è¡Œæ•°: {content.count(chr(10))}")
        
        # é‡è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ç¢ºèª
        important_methods = [
            "_detect_excessive_hours",
            "_detect_continuous_work_violations",
            "_detect_night_shift_anomalies",
            "_detect_interval_violations",
            "_calculate_severity",
            "generate_anomaly_summary"
        ]
        
        print(f"\n  é‡è¦ãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ç¢ºèª:")
        for method in important_methods:
            if f"def {method}" in content:
                # ãƒ¡ã‚½ãƒƒãƒ‰ã®é–‹å§‹ä½ç½®ã¨ä¸€éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã‚’è¡¨ç¤º
                start_pos = content.find(f"def {method}")
                method_preview = content[start_pos:start_pos+200].split('\n')[0]
                print(f"    âœ… {method_preview}")
            else:
                print(f"    âŒ {method} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    # å®Œå…¨æ€§æ¤œè¨¼å®Ÿè¡Œ
    is_complete = verify_all_backup_content()
    
    # ã‚µãƒ³ãƒ—ãƒ«å†…å®¹è¡¨ç¤º
    show_sample_content()
    
    print("\nâœ… æ¤œè¨¼å®Œäº†")