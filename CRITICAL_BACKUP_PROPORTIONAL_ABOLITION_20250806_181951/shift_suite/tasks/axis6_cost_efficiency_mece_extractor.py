#!/usr/bin/env python3
"""
è»¸6: ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§ MECEäº‹å®ŸæŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³

12è»¸åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è»¸6ã‚’æ‹…å½“
éå»ã‚·ãƒ•ãƒˆå®Ÿç¸¾ã‹ã‚‰ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã¨åŠ¹ç‡æ€§å‘ä¸Šã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’æŠ½å‡º

ä½œæˆæ—¥: 2025å¹´7æœˆ
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import json

log = logging.getLogger(__name__)

class CostEfficiencyMECEFactExtractor:
    """è»¸6: ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§ã®MECEäº‹å®ŸæŠ½å‡ºå™¨"""
    
    def __init__(self):
        self.axis_number = 6
        self.axis_name = "ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§"
        
    def extract_axis6_cost_efficiency_rules(self, long_df: pd.DataFrame, wt_df: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è»¸6: ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§ãƒ«ãƒ¼ãƒ«ã‚’MECEåˆ†è§£ã«ã‚ˆã‚ŠæŠ½å‡º
        
        Args:
            long_df: éå»ã®ã‚·ãƒ•ãƒˆå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿
            wt_df: å‹¤å‹™åŒºåˆ†ãƒã‚¹ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Dict: æŠ½å‡ºçµæœï¼ˆhuman_readable, machine_readable, extraction_metadataï¼‰
        """
        log.info(f"ğŸ¯ è»¸6: {self.axis_name} MECEäº‹å®ŸæŠ½å‡ºã‚’é–‹å§‹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            if long_df.empty:
                raise ValueError("é•·æœŸãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            
            # è»¸6ã®MECEåˆ†è§£ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆ8ã¤ï¼‰
            mece_facts = {
                "äººä»¶è²»æœ€é©åŒ–åˆ¶ç´„": self._extract_labor_cost_optimization_constraints(long_df, wt_df),
                "é›‡ç”¨å½¢æ…‹åŠ¹ç‡åˆ¶ç´„": self._extract_employment_efficiency_constraints(long_df, wt_df),
                "æ™‚é–“åŠ¹ç‡åˆ¶ç´„": self._extract_time_efficiency_constraints(long_df, wt_df),
                "æ®‹æ¥­ãƒ»è¶…éåˆ¶ç´„": self._extract_overtime_control_constraints(long_df, wt_df),
                "ç”Ÿç”£æ€§å‘ä¸Šåˆ¶ç´„": self._extract_productivity_enhancement_constraints(long_df, wt_df),
                "ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨åˆ¶ç´„": self._extract_resource_utilization_constraints(long_df, wt_df),
                "é‹å–¶åŠ¹ç‡åˆ¶ç´„": self._extract_operational_efficiency_constraints(long_df, wt_df),
                "ã‚³ã‚¹ãƒˆå‰Šæ¸›åˆ¶ç´„": self._extract_cost_reduction_constraints(long_df, wt_df)
            }
            
            # äººé–“å¯èª­å½¢å¼ã®çµæœç”Ÿæˆ
            human_readable = self._generate_human_readable_results(mece_facts, long_df)
            
            # æ©Ÿæ¢°å¯èª­å½¢å¼ã®åˆ¶ç´„ç”Ÿæˆ
            machine_readable = self._generate_machine_readable_constraints(mece_facts, long_df)
            
            # æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            extraction_metadata = self._generate_extraction_metadata(long_df, wt_df, mece_facts)
            
            log.info(f"âœ… è»¸6: {self.axis_name} MECEäº‹å®ŸæŠ½å‡ºå®Œäº†")
            
            return {
                'human_readable': human_readable,
                'machine_readable': machine_readable,
                'extraction_metadata': extraction_metadata
            }
            
        except Exception as e:
            log.error(f"âŒ è»¸6: {self.axis_name} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            raise e
    
    def _extract_labor_cost_optimization_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """äººä»¶è²»æœ€é©åŒ–åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # é›‡ç”¨å½¢æ…‹åˆ¥ã®ã‚³ã‚¹ãƒˆæ¨å®š
            if 'employment' in long_df.columns:
                employment_distribution = long_df['employment'].value_counts()
                total_shifts = len(long_df)
                
                # æ­£è¦é›‡ç”¨ã¨éæ­£è¦é›‡ç”¨ã®æ¯”ç‡
                regular_keywords = ['æ­£ç¤¾å“¡', 'æ­£è¦', 'å¸¸å‹¤', 'ãƒ•ãƒ«ã‚¿ã‚¤ãƒ ']
                irregular_keywords = ['ãƒ‘ãƒ¼ãƒˆ', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'éå¸¸å‹¤', 'æ´¾é£', 'å¥‘ç´„']
                
                regular_count = sum(
                    employment_distribution[emp] for emp in employment_distribution.index
                    if any(keyword in emp for keyword in regular_keywords)
                )
                irregular_count = sum(
                    employment_distribution[emp] for emp in employment_distribution.index
                    if any(keyword in emp for keyword in irregular_keywords)
                )
                
                if regular_count + irregular_count > 0:
                    regular_ratio = regular_count / (regular_count + irregular_count)
                    
                    if regular_ratio > 0.7:
                        constraints.append(f"é«˜äººä»¶è²»æ§‹é€ : æ­£è¦é›‡ç”¨{regular_ratio:.1%} - ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ä½™åœ°")
                    elif regular_ratio < 0.3:
                        constraints.append(f"æŸ”è»Ÿäººä»¶è²»æ§‹é€ : æ­£è¦é›‡ç”¨{regular_ratio:.1%} - ã‚³ã‚¹ãƒˆå¤‰å‹•ãƒªã‚¹ã‚¯ç®¡ç†å¿…è¦")
                    else:
                        constraints.append(f"ãƒãƒ©ãƒ³ã‚¹äººä»¶è²»æ§‹é€ : æ­£è¦é›‡ç”¨{regular_ratio:.1%}")
            
            # è·ç¨®åˆ¥ã‚³ã‚¹ãƒˆåŠ¹ç‡åˆ†æ
            if 'role' in long_df.columns:
                role_distribution = long_df['role'].value_counts()
                
                # é«˜ã‚³ã‚¹ãƒˆå°‚é–€è·ã®é…ç½®åŠ¹ç‡
                high_cost_roles = ['çœ‹è­·å¸«', 'åŒ»å¸«', 'ç†å­¦ç™‚æ³•å£«', 'ä½œæ¥­ç™‚æ³•å£«', 'PT', 'OT', 'ST']
                high_cost_count = sum(
                    role_distribution[role] for role in role_distribution.index
                    if any(hc_role in role for hc_role in high_cost_roles)
                )
                
                if high_cost_count > 0:
                    high_cost_ratio = high_cost_count / total_shifts
                    constraints.append(f"å°‚é–€è·ã‚³ã‚¹ãƒˆæ¯”ç‡: {high_cost_ratio:.1%} - å°‚é–€æ€§ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹")
                    
                    if high_cost_ratio > 0.4:
                        constraints.append("å°‚é–€è·é›†ç´„é…ç½®: é«˜åŠ¹ç‡æ´»ç”¨ãŒé‡è¦")
                    else:
                        constraints.append("å°‚é–€è·é©æ­£é…ç½®: ã‚³ã‚¹ãƒˆåŠ¹ç‡è‰¯å¥½")
            
            # æ™‚é–“å¤–ãƒ»å‰²å¢—è³ƒé‡‘ã®æ¨å®š
            if 'ds' in long_df.columns:
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                long_df['weekday'] = pd.to_datetime(long_df['ds']).dt.day_name()
                
                # å¤œé–“ãƒ»æ—©æœï¼ˆå‰²å¢—å¯¾è±¡æ™‚é–“ï¼‰
                premium_hours = list(range(22, 24)) + list(range(0, 6))
                premium_shifts = long_df[long_df['hour'].isin(premium_hours)]
                premium_ratio = len(premium_shifts) / total_shifts if total_shifts > 0 else 0
                
                constraints.append(f"å‰²å¢—æ™‚é–“å¸¯æ¯”ç‡: {premium_ratio:.1%} - äººä»¶è²»å¢—åŠ è¦å› ")
                
                # åœŸæ—¥å‹¤å‹™ï¼ˆä¼‘æ—¥å‰²å¢—ï¼‰
                weekend_shifts = long_df[long_df['weekday'].isin(['Saturday', 'Sunday'])]
                weekend_ratio = len(weekend_shifts) / total_shifts if total_shifts > 0 else 0
                
                constraints.append(f"ä¼‘æ—¥å‹¤å‹™æ¯”ç‡: {weekend_ratio:.1%} - ä¼‘æ—¥å‰²å¢—ã‚³ã‚¹ãƒˆ")
                
                if premium_ratio + weekend_ratio > 0.3:
                    constraints.append("å‰²å¢—è³ƒé‡‘é«˜: ã‚·ãƒ•ãƒˆæœ€é©åŒ–ã§ã‚³ã‚¹ãƒˆå‰Šæ¸›å¯èƒ½")
                else:
                    constraints.append("å‰²å¢—è³ƒé‡‘é©æ­£: åŠ¹ç‡çš„ã‚·ãƒ•ãƒˆé…ç½®")
                
        except Exception as e:
            log.warning(f"äººä»¶è²»æœ€é©åŒ–åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("äººä»¶è²»æœ€é©åŒ–åˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["äººä»¶è²»æœ€é©åŒ–ã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_employment_efficiency_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """é›‡ç”¨å½¢æ…‹åŠ¹ç‡åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # é›‡ç”¨å½¢æ…‹ã®å¤šæ§˜æ€§ã¨åŠ¹ç‡æ€§
            if 'employment' in long_df.columns:
                employment_types = long_df['employment'].nunique()
                employment_distribution = long_df['employment'].value_counts()
                
                # æœ€é©ãªé›‡ç”¨å½¢æ…‹ãƒŸãƒƒã‚¯ã‚¹ã®åˆ†æ
                total_shifts = len(long_df)
                diversity_score = employment_types / total_shifts * 100
                
                constraints.append(f"é›‡ç”¨å½¢æ…‹å¤šæ§˜æ€§: {employment_types}ç¨®é¡ (å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢: {diversity_score:.1f})")
                
                if employment_types >= 4:
                    constraints.append("é«˜å¤šæ§˜æ€§é›‡ç”¨: æŸ”è»Ÿæ€§é«˜ã„ãŒç®¡ç†ã‚³ã‚¹ãƒˆå¢—")
                elif employment_types <= 2:
                    constraints.append("ä½å¤šæ§˜æ€§é›‡ç”¨: ç®¡ç†åŠ¹ç‡è‰¯ã„ãŒæŸ”è»Ÿæ€§åˆ¶é™")
                else:
                    constraints.append("é©åº¦å¤šæ§˜æ€§é›‡ç”¨: ãƒãƒ©ãƒ³ã‚¹è‰¯ã„é›‡ç”¨å½¢æ…‹")
                
                # é›‡ç”¨å½¢æ…‹åˆ¥ã®ç¨¼åƒåŠ¹ç‡
                for emp_type in employment_distribution.index[:3]:  # ä¸Šä½3ç¨®é¡
                    emp_shifts = employment_distribution[emp_type]
                    emp_ratio = emp_shifts / total_shifts
                    
                    if emp_ratio > 0.4:
                        constraints.append(f"{emp_type}ä¸»åŠ›: {emp_ratio:.1%} - å®‰å®šç¨¼åƒãƒ»é«˜ä¾å­˜ãƒªã‚¹ã‚¯")
                    elif emp_ratio < 0.1:
                        constraints.append(f"{emp_type}è£œåŠ©: {emp_ratio:.1%} - ç‰¹å®šç”¨é€”æ´»ç”¨")
            
            # ãƒ•ãƒ«ã‚¿ã‚¤ãƒ ãƒ»ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒ ã®åŠ¹ç‡çš„é…ç½®
            if 'employment' in long_df.columns and 'ds' in long_df.columns:
                # æ™‚é–“å¸¯åˆ¥é›‡ç”¨å½¢æ…‹åˆ†æ
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                
                # æ—¥ä¸­æ™‚é–“å¸¯ã§ã®ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒ æ´»ç”¨
                daytime_hours = range(9, 17)
                daytime_data = long_df[long_df['hour'].isin(daytime_hours)]
                
                if not daytime_data.empty:
                    part_time_keywords = ['ãƒ‘ãƒ¼ãƒˆ', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'éå¸¸å‹¤']
                    part_time_daytime = sum(
                        daytime_data['employment'].str.contains(keyword, case=False, na=False).sum()
                        for keyword in part_time_keywords
                    )
                    
                    part_time_efficiency = part_time_daytime / len(daytime_data) if len(daytime_data) > 0 else 0
                    constraints.append(f"æ—¥ä¸­ãƒ‘ãƒ¼ãƒˆã‚¿ã‚¤ãƒ æ´»ç”¨: {part_time_efficiency:.1%} - ã‚³ã‚¹ãƒˆåŠ¹ç‡çš„é…ç½®")
                
                # å¤œé–“ãƒ»ä¼‘æ—¥ã§ã®ãƒ•ãƒ«ã‚¿ã‚¤ãƒ é…ç½®
                night_weekend_hours = list(range(22, 24)) + list(range(0, 6))
                long_df['weekday'] = pd.to_datetime(long_df['ds']).dt.day_name()
                
                challenging_shifts = long_df[
                    (long_df['hour'].isin(night_weekend_hours)) |
                    (long_df['weekday'].isin(['Saturday', 'Sunday']))
                ]
                
                if not challenging_shifts.empty:
                    full_time_keywords = ['æ­£ç¤¾å“¡', 'æ­£è¦', 'å¸¸å‹¤']
                    full_time_challenging = sum(
                        challenging_shifts['employment'].str.contains(keyword, case=False, na=False).sum()
                        for keyword in full_time_keywords
                    )
                    
                    full_time_coverage = full_time_challenging / len(challenging_shifts) if len(challenging_shifts) > 0 else 0
                    constraints.append(f"å›°é›£æ™‚é–“å¸¯æ­£è¦é›‡ç”¨: {full_time_coverage:.1%} - è²¬ä»»ä½“åˆ¶ç¢ºä¿")
                
        except Exception as e:
            log.warning(f"é›‡ç”¨å½¢æ…‹åŠ¹ç‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("é›‡ç”¨å½¢æ…‹åŠ¹ç‡åˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["é›‡ç”¨å½¢æ…‹åŠ¹ç‡ã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_time_efficiency_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æ™‚é–“åŠ¹ç‡åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ã‚·ãƒ•ãƒˆé•·ã®åŠ¹ç‡æ€§åˆ†æ
            if 'ds' in long_df.columns:
                # 1æ—¥ã®ã‚·ãƒ•ãƒˆæ™‚é–“åˆ†å¸ƒæ¨å®š
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                
                daily_shift_spans = []
                for date in long_df['ds'].dt.date.unique():
                    daily_data = long_df[long_df['ds'].dt.date == date]
                    if len(daily_data) > 1:
                        min_hour = daily_data['hour'].min()
                        max_hour = daily_data['hour'].max()
                        span = max_hour - min_hour + 1  # +1 for inclusive range
                        daily_shift_spans.append(span)
                
                if daily_shift_spans:
                    avg_operational_hours = np.mean(daily_shift_spans)
                    constraints.append(f"å¹³å‡ç¨¼åƒæ™‚é–“: {avg_operational_hours:.1f}æ™‚é–“/æ—¥")
                    
                    if avg_operational_hours >= 16:
                        constraints.append("é•·æ™‚é–“ç¨¼åƒ: 24æ™‚é–“ä½“åˆ¶ã«è¿‘ã„åŠ¹ç‡çš„é‹å–¶")
                    elif avg_operational_hours <= 8:
                        constraints.append("çŸ­æ™‚é–“ç¨¼åƒ: é›†ä¸­çš„ãƒ»åŠ¹ç‡çš„é‹å–¶")
                    else:
                        constraints.append("ä¸­æ™‚é–“ç¨¼åƒ: ä¸€èˆ¬çš„ãªé‹å–¶æ™‚é–“")
            
            # ã‚¹ã‚¿ãƒƒãƒ•ç¨¼åƒåŠ¹ç‡
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                # ã‚¹ã‚¿ãƒƒãƒ•ã”ã¨ã®ç¨¼åƒé »åº¦
                staff_workload = long_df['staff'].value_counts()
                
                # ç¨¼åƒåŠ¹ç‡ã®åˆ†æ
                high_utilization_staff = staff_workload[staff_workload >= staff_workload.quantile(0.8)]
                low_utilization_staff = staff_workload[staff_workload <= staff_workload.quantile(0.2)]
                
                total_staff = len(staff_workload)
                high_util_ratio = len(high_utilization_staff) / total_staff if total_staff > 0 else 0
                low_util_ratio = len(low_utilization_staff) / total_staff if total_staff > 0 else 0
                
                constraints.append(f"é«˜ç¨¼åƒã‚¹ã‚¿ãƒƒãƒ•: {high_util_ratio:.1%} - åŠ¹ç‡æ´»ç”¨")
                constraints.append(f"ä½ç¨¼åƒã‚¹ã‚¿ãƒƒãƒ•: {low_util_ratio:.1%} - æ´»ç”¨ä½™åœ°")
                
                # ç¨¼åƒã®ã°ã‚‰ã¤ã
                workload_cv = staff_workload.std() / staff_workload.mean() if staff_workload.mean() > 0 else 0
                if workload_cv > 0.5:
                    constraints.append(f"ç¨¼åƒä¸å‡ç­‰: CV={workload_cv:.2f} - åŠ¹ç‡æ€§æ”¹å–„ä½™åœ°")
                else:
                    constraints.append(f"ç¨¼åƒå‡ç­‰: CV={workload_cv:.2f} - åŠ¹ç‡çš„äººå“¡æ´»ç”¨")
            
            # æ™‚é–“å¸¯åˆ¥åŠ¹ç‡æ€§
            if 'ds' in long_df.columns and 'staff' in long_df.columns:
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                hourly_staff_count = long_df.groupby('hour')['staff'].nunique()
                
                # ãƒ”ãƒ¼ã‚¯åŠ¹ç‡ã®åˆ†æ
                peak_hours = hourly_staff_count.nlargest(3).index.tolist()
                off_peak_hours = hourly_staff_count.nsmallest(3).index.tolist()
                
                peak_efficiency = hourly_staff_count.max() / hourly_staff_count.mean() if hourly_staff_count.mean() > 0 else 0
                constraints.append(f"æ™‚é–“å¸¯åŠ¹ç‡å·®: ãƒ”ãƒ¼ã‚¯{peak_efficiency:.1f}å€ - éœ€è¦å¤‰å‹•å¯¾å¿œ")
                
                # 24æ™‚é–“åŠ¹ç‡æ€§
                if len(hourly_staff_count) >= 12:  # åŠæ—¥ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿
                    night_hours = [22, 23, 0, 1, 2, 3, 4, 5]
                    night_efficiency = hourly_staff_count[hourly_staff_count.index.isin(night_hours)].mean()
                    day_efficiency = hourly_staff_count[~hourly_staff_count.index.isin(night_hours)].mean()
                    
                    if day_efficiency > 0:
                        night_day_ratio = night_efficiency / day_efficiency
                        constraints.append(f"å¤œé–“åŠ¹ç‡æ¯”: {night_day_ratio:.2f} - 24æ™‚é–“é‹å–¶åŠ¹ç‡")
                
        except Exception as e:
            log.warning(f"æ™‚é–“åŠ¹ç‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("æ™‚é–“åŠ¹ç‡åˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["æ™‚é–“åŠ¹ç‡ã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_overtime_control_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æ®‹æ¥­ãƒ»è¶…éåˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # é€£ç¶šå‹¤å‹™ã«ã‚ˆã‚‹æ®‹æ¥­ãƒªã‚¹ã‚¯
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                overtime_risks = []
                
                for staff_id in long_df['staff'].unique():
                    staff_dates = pd.to_datetime(long_df[long_df['staff'] == staff_id]['ds']).sort_values()
                    
                    if len(staff_dates) > 1:
                        # é€£ç¶šå‹¤å‹™æ—¥æ•°
                        consecutive_days = self._calculate_max_consecutive_days(staff_dates)
                        if consecutive_days >= 5:
                            overtime_risks.append(consecutive_days)
                
                if overtime_risks:
                    avg_consecutive = np.mean(overtime_risks)
                    risk_staff_ratio = len(overtime_risks) / long_df['staff'].nunique()
                    
                    constraints.append(f"æ®‹æ¥­ãƒªã‚¹ã‚¯ã‚¹ã‚¿ãƒƒãƒ•: {risk_staff_ratio:.1%} (å¹³å‡{avg_consecutive:.1f}æ—¥é€£ç¶š)")
                    
                    if risk_staff_ratio > 0.3:
                        constraints.append("é«˜æ®‹æ¥­ãƒªã‚¹ã‚¯: é€£ç¶šå‹¤å‹™åˆ¶é™ã®å¼·åŒ–å¿…è¦")
                    else:
                        constraints.append("æ®‹æ¥­ãƒªã‚¹ã‚¯ç®¡ç†è‰¯å¥½: é©åˆ‡ãªä¼‘æ†©é…ç½®")
                else:
                    constraints.append("æ®‹æ¥­ãƒªã‚¹ã‚¯ä½: åŠ¹æœçš„ãªå‹¤å‹™åˆ†æ•£")
            
            # æ™‚é–“å¤–å‹¤å‹™ã®é »åº¦æ¨å®š
            if 'ds' in long_df.columns:
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                
                # æ³•å®šæ™‚é–“å¤–ï¼ˆ22æ™‚-6æ™‚ï¼‰
                overtime_hours = list(range(22, 24)) + list(range(0, 6))
                overtime_shifts = long_df[long_df['hour'].isin(overtime_hours)]
                overtime_ratio = len(overtime_shifts) / len(long_df) if len(long_df) > 0 else 0
                
                constraints.append(f"æ™‚é–“å¤–å‹¤å‹™ç‡: {overtime_ratio:.1%} - è¶…éå‹¤å‹™æ‰‹å½“å¯¾è±¡")
                
                if overtime_ratio > 0.25:
                    constraints.append("é«˜æ™‚é–“å¤–æ¯”ç‡: å‹¤å‹™æ™‚é–“æœ€é©åŒ–ã§ã‚³ã‚¹ãƒˆå‰Šæ¸›å¯èƒ½")
                elif overtime_ratio < 0.1:
                    constraints.append("ä½æ™‚é–“å¤–æ¯”ç‡: åŠ¹ç‡çš„å‹¤å‹™æ™‚é–“ç®¡ç†")
                else:
                    constraints.append("é©æ­£æ™‚é–“å¤–æ¯”ç‡: ãƒãƒ©ãƒ³ã‚¹è‰¯ã„å‹¤å‹™é…ç½®")
            
            # é€±40æ™‚é–“è¶…éæ¨å®š
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                long_df['week'] = pd.to_datetime(long_df['ds']).dt.isocalendar().week
                
                weekly_overtime_staff = []
                for staff_id in long_df['staff'].unique():
                    staff_data = long_df[long_df['staff'] == staff_id]
                    weekly_hours = staff_data.groupby('week').size()
                    
                    # 1é€±5æ—¥ä»¥ä¸Šã‚’è¶…éå‹¤å‹™ã¨ã¿ãªã™ï¼ˆ8æ™‚é–“Ã—5æ—¥=40æ™‚é–“ï¼‰
                    overtime_weeks = weekly_hours[weekly_hours > 5]
                    if len(overtime_weeks) > 0:
                        weekly_overtime_staff.append(staff_id)
                
                if long_df['staff'].nunique() > 0:
                    weekly_overtime_ratio = len(weekly_overtime_staff) / long_df['staff'].nunique()
                    constraints.append(f"é€±æ¬¡è¶…éå‹¤å‹™ç‡: {weekly_overtime_ratio:.1%} - åŠ´åƒåŸºæº–æ³•å¯¾å¿œ")
                    
                    if weekly_overtime_ratio > 0.2:
                        constraints.append("é€±æ¬¡è¶…éå¤š: å‹¤å‹™æ™‚é–“ç®¡ç†ã®æ”¹å–„å¿…è¦")
                    else:
                        constraints.append("é€±æ¬¡è¶…éç®¡ç†è‰¯å¥½: é©æ­£å‹¤å‹™æ™‚é–“ç¶­æŒ")
                
        except Exception as e:
            log.warning(f"æ®‹æ¥­ãƒ»è¶…éåˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("æ®‹æ¥­ãƒ»è¶…éåˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["æ®‹æ¥­ãƒ»è¶…éã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_productivity_enhancement_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ç”Ÿç”£æ€§å‘ä¸Šåˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ã®ç”Ÿç”£æ€§åˆ†æ
            if 'staff' in long_df.columns and 'role' in long_df.columns:
                # è·ç¨®ã”ã¨ã®äººå“¡é…ç½®åŠ¹ç‡
                role_staff_matrix = pd.crosstab(long_df['role'], long_df['staff'])
                
                # å¤šæŠ€èƒ½ã‚¹ã‚¿ãƒƒãƒ•ã®æ´»ç”¨
                multi_role_staff = []
                for staff_id in role_staff_matrix.columns:
                    roles_count = (role_staff_matrix[staff_id] > 0).sum()
                    if roles_count > 1:
                        multi_role_staff.append((staff_id, roles_count))
                
                if multi_role_staff:
                    multi_role_ratio = len(multi_role_staff) / len(role_staff_matrix.columns)
                    avg_roles_per_staff = np.mean([roles for _, roles in multi_role_staff])
                    
                    constraints.append(f"å¤šæŠ€èƒ½ã‚¹ã‚¿ãƒƒãƒ•: {multi_role_ratio:.1%} (å¹³å‡{avg_roles_per_staff:.1f}è·ç¨®)")
                    
                    if multi_role_ratio > 0.3:
                        constraints.append("é«˜å¤šæŠ€èƒ½æ´»ç”¨: æŸ”è»Ÿæ€§ãƒ»ç”Ÿç”£æ€§å‘ä¸Š")
                    else:
                        constraints.append("å°‚é–€æ€§é‡è¦–é…ç½®: å°‚é–€ç‰¹åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–")
                else:
                    constraints.append("å˜ä¸€è·ç¨®é…ç½®: å°‚é–€æ€§é›†ä¸­ã«ã‚ˆã‚‹ç”Ÿç”£æ€§ç¢ºä¿")
            
            # ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯åŠ¹ç‡
            if 'ds' in long_df.columns and 'staff' in long_df.columns:
                # åŒæ—¥å‹¤å‹™ã§ã®ãƒãƒ¼ãƒ ç·¨æˆ
                daily_team_sizes = []
                team_stability_scores = []
                
                for date in long_df['ds'].dt.date.unique():
                    daily_staff = long_df[long_df['ds'].dt.date == date]['staff'].unique()
                    daily_team_sizes.append(len(daily_staff))
                
                if daily_team_sizes:
                    avg_team_size = np.mean(daily_team_sizes)
                    team_size_cv = np.std(daily_team_sizes) / avg_team_size if avg_team_size > 0 else 0
                    
                    constraints.append(f"å¹³å‡ãƒãƒ¼ãƒ ã‚µã‚¤ã‚º: {avg_team_size:.1f}å (å¤‰å‹•CV={team_size_cv:.2f})")
                    
                    if 3 <= avg_team_size <= 7:
                        constraints.append("æœ€é©ãƒãƒ¼ãƒ ã‚µã‚¤ã‚º: åŠ¹æœçš„ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»å”åƒ")
                    elif avg_team_size > 7:
                        constraints.append("å¤§è¦æ¨¡ãƒãƒ¼ãƒ : ç®¡ç†ã‚³ã‚¹ãƒˆå¢—ãƒ»åŠ¹ç‡æ€§è¦æ¤œè¨")
                    else:
                        constraints.append("å°è¦æ¨¡ãƒãƒ¼ãƒ : é«˜å¯†åº¦é€£æºãƒ»å°‚é–€æ€§æ´»ç”¨")
                
                # ãƒãƒ¼ãƒ å®‰å®šæ€§ï¼ˆå›ºå®šãƒ¡ãƒ³ãƒãƒ¼æ¯”ç‡ï¼‰
                if len(daily_team_sizes) > 7:  # 1é€±é–“ä»¥ä¸Šã®ãƒ‡ãƒ¼ã‚¿
                    all_dates = sorted(long_df['ds'].dt.date.unique())
                    stable_pairs = 0
                    total_pairs = 0
                    
                    for i in range(len(all_dates) - 1):
                        today_staff = set(long_df[long_df['ds'].dt.date == all_dates[i]]['staff'])
                        tomorrow_staff = set(long_df[long_df['ds'].dt.date == all_dates[i+1]]['staff'])
                        
                        if today_staff and tomorrow_staff:
                            overlap = len(today_staff.intersection(tomorrow_staff))
                            total = len(today_staff.union(tomorrow_staff))
                            
                            if total > 0:
                                stability = overlap / total
                                if stability >= 0.5:
                                    stable_pairs += 1
                                total_pairs += 1
                    
                    if total_pairs > 0:
                        team_stability = stable_pairs / total_pairs
                        constraints.append(f"ãƒãƒ¼ãƒ å®‰å®šæ€§: {team_stability:.1%} - ç¶™ç¶šæ€§ã«ã‚ˆã‚‹ç”Ÿç”£æ€§")
            
            # å°‚é–€æ€§æ´»ç”¨åŠ¹ç‡
            if 'role' in long_df.columns:
                specialized_roles = ['çœ‹è­·å¸«', 'ç†å­¦ç™‚æ³•å£«', 'ä½œæ¥­ç™‚æ³•å£«', 'PT', 'OT', 'ST', 'åŒ»å¸«']
                general_roles = ['ä»‹è­·å£«', 'ä»‹è­·ç¦ç¥‰å£«', 'ãƒ˜ãƒ«ãƒ‘ãƒ¼', 'ã‚±ã‚¢ãƒ¯ãƒ¼ã‚«ãƒ¼']
                
                specialized_count = sum(
                    long_df['role'].str.contains(role, case=False, na=False).sum()
                    for role in specialized_roles
                )
                general_count = sum(
                    long_df['role'].str.contains(role, case=False, na=False).sum()
                    for role in general_roles
                )
                
                total_shifts = len(long_df)
                if total_shifts > 0:
                    specialization_ratio = specialized_count / total_shifts
                    constraints.append(f"å°‚é–€æ€§æ´»ç”¨ç‡: {specialization_ratio:.1%} - é«˜ä»˜åŠ ä¾¡å€¤æ¥­å‹™æ¯”ç‡")
                    
                    if specialization_ratio > 0.4:
                        constraints.append("é«˜å°‚é–€æ€§é…ç½®: ä»˜åŠ ä¾¡å€¤å‰µå‡ºãƒ»ã‚³ã‚¹ãƒˆåŠ¹æœè¦æ¤œè¨¼")
                    elif specialization_ratio < 0.2:
                        constraints.append("æ±ç”¨æ€§é‡è¦–é…ç½®: ã‚³ã‚¹ãƒˆåŠ¹ç‡ãƒ»æŸ”è»Ÿæ€§ç¢ºä¿")
                    else:
                        constraints.append("å°‚é–€æ€§ãƒãƒ©ãƒ³ã‚¹é…ç½®: åŠ¹ç‡æ€§ã¨å°‚é–€æ€§ã®èª¿å’Œ")
                
        except Exception as e:
            log.warning(f"ç”Ÿç”£æ€§å‘ä¸Šåˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("ç”Ÿç”£æ€§å‘ä¸Šåˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["ç”Ÿç”£æ€§å‘ä¸Šã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_resource_utilization_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # äººçš„ãƒªã‚½ãƒ¼ã‚¹ã®ç¨¼åƒç‡
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                total_staff = long_df['staff'].nunique()
                total_days = long_df['ds'].dt.date.nunique()
                total_possible_shifts = total_staff * total_days
                actual_shifts = len(long_df)
                
                if total_possible_shifts > 0:
                    utilization_rate = actual_shifts / total_possible_shifts
                    constraints.append(f"äººå“¡ç¨¼åƒç‡: {utilization_rate:.1%} - ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨åŠ¹ç‡")
                    
                    if utilization_rate > 0.7:
                        constraints.append("é«˜ç¨¼åƒç‡: åŠ¹ç‡çš„äººå“¡æ´»ç”¨ãƒ»éåŠ´ãƒªã‚¹ã‚¯è¦æ³¨æ„")
                    elif utilization_rate < 0.3:
                        constraints.append("ä½ç¨¼åƒç‡: äººå“¡ä½™å‰°ãƒ»ã‚³ã‚¹ãƒˆæœ€é©åŒ–ä½™åœ°")
                    else:
                        constraints.append("é©æ­£ç¨¼åƒç‡: ãƒãƒ©ãƒ³ã‚¹è‰¯ã„ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨")
                
                # ã‚¹ã‚¿ãƒƒãƒ•å€‹åˆ¥ã®ç¨¼åƒåŠ¹ç‡
                staff_utilization = long_df['staff'].value_counts()
                avg_workdays = staff_utilization.mean()
                max_workdays = staff_utilization.max()
                min_workdays = staff_utilization.min()
                
                utilization_range = max_workdays - min_workdays
                constraints.append(f"å€‹äººç¨¼åƒå·®: æœ€å¤§{max_workdays}æ—¥ - æœ€å°{min_workdays}æ—¥ (å·®{utilization_range}æ—¥)")
                
                if utilization_range > total_days * 0.5:
                    constraints.append("ç¨¼åƒæ ¼å·®å¤§: äººå“¡é…ç½®ã®å‡ç­‰åŒ–è¦æ¤œè¨")
                else:
                    constraints.append("ç¨¼åƒæ ¼å·®å°: å‡ç­‰ãªãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨")
            
            # æ™‚é–“å¸¯åˆ¥ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡
            if 'ds' in long_df.columns:
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                hourly_distribution = long_df['hour'].value_counts().sort_index()
                
                # ç¨¼åƒæ™‚é–“å¸¯ã®é›†ä¸­åº¦
                if len(hourly_distribution) > 0:
                    peak_hours = hourly_distribution.nlargest(6).index.tolist()  # ä¸Šä½6æ™‚é–“
                    peak_concentration = hourly_distribution.nlargest(6).sum() / len(long_df)
                    
                    constraints.append(f"ãƒ”ãƒ¼ã‚¯æ™‚é–“é›†ä¸­: {peak_concentration:.1%} (æ™‚é–“å¸¯: {sorted(peak_hours)})")
                    
                    if peak_concentration > 0.6:
                        constraints.append("é«˜æ™‚é–“é›†ä¸­: åŠ¹ç‡çš„ã ãŒæŸ”è»Ÿæ€§åˆ¶é™")
                    else:
                        constraints.append("åˆ†æ•£æ™‚é–“é…ç½®: æŸ”è»Ÿæ€§é«˜ãƒ»åŠ¹ç‡æ€§è¦æ¤œè¨")
            
            # è·ç¨®ãƒªã‚½ãƒ¼ã‚¹ã®æ´»ç”¨ãƒãƒ©ãƒ³ã‚¹
            if 'role' in long_df.columns:
                role_distribution = long_df['role'].value_counts()
                role_concentration = role_distribution.iloc[0] / len(long_df) if len(role_distribution) > 0 else 0
                
                constraints.append(f"ä¸»è¦è·ç¨®é›†ä¸­: {role_concentration:.1%} - ãƒªã‚½ãƒ¼ã‚¹é›†ç´„åº¦")
                
                # è·ç¨®å¤šæ§˜æ€§æŒ‡æ¨™
                role_diversity = len(role_distribution) / len(long_df) * 100
                if role_diversity > 15:
                    constraints.append("é«˜è·ç¨®å¤šæ§˜æ€§: å¤šè§’çš„ã‚µãƒ¼ãƒ“ã‚¹ãƒ»ç®¡ç†ã‚³ã‚¹ãƒˆå¢—")
                elif role_diversity < 5:
                    constraints.append("ä½è·ç¨®å¤šæ§˜æ€§: åŠ¹ç‡çš„ç®¡ç†ãƒ»å°‚é–€æ€§åˆ¶é™")
                else:
                    constraints.append("é©åº¦è·ç¨®å¤šæ§˜æ€§: ãƒãƒ©ãƒ³ã‚¹è‰¯ã„ãƒªã‚½ãƒ¼ã‚¹æ§‹æˆ")
            
            # é›‡ç”¨å½¢æ…‹ãƒªã‚½ãƒ¼ã‚¹ã®åŠ¹ç‡æ´»ç”¨
            if 'employment' in long_df.columns and 'ds' in long_df.columns:
                # é›‡ç”¨å½¢æ…‹åˆ¥ã®æ™‚é–“å¸¯æ´»ç”¨
                employment_time_matrix = pd.crosstab(
                    long_df['employment'], 
                    pd.to_datetime(long_df['ds']).dt.hour
                )
                
                # å„é›‡ç”¨å½¢æ…‹ã®æ™‚é–“åˆ†æ•£åº¦
                for emp_type in employment_time_matrix.index[:3]:  # ä¸Šä½3ç¨®é¡
                    time_distribution = employment_time_matrix.loc[emp_type]
                    time_cv = time_distribution.std() / time_distribution.mean() if time_distribution.mean() > 0 else 0
                    
                    if time_cv > 1.0:
                        constraints.append(f"{emp_type}æ™‚é–“é›†ä¸­é…ç½®: CV={time_cv:.2f} - ç‰¹å®šæ™‚é–“å¸¯æ´»ç”¨")
                    else:
                        constraints.append(f"{emp_type}æ™‚é–“åˆ†æ•£é…ç½®: CV={time_cv:.2f} - å‡ç­‰æ™‚é–“æ´»ç”¨")
                
        except Exception as e:
            log.warning(f"ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨åˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨ã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_operational_efficiency_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """é‹å–¶åŠ¹ç‡åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ã‚·ãƒ•ãƒˆå¤‰æ›´ãƒ»èª¿æ•´ã®é »åº¦æ¨å®š
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                # ã‚¹ã‚¿ãƒƒãƒ•ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                staff_patterns = {}
                for staff_id in long_df['staff'].unique():
                    staff_dates = pd.to_datetime(long_df[long_df['staff'] == staff_id]['ds'])
                    if len(staff_dates) > 1:
                        # å‹¤å‹™é–“éš”ã®æ¨™æº–åå·®ï¼ˆè¦å‰‡æ€§ã®æŒ‡æ¨™ï¼‰
                        intervals = staff_dates.sort_values().diff().dt.days.dropna()
                        if len(intervals) > 0:
                            pattern_regularity = intervals.std()
                            staff_patterns[staff_id] = pattern_regularity
                
                if staff_patterns:
                    avg_irregularity = np.mean(list(staff_patterns.values()))
                    regular_staff_ratio = sum(1 for irregularity in staff_patterns.values() if irregularity <= 2) / len(staff_patterns)
                    
                    constraints.append(f"å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³è¦å‰‡æ€§: {regular_staff_ratio:.1%}ã®ã‚¹ã‚¿ãƒƒãƒ•ãŒè¦å‰‡çš„ (å¹³å‡ä¸è¦å‰‡åº¦: {avg_irregularity:.1f}æ—¥)")
                    
                    if regular_staff_ratio > 0.7:
                        constraints.append("é«˜è¦å‰‡æ€§å‹¤å‹™: é‹å–¶åŠ¹ç‡è‰¯ãƒ»äºˆæ¸¬å¯èƒ½æ€§é«˜")
                    else:
                        constraints.append("ä¸è¦å‰‡å‹¤å‹™å¤š: æŸ”è»Ÿæ€§é«˜ãƒ»ç®¡ç†ã‚³ã‚¹ãƒˆå¢—")
            
            # å¼•ãç¶™ãåŠ¹ç‡
            if 'ds' in long_df.columns and 'staff' in long_df.columns and 'code' in long_df.columns:
                # ã‚·ãƒ•ãƒˆé–“ã®é‡è¤‡ãƒ»å¼•ãç¶™ãåˆ†æ
                handover_opportunities = 0
                total_shift_changes = 0
                
                for date in long_df['ds'].dt.date.unique():
                    daily_data = long_df[long_df['ds'].dt.date == date]
                    
                    # ç•°ãªã‚‹ã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›
                    shift_codes = daily_data['code'].unique()
                    if len(shift_codes) > 1:
                        total_shift_changes += 1
                        
                        # ã‚¹ã‚¿ãƒƒãƒ•ã®é‡è¤‡ï¼ˆå¼•ãç¶™ãæ©Ÿä¼šï¼‰
                        shift_staff_overlap = False
                        for i, code1 in enumerate(shift_codes):
                            for code2 in shift_codes[i+1:]:
                                staff1 = set(daily_data[daily_data['code'] == code1]['staff'])
                                staff2 = set(daily_data[daily_data['code'] == code2]['staff'])
                                if staff1.intersection(staff2):
                                    shift_staff_overlap = True
                                    break
                            if shift_staff_overlap:
                                break
                        
                        if shift_staff_overlap:
                            handover_opportunities += 1
                
                if total_shift_changes > 0:
                    handover_efficiency = handover_opportunities / total_shift_changes
                    constraints.append(f"å¼•ãç¶™ãåŠ¹ç‡: {handover_efficiency:.1%} - æƒ…å ±å…±æœ‰æ©Ÿä¼š")
                    
                    if handover_efficiency > 0.6:
                        constraints.append("é«˜å¼•ãç¶™ãåŠ¹ç‡: æƒ…å ±å…±æœ‰å……å®Ÿãƒ»é‹å–¶ç¶™ç¶šæ€§ç¢ºä¿")
                    else:
                        constraints.append("å¼•ãç¶™ãæ©Ÿä¼šå°‘: æƒ…å ±å…±æœ‰ä½“åˆ¶ã®æ”¹å–„ä½™åœ°")
            
            # ç®¡ç†è² è·ã®æ¨å®š
            if 'staff' in long_df.columns and 'employment' in long_df.columns and 'role' in long_df.columns:
                # ç®¡ç†å¯¾è±¡ã®è¤‡é›‘æ€§
                unique_staff = long_df['staff'].nunique()
                unique_employments = long_df['employment'].nunique()
                unique_roles = long_df['role'].nunique()
                
                management_complexity = (unique_staff * unique_employments * unique_roles) / len(long_df) * 100
                constraints.append(f"ç®¡ç†è¤‡é›‘åº¦: {management_complexity:.1f} - é‹å–¶ç®¡ç†è² è·æŒ‡æ¨™")
                
                if management_complexity > 50:
                    constraints.append("é«˜ç®¡ç†è¤‡é›‘åº¦: ã‚·ã‚¹ãƒ†ãƒ åŒ–ãƒ»æ¨™æº–åŒ–ã§åŠ¹ç‡åŒ–å¿…è¦")
                elif management_complexity < 10:
                    constraints.append("ä½ç®¡ç†è¤‡é›‘åº¦: ã‚·ãƒ³ãƒ—ãƒ«é‹å–¶ãƒ»åŠ¹ç‡æ€§ç¢ºä¿")
                else:
                    constraints.append("é©åº¦ç®¡ç†è¤‡é›‘åº¦: ãƒãƒ©ãƒ³ã‚¹è‰¯ã„é‹å–¶ä½“åˆ¶")
            
            # é‹å–¶ã®å®‰å®šæ€§
            if 'ds' in long_df.columns:
                # æ—¥åˆ¥ã®å‹¤å‹™è€…æ•°ã®å®‰å®šæ€§
                daily_staff_counts = long_df.groupby(long_df['ds'].dt.date)['staff'].nunique()
                
                if len(daily_staff_counts) > 1:
                    stability_cv = daily_staff_counts.std() / daily_staff_counts.mean()
                    constraints.append(f"é‹å–¶å®‰å®šæ€§: æ—¥åˆ¥äººå“¡å¤‰å‹•CV={stability_cv:.2f}")
                    
                    if stability_cv < 0.2:
                        constraints.append("é«˜é‹å–¶å®‰å®šæ€§: äºˆæ¸¬å¯èƒ½ãƒ»åŠ¹ç‡çš„é‹å–¶")
                    elif stability_cv > 0.5:
                        constraints.append("é‹å–¶å¤‰å‹•å¤§: éœ€è¦å¯¾å¿œåŠ›é«˜ãƒ»ç®¡ç†è² è·å¢—")
                    else:
                        constraints.append("é©åº¦é‹å–¶å¤‰å‹•: æŸ”è»Ÿæ€§ã¨å®‰å®šæ€§ã®ãƒãƒ©ãƒ³ã‚¹")
                
        except Exception as e:
            log.warning(f"é‹å–¶åŠ¹ç‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("é‹å–¶åŠ¹ç‡åˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["é‹å–¶åŠ¹ç‡ã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _extract_cost_reduction_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ã‚³ã‚¹ãƒˆå‰Šæ¸›åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ç„¡é§„ãªäººå“¡é…ç½®ã®ç‰¹å®š
            if 'ds' in long_df.columns and 'staff' in long_df.columns:
                # æœ€å°é™äººå“¡ã§ã®é‹å–¶å¯èƒ½æ€§
                daily_staff_counts = long_df.groupby(long_df['ds'].dt.date)['staff'].nunique()
                min_staff_needed = daily_staff_counts.min()
                max_staff_used = daily_staff_counts.max()
                avg_staff = daily_staff_counts.mean()
                
                efficiency_ratio = min_staff_needed / avg_staff if avg_staff > 0 else 0
                constraints.append(f"äººå“¡åŠ¹ç‡åŒ–ä½™åœ°: æœ€å°{min_staff_needed}å vs å¹³å‡{avg_staff:.1f}å (åŠ¹ç‡æ¯”{efficiency_ratio:.2f})")
                
                if efficiency_ratio < 0.7:
                    constraints.append("äººå“¡å‰Šæ¸›ä½™åœ°: æœ€å°é…ç½®åŸºæº–ã§ã®é‹å–¶æ¤œè¨å¯èƒ½")
                else:
                    constraints.append("åŠ¹ç‡çš„äººå“¡é…ç½®: é©æ­£ãƒ¬ãƒ™ãƒ«ã®äººå“¡æ´»ç”¨")
                
                # éå‰°é…ç½®æ—¥ã®ç‰¹å®š
                median_staff = daily_staff_counts.median()
                excess_days = sum(daily_staff_counts > median_staff * 1.5)
                total_days = len(daily_staff_counts)
                excess_ratio = excess_days / total_days if total_days > 0 else 0
                
                if excess_ratio > 0.2:
                    constraints.append(f"éå‰°é…ç½®æ—¥: {excess_ratio:.1%} - ã‚³ã‚¹ãƒˆå‰Šæ¸›æ©Ÿä¼š")
                else:
                    constraints.append(f"é©æ­£é…ç½®ç¶­æŒ: éå‰°é…ç½®{excess_ratio:.1%}ã®ã¿")
            
            # é«˜ã‚³ã‚¹ãƒˆæ™‚é–“å¸¯ã®æœ€é©åŒ–
            if 'ds' in long_df.columns:
                long_df['hour'] = pd.to_datetime(long_df['ds']).dt.hour
                long_df['weekday'] = pd.to_datetime(long_df['ds']).dt.day_name()
                
                # å‰²å¢—æ™‚é–“å¸¯ã®ã‚³ã‚¹ãƒˆåŠ¹ç‡
                premium_hours = list(range(22, 24)) + list(range(0, 6))  # å¤œé–“å‰²å¢—
                weekend_days = ['Saturday', 'Sunday']  # ä¼‘æ—¥å‰²å¢—
                
                premium_time_shifts = long_df[long_df['hour'].isin(premium_hours)]
                weekend_shifts = long_df[long_df['weekday'].isin(weekend_days)]
                
                total_shifts = len(long_df)
                premium_time_ratio = len(premium_time_shifts) / total_shifts if total_shifts > 0 else 0
                weekend_ratio = len(weekend_shifts) / total_shifts if total_shifts > 0 else 0
                
                total_premium_cost_ratio = premium_time_ratio + weekend_ratio
                constraints.append(f"é«˜ã‚³ã‚¹ãƒˆæ™‚é–“æ¯”ç‡: å¤œé–“{premium_time_ratio:.1%} + ä¼‘æ—¥{weekend_ratio:.1%} = {total_premium_cost_ratio:.1%}")
                
                if total_premium_cost_ratio > 0.4:
                    constraints.append("é«˜ã‚³ã‚¹ãƒˆæ™‚é–“å¤š: æ—¥ä¸­ãƒ»å¹³æ—¥ã‚·ãƒ•ãƒˆã¸ã®ç§»è¡Œã§ã‚³ã‚¹ãƒˆå‰Šæ¸›")
                elif total_premium_cost_ratio < 0.2:
                    constraints.append("ä½ã‚³ã‚¹ãƒˆæ™‚é–“ä¸­å¿ƒ: åŠ¹ç‡çš„ã‚³ã‚¹ãƒˆç®¡ç†")
                else:
                    constraints.append("ãƒãƒ©ãƒ³ã‚¹æ™‚é–“é…ç½®: å¿…è¦æ€§ã«å¿œã˜ãŸã‚³ã‚¹ãƒˆè² æ‹…")
            
            # é›‡ç”¨å½¢æ…‹æœ€é©åŒ–ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆå‰Šæ¸›
            if 'employment' in long_df.columns and 'role' in long_df.columns:
                # æ­£è¦é›‡ç”¨ã®å¿…è¦æ€§åˆ†æ
                regular_keywords = ['æ­£ç¤¾å“¡', 'æ­£è¦', 'å¸¸å‹¤']
                non_regular_keywords = ['ãƒ‘ãƒ¼ãƒˆ', 'ã‚¢ãƒ«ãƒã‚¤ãƒˆ', 'éå¸¸å‹¤', 'æ´¾é£']
                
                regular_shifts = sum(
                    long_df['employment'].str.contains(keyword, case=False, na=False).sum()
                    for keyword in regular_keywords
                )
                non_regular_shifts = sum(
                    long_df['employment'].str.contains(keyword, case=False, na=False).sum()
                    for keyword in non_regular_keywords
                )
                
                if regular_shifts + non_regular_shifts > 0:
                    regular_ratio = regular_shifts / (regular_shifts + non_regular_shifts)
                    
                    # è·ç¨®åˆ¥ã®é›‡ç”¨å½¢æ…‹é©æ­£æ€§
                    specialist_roles = ['çœ‹è­·å¸«', 'åŒ»å¸«', 'ç†å­¦ç™‚æ³•å£«', 'ä½œæ¥­ç™‚æ³•å£«']
                    general_roles = ['ä»‹è­·å£«', 'ãƒ˜ãƒ«ãƒ‘ãƒ¼', 'ã‚±ã‚¢ãƒ¯ãƒ¼ã‚«ãƒ¼']
                    
                    specialist_count = sum(
                        long_df['role'].str.contains(role, case=False, na=False).sum()
                        for role in specialist_roles
                    )
                    general_count = sum(
                        long_df['role'].str.contains(role, case=False, na=False).sum()
                        for role in general_roles
                    )
                    
                    if specialist_count > 0 and general_count > 0:
                        specialist_ratio = specialist_count / (specialist_count + general_count)
                        
                        if regular_ratio > specialist_ratio + 0.2:
                            constraints.append(f"æ­£è¦é›‡ç”¨éå¤šå¯èƒ½æ€§: æ­£è¦{regular_ratio:.1%} vs å°‚é–€è·{specialist_ratio:.1%}")
                        else:
                            constraints.append(f"é›‡ç”¨å½¢æ…‹é©æ­£: æ­£è¦{regular_ratio:.1%}ãƒ»å°‚é–€è·{specialist_ratio:.1%}ãƒãƒ©ãƒ³ã‚¹")
            
            # äº¤é€šè²»ãƒ»è«¸æ‰‹å½“å‰Šæ¸›æ©Ÿä¼š
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                # çŸ­æ™‚é–“ãƒ»çŸ­æœŸé–“å‹¤å‹™ã®åˆ†æ
                staff_workdays = long_df['staff'].value_counts()
                
                # æœˆ4æ—¥æœªæº€å‹¤å‹™ï¼ˆäº¤é€šè²»åŠ¹ç‡æ‚ªï¼‰
                low_frequency_staff = staff_workdays[staff_workdays < 4]
                low_frequency_ratio = len(low_frequency_staff) / len(staff_workdays) if len(staff_workdays) > 0 else 0
                
                if low_frequency_ratio > 0.3:
                    constraints.append(f"ä½é »åº¦å‹¤å‹™ã‚¹ã‚¿ãƒƒãƒ•: {low_frequency_ratio:.1%} - äº¤é€šè²»åŠ¹ç‡è¦æ¤œè¨")
                else:
                    constraints.append(f"åŠ¹ç‡çš„å‹¤å‹™é »åº¦: ä½é »åº¦{low_frequency_ratio:.1%}ã®ã¿")
                
                # å‹¤å‹™é›†ç´„ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
                total_staff = long_df['staff'].nunique()
                total_workdays = len(long_df)
                consolidation_potential = total_workdays / total_staff
                
                constraints.append(f"å‹¤å‹™é›†ç´„åº¦: å¹³å‡{consolidation_potential:.1f}æ—¥/äºº - åŠ¹ç‡åŒ–æŒ‡æ¨™")
                
                if consolidation_potential < 5:
                    constraints.append("å‹¤å‹™åˆ†æ•£: é›†ç´„åŒ–ã§ã‚³ã‚¹ãƒˆå‰Šæ¸›ä½™åœ°")
                else:
                    constraints.append("å‹¤å‹™é›†ç´„æ¸ˆ: åŠ¹ç‡çš„ã‚¹ã‚¿ãƒƒãƒ•æ´»ç”¨")
                
        except Exception as e:
            log.warning(f"ã‚³ã‚¹ãƒˆå‰Šæ¸›åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            constraints.append("ã‚³ã‚¹ãƒˆå‰Šæ¸›åˆ¶ç´„ã®æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        
        return constraints if constraints else ["ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«é–¢ã™ã‚‹åˆ¶ç´„ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
    
    def _calculate_max_consecutive_days(self, dates: pd.Series) -> int:
        """æœ€å¤§é€£ç¶šå‹¤å‹™æ—¥æ•°ã®è¨ˆç®—"""
        if len(dates) <= 1:
            return len(dates)
        
        sorted_dates = sorted(dates)
        max_consecutive = 1
        current_consecutive = 1
        
        for i in range(1, len(sorted_dates)):
            if (sorted_dates[i] - sorted_dates[i-1]).days == 1:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 1
        
        return max_consecutive
    
    def _generate_human_readable_results(self, mece_facts: Dict[str, List[str]], long_df: pd.DataFrame) -> Dict[str, Any]:
        """äººé–“å¯èª­å½¢å¼ã®çµæœç”Ÿæˆ"""
        
        # äº‹å®Ÿç·æ•°è¨ˆç®—
        total_facts = sum(len(facts) for facts in mece_facts.values())
        
        # ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§é‡è¦åº¦åˆ¥åˆ†é¡
        high_impact = [fact for facts in mece_facts.values() for fact in facts if any(keyword in fact for keyword in ['å‰Šæ¸›', 'æœ€é©åŒ–', 'åŠ¹ç‡', 'ä½™åœ°'])]
        cost_focus = [fact for facts in mece_facts.values() for fact in facts if any(keyword in fact for keyword in ['ã‚³ã‚¹ãƒˆ', 'äººä»¶è²»', 'å‰²å¢—', 'è¶…é'])]
        efficiency_focus = [fact for facts in mece_facts.values() for fact in facts if any(keyword in fact for keyword in ['ç”Ÿç”£æ€§', 'ç¨¼åƒ', 'æ´»ç”¨', 'åŠ¹ç‡'])]
        
        return {
            'æŠ½å‡ºäº‹å®Ÿã‚µãƒãƒªãƒ¼': {
                'ç·äº‹å®Ÿæ•°': total_facts,
                'åˆ†æè»¸': f'è»¸{self.axis_number}: {self.axis_name}',
                'åˆ†æå¯¾è±¡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°': len(long_df),
                'MECEã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°': len(mece_facts),
                **{category: len(facts) for category, facts in mece_facts.items()}
            },
            'MECEåˆ†è§£äº‹å®Ÿ': mece_facts,
            'ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§åˆ†é¡': {
                'é«˜ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆäº‹å®Ÿï¼ˆå‰Šæ¸›ãƒ»æœ€é©åŒ–ï¼‰': high_impact,
                'ã‚³ã‚¹ãƒˆé‡ç‚¹äº‹å®Ÿï¼ˆäººä»¶è²»ãƒ»å‰²å¢—ï¼‰': cost_focus,
                'åŠ¹ç‡æ€§é‡ç‚¹äº‹å®Ÿï¼ˆç”Ÿç”£æ€§ãƒ»ç¨¼åƒï¼‰': efficiency_focus,
                'è¦æ¤œè¨¼äº‹å®Ÿ': [fact for facts in mece_facts.values() for fact in facts if 'ã‚¨ãƒ©ãƒ¼' in fact or 'æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ' in fact]
            }
        }
    
    def _generate_machine_readable_constraints(self, mece_facts: Dict[str, List[str]], long_df: pd.DataFrame) -> Dict[str, Any]:
        """æ©Ÿæ¢°å¯èª­å½¢å¼ã®åˆ¶ç´„ç”Ÿæˆ"""
        
        hard_constraints = []
        soft_constraints = []
        preferences = []
        
        # MECEã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥åˆ¶ç´„åˆ†é¡
        for category, facts in mece_facts.items():
            for i, fact in enumerate(facts):
                constraint_id = f"axis6_{category.lower().replace('åˆ¶ç´„', '')}_{i+1}"
                
                # ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§ã®åˆ¶ç´„å¼·åº¦åˆ¤å®š
                if any(keyword in fact for keyword in ['å‰Šæ¸›', 'æœ€é©åŒ–', 'è¶…é', 'å¿…è¦', 'åˆ¶é™']):
                    hard_constraints.append({
                        'id': constraint_id,
                        'type': 'cost_efficiency',
                        'category': category,
                        'description': fact,
                        'priority': 'high',
                        'confidence': 0.8,
                        'cost_impact': self._assess_cost_impact(fact),
                        'efficiency_aspect': self._categorize_efficiency_aspect(fact)
                    })
                elif any(keyword in fact for keyword in ['æ”¹å–„', 'å‘ä¸Š', 'åŠ¹ç‡åŒ–', 'æ´»ç”¨', 'æ¤œè¨']):
                    soft_constraints.append({
                        'id': constraint_id,
                        'type': 'cost_efficiency',
                        'category': category,
                        'description': fact,
                        'priority': 'medium',
                        'confidence': 0.6,
                        'cost_impact': self._assess_cost_impact(fact),
                        'efficiency_aspect': self._categorize_efficiency_aspect(fact)
                    })
                else:
                    preferences.append({
                        'id': constraint_id,
                        'type': 'cost_efficiency',
                        'category': category,
                        'description': fact,
                        'priority': 'low',
                        'confidence': 0.4,
                        'cost_impact': self._assess_cost_impact(fact),
                        'efficiency_aspect': self._categorize_efficiency_aspect(fact)
                    })
        
        return {
            'hard_constraints': hard_constraints,
            'soft_constraints': soft_constraints,
            'preferences': preferences,
            'constraint_relationships': [
                {
                    'relationship_id': 'cost_efficiency_tradeoff',
                    'type': 'conflicts',
                    'from_category': 'äººä»¶è²»æœ€é©åŒ–åˆ¶ç´„',
                    'to_category': 'ç”Ÿç”£æ€§å‘ä¸Šåˆ¶ç´„',
                    'description': 'ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨ç”Ÿç”£æ€§å‘ä¸Šã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•é–¢ä¿‚'
                },
                {
                    'relationship_id': 'efficiency_synergy',
                    'type': 'enhances',
                    'from_category': 'æ™‚é–“åŠ¹ç‡åˆ¶ç´„',
                    'to_category': 'ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨åˆ¶ç´„',
                    'description': 'æ™‚é–“åŠ¹ç‡ã¨ãƒªã‚½ãƒ¼ã‚¹æ´»ç”¨ã®ç›¸ä¹—åŠ¹æœ'
                }
            ],
            'validation_rules': [
                {
                    'rule_id': 'axis6_cost_threshold_check',
                    'description': 'ã‚³ã‚¹ãƒˆä¸Šé™ã‚’è¶…ãˆãªã„ã“ã¨ã‚’ç¢ºèª',
                    'validation_type': 'cost_control'
                },
                {
                    'rule_id': 'axis6_efficiency_minimum_check',
                    'description': 'æœ€ä½åŠ¹ç‡åŸºæº–ã‚’æº€ãŸã™ã“ã¨ã‚’ç¢ºèª',
                    'validation_type': 'efficiency_compliance'
                },
                {
                    'rule_id': 'axis6_overtime_limit_check',
                    'description': 'æ®‹æ¥­æ™‚é–“åˆ¶é™ã‚’éµå®ˆã™ã‚‹ã“ã¨ã‚’ç¢ºèª',
                    'validation_type': 'overtime_control'
                }
            ]
        }
    
    def _assess_cost_impact(self, fact: str) -> str:
        """ã‚³ã‚¹ãƒˆã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆã®è©•ä¾¡"""
        if any(keyword in fact for keyword in ['å‰Šæ¸›', 'ä½™åœ°', 'éå¤š', 'è¶…é']):
            return 'high_reduction_potential'
        elif any(keyword in fact for keyword in ['æœ€é©åŒ–', 'åŠ¹ç‡åŒ–', 'æ”¹å–„']):
            return 'medium_optimization'
        elif any(keyword in fact for keyword in ['å¢—', 'ãƒªã‚¹ã‚¯', 'è² æ‹…']):
            return 'cost_increase_risk'
        else:
            return 'neutral'
    
    def _categorize_efficiency_aspect(self, fact: str) -> str:
        """åŠ¹ç‡æ€§å´é¢ã®åˆ†é¡"""
        if any(keyword in fact for keyword in ['äººä»¶è²»', 'ã‚³ã‚¹ãƒˆ', 'å‰²å¢—']):
            return 'cost_efficiency'
        elif any(keyword in fact for keyword in ['æ™‚é–“', 'ç¨¼åƒ', 'æ´»ç”¨']):
            return 'time_efficiency'
        elif any(keyword in fact for keyword in ['ç”Ÿç”£æ€§', 'ãƒãƒ¼ãƒ ', 'å”åƒ']):
            return 'productivity'
        elif any(keyword in fact for keyword in ['æ®‹æ¥­', 'è¶…é', 'é€£ç¶š']):
            return 'overtime_control'
        elif any(keyword in fact for keyword in ['é›‡ç”¨', 'å½¢æ…‹', 'é…ç½®']):
            return 'employment_optimization'
        elif any(keyword in fact for keyword in ['é‹å–¶', 'ç®¡ç†', 'å®‰å®š']):
            return 'operational_efficiency'
        else:
            return 'general_efficiency'
    
    def _generate_extraction_metadata(self, long_df: pd.DataFrame, wt_df: pd.DataFrame, 
                                     mece_facts: Dict[str, List[str]]) -> Dict[str, Any]:
        """æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        
        # ãƒ‡ãƒ¼ã‚¿æœŸé–“ã®è¨ˆç®—
        date_range = {}
        if 'ds' in long_df.columns:
            dates = pd.to_datetime(long_df['ds'])
            date_range = {
                'start_date': dates.min().isoformat(),
                'end_date': dates.max().isoformat(),
                'total_days': (dates.max() - dates.min()).days
            }
        
        # ã‚³ã‚¹ãƒˆãƒ»åŠ¹ç‡æ€§æŒ‡æ¨™
        cost_efficiency_indicators = {
            'regular_employment_ratio': len(long_df[long_df['employment'].str.contains('æ­£ç¤¾å“¡|æ­£è¦', case=False, na=False)]) / len(long_df) if 'employment' in long_df.columns and len(long_df) > 0 else 0,
            'overtime_shift_ratio': len(long_df[pd.to_datetime(long_df['ds']).dt.hour.isin(list(range(22, 24)) + list(range(0, 6)))]) / len(long_df) if 'ds' in long_df.columns and len(long_df) > 0 else 0,
            'staff_utilization_cv': long_df['staff'].value_counts().std() / long_df['staff'].value_counts().mean() if 'staff' in long_df.columns and long_df['staff'].value_counts().mean() > 0 else 0,
            'cost_optimization_potential': len([f for facts in mece_facts.values() for f in facts if 'å‰Šæ¸›' in f or 'ä½™åœ°' in f]) / sum(len(facts) for facts in mece_facts.values()) if sum(len(facts) for facts in mece_facts.values()) > 0 else 0
        }
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™
        data_quality = {
            'completeness': 1.0 - (long_df.isnull().sum().sum() / (len(long_df) * len(long_df.columns))),
            'record_count': len(long_df),
            'unique_staff_count': long_df['staff'].nunique() if 'staff' in long_df.columns else 0,
            'unique_employment_types': long_df['employment'].nunique() if 'employment' in long_df.columns else 0,
            'efficiency_focus_ratio': len([f for facts in mece_facts.values() for f in facts if any(e in f for e in ['åŠ¹ç‡', 'ã‚³ã‚¹ãƒˆ', 'å‰Šæ¸›', 'æœ€é©'])]) / sum(len(facts) for facts in mece_facts.values()) if sum(len(facts) for facts in mece_facts.values()) > 0 else 0
        }
        
        return {
            'extraction_timestamp': datetime.now().isoformat(),
            'axis_info': {
                'axis_number': self.axis_number,
                'axis_name': self.axis_name,
                'mece_categories': list(mece_facts.keys()),
                'focus_area': 'ã‚³ã‚¹ãƒˆæœ€é©åŒ–ãƒ»åŠ¹ç‡æ€§å‘ä¸Šåˆ¶ç´„'
            },
            'data_period': date_range,
            'cost_efficiency_indicators': cost_efficiency_indicators,
            'data_quality': data_quality,
            'extraction_statistics': {
                'total_facts_extracted': sum(len(facts) for facts in mece_facts.values()),
                'cost_reduction_facts': len([f for facts in mece_facts.values() for f in facts if 'å‰Šæ¸›' in f or 'ã‚³ã‚¹ãƒˆ' in f]),
                'efficiency_improvement_facts': len([f for facts in mece_facts.values() for f in facts if 'åŠ¹ç‡' in f or 'ç”Ÿç”£æ€§' in f]),
                'optimization_opportunities': len([f for facts in mece_facts.values() for f in facts if 'æœ€é©åŒ–' in f or 'ä½™åœ°' in f]),
                'categories_with_facts': len([cat for cat, facts in mece_facts.items() if facts and not any('æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ' in f for f in facts)])
            }
        }