from __future__ import annotations

import logging
from collections import defaultdict
from itertools import combinations
from typing import Any, Dict

import numpy as np
import pandas as pd
from shift_suite.tasks.constants import STATISTICAL_THRESHOLDS, FATIGUE_PARAMETERS, NIGHT_START_HOUR, NIGHT_END_HOUR
from shift_suite.tasks.utils import validate_and_convert_slot_minutes, safe_slot_calculation

# --- analysis thresholds (çµ±ä¸€ã•ã‚ŒãŸå®šæ•°ã‚’ä½¿ç”¨) ---
SYNERGY_HIGH_THRESHOLD = STATISTICAL_THRESHOLDS["synergy_high_threshold"]
SYNERGY_LOW_THRESHOLD = STATISTICAL_THRESHOLDS["synergy_low_threshold"]
VETERAN_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["veteran_ratio_threshold"]
ROLE_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["role_ratio_threshold"]
MONTH_END_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["month_end_ratio_threshold"]
WEEKEND_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["weekend_ratio_threshold"]
EARLY_MONTH_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["early_month_ratio_threshold"]
ROLE_COMBO_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["role_combo_ratio_threshold"]
NEW_STAFF_ONLY_RATIO_THRESHOLD = STATISTICAL_THRESHOLDS["new_staff_only_ratio_threshold"]

log = logging.getLogger(__name__)


def _analyze_skill_synergy(long_df: pd.DataFrame) -> list:
    """ã‚¹ã‚­ãƒ«ç›¸æ€§ãƒãƒˆãƒªã‚¯ã‚¹åˆ†æï¼šèª°ã¨èª°ã‚’çµ„ã¾ã›ã‚‹ã¨ä¸Šæ‰‹ãã„ãã‹"""
    rules = []

    if 'staff' not in long_df.columns:
        return rules

    # åŒæ™‚å‹¤å‹™ã®å®Ÿç¸¾ã‚’é›†è¨ˆ
    daily_staff = long_df[long_df['parsed_slots_count'] > 0].groupby(
        ['ds', 'code']
    )['staff'].apply(list).reset_index()

    # ãƒšã‚¢ã”ã¨ã®å…±åƒå›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    pair_counts = defaultdict(int)
    total_shifts_per_staff = long_df.groupby('staff')['ds'].nunique()

    for _, row in daily_staff.iterrows():
        staff_list = row['staff']
        if len(staff_list) >= 2:
            for pair in combinations(sorted(set(staff_list)), 2):
                pair_counts[pair] += 1

    # æœŸå¾…å€¤ã¨ã®ä¹–é›¢ã‚’è¨ˆç®—
    for (staff1, staff2), actual_count in pair_counts.items():
        if staff1 not in total_shifts_per_staff.index or staff2 not in total_shifts_per_staff.index:
            continue

        # ç‹¬ç«‹ã—ãŸå ´åˆã®æœŸå¾…å…±åƒå›æ•°
        total_days = long_df['ds'].dt.date.nunique()
        prob1 = total_shifts_per_staff[staff1] / total_days
        prob2 = total_shifts_per_staff[staff2] / total_days
        expected_count = prob1 * prob2 * total_days

        if expected_count > 0:
            synergy_score = actual_count / expected_count

            # æœŸå¾…å€¤ã‹ã‚‰å¤§ããä¹–é›¢ã—ã¦ã„ã‚‹çµ„ã¿åˆã‚ã›ã‚’æ¤œå‡º
            if synergy_score > SYNERGY_HIGH_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ã‚¹ã‚­ãƒ«ç›¸æ€§",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff1}ã€ã¨ã€Œ{staff2}ã€ã¯æ„å›³çš„ã«åŒã˜ã‚·ãƒ•ãƒˆã«é…ç½®ã•ã‚Œã‚‹ï¼ˆç›¸æ€§â—ï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(min(synergy_score / 2, 1.0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å®Ÿç¸¾": actual_count, "æœŸå¾…å€¤": round(expected_count, 1)}
                })
            elif synergy_score < SYNERGY_LOW_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ã‚¹ã‚­ãƒ«ç›¸æ€§",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff1}ã€ã¨ã€Œ{staff2}ã€ã¯æ„å›³çš„ã«åˆ¥ã‚·ãƒ•ãƒˆã«é…ç½®ã•ã‚Œã‚‹ï¼ˆç›¸æ€§Ã—ï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - synergy_score, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å®Ÿç¸¾": actual_count, "æœŸå¾…å€¤": round(expected_count, 1)}
                })

    return rules


def _analyze_workload_distribution(long_df: pd.DataFrame) -> list:
    """è² è·ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°æˆ¦ç•¥ï¼šç¹å¿™æ™‚é–“å¸¯ã§ã®äººå“¡é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    rules = []

    if not {'staff', 'role'}.issubset(long_df.columns):
        return rules

    # æ™‚é–“å¸¯åˆ¥ã®å¹³å‡äººå“¡æ•°ã‚’è¨ˆç®—
    time_staff_counts = long_df[long_df['parsed_slots_count'] > 0].groupby(
        long_df['ds'].dt.time
    )['staff'].nunique().reset_index()
    time_staff_counts.columns = ['time', 'avg_staff']

    # ç¹å¿™æ™‚é–“å¸¯ã‚’ç‰¹å®šï¼ˆä¸Šä½25%ï¼‰
    threshold = time_staff_counts['avg_staff'].quantile(STATISTICAL_THRESHOLDS["quantile_75"])
    busy_times = time_staff_counts[time_staff_counts['avg_staff'] >= threshold]['time'].tolist()

    if not busy_times:
        return rules

    # ç¹å¿™æ™‚é–“å¸¯ã§ã®è·å“¡ã®çµŒé¨“å€¤åˆ†å¸ƒã‚’åˆ†æ
    busy_df = long_df[long_df['ds'].dt.time.isin(busy_times)]

    # å„è·å“¡ã®ç·å‹¤å‹™æ—¥æ•°ï¼ˆçµŒé¨“å€¤ã®ä»£ç†æŒ‡æ¨™ï¼‰
    staff_experience = long_df.groupby('staff')['ds'].nunique().sort_values(ascending=False)
    median_exp = staff_experience.median()

    # ç¹å¿™æ™‚é–“å¸¯ã§ã®ãƒ™ãƒ†ãƒ©ãƒ³é…ç½®ç‡
    busy_staff = busy_df['staff'].unique()
    veteran_staff = staff_experience[staff_experience > median_exp].index
    veteran_ratio = len(set(busy_staff) & set(veteran_staff)) / len(busy_staff) if len(busy_staff) > 0 else 0

    if veteran_ratio > VETERAN_RATIO_THRESHOLD:
        rules.append({
            "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "è² è·åˆ†æ•£æˆ¦ç•¥",
            "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "ç¹å¿™æ™‚é–“å¸¯ã«ã¯å¿…ãšãƒ™ãƒ†ãƒ©ãƒ³è·å“¡ã‚’å„ªå…ˆé…ç½®ã—ã¦ã„ã‚‹",
            "æ³•å‰‡ã®å¼·åº¦": round(veteran_ratio, 2),
            "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"ç¹å¿™æ™‚é–“å¸¯": [str(t) for t in busy_times[:5]], "ãƒ™ãƒ†ãƒ©ãƒ³é…ç½®ç‡": f"{veteran_ratio:.1%}"}
        })

    # å½¹å‰²åˆ¥ã®åˆ†å¸ƒã‚‚åˆ†æ
    for role in busy_df['role'].unique():
        role_ratio = len(busy_df[busy_df['role'] == role]) / len(busy_df)
        if role_ratio > ROLE_RATIO_THRESHOLD:
            rules.append({
                "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "è² è·åˆ†æ•£æˆ¦ç•¥",
                "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ç¹å¿™æ™‚é–“å¸¯ã§ã¯ã€Œ{role}ã€ã®é…ç½®ã‚’é‡è¦–ã—ã¦ã„ã‚‹",
                "æ³•å‰‡ã®å¼·åº¦": round(role_ratio, 2),
                "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å½¹å‰²æ¯”ç‡": f"{role_ratio:.1%}"}
            })

    return rules


def _analyze_personal_consideration(long_df: pd.DataFrame) -> list:
    """å€‹äººäº‹æƒ…é…æ…®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šç‰¹å®šè·å“¡ã¸ã®å®šæœŸçš„ãªé…æ…®"""
    rules = []

    if 'staff' not in long_df.columns:
        return rules

    # å„è·å“¡ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    for staff in long_df['staff'].unique():
        staff_df = long_df[long_df['staff'] == staff].copy()

        # æ›œæ—¥Ã—æ™‚é–“å¸¯ã®å‹¤å‹™é »åº¦ã‚’è¨ˆç®—
        staff_df['dow'] = staff_df['ds'].dt.dayofweek
        staff_df['hour'] = staff_df['ds'].dt.hour

        # ç‰¹å®šã®æ›œæ—¥ãƒ»æ™‚é–“å¸¯ã‚’é¿ã‘ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        dow_hour_counts = staff_df.groupby(['dow', 'hour']).size()
        total_weeks = staff_df['ds'].dt.isocalendar().week.nunique()

        # æœŸå¾…é »åº¦ã¨å®Ÿéš›ã®é »åº¦ã‚’æ¯”è¼ƒ
        for (dow, hour), count in dow_hour_counts.items():
            expected_count = total_weeks * 0.8  # 80%ã®å‡ºç¾ã‚’æœŸå¾…å€¤ã¨ã™ã‚‹

            if count < expected_count * 0.2:  # æœŸå¾…å€¤ã®20%æœªæº€
                dow_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "å€‹äººé…æ…®",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã¯{dow_names[dow]}æ›œæ—¥ã®{hour}æ™‚å°ã‚’ã»ã¼é¿ã‘ã¦ã„ã‚‹ï¼ˆå€‹äººäº‹æƒ…ï¼Ÿï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - (count / expected_count if expected_count > 0 else 0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å‡ºç¾å›æ•°": count, "æœŸå¾…å›æ•°": round(expected_count, 1)}
                })

        # æœˆåˆãƒ»æœˆæœ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚åˆ†æ
        staff_df['day'] = staff_df['ds'].dt.day
        month_pattern = staff_df.groupby(staff_df['day'] <= 5)['ds'].count()

        if len(month_pattern) == 2:
            early_month_ratio = month_pattern[True] / month_pattern.sum()
            if early_month_ratio < EARLY_MONTH_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "å€‹äººé…æ…®",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã¯æœˆåˆï¼ˆ1-5æ—¥ï¼‰ã®å‹¤å‹™ã‚’ã»ã¼é¿ã‘ã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - early_month_ratio, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœˆåˆå‹¤å‹™ç‡": f"{early_month_ratio:.1%}"}
                })

    return rules


def _analyze_rotation_strategy(long_df: pd.DataFrame) -> list:
    """ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ï¼šå…¬å¹³æ€§ã‚’ä¿ã¤ãŸã‚ã®è¤‡é›‘ãªãƒ«ãƒ¼ãƒ«"""
    rules = []

    if not {'staff', 'code'}.issubset(long_df.columns):
        return rules

    # å„è·å“¡ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é€£ç¶šæ€§ã‚’åˆ†æ
    for staff in long_df['staff'].unique():
        staff_df = long_df[long_df['staff'] == staff].sort_values('ds')

        # é€£ç¶šå‹¤å‹™æ—¥æ•°ã®è¨ˆç®—
        staff_dates = staff_df[staff_df['parsed_slots_count'] > 0]['ds'].dt.date.unique()

        if len(staff_dates) < 2:
            continue

        # é€£ç¶šå‹¤å‹™ã®ã‚«ã‚¦ãƒ³ãƒˆ
        consecutive_counts = []
        current_streak = 1

        for i in range(1, len(staff_dates)):
            if (staff_dates[i] - staff_dates[i-1]).days == 1:
                current_streak += 1
            else:
                if current_streak > 1:
                    consecutive_counts.append(current_streak)
                current_streak = 1

        if current_streak > 1:
            consecutive_counts.append(current_streak)

        # é•·æœŸé€£å‹¤ã®æ¤œå‡º
        if consecutive_counts:
            max_consecutive = max(consecutive_counts)
            avg_consecutive = np.mean(consecutive_counts)

            if max_consecutive <= 3 and avg_consecutive < 2.5:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã®é€£ç¶šå‹¤å‹™ã¯å¿…ãš3æ—¥ä»¥å†…ã«åˆ¶é™ã•ã‚Œã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - (max_consecutive - 3) / 7, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœ€å¤§é€£ç¶š": max_consecutive, "å¹³å‡é€£ç¶š": round(avg_consecutive, 1)}
                })

        # å‹¤å‹™ã‚³ãƒ¼ãƒ‰ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
        code_sequence = staff_df[staff_df['parsed_slots_count'] > 0]['code'].tolist()

        if len(code_sequence) >= 3:
            # åŒã˜ã‚³ãƒ¼ãƒ‰ã®é€£ç¶šã‚’é¿ã‘ã¦ã„ã‚‹ã‹
            same_code_runs = []
            current_run = 1

            for i in range(1, len(code_sequence)):
                if code_sequence[i] == code_sequence[i-1]:
                    current_run += 1
                else:
                    if current_run > 1:
                        same_code_runs.append(current_run)
                    current_run = 1

            if same_code_runs and max(same_code_runs) <= 2:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã¯åŒã˜å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ3æ—¥ä»¥ä¸Šç¶šã‹ãªã„ã‚ˆã†ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": 0.8,
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœ€å¤§é€£ç¶šåŒä¸€å‹¤å‹™": max(same_code_runs)}
                })

    return rules


def _analyze_risk_mitigation(long_df: pd.DataFrame) -> list:
    """ãƒªã‚¹ã‚¯å›é¿ãƒ«ãƒ¼ãƒ«ï¼šãƒˆãƒ©ãƒ–ãƒ«é˜²æ­¢ã®ãŸã‚ã®æš—é»™ã®é…ç½®ãƒ«ãƒ¼ãƒ«"""
    rules = []

    if not {'staff', 'role'}.issubset(long_df.columns):
        return rules

    # å„æ™‚é–“å¸¯ã§ã®æ–°äººæ¯”ç‡ã‚’åˆ†æ
    staff_experience = long_df.groupby('staff')['ds'].nunique()
    experience_threshold = staff_experience.quantile(0.25)  # ä¸‹ä½25%ã‚’æ–°äººã¨ã™ã‚‹
    new_staff = staff_experience[staff_experience <= experience_threshold].index

    # æ™‚é–“å¸¯åˆ¥ã®æ–°äººæ¯”ç‡
    time_groups = long_df[long_df['parsed_slots_count'] > 0].groupby(
        [long_df['ds'].dt.date, long_df['ds'].dt.hour]
    )

    new_staff_only_count = 0
    total_time_slots = 0

    for (date, hour), group in time_groups:
        unique_staff = group['staff'].unique()
        if len(unique_staff) > 1:  # è¤‡æ•°äººå‹¤å‹™ã®å ´åˆã®ã¿
            total_time_slots += 1
            if all(s in new_staff for s in unique_staff):
                new_staff_only_count += 1

    if total_time_slots > 0:
        new_staff_only_ratio = new_staff_only_count / total_time_slots

        if new_staff_only_ratio < NEW_STAFF_ONLY_RATIO_THRESHOLD:
            rules.append({
                "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒªã‚¹ã‚¯å›é¿",
                "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "æ–°äººã ã‘ã®ã‚·ãƒ•ãƒˆã¯çµ¶å¯¾ã«ä½œã‚‰ãªã„ï¼ˆå¿…ãšãƒ™ãƒ†ãƒ©ãƒ³ã‚’1äººã¯é…ç½®ï¼‰",
                "æ³•å‰‡ã®å¼·åº¦": round(1.0 - new_staff_only_ratio, 2),
                "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æ–°äººã®ã¿ã‚·ãƒ•ãƒˆç™ºç”Ÿç‡": f"{new_staff_only_ratio:.1%}"}
            })

    # ç‰¹å®šã®å½¹å‰²ã®çµ„ã¿åˆã‚ã›åˆ†æ
    role_combinations = defaultdict(int)

    for (date, time), group in long_df.groupby([long_df['ds'].dt.date, long_df['ds'].dt.time]):
        roles = group['role'].unique()
        if len(roles) >= 2:
            for combo in combinations(sorted(roles), 2):
                role_combinations[combo] += 1

    # æœŸå¾…å€¤ã¨æ¯”è¼ƒã—ã¦ç•°å¸¸ã«å°‘ãªã„çµ„ã¿åˆã‚ã›ã‚’æ¤œå‡º
    if role_combinations:
        avg_count = np.mean(list(role_combinations.values()))
        for combo, count in role_combinations.items():
            if count < avg_count * ROLE_COMBO_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒªã‚¹ã‚¯å›é¿",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{combo[0]}ã€ã¨ã€Œ{combo[1]}ã€ã¯åŒæ™‚é…ç½®ã‚’é¿ã‘ã¦ã„ã‚‹ï¼ˆæ¥­å‹™ä¸Šã®ç†ç”±ï¼Ÿï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - (count / avg_count if avg_count > 0 else 0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å‡ºç¾å›æ•°": count, "å¹³å‡": round(avg_count, 1)}
                })

    return rules


def _analyze_temporal_context(long_df: pd.DataFrame) -> list:
    """æ™‚ç³»åˆ—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼šæ™‚æœŸã«ã‚ˆã‚‹é…ç½®æˆ¦ç•¥ã®å¤‰åŒ–"""
    rules = []

    if 'ds' not in long_df.columns:
        return rules

    # æœˆåˆ¥ã®å‚¾å‘åˆ†æ
    long_df['month'] = long_df['ds'].dt.month
    long_df['day'] = long_df['ds'].dt.day

    # æœˆåˆãƒ»æœˆä¸­ãƒ»æœˆæœ«ã§ã®äººå“¡é…ç½®ã®é•ã„
    long_df['period'] = pd.cut(long_df['day'], bins=[0, 10, 20, 31], labels=['æœˆåˆ', 'æœˆä¸­', 'æœˆæœ«'])

    period_stats = long_df[long_df['parsed_slots_count'] > 0].groupby(['period']).agg({
        'staff': 'nunique',
        'ds': 'count'
    })

    if len(period_stats) > 1 and 'æœˆæœ«' in period_stats.index:
        period_stats['avg_staff_per_slot'] = period_stats['ds'] / period_stats['staff']

        mean_staff_per_slot = period_stats['avg_staff_per_slot'].mean()
        if mean_staff_per_slot > 0:
            month_end_ratio = period_stats.loc['æœˆæœ«', 'avg_staff_per_slot'] / mean_staff_per_slot
            if month_end_ratio > MONTH_END_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "æ™‚ç³»åˆ—æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "æœˆæœ«ã¯é€šå¸¸ã‚ˆã‚Šæ‰‹åšã„äººå“¡é…ç½®ã‚’è¡Œã£ã¦ã„ã‚‹ï¼ˆç· ã‚ä½œæ¥­å¯¾å¿œï¼Ÿï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(min(month_end_ratio - 1.0, 1.0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœˆæœ«é…ç½®å€ç‡": f"{month_end_ratio:.2f}å€"}
                })

    # æ›œæ—¥ã«ã‚ˆã‚‹æˆ¦ç•¥ã®é•ã„
    long_df['dow'] = long_df['ds'].dt.dayofweek

    dow_stats = long_df[long_df['parsed_slots_count'] > 0].groupby('dow').agg({
        'staff': 'nunique',
        'code': lambda x: x.value_counts().to_dict()
    })

    # é€±æœ«ã®ç‰¹åˆ¥ãªé…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³
    if 5 in dow_stats.index and 6 in dow_stats.index:  # åœŸæ—¥
        weekend_staff = dow_stats.loc[[5, 6], 'staff'].mean()
        weekday_staff = dow_stats.loc[[0, 1, 2, 3, 4], 'staff'].mean() if len(dow_stats) > 2 else 0

        if weekday_staff > 0:
            weekend_ratio = weekend_staff / weekday_staff
            if weekend_ratio < WEEKEND_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "æ™‚ç³»åˆ—æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "é€±æœ«ã¯å¹³æ—¥ã®70%ä»¥ä¸‹ã®çœåŠ›ä½“åˆ¶ã§é‹å–¶ã—ã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - weekend_ratio, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"é€±æœ«/å¹³æ—¥æ¯”": f"{weekend_ratio:.1%}"}
                })

    return rules


def _extract_surprising_insights(rules_df: pd.DataFrame) -> list:
    """æ„å¤–æ€§ã®é«˜ã„ç™ºè¦‹ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    if rules_df.empty:
        return []

    surprising = []

    # å€‹äººé…æ…®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æ„å¤–ãªã‚‚ã®ã‚’æŠ½å‡º
    personal_rules = rules_df[rules_df['æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼'] == 'å€‹äººé…æ…®']
    if not personal_rules.empty:
        top_personal = personal_rules.nlargest(3, 'æ³•å‰‡ã®å¼·åº¦')
        for _, rule in top_personal.iterrows():
            surprising.append({
                "ç™ºè¦‹": rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'],
                "æ„å¤–æ€§": "ç‰¹å®šå€‹äººã¸ã®é…æ…®ãŒæ˜ç¢ºã«ãƒ‡ãƒ¼ã‚¿ã«ç¾ã‚Œã¦ã„ã‚‹"
            })

    # ã‚¹ã‚­ãƒ«ç›¸æ€§ã§ç›¸æ€§ãŒæ‚ªã„ãƒšã‚¢
    bad_synergy = rules_df[
        (rules_df['æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼'] == 'ã‚¹ã‚­ãƒ«ç›¸æ€§') &
        (rules_df['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'].str.contains('åˆ¥ã‚·ãƒ•ãƒˆ'))
    ]
    if not bad_synergy.empty:
        for _, rule in bad_synergy.iterrows():
            surprising.append({
                "ç™ºè¦‹": rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'],
                "æ„å¤–æ€§": "æ„å›³çš„ã«çµ„ã¿åˆã‚ã›ã‚’é¿ã‘ã¦ã„ã‚‹è·å“¡ãƒšã‚¢ã®å­˜åœ¨"
            })

    return surprising


def _generate_deep_insights_summary(rules_df: pd.DataFrame) -> str:
    """ã‚ˆã‚Šæ´å¯Ÿçš„ãªã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
    if rules_df.empty:
        return "åˆ†æå¯èƒ½ãªæ³•å‰‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    summary_parts = []

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®æ³•å‰‡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    category_counts = rules_df['æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼'].value_counts()

    summary_parts.append("## ğŸ” ã‚·ãƒ•ãƒˆä½œæˆã®æ·±å±¤åˆ†æçµæœ\n")
    summary_parts.append(f"åˆè¨ˆ **{len(rules_df)}å€‹** ã®æš—é»™ã®ãƒ«ãƒ¼ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚\n")

    # æœ€ã‚‚å¼·ã„æ³•å‰‡ãƒˆãƒƒãƒ—3
    top_rules = rules_df.nlargest(3, 'æ³•å‰‡ã®å¼·åº¦')
    summary_parts.append("\n### ğŸ“Š æœ€ã‚‚å½±éŸ¿åŠ›ã®å¼·ã„ãƒ«ãƒ¼ãƒ« TOP3\n")
    for i, (_, rule) in enumerate(top_rules.iterrows(), 1):
        summary_parts.append(f"{i}. **{rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡']}** (å¼·åº¦: {rule['æ³•å‰‡ã®å¼·åº¦']})")

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®ç‰¹å¾´
    summary_parts.append("\n### ğŸ¯ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®ç‰¹å¾´\n")

    if 'ã‚¹ã‚­ãƒ«ç›¸æ€§' in category_counts:
        summary_parts.append(f"- **äººé–“é–¢ä¿‚ã¸ã®é…æ…®**: {category_counts['ã‚¹ã‚­ãƒ«ç›¸æ€§']}å€‹ã®ãƒ«ãƒ¼ãƒ«")
    if 'å€‹äººé…æ…®' in category_counts:
        summary_parts.append(f"- **å€‹äººäº‹æƒ…ã¸ã®å¯¾å¿œ**: {category_counts['å€‹äººé…æ…®']}å€‹ã®ãƒ«ãƒ¼ãƒ«")
    if 'ãƒªã‚¹ã‚¯å›é¿' in category_counts:
        summary_parts.append(f"- **ãƒˆãƒ©ãƒ–ãƒ«é˜²æ­¢ç­–**: {category_counts['ãƒªã‚¹ã‚¯å›é¿']}å€‹ã®ãƒ«ãƒ¼ãƒ«")

    # ç·æ‹¬
    summary_parts.append("\n### ğŸ’¡ ç·æ‹¬")
    summary_parts.append("ã“ã®ã‚·ãƒ•ãƒˆã¯ã€è¡¨é¢çš„ãªãƒ«ãƒ¼ãƒ«ã ã‘ã§ãªãã€")
    summary_parts.append("è·å“¡é–“ã®ç›¸æ€§ã€å€‹äººã®äº‹æƒ…ã€ãƒªã‚¹ã‚¯ç®¡ç†ãªã©ã€")
    summary_parts.append("**å¤šæ¬¡å…ƒçš„ãªé…æ…®**ãŒè¤‡é›‘ã«çµ„ã¿åˆã‚ã•ã£ã¦ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚")

    return "\n".join(summary_parts)


def _calculate_fairness_score(daily_shift_df: pd.DataFrame) -> float:
    """ãã®æ—¥ã®ã‚·ãƒ•ãƒˆã®å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    work_hours = daily_shift_df.groupby("staff")["ds"].count()
    if work_hours.empty:
        return 0.0
    return 1.0 - (work_hours.std() / (work_hours.mean() + 1e-6))


def _calculate_cost_score(daily_shift_df: pd.DataFrame, max_staff: int) -> float:
    """ãã®æ—¥ã®ã‚·ãƒ•ãƒˆã®ã‚³ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    staff_count = daily_shift_df["staff"].nunique()
    if max_staff == 0:
        return 1.0
    return 1.0 - (staff_count / max_staff)


def _calculate_risk_score(daily_shift_df: pd.DataFrame, new_staff: set[str]) -> float:
    """æ–°äººã ã‘ã®ã‚·ãƒ•ãƒˆã‚’é¿ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã§ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡"""
    time_groups = daily_shift_df.groupby(daily_shift_df["ds"].dt.hour)
    new_only = 0
    total = 0
    for _, group in time_groups:
        staff = set(group["staff"].unique())
        if len(staff) > 1:
            total += 1
            if all(s in new_staff for s in staff):
                new_only += 1
    if total == 0:
        return 1.0
    return 1.0 - new_only / total


def _calculate_satisfaction_score(date: pd.Timestamp, long_df: pd.DataFrame) -> float:
    """é€£å‹¤ã®é•·ã•ã‹ã‚‰æº€è¶³åº¦ã‚’æ¨å®š"""
    day_df = long_df[long_df["ds"].dt.date == date.date()]
    staff_list = day_df["staff"].unique()
    long_run = 0
    for staff in staff_list:
        staff_dates = (
            long_df[long_df["staff"] == staff]["ds"].dt.date.drop_duplicates().sort_values()
        )
        indices = [i for i, d in enumerate(staff_dates) if d == date.date()]
        for idx in indices:
            left = idx
            while left > 0 and (staff_dates.iloc[left] - staff_dates.iloc[left - 1]).days == 1:
                left -= 1
            right = idx
            while right < len(staff_dates) - 1 and (
                staff_dates.iloc[right + 1] - staff_dates.iloc[right]
            ).days == 1:
                right += 1
            if right - left + 1 >= 4:
                long_run += 1
                break
    if len(staff_list) == 0:
        return 1.0
    return 1.0 - long_run / len(staff_list)


def create_scored_blueprint(long_df: pd.DataFrame) -> pd.DataFrame:
    """Calculate daily scores representing shift creator preferences."""
    if long_df.empty:
        return pd.DataFrame()

    df = long_df.copy()
    df["ds"] = pd.to_datetime(df["ds"])

    daily_groups = df.groupby(df["ds"].dt.date)
    max_staff = daily_groups["staff"].nunique().max()

    staff_experience = df.groupby("staff")["ds"].nunique()
    threshold = staff_experience.quantile(0.25)
    new_staff = set(staff_experience[staff_experience <= threshold].index)

    scored_days = []
    for date, group in daily_groups:
        scores = {
            "date": pd.to_datetime(date),
            "fairness_score": _calculate_fairness_score(group),
            "cost_score": _calculate_cost_score(group, int(max_staff)),
            "risk_score": _calculate_risk_score(group, new_staff),
            "satisfaction_score": _calculate_satisfaction_score(pd.to_datetime(date), df),
        }
        scored_days.append(scores)

    return pd.DataFrame(scored_days).set_index("date")


def analyze_tradeoffs(scored_blueprint_df: pd.DataFrame) -> dict:
    """ã‚¹ã‚³ã‚¢é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ†æã™ã‚‹"""
    if scored_blueprint_df.empty:
        return {}

    corr = scored_blueprint_df.corr()
    tradeoff_pairs = {}
    for col1 in corr.columns:
        for col2 in corr.columns:
            if col1 != col2 and corr.loc[col1, col2] < -0.5:
                tradeoff_pairs[f"{col1}_vs_{col2}"] = float(corr.loc[col1, col2])

    scatter_cols = {"fairness_score", "cost_score"}
    scatter_data = (
        scored_blueprint_df[list(scatter_cols)].reset_index().to_dict("records")
        if scatter_cols.issubset(scored_blueprint_df.columns)
        else []
    )

    return {
        "correlation_matrix": corr.to_dict(),
        "strongest_tradeoffs": tradeoff_pairs,
        "scatter_data": scatter_data,
    }


def create_staff_level_blueprint(
    long_df: pd.DataFrame, score_df: pd.DataFrame
) -> pd.DataFrame:
    """ã‚¹ã‚¿ãƒƒãƒ•æ¯ã®ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚¹ã‚³ã‚¢ã‚’é›†è¨ˆ"""
    if long_df.empty or score_df.empty:
        return pd.DataFrame()

    staff_by_day = long_df[["ds", "staff"]].copy()
    staff_by_day["date"] = staff_by_day["ds"].dt.date

    score_df_reset = score_df.reset_index()

    # staff_by_dayã®'date'åˆ—ã‚’ã€score_df_resetã«åˆã‚ã›ã¦æ—¥ä»˜å‹(datetime64[ns])ã«å¤‰æ›ã—ã¾ã™ã€‚
    staff_by_day["date"] = pd.to_datetime(staff_by_day["date"])

    merged = staff_by_day.merge(score_df_reset, on="date", how="left")

    if merged.empty or "staff" not in merged.columns:
        return pd.DataFrame()

    score_cols = [
        "fairness_score",
        "cost_score",
        "risk_score",
        "satisfaction_score",
    ]
    return merged.groupby("staff")[score_cols].mean()


# backward compatibility
def create_blueprint_list(long_df: pd.DataFrame) -> dict:
    """Return discovered rules and scoring analysis as a single dictionary."""
    if long_df.empty:
        return {}

    # --- legacy implicit rule discovery logic ---
    log.info("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æï¼ˆãƒ«ãƒ¼ãƒ«ç™ºè¦‹ï¼‰ã‚’é–‹å§‹ã—ã¾ã™...")
    all_rules: list[dict] = []
    all_rules.extend(_analyze_skill_synergy(long_df))
    all_rules.extend(_analyze_workload_distribution(long_df))
    all_rules.extend(_analyze_personal_consideration(long_df))
    all_rules.extend(_analyze_rotation_strategy(long_df))
    all_rules.extend(_analyze_risk_mitigation(long_df))
    all_rules.extend(_analyze_temporal_context(long_df))

    rules_df = (
        pd.DataFrame(all_rules).sort_values("æ³•å‰‡ã®å¼·åº¦", ascending=False).reset_index(drop=True)
        if all_rules
        else pd.DataFrame()
    )

    # --- scoring and trade-off analysis ---
    log.info("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æï¼ˆã‚¹ã‚³ã‚¢åŒ–ãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æï¼‰ã‚’é–‹å§‹ã—ã¾ã™...")
    scored_df = create_scored_blueprint(long_df)
    tradeoff_info = analyze_tradeoffs(scored_df)

    # --- staff level scores ---
    staff_level_scores = create_staff_level_blueprint(long_df, scored_df)

    return {
        "rules_df": rules_df,
        "scored_df": scored_df,
        "tradeoffs": tradeoff_info,
        "staff_level_scores": staff_level_scores,
    }


# backward compatibility ã‚’ä¿ã¡ã¤ã¤æ‹¡å¼µ
_original_create_blueprint_list = create_blueprint_list


class FactExtractor:
    """ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®¢è¦³çš„ãªäº‹å®Ÿã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, slot_minutes: int = 30) -> None:
        self.weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        # å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆè¨­å®š
        self.slot_hours = validate_and_convert_slot_minutes(slot_minutes, "FactExtractor.__init__")
        self.slot_minutes = slot_minutes

    def extract_all_facts(self, long_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
        """å…¨ã¦ã®å®¢è¦³çš„äº‹å®Ÿã‚’ä½“ç³»çš„ã«æŠ½å‡º"""

        if long_df.empty:
            return {}

        facts = {
            "å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³äº‹å®Ÿ": self._extract_work_pattern_facts(long_df),
            "æ›œæ—¥äº‹å®Ÿ": self._extract_weekday_facts(long_df),
            "ã‚³ãƒ¼ãƒ‰äº‹å®Ÿ": self._extract_code_facts(long_df),
            "æ™‚é–“å¸¯äº‹å®Ÿ": self._extract_time_facts(long_df),
            "ãƒšã‚¢äº‹å®Ÿ": self._extract_pair_facts(long_df),
            "çµ±è¨ˆçš„äº‹å®Ÿ": self._extract_statistical_facts(long_df),
        }

        return facts

    def _extract_work_pattern_facts(self, long_df: pd.DataFrame) -> pd.DataFrame:
        """å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã«é–¢ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªäº‹å®Ÿã‚’æŠ½å‡º"""
        return pd.DataFrame()

    def _extract_weekday_facts(self, long_df: pd.DataFrame) -> pd.DataFrame:
        """ã‚¹ã‚¿ãƒƒãƒ•ã”ã¨ã®æ›œæ—¥åˆ¥å‹¤å‹™äº‹å®Ÿã‚’æŠ½å‡º"""
        facts = []

        for staff in long_df['staff'].unique():
            staff_df = long_df[long_df['staff'] == staff]
            staff_df = staff_df[staff_df['parsed_slots_count'] > 0]

            if staff_df.empty:
                continue

            weekday_counts = staff_df.groupby(staff_df['ds'].dt.dayofweek).size()
            total_days = staff_df['ds'].dt.date.nunique()

            working_days = weekday_counts[weekday_counts > 0].index.tolist()

            if len(working_days) < 7:
                working_day_names = [self.weekday_names[d] for d in working_days]
                facts.append({
                    "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                    "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "æ›œæ—¥é™å®šå‹¤å‹™",
                    "è©³ç´°": f"{', '.join(working_day_names)}ã®ã¿å‹¤å‹™",
                    "å‹¤å‹™æ›œæ—¥æ•°": len(working_days),
                    "ç·å‹¤å‹™æ—¥æ•°": total_days,
                    "ç¢ºä¿¡åº¦": min(1.0, total_days / 10),
                })

            if len(weekday_counts) > 0 and total_days >= 5:
                for day, count in weekday_counts.items():
                    expected_ratio = 1 / 7
                    actual_ratio = count / total_days
                    if actual_ratio > expected_ratio * 2:
                        facts.append({
                            "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                            "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "æ›œæ—¥åé‡",
                            "è©³ç´°": f"{self.weekday_names[day]}æ›œæ—¥ã«{actual_ratio:.1%}ãŒé›†ä¸­",
                            "å®Ÿç¸¾æ¯”ç‡": round(actual_ratio, 3),
                            "æœŸå¾…æ¯”ç‡": round(expected_ratio, 3),
                            "åé‡åº¦": round(actual_ratio / expected_ratio, 2),
                            "ç¢ºä¿¡åº¦": min(1.0, total_days / 10),
                        })

        return pd.DataFrame(facts)

    def _extract_code_facts(self, long_df: pd.DataFrame) -> pd.DataFrame:
        """å‹¤å‹™ã‚³ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹äº‹å®Ÿã‚’æŠ½å‡º"""
        facts = []

        all_codes = sorted(long_df[long_df['code'] != '']['code'].unique())

        for staff in long_df['staff'].unique():
            staff_df = long_df[long_df['staff'] == staff]
            staff_df = staff_df[staff_df['code'] != '']

            if staff_df.empty:
                continue

            used_codes = sorted(staff_df['code'].unique())

            if len(used_codes) > 0 and len(used_codes) < len(all_codes) * 0.5:
                facts.append({
                    "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                    "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "é™å®šã‚³ãƒ¼ãƒ‰ä½¿ç”¨",
                    "è©³ç´°": f"ä½¿ç”¨ã‚³ãƒ¼ãƒ‰: {', '.join(used_codes[:5])}{'...' if len(used_codes) > 5 else ''}",
                    "ä½¿ç”¨ã‚³ãƒ¼ãƒ‰æ•°": len(used_codes),
                    "å…¨ã‚³ãƒ¼ãƒ‰æ•°": len(all_codes),
                    "ä½¿ç”¨ç‡": round(len(used_codes) / len(all_codes), 3),
                    "ç¢ºä¿¡åº¦": 1.0,
                })

            avoided_codes = sorted(set(all_codes) - set(used_codes))
            if avoided_codes and len(avoided_codes) < len(all_codes) * 0.5:
                facts.append({
                    "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                    "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "ã‚³ãƒ¼ãƒ‰å›é¿",
                    "è©³ç´°": f"æœªä½¿ç”¨ã‚³ãƒ¼ãƒ‰: {', '.join(avoided_codes[:5])}{'...' if len(avoided_codes) > 5 else ''}",
                    "å›é¿ã‚³ãƒ¼ãƒ‰æ•°": len(avoided_codes),
                    "ç¢ºä¿¡åº¦": 1.0,
                })

            if len(staff_df) > 0:
                code_freq = staff_df['code'].value_counts(normalize=True)
                if len(code_freq) > 0 and code_freq.iloc[0] > 0.7:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "ä¸»è¦ã‚³ãƒ¼ãƒ‰ä¾å­˜",
                        "è©³ç´°": f"{code_freq.index[0]}ãŒ{code_freq.iloc[0]:.1%}ã‚’å ã‚ã‚‹",
                        "ä¸»è¦ã‚³ãƒ¼ãƒ‰": code_freq.index[0],
                        "ä¾å­˜åº¦": round(code_freq.iloc[0], 3),
                        "ç¢ºä¿¡åº¦": min(1.0, len(staff_df) / 20),
                    })

        return pd.DataFrame(facts)

    def _extract_time_facts(self, long_df: pd.DataFrame) -> pd.DataFrame:
        """æ™‚é–“å¸¯ã«é–¢ã™ã‚‹äº‹å®Ÿã‚’æŠ½å‡º"""
        facts = []

        for staff in long_df['staff'].unique():
            staff_df = long_df[long_df['staff'] == staff]
            staff_df = staff_df[staff_df['parsed_slots_count'] > 0]

            if staff_df.empty:
                continue

            hours = staff_df['ds'].dt.hour
            if len(hours) > 0:
                early_morning_ratio = (hours < 6).sum() / len(hours)
                if early_morning_ratio > FATIGUE_PARAMETERS["early_morning_threshold"]:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "æ—©æœå‹¤å‹™",
                        "è©³ç´°": f"å…¨å‹¤å‹™ã®{early_morning_ratio:.1%}ãŒ6æ™‚å‰",
                        "æ¯”ç‡": round(early_morning_ratio, 3),
                        "ç¢ºä¿¡åº¦": min(1.0, len(hours) / 50),
                    })

                night_hours_mask = hours.apply(lambda h: h >= NIGHT_START_HOUR or h < NIGHT_END_HOUR)
                night_ratio = night_hours_mask.sum() / len(hours)
                if night_ratio > FATIGUE_PARAMETERS["night_shift_threshold"]:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "æ·±å¤œå‹¤å‹™",
                        "è©³ç´°": f"å…¨å‹¤å‹™ã®{night_ratio:.1%}ãŒ22æ™‚ä»¥é™",
                        "æ¯”ç‡": round(night_ratio, 3),
                        "ç¢ºä¿¡åº¦": min(1.0, len(hours) / 50),
                    })

                hour_std = hours.std()
                if hour_std < 2:
                    mode_hour = hours.mode()[0] if len(hours.mode()) > 0 else hours.mean()
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "å›ºå®šæ™‚é–“å¸¯å‹¤å‹™",
                        "è©³ç´°": f"ä¸»ã«{int(mode_hour)}æ™‚å°ã‚’ä¸­å¿ƒã«å‹¤å‹™",
                        "ä¸­å¿ƒæ™‚é–“": int(mode_hour),
                        "ã°ã‚‰ã¤ã": round(hour_std, 2),
                        "ç¢ºä¿¡åº¦": min(1.0, len(hours) / 30),
                    })

        return pd.DataFrame(facts)

    def _extract_pair_facts(self, long_df: pd.DataFrame) -> pd.DataFrame:
        """ãƒšã‚¢å‹¤å‹™ã«é–¢ã™ã‚‹äº‹å®Ÿã‚’æŠ½å‡º"""
        facts = []

        daily_staff = long_df[long_df['parsed_slots_count'] > 0].groupby(
            ['ds', 'code']
        )['staff'].apply(list).reset_index()

        pair_counts = defaultdict(int)
        total_shifts_per_staff = long_df.groupby('staff')['ds'].nunique()

        for _, row in daily_staff.iterrows():
            staff_list = row['staff']
            if len(staff_list) >= 2:
                for pair in combinations(sorted(set(staff_list)), 2):
                    pair_counts[pair] += 1

        for (staff1, staff2), count in pair_counts.items():
            if staff1 in total_shifts_per_staff.index and staff2 in total_shifts_per_staff.index:
                min_shifts = min(total_shifts_per_staff[staff1], total_shifts_per_staff[staff2])
                if min_shifts > 0:
                    pair_ratio = count / min_shifts

                    if pair_ratio > 0.5:
                        facts.append({
                            "ã‚¹ã‚¿ãƒƒãƒ•": f"{staff1} & {staff2}",
                            "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "é »ç¹ãƒšã‚¢",
                            "è©³ç´°": f"å…±åƒç‡{pair_ratio:.1%} ({count}å›)",
                            "å…±åƒå›æ•°": count,
                            "å…±åƒç‡": round(pair_ratio, 3),
                            "ç¢ºä¿¡åº¦": min(1.0, count / 10),
                        })
                    elif count == 0:
                        facts.append({
                            "ã‚¹ã‚¿ãƒƒãƒ•": f"{staff1} & {staff2}",
                            "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "éå…±åƒãƒšã‚¢",
                            "è©³ç´°": "ä¸€åº¦ã‚‚åŒã˜ã‚·ãƒ•ãƒˆã«å…¥ã£ã¦ã„ãªã„",
                            "å…±åƒå›æ•°": 0,
                            "ç¢ºä¿¡åº¦": min(1.0, min_shifts / 10),
                        })

        return pd.DataFrame(facts)

    def _extract_statistical_facts(self, long_df: pd.DataFrame) -> pd.DataFrame:
        """çµ±è¨ˆçš„ãªäº‹å®Ÿã‚’æŠ½å‡º"""
        facts = []

        total_staff = long_df['staff'].nunique()
        total_days = long_df['ds'].dt.date.nunique()

        if total_days > 0:
            daily_staff_counts = long_df[long_df['parsed_slots_count'] > 0].groupby(
                long_df['ds'].dt.date
            )['staff'].nunique()

            avg_daily_staff = daily_staff_counts.mean()

            facts.append({
                "ã‚¹ã‚¿ãƒƒãƒ•": "å…¨ä½“",
                "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "åŸºæœ¬çµ±è¨ˆ",
                "è©³ç´°": f"å¹³å‡{avg_daily_staff:.1f}äºº/æ—¥ï¼ˆç·å‹¢{total_staff}äººï¼‰",
                "å¹³å‡æ—¥æ¬¡ã‚¹ã‚¿ãƒƒãƒ•æ•°": round(avg_daily_staff, 2),
                "ç·ã‚¹ã‚¿ãƒƒãƒ•æ•°": total_staff,
                "åˆ†ææœŸé–“æ—¥æ•°": total_days,
                "ç¢ºä¿¡åº¦": 1.0,
            })

        for staff in long_df['staff'].unique():
            staff_df = long_df[long_df['staff'] == staff]
            work_days = staff_df[staff_df['parsed_slots_count'] > 0]['ds'].dt.date.nunique()

            if total_days > 0 and work_days > 0:
                work_ratio = work_days / total_days
                # ä¿®æ­£: å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆè¨­å®šå¯¾å¿œ
                avg_hours_per_day = safe_slot_calculation(
                    staff_df['parsed_slots_count'], 
                    self.slot_minutes, 
                    "sum", 
                    "extract_all_facts"
                ) / work_days

                if work_ratio < 0.2:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "ä½é »åº¦å‹¤å‹™",
                        "è©³ç´°": f"å‹¤å‹™ç‡{work_ratio:.1%} ({work_days}/{total_days}æ—¥)",
                        "å‹¤å‹™æ—¥æ•°": work_days,
                        "å‹¤å‹™ç‡": round(work_ratio, 3),
                        "ç¢ºä¿¡åº¦": min(1.0, total_days / 20),
                    })
                elif work_ratio > 0.8:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "é«˜é »åº¦å‹¤å‹™",
                        "è©³ç´°": f"å‹¤å‹™ç‡{work_ratio:.1%} ({work_days}/{total_days}æ—¥)",
                        "å‹¤å‹™æ—¥æ•°": work_days,
                        "å‹¤å‹™ç‡": round(work_ratio, 3),
                        "ç¢ºä¿¡åº¦": min(1.0, total_days / 20),
                    })

                if avg_hours_per_day < 4:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "çŸ­æ™‚é–“å‹¤å‹™",
                        "è©³ç´°": f"å¹³å‡{avg_hours_per_day:.1f}æ™‚é–“/æ—¥",
                        "å¹³å‡å‹¤å‹™æ™‚é–“": round(avg_hours_per_day, 2),
                        "ç¢ºä¿¡åº¦": min(1.0, work_days / 10),
                    })
                elif avg_hours_per_day > 10:
                    facts.append({
                        "ã‚¹ã‚¿ãƒƒãƒ•": staff,
                        "äº‹å®Ÿã‚¿ã‚¤ãƒ—": "é•·æ™‚é–“å‹¤å‹™",
                        "è©³ç´°": f"å¹³å‡{avg_hours_per_day:.1f}æ™‚é–“/æ—¥",
                        "å¹³å‡å‹¤å‹™æ™‚é–“": round(avg_hours_per_day, 2),
                        "ç¢ºä¿¡åº¦": min(1.0, work_days / 10),
                    })

        return pd.DataFrame(facts)


def create_integrated_analysis(long_df: pd.DataFrame) -> Dict[str, Any]:
    """äº‹å®Ÿã¨æš—é»™çŸ¥ã®çµ±åˆåˆ†æ"""

    blueprint_data = create_blueprint_list(long_df)

    fact_extractor = FactExtractor()
    facts_dict = fact_extractor.extract_all_facts(long_df)

    all_facts = []
    for fact_type, fact_df in facts_dict.items():
        if not fact_df.empty:
            fact_df['ã‚«ãƒ†ã‚´ãƒªãƒ¼'] = fact_type
            all_facts.append(fact_df)

    combined_facts_df = pd.concat(all_facts, ignore_index=True) if all_facts else pd.DataFrame()

    connections = []
    if not combined_facts_df.empty and 'rules_df' in blueprint_data and not blueprint_data['rules_df'].empty:
        for _, fact in combined_facts_df.iterrows():
            staff_in_fact = fact.get('ã‚¹ã‚¿ãƒƒãƒ•', '')
            related_rules = blueprint_data['rules_df'][
                blueprint_data['rules_df']['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'].str.contains(staff_in_fact, na=False)
            ]

            for _, rule in related_rules.iterrows():
                connections.append({
                    "äº‹å®Ÿ": fact['è©³ç´°'],
                    "é–¢é€£ãƒ«ãƒ¼ãƒ«": rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'],
                    "é–¢é€£æ€§": "ã‚¹ã‚¿ãƒƒãƒ•åã«ã‚ˆã‚‹é–¢é€£",
                })

    result = blueprint_data.copy()
    result['facts_df'] = combined_facts_df
    result['facts_by_category'] = facts_dict
    result['fact_rule_connections'] = pd.DataFrame(connections)

    return result


def create_blueprint_list(long_df: pd.DataFrame) -> dict:
    """Return discovered rules and scoring analysis as a single dictionary."""
    result = _original_create_blueprint_list(long_df)

    try:
        fact_extractor = FactExtractor()
        facts_dict = fact_extractor.extract_all_facts(long_df)

        all_facts = []
        for fact_type, fact_df in facts_dict.items():
            if not fact_df.empty:
                fact_df['ã‚«ãƒ†ã‚´ãƒªãƒ¼'] = fact_type
                all_facts.append(fact_df)

        combined_facts_df = pd.concat(all_facts, ignore_index=True) if all_facts else pd.DataFrame()

        result['facts_df'] = combined_facts_df
        result['facts_by_category'] = facts_dict
    except Exception as e:  # noqa: BLE001
        log.warning(f"äº‹å®ŸæŠ½å‡ºä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        result['facts_df'] = pd.DataFrame()
        result['facts_by_category'] = {}

    return result
