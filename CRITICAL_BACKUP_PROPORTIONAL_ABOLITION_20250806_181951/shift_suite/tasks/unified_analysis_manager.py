#!/usr/bin/env python3
"""
çµ±ä¸€åˆ†æçµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - Unified Analysis Manager

å…¨ä½“æœ€é©åŒ–ã‚’ç›®çš„ã¨ã—ãŸçµ±ä¸€ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ»ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ»AIé€£æºã‚·ã‚¹ãƒ†ãƒ 
å‹•çš„ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨å®Ÿãƒ‡ãƒ¼ã‚¿å„ªå…ˆã‚’åŸºæœ¬æ–¹é‡ã¨ã™ã‚‹
"""

import logging
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, List, Union
import pandas as pd
import numpy as np
from pathlib import Path

log = logging.getLogger(__name__)

class SafeDataConverter:
    """å‹å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚·ã‚¹ãƒ†ãƒ """
    
    @staticmethod
    def safe_float(value: Any, default: float = 0.0, field_name: str = "unknown") -> float:
        """å®‰å…¨ãªfloatå¤‰æ›"""
        try:
            if pd.isna(value):
                return default
            converted = float(value)
            if np.isinf(converted):
                log.warning(f"[SafeDataConverter] {field_name}ã§Infå€¤æ¤œå‡º â†’ {default}ã«å¤‰æ›")
                return default
            return converted
        except (ValueError, TypeError) as e:
            log.warning(f"[SafeDataConverter] {field_name}ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value} â†’ {default} (ç†ç”±: {e})")
            return default
    
    @staticmethod
    def safe_int(value: Any, default: int = 0, field_name: str = "unknown") -> int:
        """å®‰å…¨ãªintå¤‰æ›"""
        try:
            if pd.isna(value):
                return default
            converted = int(float(value))  # floatçµŒç”±ã§ã‚ˆã‚ŠæŸ”è»Ÿã«å¤‰æ›
            return converted
        except (ValueError, TypeError) as e:
            log.warning(f"[SafeDataConverter] {field_name}ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value} â†’ {default} (ç†ç”±: {e})")
            return default
    
    @staticmethod
    def safe_str(value: Any, default: str = "N/A", field_name: str = "unknown") -> str:
        """å®‰å…¨ãªstrå¤‰æ›"""
        try:
            if pd.isna(value):
                return default
            return str(value)
        except Exception as e:
            log.warning(f"[SafeDataConverter] {field_name}ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {value} â†’ {default} (ç†ç”±: {e})")
            return default

class DynamicKeyManager:
    """å‹•çš„ã‚­ãƒ¼ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªå¯¾å¿œå¼·åŒ–ç‰ˆ"""
    
    @staticmethod
    def generate_analysis_key(file_name: str, scenario_key: str = "default", 
                            analysis_type: str = "general") -> str:
        """çµ±ä¸€ã•ã‚ŒãŸã‚­ãƒ¼ç”Ÿæˆï¼ˆå¾“æ¥äº’æ›ï¼‰"""
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ‹¡å¼µå­ã‚’é™¤å»ã—ã€å®‰å…¨ãªæ–‡å­—ã®ã¿ä½¿ç”¨
        clean_filename = Path(file_name).stem.replace(' ', '_').replace('-', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        
        return f"{clean_filename}_{scenario_key}_{analysis_type}_{timestamp}_{unique_id}"
    
    @staticmethod  
    def generate_scenario_analysis_key(
        file_name: str, 
        scenario_key: str,
        analysis_type: str
    ) -> str:
        """ğŸ”§ æ–°æ©Ÿèƒ½: ã‚·ãƒŠãƒªã‚ªå¯¾å¿œã®çµ±ä¸€ã‚­ãƒ¼ç”Ÿæˆ"""
        clean_filename = Path(file_name).stem.replace(' ', '_').replace('-', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        
        return f"{clean_filename}_{scenario_key}_{analysis_type}_{timestamp}_{unique_id}"
    
    @staticmethod
    def parse_scenario_key(analysis_key: str) -> Dict[str, str]:
        """ğŸ”§ æ–°æ©Ÿèƒ½: ã‚·ãƒŠãƒªã‚ªã‚­ãƒ¼ã®è§£æ"""
        try:
            parts = analysis_key.split('_')
            if len(parts) >= 5:
                return {
                    "file_name": parts[0],
                    "scenario": parts[1], 
                    "analysis_type": parts[2],
                    "timestamp": f"{parts[3]}_{parts[4]}",
                    "unique_id": parts[5] if len(parts) > 5 else "unknown"
                }
        except Exception as e:
            log.warning(f"ã‚·ãƒŠãƒªã‚ªã‚­ãƒ¼è§£æã‚¨ãƒ©ãƒ¼: {analysis_key} â†’ {e}")
        
        return {"file_name": "unknown", "scenario": "default", 
                "analysis_type": "general", "timestamp": "unknown", "unique_id": "unknown"}
    
    @staticmethod
    def extract_file_info(analysis_key: str) -> Dict[str, str]:
        """ã‚­ãƒ¼ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æŠ½å‡º"""
        try:
            parts = analysis_key.split('_')
            if len(parts) >= 5:
                return {
                    "file_name": parts[0],
                    "scenario_key": parts[1],
                    "analysis_type": parts[2],
                    "timestamp": f"{parts[3]}_{parts[4]}",
                    "unique_id": parts[5] if len(parts) > 5 else "unknown"
                }
        except Exception as e:
            log.warning(f"ã‚­ãƒ¼è§£æã‚¨ãƒ©ãƒ¼: {analysis_key} â†’ {e}")
        
        return {"file_name": "unknown", "scenario_key": "default", 
                "analysis_type": "general", "timestamp": "unknown", "unique_id": "unknown"}

class UnifiedAnalysisResult:
    """çµ±ä¸€åˆ†æçµæœãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    
    def __init__(self, analysis_key: str, analysis_type: str):
        self.analysis_key = analysis_key
        self.analysis_type = analysis_type
        self.creation_time = datetime.now().isoformat()
        self.data_integrity = "valid"
        self.error_details = None
        self.core_metrics = {}
        self.extended_data = {}
        self.metadata = {
            "data_source": "real_time_analysis",
            "processing_method": "dynamic_adaptive",
            "quality_score": 1.0
        }
    
    def set_error_state(self, error: Exception, fallback_data: Dict[str, Any] = None):
        """ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã®çµ±ä¸€è¨­å®š"""
        self.data_integrity = "error"
        self.error_details = {
            "error_type": type(error).__name__,
            "error_message": str(error)[:200],
            "timestamp": datetime.now().isoformat()
        }
        self.metadata["quality_score"] = 0.0
        
        if fallback_data:
            self.core_metrics.update(fallback_data)
            log.info(f"[{self.analysis_key}] ã‚¨ãƒ©ãƒ¼æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š")
    
    def add_core_metric(self, key: str, value: Any, description: str = ""):
        """ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®‰å…¨ãªè¿½åŠ """
        self.core_metrics[key] = {
            "value": value,
            "description": description,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_ai_compatible_dict(self) -> Dict[str, Any]:
        """AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®è¾æ›¸ç”Ÿæˆ"""
        return {
            "analysis_key": self.analysis_key,
            "analysis_type": self.analysis_type,
            "creation_time": self.creation_time,
            "data_integrity": self.data_integrity,
            "error_details": self.error_details,
            "core_metrics": self.core_metrics,
            "extended_data": self.extended_data,
            "metadata": self.metadata
        }

class UnifiedAnalysisManager:
    """çµ±ä¸€åˆ†æçµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªå¯¾å¿œç‰ˆ"""
    
    def __init__(self):
        self.converter = SafeDataConverter()
        self.key_manager = DynamicKeyManager()
        self.results_registry = {}  # å¾“æ¥ã®çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒª
        
        # ğŸ”§ æ–°æ©Ÿèƒ½: ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªç®¡ç†
        self.scenario_registries = {
            "mean_based": {},
            "median_based": {},
            "p25_based": {}
        }
        self.default_scenario = "median_based"  # çµ±è¨ˆçš„ã«æœ€ã‚‚å®‰å®š
        
        log.info("[UnifiedAnalysisManager] ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªå¯¾å¿œã§åˆæœŸåŒ–å®Œäº†")
        log.info(f"[UnifiedAnalysisManager] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ª: {self.default_scenario}")
    
    def create_shortage_analysis(self, file_name: str, scenario_key: str, 
                               role_df: pd.DataFrame) -> UnifiedAnalysisResult:
        """ä¸è¶³åˆ†æçµæœã®çµ±ä¸€ä½œæˆ - ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªå¯¾å¿œ"""
        # ğŸ”§ ä¿®æ­£: ã‚·ãƒŠãƒªã‚ªå¯¾å¿œã‚­ãƒ¼ç”Ÿæˆ
        analysis_key = self.key_manager.generate_scenario_analysis_key(
            file_name, scenario_key, "shortage"
        )
        result = UnifiedAnalysisResult(analysis_key, "shortage_analysis")
        
        # ã‚·ãƒŠãƒªã‚ªæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        result.metadata["scenario"] = scenario_key
        result.metadata["file_name"] = file_name
        
        try:
            # å‹•çš„ãƒ‡ãƒ¼ã‚¿å‡¦ç† - ã‚«ãƒ©ãƒ ã®å­˜åœ¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰å‡¦ç†
            if "lack_h" in role_df.columns and not role_df.empty:
                total_shortage_hours = self.converter.safe_float(
                    role_df["lack_h"].sum(), 0.0, "total_shortage_hours"
                )
                shortage_events = self.converter.safe_int(
                    (role_df["lack_h"] > 0).sum(), 0, "shortage_events"
                )
                
                # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ¼ã‚¿è¦ç´„ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ï¼‰
                top_shortage_roles = []
                if "lack_h" in role_df.columns:
                    # å‹•çš„ã«ä¸Šä½Nä»¶ã‚’æ±ºå®šï¼ˆãƒ‡ãƒ¼ã‚¿è¦æ¨¡ã«å¿œã˜ã¦èª¿æ•´ï¼‰
                    sample_size = min(50, max(10, len(role_df) // 10))
                    top_roles_df = role_df.nlargest(sample_size, 'lack_h')
                    top_shortage_roles = [
                        self.converter.safe_str(role, "Unknown") 
                        for role in top_roles_df.get('role', pd.Series()).tolist()
                    ]
                
                # ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨­å®š
                result.add_core_metric("total_shortage_hours", total_shortage_hours, 
                                     "ç·ä¸è¶³æ™‚é–“ï¼ˆå®Ÿæ¸¬å€¤ï¼‰")
                result.add_core_metric("shortage_events_count", shortage_events,
                                     "ä¸è¶³ç™ºç”Ÿå›æ•°")
                result.add_core_metric("affected_roles_count", len(role_df),
                                     "å½±éŸ¿ã‚’å—ã‘ãŸè·ç¨®æ•°")
                
                # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ï¼ˆAIåˆ†æç”¨ï¼‰
                result.extended_data = {
                    "top_shortage_roles": top_shortage_roles[:10],
                    "severity_level": self._calculate_severity(total_shortage_hours),
                    "role_count": len(role_df),
                    "data_completeness": len(role_df[role_df["lack_h"].notna()]) / len(role_df) if len(role_df) > 0 else 0.0
                }
                
                log.info(f"[{analysis_key}] ä¸è¶³åˆ†æå®Œäº†: {total_shortage_hours:.1f}æ™‚é–“ä¸è¶³")
                
            else:
                # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                result.add_core_metric("total_shortage_hours", 0.0, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤")
                result.extended_data = {"severity_level": "unknown", "role_count": 0}
                result.metadata["quality_score"] = 0.5
                log.warning(f"[{analysis_key}] ä¸è¶³åˆ†æ: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨")
                
        except Exception as e:
            result.set_error_state(e, {
                "total_shortage_hours": 0.0,
                "shortage_events_count": 0,
                "affected_roles_count": 0
            })
            log.error(f"[{analysis_key}] ä¸è¶³åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        
        # ğŸ”§ ä¿®æ­£: çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¨ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®ä¸¡æ–¹ã«ç™»éŒ²
        self.results_registry[analysis_key] = result
        
        # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
        if scenario_key in self.scenario_registries:
            self.scenario_registries[scenario_key][analysis_key] = result
            log.debug(f"[UnifiedAnalysisManager] {scenario_key}ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²: {analysis_key}")
        
        return result
    
    def create_fatigue_analysis(self, file_name: str, scenario_key: str,
                              fatigue_df: Optional[pd.DataFrame] = None,
                              combined_df: Optional[pd.DataFrame] = None) -> UnifiedAnalysisResult:
        """ç–²åŠ´åˆ†æçµæœã®çµ±ä¸€ä½œæˆ - ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªå¯¾å¿œ"""
        # ğŸ”§ ä¿®æ­£: ã‚·ãƒŠãƒªã‚ªå¯¾å¿œã‚­ãƒ¼ç”Ÿæˆ
        analysis_key = self.key_manager.generate_scenario_analysis_key(
            file_name, scenario_key, "fatigue"
        )
        result = UnifiedAnalysisResult(analysis_key, "fatigue_analysis")
        
        # ã‚·ãƒŠãƒªã‚ªæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
        result.metadata["scenario"] = scenario_key
        result.metadata["file_name"] = file_name
        
        try:
            avg_fatigue_score = 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            high_fatigue_count = 0
            total_analyzed = 0
            
            # å‹•çš„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹é¸æŠ
            active_df = None
            data_source_type = "none"
            
            if fatigue_df is not None and not fatigue_df.empty and "fatigue_score" in fatigue_df.columns:
                active_df = fatigue_df
                data_source_type = "fatigue_score_direct"
                avg_fatigue_score = self.converter.safe_float(
                    fatigue_df["fatigue_score"].mean(), 0.5, "avg_fatigue_score"
                )
                high_fatigue_count = self.converter.safe_int(
                    (fatigue_df["fatigue_score"] > 0.7).sum(), 0, "high_fatigue_count"
                )
                total_analyzed = len(fatigue_df)
                
            elif combined_df is not None and not combined_df.empty and "final_score" in combined_df.columns:
                active_df = combined_df
                data_source_type = "combined_score_estimated"
                scores = combined_df["final_score"]
                if scores.max() > 0:
                    # ä½ã„ã‚¹ã‚³ã‚¢ = é«˜ã„ç–²åŠ´ã®æ¨å®š
                    avg_fatigue_score = self.converter.safe_float(
                        max(0, 1 - (scores.mean() / scores.max())), 0.5, "estimated_fatigue"
                    )
                    high_fatigue_count = self.converter.safe_int(
                        len(scores[scores < (scores.mean() - scores.std())]), 0, "estimated_high_fatigue"
                    )
                total_analyzed = len(combined_df)
            
            # ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨­å®š
            result.add_core_metric("avg_fatigue_score", avg_fatigue_score, 
                                 f"å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢ï¼ˆ{data_source_type}ï¼‰")
            result.add_core_metric("high_fatigue_staff_count", high_fatigue_count,
                                 "é«˜ç–²åŠ´ã‚¹ã‚¿ãƒƒãƒ•æ•°")
            result.add_core_metric("total_staff_analyzed", total_analyzed,
                                 "åˆ†æå¯¾è±¡ã‚¹ã‚¿ãƒƒãƒ•ç·æ•°")
            
            # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿
            result.extended_data = {
                "data_source_type": data_source_type,
                "fatigue_distribution": self._calculate_fatigue_distribution(active_df) if active_df is not None else {},
                "analysis_reliability": "high" if data_source_type == "fatigue_score_direct" else "medium"
            }
            
            log.info(f"[{analysis_key}] ç–²åŠ´åˆ†æå®Œäº†: å¹³å‡ã‚¹ã‚³ã‚¢ {avg_fatigue_score:.3f} ({data_source_type})")
            
        except Exception as e:
            result.set_error_state(e, {
                "avg_fatigue_score": 0.5,
                "high_fatigue_staff_count": 0,
                "total_staff_analyzed": 0
            })
            log.error(f"[{analysis_key}] ç–²åŠ´åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        
        # ğŸ”§ ä¿®æ­£: çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¨ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®ä¸¡æ–¹ã«ç™»éŒ²
        self.results_registry[analysis_key] = result
        
        # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
        if scenario_key in self.scenario_registries:
            self.scenario_registries[scenario_key][analysis_key] = result
            log.debug(f"[UnifiedAnalysisManager] {scenario_key}ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²: {analysis_key}")
        
        return result
    
    def create_fairness_analysis(self, file_name: str, scenario_key: str,
                               fairness_df: pd.DataFrame) -> List[UnifiedAnalysisResult]:
        """å…¬å¹³æ€§åˆ†æçµæœã®çµ±ä¸€ä½œæˆ - ãƒãƒ«ãƒã‚·ãƒŠãƒªã‚ªå¯¾å¿œï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
        results = []
        
        try:
            # å‹•çš„ãƒ‡ãƒ¼ã‚¿å‡¦ç† - åˆ©ç”¨å¯èƒ½ãªã‚«ãƒ©ãƒ ã‚’ç¢ºèª
            score_column = None
            if "night_ratio" in fairness_df.columns:
                score_column = "night_ratio"
            elif "fairness_score" in fairness_df.columns:
                score_column = "fairness_score"
            elif "balance_score" in fairness_df.columns:
                score_column = "balance_score"
            
            if score_column and not fairness_df.empty:
                avg_fairness_score = self.converter.safe_float(
                    fairness_df[score_column].mean(), 0.8, f"avg_{score_column}"
                )
                low_fairness_count = self.converter.safe_int(
                    len(fairness_df[fairness_df[score_column] < 0.7]), 0, "low_fairness_count"
                )
                
                # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯çµ±ä¸€çµæœã‚’ä½œæˆ
                # ğŸ”§ ä¿®æ­£: ã‚·ãƒŠãƒªã‚ªå¯¾å¿œã‚­ãƒ¼ç”Ÿæˆ
                analysis_key = self.key_manager.generate_scenario_analysis_key(
                    file_name, scenario_key, "fairness"
                )
                result = UnifiedAnalysisResult(analysis_key, "fairness_analysis")
                
                # ã‚·ãƒŠãƒªã‚ªæƒ…å ±ã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
                result.metadata["scenario"] = scenario_key
                result.metadata["file_name"] = file_name
                
                result.add_core_metric("avg_fairness_score", avg_fairness_score,
                                     f"å¹³å‡å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ï¼ˆ{score_column}ãƒ™ãƒ¼ã‚¹ï¼‰")
                result.add_core_metric("low_fairness_staff_count", low_fairness_count,
                                     "æ”¹å–„å¿…è¦ã‚¹ã‚¿ãƒƒãƒ•æ•°")
                result.add_core_metric("total_staff_analyzed", len(fairness_df),
                                     "åˆ†æå¯¾è±¡ã‚¹ã‚¿ãƒƒãƒ•ç·æ•°")
                
                result.extended_data = {
                    "score_column_used": score_column,
                    "fairness_distribution": self._calculate_fairness_distribution(fairness_df, score_column),
                    "improvement_rate": (len(fairness_df) - low_fairness_count) / len(fairness_df) if len(fairness_df) > 0 else 1.0
                }
                
                results.append(result)
                self.results_registry[analysis_key] = result
                
                log.info(f"[{analysis_key}] å…¬å¹³æ€§åˆ†æå®Œäº†: å¹³å‡ã‚¹ã‚³ã‚¢ {avg_fairness_score:.3f}")
                
            else:
                # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                analysis_key = self.key_manager.generate_analysis_key(
                    file_name, scenario_key, "fairness"
                )
                result = UnifiedAnalysisResult(analysis_key, "fairness_analysis")
                result.add_core_metric("avg_fairness_score", 0.8, "ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤")
                result.extended_data = {"score_column_used": "none", "improvement_rate": 0.0}
                results.append(result)
                self.results_registry[analysis_key] = result
                
                log.warning(f"[{analysis_key}] å…¬å¹³æ€§åˆ†æ: é©åˆ‡ãªã‚¹ã‚³ã‚¢åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨")
                
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœä½œæˆ
            analysis_key = self.key_manager.generate_analysis_key(
                file_name, scenario_key, "fairness"
            )
            result = UnifiedAnalysisResult(analysis_key, "fairness_analysis")
            result.set_error_state(e, {
                "avg_fairness_score": 0.8,
                "low_fairness_staff_count": 0,
                "total_staff_analyzed": 0
            })
            results.append(result)
            self.results_registry[analysis_key] = result
            log.error(f"[{analysis_key}] å…¬å¹³æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        
        return results
    
    def set_default_scenario(self, scenario: str):
        """ğŸ”§ æ–°æ©Ÿèƒ½: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ªã®è¨­å®š"""
        valid_scenarios = ["mean_based", "median_based", "p25_based"]
        if scenario in valid_scenarios:
            self.default_scenario = scenario
            log.info(f"[UnifiedAnalysisManager] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ªã‚’è¨­å®š: {scenario}")
        else:
            log.warning(f"[UnifiedAnalysisManager] ç„¡åŠ¹ãªã‚·ãƒŠãƒªã‚ª: {scenario}. æœ‰åŠ¹: {valid_scenarios}")
    
    def get_scenario_compatible_results(
        self, 
        file_pattern: str = None,
        scenario: str = None
    ) -> Dict[str, Any]:
        """ğŸ”§ æ–°æ©Ÿèƒ½: ã‚·ãƒŠãƒªã‚ªå¯¾å¿œã®çµæœå–å¾—"""
        target_scenario = scenario or self.default_scenario
        
        log.info(f"[get_scenario_compatible_results] å¯¾è±¡ã‚·ãƒŠãƒªã‚ª: {target_scenario}")
        log.info(f"[get_scenario_compatible_results] æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: '{file_pattern}'")
        
        # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰å–å¾—
        if target_scenario in self.scenario_registries:
            scenario_registry = self.scenario_registries[target_scenario]
            log.info(f"[get_scenario_compatible_results] {target_scenario}ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚µã‚¤ã‚º: {len(scenario_registry)}")
            
            if scenario_registry:
                # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰çµæœã‚’å–å¾—
                return self._process_scenario_results(scenario_registry, file_pattern)
            else:
                log.warning(f"[get_scenario_compatible_results] {target_scenario}ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãŒç©ºã§ã™")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰å–å¾—
        log.info("[get_scenario_compatible_results] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰å–å¾—")
        return self.get_ai_compatible_results(file_pattern)
    
    def _process_scenario_results(self, registry: Dict[str, 'UnifiedAnalysisResult'], file_pattern: str = None) -> Dict[str, Any]:
        """ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®çµæœå‡¦ç†"""
        ai_results = {}
        
        for key, result in registry.items():
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
            if file_pattern is None:
                match = True
            else:
                clean_pattern = Path(file_pattern).stem  # æ‹¡å¼µå­ã‚’é™¤å»
                match = clean_pattern in key or file_pattern in key
                
            if match:
                log.debug(f"[_process_scenario_results] ãƒãƒƒãƒ: {key}")
                # åˆ†æã‚¿ã‚¤ãƒ—ã”ã¨ã«æ•´ç†
                analysis_type = result.analysis_type
                if analysis_type not in ai_results:
                    ai_results[analysis_type] = []
                
                ai_results[analysis_type].append(result.get_ai_compatible_dict())
        
        # æœ€æ–°ã®çµæœã®ã¿ã‚’å„åˆ†æã‚¿ã‚¤ãƒ—ã‹ã‚‰é¸æŠ
        consolidated_results = {}
        for analysis_type, results_list in ai_results.items():
            if results_list:
                # ä½œæˆæ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ã‚’é¸æŠ
                latest_result = max(results_list, key=lambda x: x["creation_time"])
                
                # ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¹³å¦åŒ–
                consolidated_data = {}
                for metric_key, metric_data in latest_result["core_metrics"].items():
                    consolidated_data[metric_key] = metric_data["value"]
                
                # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚‚çµ±åˆ
                consolidated_data.update(latest_result["extended_data"])
                consolidated_data["data_integrity"] = latest_result["data_integrity"]
                consolidated_data["metadata"] = latest_result["metadata"]
                
                consolidated_results[analysis_type] = consolidated_data
        
        return consolidated_results
    
    def get_ai_compatible_results(self, file_pattern: str = None) -> Dict[str, Any]:
        """AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”¨ã®çµæœè¾æ›¸ç”Ÿæˆ"""
        ai_results = {}
        
        # ğŸ”§ ä¿®æ­£: ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’è¿½åŠ 
        log.info(f"[get_ai_compatible_results] æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³: '{file_pattern}'")
        log.info(f"[get_ai_compatible_results] ãƒ¬ã‚¸ã‚¹ãƒˆãƒªå†…ã®ã‚­ãƒ¼æ•°: {len(self.results_registry)}")
        
        # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªå†…ã®ã‚­ãƒ¼ã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if self.results_registry:
            log.debug("[get_ai_compatible_results] ãƒ¬ã‚¸ã‚¹ãƒˆãƒªå†…ã®ã‚­ãƒ¼:")
            for key in list(self.results_registry.keys())[:5]:  # æœ€åˆã®5å€‹ã®ã¿
                log.debug(f"  - {key}")
        else:
            log.warning("[get_ai_compatible_results] âš ï¸ ãƒ¬ã‚¸ã‚¹ãƒˆãƒªãŒç©ºã§ã™ï¼")
        
        for key, result in self.results_registry.items():
            # ğŸ”§ ä¿®æ­£: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã‚’æ”¹å–„
            if file_pattern is None:
                match = True
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«åã®éƒ¨åˆ†ä¸€è‡´ã‚’è¨±å¯
                from pathlib import Path
                clean_pattern = Path(file_pattern).stem  # æ‹¡å¼µå­ã‚’é™¤å»
                match = clean_pattern in key or file_pattern in key
                
            if match:
                log.debug(f"[get_ai_compatible_results] ãƒãƒƒãƒ: {key}")
                # åˆ†æã‚¿ã‚¤ãƒ—ã”ã¨ã«æ•´ç†
                analysis_type = result.analysis_type
                if analysis_type not in ai_results:
                    ai_results[analysis_type] = []
                
                ai_results[analysis_type].append(result.get_ai_compatible_dict())
        
        # æœ€æ–°ã®çµæœã®ã¿ã‚’å„åˆ†æã‚¿ã‚¤ãƒ—ã‹ã‚‰é¸æŠï¼ˆé‡è¤‡å›é¿ï¼‰
        consolidated_results = {}
        for analysis_type, results_list in ai_results.items():
            if results_list:
                # ä½œæˆæ™‚åˆ»ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ã‚’é¸æŠ
                latest_result = max(results_list, key=lambda x: x["creation_time"])
                
                # ã‚³ã‚¢ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å¹³å¦åŒ–ã—ã¦AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼ã«é©åˆ
                consolidated_data = {}
                for metric_key, metric_data in latest_result["core_metrics"].items():
                    consolidated_data[metric_key] = metric_data["value"]
                
                # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚‚çµ±åˆ
                consolidated_data.update(latest_result["extended_data"])
                consolidated_data["data_integrity"] = latest_result["data_integrity"]
                consolidated_data["metadata"] = latest_result["metadata"]
                
                consolidated_results[analysis_type] = consolidated_data
        
        return consolidated_results
    
    def _calculate_severity(self, shortage_hours: float) -> str:
        """ä¸è¶³æ™‚é–“ã‹ã‚‰é‡è¦åº¦ã‚’å‹•çš„è¨ˆç®—"""
        if shortage_hours > 100:
            return "critical"
        elif shortage_hours > 50:
            return "high"
        elif shortage_hours > 10:
            return "medium"
        else:
            return "low"
    
    def _calculate_fatigue_distribution(self, df: pd.DataFrame) -> Dict[str, int]:
        """ç–²åŠ´åˆ†å¸ƒã®å‹•çš„è¨ˆç®—"""
        if df is None or df.empty:
            return {"normal": 0, "elevated": 0, "high": 0, "critical": 0}
        
        score_col = "fatigue_score" if "fatigue_score" in df.columns else "final_score"
        if score_col not in df.columns:
            return {"normal": 0, "elevated": 0, "high": 0, "critical": 0}
        
        scores = df[score_col].fillna(0.5)
        return {
            "normal": int((scores < 0.5).sum()),
            "elevated": int(((scores >= 0.5) & (scores < 0.7)).sum()),
            "high": int(((scores >= 0.7) & (scores < 0.85)).sum()),
            "critical": int((scores >= 0.85).sum())
        }
    
    def _calculate_fairness_distribution(self, df: pd.DataFrame, score_col: str) -> Dict[str, int]:
        """å…¬å¹³æ€§åˆ†å¸ƒã®å‹•çš„è¨ˆç®—"""
        if df is None or df.empty or score_col not in df.columns:
            return {"excellent": 0, "good": 0, "needs_improvement": 0, "poor": 0}
        
        scores = df[score_col].fillna(0.8)
        return {
            "excellent": int((scores >= 0.9).sum()),
            "good": int(((scores >= 0.7) & (scores < 0.9)).sum()),
            "needs_improvement": int(((scores >= 0.5) & (scores < 0.7)).sum()),
            "poor": int((scores < 0.5).sum())
        }
    
    def cleanup_old_results(self, max_age_hours: int = 24):
        """å¤ã„çµæœã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        current_time = datetime.now()
        keys_to_remove = []
        
        for key, result in self.results_registry.items():
            result_time = datetime.fromisoformat(result.creation_time.replace('Z', '+00:00').replace('+00:00', ''))
            age_hours = (current_time - result_time).total_seconds() / 3600
            
            if age_hours > max_age_hours:
                keys_to_remove.append(key)
        
        for key in keys_to_remove:
            del self.results_registry[key]
            
        if keys_to_remove:
            log.info(f"[UnifiedAnalysisManager] å¤ã„çµæœ {len(keys_to_remove)}ä»¶ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—")