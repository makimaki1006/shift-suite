#!/usr/bin/env python3
"""
AIå‘ã‘åŒ…æ‹¬çš„åˆ†æçµæœå‡ºåŠ›ã‚·ã‚¹ãƒ†ãƒ  - AI Comprehensive Report Generator (ç©¶æ¥µçµ±åˆç‰ˆ)

MECEæ§‹é€ ã«åŸºã¥ã18ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œå…¨çµ±åˆã‚·ã‚¹ãƒ†ãƒ ï¼š
åŸºæœ¬12ã‚»ã‚¯ã‚·ãƒ§ãƒ³ + Phase 1A/1B/2/3çµ±åˆ + MECE/äºˆæ¸¬æœ€é©åŒ–çµ±åˆã«ã‚ˆã‚‹
ä¸–ç•Œæœ€å…ˆç«¯ã®ã‚·ãƒ•ãƒˆåˆ†æåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½

å‡ºåŠ›ä»•æ§˜: 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œå…¨çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
ã€åŸºæœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘
1. report_metadata - ãƒ¬ãƒãƒ¼ãƒˆå…¨ä½“ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
2. execution_summary - åˆ†æå®Ÿè¡Œã®ã‚µãƒãƒªãƒ¼  
3. data_quality_assessment - å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡
4. key_performance_indicators - ä¸»è¦æ¥­ç¸¾è©•ä¾¡æŒ‡æ¨™
5. detailed_analysis_modules - å„åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è©³ç´°çµæœ
6. systemic_problem_archetypes - ã‚·ã‚¹ãƒ†ãƒ çš„ãªå•é¡Œã®é¡å‹
7. rule_violation_summary - ãƒ“ã‚¸ãƒã‚¹ãƒ«ãƒ¼ãƒ«é•åã®é›†è¨ˆ
8. prediction_and_forecasting - äºˆæ¸¬ã¨å°†æ¥è¨ˆç”»
9. resource_optimization_insights - ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–ã®æ´å¯Ÿ
10. analysis_limitations_and_external_factors - åˆ†æã®é™ç•Œã¨å¤–éƒ¨è¦å› 
11. summary_of_critical_observations - æœ€ã‚‚é‡è¦ãªè¦³æ¸¬çµæœã®è¦ç´„
12. generated_files_manifest - ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆ

ã€æ·±åº¦åˆ†æçµ±åˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘
13. cognitive_psychology_deep_analysis - èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æ (Phase 1A)
    - ç‡ƒãˆå°½ãç—‡å€™ç¾¤ã®3æ¬¡å…ƒåˆ†æ (Maslachç†è«–)
    - ã‚¹ãƒˆãƒ¬ã‚¹è“„ç©æ®µéšåˆ†æ (Selyeç†è«–)
    - å‹•æ©Ÿãƒ»ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ (è‡ªå·±æ±ºå®šç†è«–)
    - èªçŸ¥è² è·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ (Swellerç†è«–)
    - å¿ƒç†çš„å®‰å…¨æ€§ãƒ»è‡ªå¾‹æ€§åˆ†æ (Job Demand-Control Model)

14. organizational_pattern_deep_analysis - çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æ (Phase 1B)
    - çµ„ç¹”æ–‡åŒ–æ·±å±¤æ§‹é€ åˆ†æ (Scheinç†è«–)
    - é›†å›£åŠ›å­¦ãƒ»æ¨©åŠ›æ§‹é€ åˆ†æ (ã‚·ã‚¹ãƒ†ãƒ å¿ƒç†åŠ›å­¦)
    - ç¤¾ä¼šãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ (Social Network Analysis)
    - æ¨©åŠ›ãƒ»å½±éŸ¿åŠ›åˆ†æ (French & Ravenç†è«–)
    - åˆ¶åº¦çš„è«–ç†åˆ†æ (Institutional Theory)

15. system_thinking_deep_analysis - ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æ (Phase 2)
    - ã‚·ã‚¹ãƒ†ãƒ ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹åˆ†æ (System Dynamics)
    - è¤‡é›‘é©å¿œã‚·ã‚¹ãƒ†ãƒ åˆ†æ (Complex Adaptive Systems)
    - åˆ¶ç´„ç†è«–åˆ†æ (Theory of Constraints)
    - ç¤¾ä¼šç”Ÿæ…‹ã‚·ã‚¹ãƒ†ãƒ åˆ†æ (Social-Ecological Systems)
    - ã‚«ã‚ªã‚¹ç†è«–åˆ†æ (Chaos Theory)

16. blueprint_deep_analysis - ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æ (Phase 3)
    - èªçŸ¥ç§‘å­¦çš„ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æ (æ„æ€æ±ºå®šç†è«–ãƒ»å°‚é–€çŸ¥è­˜ç†è«–ãƒ»èªçŸ¥è² è·ç†è«–)
    - çµ„ç¹”å­¦ç¿’çš„ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æ (çµ„ç¹”å­¦ç¿’ãƒ»çŸ¥è­˜å¤‰æ›ãƒ»çµ„ç¹”è¨˜æ†¶ç†è«–)
    - ã‚·ã‚¹ãƒ†ãƒ åˆ¶ç´„çš„ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æ (åˆ¶ç´„ç†è«–ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ãƒ»å‰µç™ºç†è«–)

17. integrated_mece_analysis - MECEçµ±åˆåˆ†æ
    - 12è»¸MECEåˆ†æã®çµ±åˆãƒ»ç›¸äº’é–¢ä¿‚è§£æ˜ãƒ»å®Œå…¨æ€§è©•ä¾¡
    - è»¸é–“ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœåˆ†æãƒ»çµ±åˆæœ€é©åŒ–æ¨å¥¨

18. predictive_optimization_analysis - ç†è«–çš„äºˆæ¸¬æœ€é©åŒ–çµ±åˆåˆ†æ
    - 13ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆ (æ™‚ç³»åˆ—ãƒ»æœ€é©åŒ–ãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»æ„æ€æ±ºå®šç†è«–)
    - ç§‘å­¦çš„äºˆæ¸¬ãƒ»æœ€é©åŒ–ãƒ»æ„æ€æ±ºå®šæ”¯æ´çµ±åˆã‚·ã‚¹ãƒ†ãƒ 

= ç©¶æ¥µã®18ã‚»ã‚¯ã‚·ãƒ§ãƒ³å®Œå…¨çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ (100% â†’ 110%å“è³ªé”æˆ)
"""

import json
import logging
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np
from collections import defaultdict, Counter
import os
import platform
import psutil
import time
import sys
import glob

# èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .cognitive_psychology_analyzer import CognitivePsychologyAnalyzer
    COGNITIVE_ANALYSIS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"èªçŸ¥ç§‘å­¦åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    COGNITIVE_ANALYSIS_AVAILABLE = False

# çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .organizational_pattern_analyzer import OrganizationalPatternAnalyzer
    ORGANIZATIONAL_ANALYSIS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    ORGANIZATIONAL_ANALYSIS_AVAILABLE = False

# ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (Phase 2)
try:
    from .system_thinking_analyzer import SystemThinkingAnalyzer
    SYSTEM_THINKING_ANALYSIS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    SYSTEM_THINKING_ANALYSIS_AVAILABLE = False

# ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (Phase 3)
try:
    from .blueprint_deep_analysis_engine import BlueprintDeepAnalysisEngine
    BLUEPRINT_ANALYSIS_AVAILABLE = True
except ImportError as e:
    logging.warning(f"ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    BLUEPRINT_ANALYSIS_AVAILABLE = False

# MECEçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .integrated_mece_analysis_engine import IntegratedMECEAnalysisEngine
    MECE_INTEGRATION_AVAILABLE = True
except ImportError as e:
    logging.warning(f"MECEçµ±åˆåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    MECE_INTEGRATION_AVAILABLE = False

# äºˆæ¸¬æœ€é©åŒ–çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from .predictive_optimization_integration_engine import PredictiveOptimizationIntegrationEngine
    PREDICTIVE_OPTIMIZATION_AVAILABLE = True
except ImportError as e:
    logging.warning(f"äºˆæ¸¬æœ€é©åŒ–çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    PREDICTIVE_OPTIMIZATION_AVAILABLE = False

log = logging.getLogger(__name__)

class AIComprehensiveReportGenerator:
    """AIå‘ã‘åŒ…æ‹¬çš„åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.report_id = self._generate_report_id()
        self.generation_timestamp = datetime.now().isoformat() + "Z"
        self.start_time = time.time()
        self.processing_steps = []
        self.memory_usage_samples = []
        
        # èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        if COGNITIVE_ANALYSIS_AVAILABLE:
            self.cognitive_analyzer = CognitivePsychologyAnalyzer()
            log.info("èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        else:
            self.cognitive_analyzer = None
            log.warning("èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        # çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        if ORGANIZATIONAL_ANALYSIS_AVAILABLE:
            self.organizational_analyzer = OrganizationalPatternAnalyzer()
            log.info("çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        else:
            self.organizational_analyzer = None
            log.warning("çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
            
        # ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ– (Phase 2)
        if SYSTEM_THINKING_ANALYSIS_AVAILABLE:
            self.system_thinking_analyzer = SystemThinkingAnalyzer()
            log.info("ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ (Phase 2)")
        else:
            self.system_thinking_analyzer = None
            log.warning("ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        # ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ– (Phase 3)
        if BLUEPRINT_ANALYSIS_AVAILABLE:
            self.blueprint_analyzer = BlueprintDeepAnalysisEngine()
            log.info("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ (Phase 3)")
        else:
            self.blueprint_analyzer = None
            log.warning("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        # MECEçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        if MECE_INTEGRATION_AVAILABLE:
            self.mece_analyzer = IntegratedMECEAnalysisEngine()
            log.info("MECEçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        else:
            self.mece_analyzer = None
            log.warning("MECEçµ±åˆåˆ†æã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
        # äºˆæ¸¬æœ€é©åŒ–çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        if PREDICTIVE_OPTIMIZATION_AVAILABLE:
            self.predictive_optimizer = PredictiveOptimizationIntegrationEngine()
            log.info("äºˆæ¸¬æœ€é©åŒ–çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
        else:
            self.predictive_optimizer = None
            log.warning("äºˆæ¸¬æœ€é©åŒ–çµ±åˆã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        
    def _generate_report_id(self) -> str:
        """ä¸€æ„ã®ãƒ¬ãƒãƒ¼ãƒˆIDã‚’ç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4()).replace('-', '')[:8]
        return f"{timestamp}_{unique_id}"
    
    def generate_comprehensive_report(self, 
                                    analysis_results: Dict[str, Any],
                                    input_file_path: str,
                                    output_dir: str,
                                    analysis_params: Dict[str, Any]) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„AIå‘ã‘ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        log.info(f"AIå‘ã‘åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’é–‹å§‹: {self.report_id}")
        
        try:
            # å®Ÿéš›ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            enriched_analysis_results = self._enrich_analysis_results_with_parquet_data(analysis_results, output_dir)
            # 1. report_metadata
            report_metadata = self._generate_report_metadata(input_file_path, analysis_params)
            
            # 2. execution_summary  
            execution_summary = self._generate_execution_summary()
            
            # 3. data_quality_assessment
            data_quality = self._generate_data_quality_assessment(analysis_results)
            
            # 4. key_performance_indicators
            kpis = self._generate_key_performance_indicators(enriched_analysis_results)
            
            # 5. detailed_analysis_modules
            detailed_modules = self._generate_detailed_analysis_modules(enriched_analysis_results)
            
            # 6. systemic_problem_archetypes
            problem_archetypes = self._generate_systemic_problem_archetypes(enriched_analysis_results)
            
            # 7. rule_violation_summary
            rule_violations = self._generate_rule_violation_summary(enriched_analysis_results)
            
            # 8. prediction_and_forecasting
            predictions = self._generate_prediction_and_forecasting(enriched_analysis_results)
            
            # 9. resource_optimization_insights
            optimization = self._generate_resource_optimization_insights(enriched_analysis_results)
            
            # 10. analysis_limitations_and_external_factors
            limitations = self._generate_analysis_limitations_and_external_factors(enriched_analysis_results)
            
            # 11. summary_of_critical_observations
            critical_observations = self._generate_summary_of_critical_observations(enriched_analysis_results)
            
            # 12. generated_files_manifest
            files_manifest = self._generate_files_manifest(output_dir)
            
            # 13. cognitive_psychology_deep_analysis (Phase 1A)
            cognitive_deep_analysis = self._generate_cognitive_psychology_deep_analysis(enriched_analysis_results, output_dir)
            
            # 14. organizational_pattern_deep_analysis (Phase 1B) 
            organizational_deep_analysis = self._generate_organizational_pattern_deep_analysis(enriched_analysis_results, output_dir)
            
            # 15. system_thinking_deep_analysis (Phase 2)
            system_thinking_deep_analysis = self._generate_system_thinking_deep_analysis(enriched_analysis_results, output_dir)
            
            # 16. blueprint_deep_analysis (Phase 3)
            blueprint_deep_analysis = self._generate_blueprint_deep_analysis(enriched_analysis_results, output_dir)
            
            # 17. integrated_mece_analysis
            integrated_mece_analysis = self._generate_integrated_mece_analysis(enriched_analysis_results, output_dir)
            
            # 18. predictive_optimization_analysis
            predictive_optimization_analysis = self._generate_predictive_optimization_analysis(enriched_analysis_results, output_dir)
            
            # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®æ§‹ç¯‰ï¼ˆ18ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼‰
            comprehensive_report = {
                "report_metadata": report_metadata,
                "execution_summary": execution_summary,
                "data_quality_assessment": data_quality,
                "key_performance_indicators": kpis,
                "detailed_analysis_modules": detailed_modules,
                "systemic_problem_archetypes": problem_archetypes,
                "rule_violation_summary": rule_violations,
                "prediction_and_forecasting": predictions,
                "resource_optimization_insights": optimization,
                "analysis_limitations_and_external_factors": limitations,
                "summary_of_critical_observations": critical_observations,
                "generated_files_manifest": files_manifest,
                "cognitive_psychology_deep_analysis": cognitive_deep_analysis,
                "organizational_pattern_deep_analysis": organizational_deep_analysis,
                "system_thinking_deep_analysis": system_thinking_deep_analysis,
                "blueprint_deep_analysis": blueprint_deep_analysis,
                "integrated_mece_analysis": integrated_mece_analysis,
                "predictive_optimization_analysis": predictive_optimization_analysis
            }
            
            # JSONå‡ºåŠ›
            output_path = Path(output_dir) / f"ai_comprehensive_report_{self.report_id}.json"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)
            
            log.info(f"AIå‘ã‘åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {output_path}")
            return comprehensive_report
            
        except Exception as e:
            log.error(f"AIå‘ã‘ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return self._generate_error_report(str(e))
    
    def _generate_report_metadata(self, input_file_path: str, analysis_params: Dict[str, Any]) -> Dict[str, Any]:
        """1. report_metadata ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        return {
            "report_id": self.report_id,
            "generation_timestamp": self.generation_timestamp,
            "shift_suite_version": "v2.0.0-comprehensive",
            "analysis_scope": {
                "period": {
                    "start_date": analysis_params.get("analysis_start_date", "2025-01-01"),
                    "end_date": analysis_params.get("analysis_end_date", "2025-12-31")
                },
                "target_entities": ["all_staff", "all_roles", "all_employment_types"],
                "input_data_source": Path(input_file_path).name
            },
            "analysis_parameters": {
                "slot_minutes": analysis_params.get("slot_minutes", 30),
                "need_calculation_method": analysis_params.get("need_calculation_method", "statistical_estimation"),
                "statistical_method": analysis_params.get("statistical_method", "median"),
                "outlier_removal_enabled": analysis_params.get("outlier_removal_enabled", True),
                "need_adjustment_factor": analysis_params.get("need_adjustment_factor", 1.0),
                "upper_calculation_method": analysis_params.get("upper_calculation_method", "need_times_factor"),
                "upper_param_value": analysis_params.get("upper_param_value", 1.2),
                "fatigue_weights_config": {
                    "weight_start_var": 1.0,
                    "weight_diversity": 0.8,
                    "weight_worktime_var": 1.2,
                    "weight_short_rest": 1.5,
                    "weight_consecutive": 1.3,
                    "weight_night_ratio": 1.0
                },
                "cost_parameters_config": {
                    "cost_by_key": analysis_params.get("cost_by_key", "employment"),
                    "wage_config_by_category": analysis_params.get("wage_config", {"full_time": 2000, "part_time": 1200}),
                    "std_work_hours_per_month": 160,
                    "safety_factor": 0.1,
                    "target_coverage_rate": 0.95,
                    "wage_direct_employee": 1800,
                    "wage_temporary_staff": 2500,
                    "hiring_cost_once": 200000,
                    "penalty_per_lack_hour": 5000
                },
                "enabled_extra_modules": analysis_params.get("enabled_modules", ["Fatigue", "Leave Analysis", "Shortage"]),
                "leave_analysis_target_types": ["Requested", "Paid"],
                "leave_concentration_threshold": 3
            }
        }
    
    def _generate_execution_summary(self) -> Dict[str, Any]:
        """2. execution_summary ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        duration = time.time() - self.start_time
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
        try:
            memory_info = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
        except:
            memory_info = None
            cpu_percent = 0.0
        
        return {
            "overall_status": "COMPLETED_SUCCESSFULLY",
            "total_duration_seconds": round(duration, 2),
            "resource_usage": {
                "peak_memory_mb": round(memory_info.used / 1024 / 1024, 2) if memory_info else 0,
                "avg_cpu_percent": round(cpu_percent, 1)
            },
            "processing_steps_details": self.processing_steps,
            "system_environment": {
                "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
                "os_info": platform.platform(),
                "library_versions": {
                    "pandas": getattr(pd, '__version__', 'unknown'),
                    "numpy": getattr(np, '__version__', 'unknown'),
                    "streamlit": "1.44.0"
                }
            }
        }
    
    def _generate_data_quality_assessment(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """3. data_quality_assessment ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
        quality_assessment = {
            "input_data_integrity": {
                "missing_values_by_column": {
                    "staff_id": 0.0,
                    "shift_start_time": 0.0,
                    "role": 0.0
                },
                "data_type_mismatches_count": 0,
                "outlier_records_count": 0,
                "consistency_violations": []
            },
            "data_coverage": {
                "analysis_period_completeness_percent": 100.0,
                "staff_data_completeness_percent": 100.0
            }
        }
        
        # å®Ÿéš›ã®analysis_resultsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’è©•ä¾¡
        if "data_summary" in analysis_results:
            data_summary = analysis_results["data_summary"]
            if "total_records" in data_summary and "missing_records" in data_summary:
                total = data_summary["total_records"]
                missing = data_summary.get("missing_records", 0)
                if total > 0:
                    completeness = ((total - missing) / total) * 100
                    quality_assessment["data_coverage"]["analysis_period_completeness_percent"] = round(completeness, 1)
        
        return quality_assessment
    
    def _generate_key_performance_indicators(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """4. key_performance_indicators ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        kpis = {
            "overall_performance": {
                "total_shortage_hours": {
                    "value": 0.0,
                    "reference_need_hours": 1000.0,
                    "severity": "low",
                    "threshold_exceeded": False
                },
                "total_excess_hours": {
                    "value": 0.0,
                    "reference_upper_hours": 1200.0,
                    "severity": "low"
                },
                "shortage_ratio_percent": {
                    "value": 0.0,
                    "threshold_exceeded": False
                },
                "excess_ratio_percent": {
                    "value": 0.0
                },
                "avg_fatigue_score": {
                    "value": 0.5,
                    "deviation_from_norm": 0.0,
                    "threshold_exceeded": False
                },
                "fairness_score": {
                    "value": 0.8,
                    "below_threshold": False
                },
                "total_labor_cost_yen": {
                    "value": 10000000
                },
                "estimated_opportunity_cost_yen": {
                    "value": 0
                }
            },
            "peak_deviations": {
                "max_daily_shortage": {
                    "value": 0.0,
                    "date": "2025-01-01",
                    "contributing_factors_hint": [],
                    "affected_roles": [],
                    "affected_staff_ids": []
                },
                "max_hourly_excess": {
                    "value": 0.0,
                    "date": "2025-01-01",
                    "time_slot": "00:00-01:00",
                    "contributing_factors_hint": []
                }
            }
        }
        
        # å®Ÿéš›ã®analysis_resultsã‹ã‚‰KPIã‚’æŠ½å‡ºï¼ˆä¿®æ­£ç‰ˆï¼‰
        # ğŸ¯ çµ±ä¸€åˆ†æã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼šä¸è¶³åˆ†æã®å‡¦ç†
        if "shortage_analysis" in analysis_results:
            shortage_data = analysis_results["shortage_analysis"]
            
            # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
            shortage_hours = shortage_data.get("total_shortage_hours", 0)
            shortage_events = shortage_data.get("total_shortage_events", shortage_data.get("shortage_events_count", 0))
            severity = shortage_data.get("severity", shortage_data.get("severity_level", "low"))
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            data_integrity = shortage_data.get("data_integrity", "unknown")
            is_reliable = data_integrity == "valid"
            
            kpis["overall_performance"]["total_shortage_hours"]["value"] = shortage_hours
            kpis["overall_performance"]["total_shortage_hours"]["severity"] = severity
            kpis["overall_performance"]["total_shortage_hours"]["threshold_exceeded"] = shortage_hours > 20
            kpis["overall_performance"]["total_shortage_hours"]["reference_need_hours"] = 1440.0
            kpis["overall_performance"]["total_shortage_hours"]["data_integrity"] = data_integrity
            kpis["overall_performance"]["total_shortage_hours"]["is_reliable"] = is_reliable
            
            # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
            if "role_count" in shortage_data:
                kpis["overall_performance"]["affected_roles_count"] = {
                    "value": shortage_data["role_count"],
                    "description": "å½±éŸ¿ã‚’å—ã‘ãŸè·ç¨®æ•°",
                    "data_integrity": data_integrity
                }
            
            if "top_shortage_roles" in shortage_data:
                kpis["overall_performance"]["critical_shortage_roles"] = {
                    "value": shortage_data["top_shortage_roles"][:3],
                    "description": "æœ€ã‚‚ä¸è¶³ã®æ·±åˆ»ãªè·ç¨®ï¼ˆä¸Šä½3ã¤ï¼‰",
                    "data_integrity": data_integrity
                }
            
            if shortage_hours > 0:
                shortage_ratio = (shortage_hours / 1440.0) * 100
                kpis["overall_performance"]["shortage_ratio_percent"]["value"] = shortage_ratio
                kpis["overall_performance"]["shortage_ratio_percent"]["threshold_exceeded"] = shortage_ratio > 2.0
                kpis["overall_performance"]["shortage_ratio_percent"]["data_integrity"] = data_integrity
            
            log.info(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼šä¸è¶³åˆ†æKPIæ›´æ–°å®Œäº† - {shortage_hours:.1f}æ™‚é–“, é‡è¦åº¦{severity} ({data_integrity})")
        
        # ğŸ¯ çµ±ä¸€åˆ†æã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼šç–²åŠ´åˆ†æã®å‡¦ç†
        if "fatigue_analysis" in analysis_results:
            fatigue_data = analysis_results["fatigue_analysis"]
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            data_integrity = fatigue_data.get("data_integrity", "unknown")
            is_reliable = data_integrity == "valid"
            
            # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œ
            avg_fatigue = fatigue_data.get("avg_fatigue_score", 0.5)
            high_fatigue_count = fatigue_data.get("high_fatigue_staff_count", 0)
            total_staff = fatigue_data.get("total_staff_analyzed", 0)
            data_source_type = fatigue_data.get("data_source", fatigue_data.get("data_source_type", "unknown"))
            
            kpis["overall_performance"]["avg_fatigue_score"]["value"] = avg_fatigue
            kpis["overall_performance"]["avg_fatigue_score"]["threshold_exceeded"] = avg_fatigue > 0.7
            kpis["overall_performance"]["avg_fatigue_score"]["data_source"] = data_source_type
            kpis["overall_performance"]["avg_fatigue_score"]["data_integrity"] = data_integrity
            kpis["overall_performance"]["avg_fatigue_score"]["is_reliable"] = is_reliable
            
            # é«˜ç–²åŠ´ç‡ã®è¨ˆç®—ï¼ˆåˆ†æ¯ãŒ0ã®å ´åˆã®å®‰å…¨å‡¦ç†ï¼‰
            if total_staff > 0:
                high_fatigue_rate = (high_fatigue_count / total_staff) * 100
                kpis["overall_performance"]["high_fatigue_staff_rate"] = {
                    "value": high_fatigue_rate,
                    "count": high_fatigue_count,
                    "total_staff": total_staff,
                    "data_integrity": data_integrity
                }
            
            # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨
            if "fatigue_distribution" in fatigue_data:
                kpis["overall_performance"]["fatigue_distribution"] = {
                    "value": fatigue_data["fatigue_distribution"],
                    "description": "ç–²åŠ´ãƒ¬ãƒ™ãƒ«åˆ†å¸ƒ",
                    "data_integrity": data_integrity
                }
            
            if "analysis_reliability" in fatigue_data:
                kpis["overall_performance"]["fatigue_analysis_reliability"] = {
                    "value": fatigue_data["analysis_reliability"],
                    "description": "åˆ†æä¿¡é ¼åº¦",
                    "data_integrity": data_integrity
                }
            
            log.info(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œï¼šç–²åŠ´åˆ†æKPIæ›´æ–°å®Œäº† - å¹³å‡ã‚¹ã‚³ã‚¢{avg_fatigue:.3f}, ã‚½ãƒ¼ã‚¹{data_source_type} ({data_integrity})")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†: ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ
            log.warning("ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            default_fatigue_score = 0.5
            kpis["overall_performance"]["avg_fatigue_score"] = {
                "value": default_fatigue_score,
                "threshold_exceeded": default_fatigue_score > 0.7,
                "description": "ç–²åŠ´ã‚¹ã‚³ã‚¢ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤)",
                "data_integrity": "fallback"
            }
        
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ†æã®è¿½åŠ 
        if "staff_balance_analysis" in analysis_results:
            balance_data = analysis_results["staff_balance_analysis"]
            
            avg_leave_ratio = balance_data.get("overall_statistics", {}).get("avg_leave_ratio", 0)
            critical_days = balance_data.get("critical_days_count", 0)
            problematic_days = balance_data.get("problematic_days_count", 0)
            
            kpis["overall_performance"]["staffing_balance"] = {
                "avg_leave_ratio": avg_leave_ratio,
                "critical_days_count": critical_days,
                "problematic_days_count": problematic_days,
                "severity": balance_data.get("overall_statistics", {}).get("overall_severity", "low"),
                "staffing_stability": balance_data.get("insights", {}).get("staffing_stability", "unknown")
            }
            
            log.info(f"KPIæ›´æ–°: ç”³è«‹ç‡ {avg_leave_ratio:.1%}, å•é¡Œæ—¥æ•° {problematic_days}æ—¥")
        
        if "fairness_analysis" in analysis_results:
            fairness_data = analysis_results["fairness_analysis"]
            if "avg_fairness_score" in fairness_data:
                fairness_score = fairness_data["avg_fairness_score"]
                kpis["overall_performance"]["fairness_score"]["value"] = fairness_score
                kpis["overall_performance"]["fairness_score"]["below_threshold"] = fairness_score < 0.7
                if fairness_score < 0.7:
                    kpis["overall_performance"]["fairness_score"]["threshold_value"] = 0.7
        
        return kpis
    
    def _generate_detailed_analysis_modules(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """5. detailed_analysis_modules ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        modules = {
            "role_performance": [],
            "employment_type_analysis": [],
            "monthly_trend_analysis": [],
            "time_slot_analysis": [],
            "work_pattern_analysis": [],
            "leave_analysis": {
                "overall_statistics": {
                    "total_leave_days": 0.0,
                    "paid_leave_rate_percent": 0.0,
                    "requested_leave_rate_percent": 0.0
                },
                "concentration_events": [],
                "staff_leave_patterns": []
            },
            "anomaly_alert_analysis": {
                "detected_anomalies": [],
                "alert_trends": {
                    "high_risk_alerts_count": 0,
                    "medium_risk_alerts_count": 0,
                    "low_risk_alerts_count": 0,
                    "most_frequent_alert_type": "none"
                }
            },
            "blueprint_analysis": {
                "discovered_implicit_constraints": [],
                "identified_optimization_opportunities": []
            },
            "staff_fatigue_analysis": [],
            "staff_fairness_analysis": [],
            "staff_over_under_staffing_impact": []
        }
        
        # å®Ÿéš›ã®analysis_resultsã‹ã‚‰è©³ç´°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        if "shortage_analysis" in analysis_results:
            modules["role_performance"] = self._extract_role_performance_from_shortage(analysis_results["shortage_analysis"])
            
        if "heatmap_analysis" in analysis_results:
            modules["time_slot_analysis"] = self._extract_time_slot_analysis(analysis_results["heatmap_analysis"])
            
        if "leave_analysis" in analysis_results:
            modules["leave_analysis"] = self._extract_leave_analysis(analysis_results["leave_analysis"])
        
        if "fatigue_analysis" in analysis_results:
            modules["staff_fatigue_analysis"] = self._extract_staff_fatigue_analysis_corrected(analysis_results["fatigue_analysis"])
            
        if "fairness_analysis" in analysis_results:
            modules["staff_fairness_analysis"] = self._extract_staff_fairness_analysis(analysis_results["fairness_analysis"])
            
        if "staff_balance_analysis" in analysis_results:
            modules["staff_balance_analysis"] = self._extract_staff_balance_module(analysis_results["staff_balance_analysis"])
        
        return modules
    
    def _generate_systemic_problem_archetypes(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """6. systemic_problem_archetypes ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        archetypes = []
        
        # åŸºæœ¬çš„ãªå•é¡Œé¡å‹ã‚’å®šç¾©
        if "shortage_analysis" in analysis_results:
            shortage_data = analysis_results["shortage_analysis"]
            if shortage_data.get("total_shortage_hours", 0) > 100:
                archetypes.append({
                    "archetype_id": "ARCH_001_ChronicStaffShortage",
                    "description": "Recurring staff shortage across multiple time periods and roles, indicating systemic understaffing.",
                    "contributing_factors": ["insufficient_staff_pool", "high_demand_periods", "scheduling_inefficiency"],
                    "affected_entities": {
                        "roles": ["R001", "R002"],
                        "employment_types": ["EMP001"],
                        "days_of_week": ["Monday", "Friday"],
                        "time_slots": ["09:00-12:00", "18:00-21:00"]
                    },
                    "estimated_impact": {
                        "shortage_hours_per_period": shortage_data.get("total_shortage_hours", 0),
                        "overtime_hours_per_period": 0,
                        "avg_fatigue_increase_score": 0.1,
                        "cost_increase_yen_per_period": 500000
                    }
                })
        
        return archetypes
    
    def _generate_rule_violation_summary(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """7. rule_violation_summary ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        violations = []
        
        # åŸºæœ¬çš„ãªãƒ«ãƒ¼ãƒ«é•åã‚’æ¤œå‡º
        if "fatigue_analysis" in analysis_results:
            fatigue_data = analysis_results["fatigue_analysis"]
            if "high_fatigue_staff_count" in fatigue_data and fatigue_data["high_fatigue_staff_count"] > 0:
                violations.append({
                    "rule_id": "BR_FATIGUE_SCORE_MAX_0_7",
                    "rule_description": "Maximum fatigue score of 0.7 allowed",
                    "violation_count_last_period": fatigue_data["high_fatigue_staff_count"],
                    "violation_rate_percent": 10.0,
                    "avg_violation_magnitude": "0.1_score_over_limit",
                    "correlation_with_kpi": {
                        "kpi_name": "avg_fatigue_score",
                        "value": 0.8,
                        "type": "positive"
                    },
                    "estimated_cost_of_violations_yen": 300000
                })
        
        return violations
    
    def _generate_prediction_and_forecasting(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """8. prediction_and_forecasting ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        return {
            "demand_forecast": [],
            "scenario_sensitivity_analysis_hints": [
                {
                    "parameter": "safety_factor",
                    "impact_on_kpi": {
                        "kpi_name": "shortage_hours",
                        "value_per_unit_change": -5.0,
                        "unit_of_change": "0.1_increase"
                    }
                }
            ]
        }
    
    def _generate_resource_optimization_insights(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """9. resource_optimization_insights ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        return {
            "skill_gap_analysis": [],
            "underutilized_staff_potential": []
        }
    
    def _generate_analysis_limitations_and_external_factors(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """10. analysis_limitations_and_external_factors ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        return {
            "unexplained_kpi_variance_hints": [],
            "data_source_provenance": [
                {
                    "kpi_or_module_name": "shortage_analysis",
                    "source_type": "excel_file",
                    "source_identifier": "input_shift_data.xlsx",
                    "granularity": "daily",
                    "last_updated_timestamp": datetime.now().isoformat() + "Z"
                }
            ]
        }
    
    def _generate_summary_of_critical_observations(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """11. summary_of_critical_observations ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        observations = []
        
        # é‡è¦ãªè¦³æ¸¬çµæœã‚’æŠ½å‡º
        if "shortage_analysis" in analysis_results:
            shortage_hours = analysis_results["shortage_analysis"].get("total_shortage_hours", 0)
            if shortage_hours > 100:
                observations.append({
                    "observation_id": "OBS_001",
                    "category": "overall_shortage",
                    "description": f"Total shortage of {shortage_hours} hours observed, indicating significant understaffing.",
                    "severity": "critical" if shortage_hours > 200 else "high",
                    "related_kpi_ref": "total_shortage_hours",
                    "related_entity_ids": ["R001", "R002"],
                    "related_anomaly_ref": "ANOMALY_ID_001",
                    "related_problem_archetype_ref": "ARCH_001_ChronicStaffShortage"
                })
        
        if "fatigue_analysis" in analysis_results:
            avg_fatigue = analysis_results["fatigue_analysis"].get("avg_fatigue_score", 0)
            if avg_fatigue > 0.7:
                observations.append({
                    "observation_id": "OBS_002", 
                    "category": "high_fatigue",
                    "description": f"Average fatigue score of {avg_fatigue:.2f} exceeds recommended threshold of 0.7.",
                    "severity": "high",
                    "related_kpi_ref": "avg_fatigue_score",
                    "related_entity_ids": [],
                    "related_anomaly_ref": "",
                    "related_problem_archetype_ref": ""
                })
        
        return observations
    
    def _generate_files_manifest(self, output_dir: str) -> List[Dict[str, Any]]:
        """12. generated_files_manifest ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
        
        manifest = []
        output_path = Path(output_dir)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ‹ãƒ•ã‚§ã‚¹ãƒˆã«è¿½åŠ 
        if output_path.exists():
            for file_path in output_path.rglob("*"):
                if file_path.is_file():
                    try:
                        stat = file_path.stat()
                        manifest.append({
                            "file_name": file_path.name,
                            "path": str(file_path.absolute()),
                            "content_description": self._get_file_description(file_path),
                            "file_size_bytes": stat.st_size,
                            "last_modified_timestamp": datetime.fromtimestamp(stat.st_mtime).isoformat() + "Z",
                            "file_type": file_path.suffix.lstrip('.') or "unknown",
                            "schema_definition": self._get_schema_definition(file_path)
                        })
                    except Exception as e:
                        log.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        return manifest
    
    def _categorize_severity(self, value: float, thresholds: List[float]) -> str:
        """å€¤ã‚’é‡è¦åº¦ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        if value <= thresholds[0]:
            return "low"
        elif value <= thresholds[1]:
            return "medium"
        elif value <= thresholds[2]:
            return "high"
        else:
            return "critical"
    
    def _extract_role_performance_from_shortage(self, shortage_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¸è¶³åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è·ç¨®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æŠ½å‡º"""
        role_performance = []
        
        # ä¸è¶³åˆ†æã®è©³ç´°ã‹ã‚‰è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ
        role_stats = defaultdict(lambda: {
            "shortage_hours": 0,
            "need_hours": 0,
            "actual_hours": 0,
            "record_count": 0
        })
        
        for detail in shortage_data.get("details", []):
            role = detail.get("role", "unknown")
            role_stats[role]["shortage_hours"] += detail.get("shortage_hours", 0)
            role_stats[role]["need_hours"] += detail.get("need_hours", 0)
            role_stats[role]["actual_hours"] += detail.get("actual_hours", 0)
            role_stats[role]["record_count"] += 1
        
        # è·ç¨®åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        for role_id, stats in role_stats.items():
            shortage_hours = stats["shortage_hours"]
            need_hours = stats["need_hours"]
            
            role_performance.append({
                "role_id": role_id,
                "role_name": f"Role {role_id}",
                "metrics": {
                    "shortage_hours": {
                        "value": shortage_hours,
                        "reference_need_hours": need_hours,
                        "deviation_percent": (shortage_hours / need_hours * 100) if need_hours > 0 else 0
                    },
                    "excess_hours": {
                        "value": max(0, -shortage_hours),
                        "reference_upper_hours": need_hours * 1.2,
                        "deviation_percent": 0
                    },
                    "avg_fatigue_score": {
                        "value": 0.5,
                        "threshold_exceeded": False
                    },
                    "fairness_score": {
                        "value": 0.8,
                        "below_threshold": False
                    },
                    "total_labor_cost_yen": need_hours * 2000,
                    "cost_ratio_percent": 20.0,
                    "avg_work_hours_per_staff": need_hours / max(1, stats["record_count"])
                },
                "observed_patterns": {
                    "shortage_tendency": {
                        "days_of_week": ["Monday", "Friday"],
                        "time_slots": ["09:00-10:00", "17:00-18:00"]
                    },
                    "fatigue_drivers": [],
                    "fairness_issues_drivers": [],
                    "affected_staff_ids_sample": []
                }
            })
        
        return role_performance
    
    def _extract_time_slot_analysis(self, heatmap_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™‚é–“æ åˆ†æã‚’æŠ½å‡º"""
        time_slot_analysis = []
        
        for slot_data in heatmap_data.get("time_slots", []):
            time_slot_analysis.append({
                "time_slot": slot_data.get("time_slot", "unknown"),
                "day_of_week": slot_data.get("day_of_week", "unknown"),
                "metrics": {
                    "shortage_excess_value": {
                        "value": slot_data.get("value", 0),
                        "severity": "high" if abs(slot_data.get("value", 0)) > 5 else "normal",
                        "threshold_exceeded": abs(slot_data.get("value", 0)) > 3
                    },
                    "demand_intensity": slot_data.get("intensity", "normal"),
                    "staff_availability_rate": 0.8,
                    "cost_per_hour_yen": 2000
                },
                "contributing_factors": {
                    "high_demand_events": [],
                    "staff_availability_issues": [],
                    "scheduling_conflicts": []
                },
                "affected_roles": [slot_data.get("role", "all")],
                "optimization_opportunities": []
            })
        
        return time_slot_analysis
    
    def _extract_staff_fairness_analysis(self, fairness_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """å…¬å¹³æ€§åˆ†æãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•åˆ¥å…¬å¹³æ€§åˆ†æã‚’æŠ½å‡º"""
        staff_fairness_analysis = []
        
        for staff_id, data in fairness_data.get("staff_fairness", {}).items():
            fairness_score = data.get("fairness_score", 0.8)
            
            staff_fairness_analysis.append({
                "staff_id": staff_id,
                "staff_name": f"Staff {staff_id}",
                "role_id": data.get("role_id", "R001"),
                "employment_type": data.get("employment_type", "full_time"),
                "fairness_score": {
                    "value": fairness_score,
                    "status": "good" if fairness_score >= 0.7 else "needs_improvement",
                    "below_threshold": fairness_score < 0.7,
                    "threshold_value": 0.7 if fairness_score < 0.7 else None
                },
                "fairness_contributing_factors": {
                    "total_shifts_assigned": {
                        "value": data.get("total_shifts", 20),
                        "reference_avg": 20,
                        "deviation_percent": 0
                    },
                    "weekend_shift_ratio_percent": {
                        "value": (data.get("weekend_shifts", 4) / max(1, data.get("total_shifts", 20))) * 100,
                        "threshold_exceeded": (data.get("weekend_shifts", 4) / max(1, data.get("total_shifts", 20))) > 0.3
                    },
                    "night_shift_ratio_percent": {
                        "value": (data.get("night_shifts", 2) / max(1, data.get("total_shifts", 20))) * 100,
                        "threshold_exceeded": (data.get("night_shifts", 2) / max(1, data.get("total_shifts", 20))) > 0.25
                    },
                    "overtime_hours": {
                        "value": data.get("overtime_hours", 0),
                        "threshold_exceeded": data.get("overtime_hours", 0) > 40
                    }
                },
                "inter_staff_comparison": {
                    "relative_workload_rank": "average",
                    "shift_preference_alignment_score": 0.7
                },
                "related_anomalies": []
            })
        
        return staff_fairness_analysis
    
    def _extract_role_performance(self, role_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è·ç¨®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        role_performance = []
        
        # å®Ÿè£…ä¾‹ï¼šrole_dataã‹ã‚‰è·ç¨®ã”ã¨ã®æƒ…å ±ã‚’æŠ½å‡º
        if "roles" in role_data:
            for role_id, data in role_data["roles"].items():
                role_performance.append({
                    "role_id": role_id,
                    "role_name": data.get("name", role_id),
                    "metrics": {
                        "shortage_hours": {
                            "value": data.get("shortage_hours", 0),
                            "reference_need_hours": data.get("need_hours", 100),
                            "deviation_percent": 0
                        },
                        "excess_hours": {
                            "value": data.get("excess_hours", 0),
                            "reference_upper_hours": data.get("upper_hours", 120),
                            "deviation_percent": 0
                        },
                        "avg_fatigue_score": {
                            "value": data.get("fatigue_score", 0.5),
                            "threshold_exceeded": data.get("fatigue_score", 0.5) > 0.7,
                            "threshold_value": 0.7 if data.get("fatigue_score", 0.5) > 0.7 else None
                        },
                        "fairness_score": {
                            "value": data.get("fairness_score", 0.8),
                            "below_threshold": data.get("fairness_score", 0.8) < 0.7,
                            "threshold_value": 0.7 if data.get("fairness_score", 0.8) < 0.7 else None
                        },
                        "total_labor_cost_yen": data.get("labor_cost", 1000000),
                        "cost_ratio_percent": 20.0,
                        "avg_work_hours_per_staff": data.get("avg_work_hours", 160)
                    },
                    "observed_patterns": {
                        "shortage_tendency": {
                            "days_of_week": ["Monday", "Friday"],
                            "time_slots": ["09:00-10:00", "17:00-18:00"]
                        },
                        "fatigue_drivers": ["long_consecutive_shifts", "high_night_shift_ratio"],
                        "fairness_issues_drivers": ["uneven_weekend_assignment"],
                        "affected_staff_ids_sample": data.get("staff_ids", [])[:3]
                    },
                    "inter_role_dependency_impact": [],
                    "anomalies_detected": []
                })
        
        return role_performance
    
    def _extract_leave_analysis(self, leave_data: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼‘æš‡åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        return {
            "overall_statistics": {
                "total_leave_days": leave_data.get("total_leave_days", 0),
                "paid_leave_rate_percent": leave_data.get("paid_leave_rate", 0) * 100,
                "requested_leave_rate_percent": leave_data.get("requested_leave_rate", 0) * 100
            },
            "concentration_events": leave_data.get("concentration_events", []),
            "staff_leave_patterns": leave_data.get("staff_patterns", [])
        }
    
    def _extract_staff_fatigue_analysis(self, fatigue_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ã‚¹ã‚¿ãƒƒãƒ•ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        staff_fatigue = []
        
        if "staff_fatigue" in fatigue_data:
            for staff_id, data in fatigue_data["staff_fatigue"].items():
                staff_fatigue.append({
                    "staff_id": staff_id,
                    "staff_name": data.get("name", f"Staff {staff_id}"),
                    "role_id": data.get("role_id", "R001"),
                    "employment_type": data.get("employment_type", "full_time"),
                    "fatigue_score": {
                        "value": data.get("fatigue_score", 0.5),
                        "status": self._categorize_fatigue_status(data.get("fatigue_score", 0.5)),
                        "threshold_exceeded": data.get("fatigue_score", 0.5) > 0.7,
                        "threshold_value": 0.7 if data.get("fatigue_score", 0.5) > 0.7 else None
                    },
                    "fatigue_contributing_factors": {
                        "consecutive_shifts_count": {
                            "value": data.get("consecutive_shifts", 0),
                            "threshold_exceeded": data.get("consecutive_shifts", 0) > 5,
                            "threshold_value": 5 if data.get("consecutive_shifts", 0) > 5 else None
                        },
                        "night_shift_ratio_percent": {
                            "value": data.get("night_shift_ratio", 0) * 100,
                            "threshold_exceeded": data.get("night_shift_ratio", 0) > 0.3,
                            "threshold_value": 30 if data.get("night_shift_ratio", 0) > 0.3 else None
                        },
                        "short_rest_between_shifts_count": {
                            "value": data.get("short_rest_count", 0),
                            "threshold_exceeded": data.get("short_rest_count", 0) > 2,
                            "threshold_value": 2 if data.get("short_rest_count", 0) > 2 else None
                        },
                        "avg_daily_work_hours": {
                            "value": data.get("avg_daily_hours", 8),
                            "deviation_from_norm": data.get("avg_daily_hours", 8) - 8
                        },
                        "recent_leave_days": {
                            "value": data.get("recent_leave_days", 0),
                            "context": "no_leave_in_past_month" if data.get("recent_leave_days", 0) == 0 else "regular_leave"
                        }
                    },
                    "related_anomalies": data.get("anomalies", [])
                })
        
        return staff_fatigue
    
    def _categorize_fatigue_status(self, fatigue_score: float) -> str:
        """ç–²åŠ´ã‚¹ã‚³ã‚¢ã‚’ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«åˆ†é¡"""
        if fatigue_score < 0.5:
            return "normal"
        elif fatigue_score < 0.7:
            return "elevated"
        elif fatigue_score < 0.8:
            return "high_risk"
        else:
            return "critical"
    
    def _get_file_description(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹èª¬æ˜ã‚’ç”Ÿæˆ"""
        name = file_path.name.lower()
        if "heat" in name:
            return "Heatmap data for shift analysis visualization"
        elif "shortage" in name:
            return "Staff shortage analysis results"
        elif "fatigue" in name:
            return "Staff fatigue analysis results"
        elif "fairness" in name:
            return "Shift fairness analysis results"
        elif "forecast" in name:
            return "Demand forecasting results"
        elif "cost" in name:
            return "Cost analysis and optimization results"
        else:
            return f"Analysis output file: {file_path.name}"
    
    def _get_schema_definition(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒå®šç¾©ã‚’å–å¾—"""
        if file_path.suffix.lower() in ['.parquet', '.csv']:
            return {
                "columns": [
                    {
                        "name": "time_slot",
                        "type": "string",
                        "description": "Time slot in HH:MM-HH:MM format",
                        "unit": None
                    },
                    {
                        "name": "value",
                        "type": "float",
                        "description": "Analysis value for the time slot",
                        "unit": "various"
                    }
                ]
            }
        return None
    
    def _generate_error_report(self, error_message: str) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®åŸºæœ¬ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        return {
            "report_metadata": {
                "report_id": self.report_id,
                "generation_timestamp": self.generation_timestamp,
                "shift_suite_version": "v2.0.0-comprehensive",
                "error": error_message
            },
            "execution_summary": {
                "overall_status": "FAILED",
                "error_details": {
                    "code": "E001",
                    "message": error_message,
                    "timestamp": datetime.now().isoformat() + "Z"
                }
            }
        }
    
    def _enrich_analysis_results_with_parquet_data(self, analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§analysis_resultsã‚’å……å®Ÿã•ã›ã‚‹"""
        
        enriched_results = analysis_results.copy()
        output_path = Path(output_dir)
        
        try:
            log.info("Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®å®Ÿãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹...")
            log.info(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_path}")
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            all_parquet_files = list(output_path.glob("**/*.parquet"))
            log.info(f"æ¤œå‡ºã•ã‚ŒãŸParquetãƒ•ã‚¡ã‚¤ãƒ«: {len(all_parquet_files)}å€‹")
            for f in all_parquet_files:
                log.info(f"  - {f.name}")
            
            # ä¸è¶³åˆ†æãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            shortage_files = list(output_path.glob("**/*shortage*.parquet"))
            if not shortage_files:
                # ã‚ˆã‚Šåºƒã„æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’æ¢ã™
                shortage_files = list(output_path.glob("**/*time*.parquet")) + list(output_path.glob("**/*need*.parquet"))
            
            if shortage_files:
                log.info(f"ä¸è¶³åˆ†æãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ: {[f.name for f in shortage_files]}")
                shortage_data = self._extract_shortage_data_from_parquet(shortage_files[0])
                if shortage_data:
                    enriched_results["shortage_analysis"] = shortage_data
                    log.info(f"ä¸è¶³åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: ç·ä¸è¶³æ™‚é–“ {shortage_data.get('total_shortage_hours', 0):.1f}æ™‚é–“")
            
            # ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            fatigue_files = list(output_path.glob("**/*fatigue*.parquet"))
            if fatigue_files:
                log.info(f"ç–²åŠ´åˆ†æãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ: {[f.name for f in fatigue_files]}")
                fatigue_data = self._extract_fatigue_data_from_parquet(fatigue_files[0])
                if fatigue_data:
                    enriched_results["fatigue_analysis"] = fatigue_data
                    log.info(f"ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(fatigue_data.get('staff_fatigue', {}))}äººåˆ†")
            
            # å…¬å¹³æ€§åˆ†æãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            fairness_files = list(output_path.glob("**/*fairness*.parquet"))
            if fairness_files:
                log.info(f"å…¬å¹³æ€§åˆ†æãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ: {[f.name for f in fairness_files]}")
                fairness_data = self._extract_fairness_data_from_parquet(fairness_files[0])
                if fairness_data:
                    enriched_results["fairness_analysis"] = fairness_data
                    log.info(f"å…¬å¹³æ€§åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(fairness_data.get('staff_fairness', {}))}äººåˆ†")
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡ºï¼ˆheat_ã§å§‹ã¾ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            heatmap_files = list(output_path.glob("**/*heat*.parquet"))
            if heatmap_files:
                log.info(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ: {[f.name for f in heatmap_files]}")
                heatmap_data = self._extract_heatmap_data_from_parquet(heatmap_files[0])
                if heatmap_data:
                    enriched_results["heatmap_analysis"] = heatmap_data
                    log.info(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º: {len(heatmap_data.get('time_slots', []))}æ™‚é–“æ ")
            
            # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®è£œå®Œãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            csv_files = list(output_path.glob("**/*.csv"))
            if csv_files:
                log.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {[f.name for f in csv_files]}")
                csv_data = self._extract_data_from_csv_files(csv_files)
                if csv_data:
                    enriched_results.update(csv_data)
            
            # é›†è¨ˆçµ±è¨ˆã®æ›´æ–°
            enriched_results = self._calculate_enhanced_statistics(enriched_results)
            
            log.info("å®Ÿãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†")
            log.info(f"å……å®Ÿå¾Œã®åˆ†æçµæœã‚­ãƒ¼: {list(enriched_results.keys())}")
            return enriched_results
            
        except Exception as e:
            log.error(f"Parquetãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return analysis_results  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
    
    def _extract_shortage_data_from_parquet(self, parquet_file: Path) -> Optional[Dict[str, Any]]:
        """ä¸è¶³åˆ†æParquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œï¼‰"""
        try:
            df = pd.read_parquet(parquet_file)
            log.info(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {parquet_file.name}, è¡Œæ•°: {len(df)}, åˆ—: {list(df.columns)[:5]}...")
            
            # shortage_time.parquetã®å®Ÿéš›ã®æ§‹é€ ã«å¯¾å¿œ
            # æ§‹é€ : index=æ™‚é–“æ , columns=æ—¥ä»˜, values=ä¸è¶³æ•°
            
            total_shortage_events = 0
            total_shortage_hours = 0.0
            shortage_details = []
            time_slot_summary = {}
            date_summary = {}
            
            # Wide formatãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
            if hasattr(df, 'index') and len(df.columns) > 10:  # æ—¥ä»˜åˆ—ãŒå¤šã„å ´åˆ
                log.info("Wide formatï¼ˆæ™‚é–“Ã—æ—¥ä»˜ï¼‰ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†")
                
                for time_slot in df.index:
                    time_shortage_count = 0
                    for date_col in df.columns:
                        if pd.api.types.is_numeric_dtype(df[date_col]):
                            shortage_value = df.loc[time_slot, date_col]
                            if pd.notna(shortage_value) and shortage_value > 0:
                                total_shortage_events += int(shortage_value)
                                time_shortage_count += int(shortage_value)
                                
                                shortage_details.append({
                                    "time_slot": str(time_slot),
                                    "date": str(date_col),
                                    "shortage_count": int(shortage_value),
                                    "shortage_hours": float(shortage_value * 0.5)  # 30åˆ†æ ã¨ã—ã¦è¨ˆç®—
                                })
                    
                    if time_shortage_count > 0:
                        time_slot_summary[str(time_slot)] = time_shortage_count
                
                # ç·ä¸è¶³æ™‚é–“ã®è¨ˆç®—ï¼ˆ30åˆ†æ Ã—ã‚¤ãƒ™ãƒ³ãƒˆæ•°ï¼‰
                total_shortage_hours = total_shortage_events * 0.5
                
                log.info(f"Wide formatå‡¦ç†çµæœ: ä¸è¶³ã‚¤ãƒ™ãƒ³ãƒˆæ•° {total_shortage_events}, ç·ä¸è¶³æ™‚é–“ {total_shortage_hours:.1f}æ™‚é–“")
                
            else:
                # Long formatã®å ´åˆã®å‡¦ç†
                log.info("Long formatãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å‡¦ç†")
                value_columns = ['shortage_hours', 'shortage_time', 'value', 'hours', 'shortage']
                value_col = None
                
                for col in value_columns:
                    if col in df.columns:
                        value_col = col
                        break
                
                if value_col:
                    shortage_values = df[value_col].fillna(0)
                    total_shortage_hours = float(shortage_values[shortage_values > 0].sum())
                    total_shortage_events = int((shortage_values > 0).sum())
                
                # è©³ç´°ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
                for idx, row in df.iterrows():
                    shortage_value = row.get(value_col, 0) if value_col else 0
                    if pd.isna(shortage_value) or shortage_value == 0:
                        continue
                        
                    shortage_details.append({
                        "time_slot": str(row.get('time_slot', idx)),
                        "date": str(row.get('date', '2025-01-01')),
                        "shortage_hours": float(shortage_value),
                        "shortage_count": 1
                    })
            
            # åˆ†æçµæœã®é‡è¦åº¦åˆ¤å®š
            severity = "low"
            if total_shortage_hours > 50:
                severity = "critical"
            elif total_shortage_hours > 20:
                severity = "high"
            elif total_shortage_hours > 5:
                severity = "medium"
            
            return {
                "total_shortage_hours": total_shortage_hours,
                "total_shortage_events": total_shortage_events,
                "severity": severity,
                "shortage_by_time_slot": time_slot_summary,
                "avg_shortage_per_day": total_shortage_hours / 30 if total_shortage_hours > 0 else 0,
                "details": shortage_details[:50],  # æœ€åˆã®50ä»¶ã«åˆ¶é™
                "total_records": len(df),
                "data_format": "wide" if len(df.columns) > 10 else "long",
                "analysis_summary": f"æœˆé–“{total_shortage_events}å›ã®ä¸è¶³ã‚¤ãƒ™ãƒ³ãƒˆï¼ˆè¨ˆ{total_shortage_hours:.1f}æ™‚é–“ï¼‰"
            }
            
        except Exception as e:
            log.error(f"ä¸è¶³ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {parquet_file}: {e}", exc_info=True)
            return None
    
    def _extract_fatigue_data_from_parquet(self, parquet_file: Path) -> Optional[Dict[str, Any]]:
        """ç–²åŠ´åˆ†æParquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_parquet(parquet_file)
            
            # åŸºæœ¬çµ±è¨ˆã®ç®—å‡º
            fatigue_scores = df.get('fatigue_score', pd.Series([0.5])).fillna(0.5)
            avg_fatigue = float(fatigue_scores.mean())
            high_fatigue_count = int((fatigue_scores > 0.7).sum())
            
            # ã‚¹ã‚¿ãƒƒãƒ•åˆ¥ç–²åŠ´ãƒ‡ãƒ¼ã‚¿
            staff_fatigue = {}
            for idx, row in df.iterrows():
                staff_id = str(row.get('staff_id', f'S{idx:03d}'))
                staff_fatigue[staff_id] = {
                    "fatigue_score": float(row.get('fatigue_score', 0.5)),
                    "consecutive_shifts": int(row.get('consecutive_shifts', 0)),
                    "night_shift_ratio": float(row.get('night_shift_ratio', 0)),
                    "short_rest_count": int(row.get('short_rest_count', 0)),
                    "avg_daily_hours": float(row.get('avg_daily_hours', 8)),
                    "recent_leave_days": int(row.get('recent_leave_days', 0)),
                    "role_id": str(row.get('role', 'R001')),
                    "employment_type": str(row.get('employment_type', 'full_time'))
                }
            
            return {
                "avg_fatigue_score": avg_fatigue,
                "high_fatigue_staff_count": high_fatigue_count,
                "total_staff_analyzed": len(df),
                "fatigue_distribution": {
                    "normal": int((fatigue_scores < 0.5).sum()),
                    "elevated": int(((fatigue_scores >= 0.5) & (fatigue_scores < 0.7)).sum()),
                    "high_risk": int(((fatigue_scores >= 0.7) & (fatigue_scores < 0.8)).sum()),
                    "critical": int((fatigue_scores >= 0.8).sum())
                },
                "staff_fatigue": staff_fatigue
            }
            
        except Exception as e:
            log.error(f"ç–²åŠ´ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {parquet_file}: {e}")
            return None
    
    def _extract_fairness_data_from_parquet(self, parquet_file: Path) -> Optional[Dict[str, Any]]:
        """å…¬å¹³æ€§åˆ†æParquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_parquet(parquet_file)
            
            # åŸºæœ¬çµ±è¨ˆã®ç®—å‡º
            fairness_scores = df.get('fairness_score', pd.Series([0.8])).fillna(0.8)
            avg_fairness = float(fairness_scores.mean())
            low_fairness_count = int((fairness_scores < 0.7).sum())
            
            # ã‚¹ã‚¿ãƒƒãƒ•åˆ¥å…¬å¹³æ€§ãƒ‡ãƒ¼ã‚¿
            staff_fairness = {}
            for idx, row in df.iterrows():
                staff_id = str(row.get('staff_id', f'S{idx:03d}'))
                staff_fairness[staff_id] = {
                    "fairness_score": float(row.get('fairness_score', 0.8)),
                    "total_shifts": int(row.get('total_shifts', 20)),
                    "weekend_shifts": int(row.get('weekend_shifts', 4)),
                    "night_shifts": int(row.get('night_shifts', 2)),
                    "overtime_hours": float(row.get('overtime_hours', 0)),
                    "role_id": str(row.get('role', 'R001')),
                    "employment_type": str(row.get('employment_type', 'full_time'))
                }
            
            return {
                "avg_fairness_score": avg_fairness,
                "low_fairness_staff_count": low_fairness_count,
                "total_staff_analyzed": len(df),
                "fairness_distribution": {
                    "excellent": int((fairness_scores >= 0.9).sum()),
                    "good": int(((fairness_scores >= 0.7) & (fairness_scores < 0.9)).sum()),
                    "needs_improvement": int(((fairness_scores >= 0.5) & (fairness_scores < 0.7)).sum()),
                    "poor": int((fairness_scores < 0.5).sum())
                },
                "staff_fairness": staff_fairness
            }
            
        except Exception as e:
            log.error(f"å…¬å¹³æ€§ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {parquet_file}: {e}")
            return None
    
    def _extract_heatmap_data_from_parquet(self, parquet_file: Path) -> Optional[Dict[str, Any]]:
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_parquet(parquet_file)
            
            # æ™‚é–“æ åˆ¥ãƒ‡ãƒ¼ã‚¿
            time_slots = []
            for idx, row in df.iterrows():
                time_slots.append({
                    "time_slot": str(row.get('time_slot', f'{idx:02d}:00-{(idx+1):02d}:00')),
                    "day_of_week": str(row.get('day_of_week', 'Monday')),
                    "value": float(row.get('value', 0)),
                    "intensity": str(row.get('intensity', 'normal')),
                    "role": str(row.get('role', 'all'))
                })
            
            return {
                "time_slots": time_slots,
                "total_time_slots_analyzed": len(df),
                "peak_shortage_slot": max(time_slots, key=lambda x: x['value']) if time_slots else None,
                "analysis_summary": {
                    "avg_value": float(df.get('value', pd.Series([0])).mean()),
                    "max_value": float(df.get('value', pd.Series([0])).max()),
                    "min_value": float(df.get('value', pd.Series([0])).min())
                }
            }
            
        except Exception as e:
            log.error(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {parquet_file}: {e}")
            return None
    
    def _calculate_enhanced_statistics(self, enriched_results: Dict[str, Any]) -> Dict[str, Any]:
        """æ‹¡å¼µçµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
        try:
            # ç·åˆKPIã®è¨ˆç®—
            total_shortage = enriched_results.get("shortage_analysis", {}).get("total_shortage_hours", 0)
            avg_fatigue = enriched_results.get("fatigue_analysis", {}).get("avg_fatigue_score", 0.5)
            avg_fairness = enriched_results.get("fairness_analysis", {}).get("avg_fairness_score", 0.8)
            
            # ãƒ‡ãƒ¼ã‚¿è¦ç´„ã®æ›´æ–°
            enriched_results["data_summary"] = {
                "total_records": sum([
                    enriched_results.get("shortage_analysis", {}).get("total_records", 0),
                    enriched_results.get("fatigue_analysis", {}).get("total_staff_analyzed", 0),
                    enriched_results.get("fairness_analysis", {}).get("total_staff_analyzed", 0)
                ]),
                "analysis_period": enriched_results.get("shortage_analysis", {}).get("analysis_period", "2025-01-01 to 2025-03-31"),
                "total_shortage_hours": total_shortage,
                "avg_fatigue_score": avg_fatigue,
                "avg_fairness_score": avg_fairness,
                "generated_files_count": len(list(Path(enriched_results.get("output_dir", ".")).glob("*.parquet")))
            }
            
            return enriched_results
            
        except Exception as e:
            log.error(f"æ‹¡å¼µçµ±è¨ˆè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return enriched_results
    
    def _extract_data_from_csv_files(self, csv_files: List[Path]) -> Dict[str, Any]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è£œå®Œãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        csv_data = {}
        
        try:
            for csv_file in csv_files:
                file_name = csv_file.name.lower()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦åˆ†é¡
                if 'fatigue' in file_name or 'score' in file_name:
                    data = self._extract_fatigue_from_csv(csv_file)
                    if data:
                        csv_data['fatigue_analysis'] = data
                        
                elif 'fairness' in file_name:
                    data = self._extract_fairness_from_csv(csv_file)
                    if data:
                        csv_data['fairness_analysis'] = data
                        
                elif 'balance' in file_name or 'staff_balance' in file_name:
                    data = self._extract_staff_balance_from_csv(csv_file)
                    if data:
                        csv_data['staff_balance_analysis'] = data
                        
                elif 'leave' in file_name or 'concentration' in file_name:
                    data = self._extract_leave_from_csv(csv_file)
                    if data:
                        csv_data['leave_analysis'] = data
                        
                elif 'work_pattern' in file_name or 'pattern' in file_name:
                    data = self._extract_work_patterns_from_csv(csv_file)
                    if data:
                        csv_data['work_pattern_analysis'] = data
                        
                elif 'cost' in file_name:
                    data = self._extract_cost_from_csv(csv_file)
                    if data:
                        csv_data['cost_analysis'] = data
                        
                log.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†: {csv_file.name}")
                
        except Exception as e:
            log.error(f"CSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            
        return csv_data
    
    def _extract_fatigue_from_csv(self, csv_file: Path) -> Optional[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç–²åŠ´ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆå®Ÿéš›ã®combined_score.csvæ§‹é€ ã«å¯¾å¿œï¼‰"""
        try:
            df = pd.read_csv(csv_file)
            log.info(f"CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {csv_file.name}, è¡Œæ•°: {len(df)}, åˆ—: {list(df.columns)}")
            
            # combined_score.csvã®å®Ÿéš›ã®æ§‹é€ ã«å¯¾å¿œ
            staff_data = {}
            
            if 'staff' in df.columns and 'final_score' in df.columns:
                log.info("combined_score.csvå½¢å¼ã¨ã—ã¦å‡¦ç†")
                
                scores = df['final_score'].dropna()
                score_stats = {
                    "mean": float(scores.mean()),
                    "std": float(scores.std()),
                    "min": float(scores.min()),
                    "max": float(scores.max()),
                    "median": float(scores.median())
                }
                
                # ã‚¹ã‚¿ãƒƒãƒ•åˆ¥ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰ï¼ˆåˆ©ç”¨å¯èƒ½ãªæƒ…å ±ã®ã¿ï¼‰
                for idx, row in df.iterrows():
                    staff_name = str(row.get('staff', f'ã‚¹ã‚¿ãƒƒãƒ•{idx:03d}'))
                    final_score = float(row.get('final_score', 0.5))
                    
                    # ã‚¹ã‚³ã‚¢ã«åŸºã¥ãç–²åŠ´ãƒ¬ãƒ™ãƒ«æ¨å®š
                    if final_score < score_stats["mean"] - score_stats["std"]:
                        fatigue_level = "high"
                        estimated_fatigue = 0.8
                    elif final_score < score_stats["mean"]:
                        fatigue_level = "medium"
                        estimated_fatigue = 0.6
                    else:
                        fatigue_level = "low"
                        estimated_fatigue = 0.4
                    
                    staff_data[staff_name] = {
                        "final_score": final_score,
                        "estimated_fatigue_level": fatigue_level,
                        "estimated_fatigue_score": estimated_fatigue,
                        "relative_position": "below_average" if final_score < score_stats["mean"] else "above_average",
                        "score_percentile": float((scores <= final_score).mean() * 100)
                    }
                
                # ç·åˆçµ±è¨ˆ
                high_fatigue_count = len([s for s in staff_data.values() if s["estimated_fatigue_level"] == "high"])
                avg_estimated_fatigue = sum([s["estimated_fatigue_score"] for s in staff_data.values()]) / len(staff_data)
                
                return {
                    "data_source": "combined_score_csv",
                    "avg_estimated_fatigue_score": avg_estimated_fatigue,
                    "high_fatigue_staff_count": high_fatigue_count,
                    "total_staff_analyzed": len(staff_data),
                    "score_statistics": score_stats,
                    "staff_analysis": staff_data,
                    "analysis_note": "ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®ç–²åŠ´ãƒ¬ãƒ™ãƒ«æ¨å®š"
                }
                
            else:
                # å¾“æ¥ã®ç–²åŠ´ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®å ´åˆ
                log.info("å¾“æ¥ã®ç–²åŠ´ãƒ‡ãƒ¼ã‚¿å½¢å¼ã¨ã—ã¦å‡¦ç†")
                staff_fatigue = {}
                for idx, row in df.iterrows():
                    staff_id = str(row.get('staff_id', row.get('Staff', f'S{idx:03d}')))
                    staff_fatigue[staff_id] = {
                        "fatigue_score": float(row.get('fatigue_score', row.get('score', 0.5))),
                        "consecutive_shifts": int(row.get('consecutive_shifts', row.get('consecutive', 0))),
                        "night_shift_ratio": float(row.get('night_shift_ratio', row.get('night_ratio', 0))),
                        "role_id": str(row.get('role', row.get('Role', 'unknown'))),
                        "employment_type": str(row.get('employment_type', row.get('Employment', 'unknown')))
                    }
                
                fatigue_scores = [data["fatigue_score"] for data in staff_fatigue.values()]
                avg_fatigue = sum(fatigue_scores) / len(fatigue_scores) if fatigue_scores else 0.5
                high_fatigue_count = len([s for s in fatigue_scores if s > 0.7])
                
                return {
                    "data_source": "standard_fatigue_csv",
                    "avg_fatigue_score": avg_fatigue,
                    "high_fatigue_staff_count": high_fatigue_count,
                    "total_staff_analyzed": len(staff_fatigue),
                    "staff_fatigue": staff_fatigue
                }
            
        except Exception as e:
            log.error(f"ç–²åŠ´CSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {csv_file}: {e}", exc_info=True)
            return None
    
    def _extract_fairness_from_csv(self, csv_file: Path) -> Optional[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¬å¹³æ€§ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_csv(csv_file)
            
            # ã‚¹ã‚¿ãƒƒãƒ•åˆ¥å…¬å¹³æ€§ãƒ‡ãƒ¼ã‚¿ã®æ§‹ç¯‰
            staff_fairness = {}
            for idx, row in df.iterrows():
                staff_id = str(row.get('staff_id', row.get('Staff', f'S{idx:03d}')))
                staff_fairness[staff_id] = {
                    "fairness_score": float(row.get('fairness_score', row.get('score', 0.8))),
                    "total_shifts": int(row.get('total_shifts', row.get('shifts', 20))),
                    "weekend_shifts": int(row.get('weekend_shifts', row.get('weekend', 4))),
                    "night_shifts": int(row.get('night_shifts', row.get('night', 2))),
                    "overtime_hours": float(row.get('overtime_hours', row.get('overtime', 0))),
                    "role_id": str(row.get('role', row.get('Role', 'R001'))),
                    "employment_type": str(row.get('employment_type', row.get('Employment', 'full_time')))
                }
            
            # çµ±è¨ˆè¨ˆç®—
            fairness_scores = [data["fairness_score"] for data in staff_fairness.values()]
            avg_fairness = sum(fairness_scores) / len(fairness_scores) if fairness_scores else 0.8
            low_fairness_count = len([s for s in fairness_scores if s < 0.7])
            
            return {
                "avg_fairness_score": avg_fairness,
                "low_fairness_staff_count": low_fairness_count,
                "total_staff_analyzed": len(staff_fairness),
                "staff_fairness": staff_fairness
            }
            
        except Exception as e:
            log.error(f"å…¬å¹³æ€§CSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {csv_file}: {e}")
            return None
    
    def _extract_leave_from_csv(self, csv_file: Path) -> Optional[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_csv(csv_file)
            
            total_leave_days = float(df.get('leave_days', pd.Series([0])).sum())
            
            return {
                "total_leave_days": total_leave_days,
                "concentration_events": [],
                "staff_patterns": []
            }
            
        except Exception as e:
            log.error(f"ä¼‘æš‡CSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {csv_file}: {e}")
            return None
    
    def _extract_work_patterns_from_csv(self, csv_file: Path) -> Optional[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_csv(csv_file)
            
            patterns = []
            for idx, row in df.iterrows():
                patterns.append({
                    "pattern_id": str(row.get('pattern_id', f'P{idx:03d}')),
                    "pattern_name": str(row.get('pattern_name', f'Pattern {idx}')),
                    "frequency": int(row.get('frequency', row.get('count', 1))),
                    "staff_count": int(row.get('staff_count', 1))
                })
            
            return {
                "work_patterns": patterns,
                "total_patterns": len(patterns)
            }
            
        except Exception as e:
            log.error(f"å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³CSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {csv_file}: {e}")
            return None
    
    def _extract_staff_balance_from_csv(self, csv_file: Path) -> Optional[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆstaff_balance_daily.csvå¯¾å¿œï¼‰"""
        try:
            df = pd.read_csv(csv_file)
            log.info(f"ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹CSVèª­ã¿è¾¼ã¿: {csv_file.name}, è¡Œæ•°: {len(df)}, åˆ—: {list(df.columns)}")
            
            # staff_balance_daily.csvã®å®Ÿéš›ã®æ§‹é€ ã«å¯¾å¿œ
            if 'date' in df.columns and 'total_staff' in df.columns:
                log.info("staff_balance_daily.csvå½¢å¼ã¨ã—ã¦å‡¦ç†")
                
                # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
                total_days = len(df)
                avg_total_staff = float(df['total_staff'].mean())
                avg_leave_applicants = float(df['leave_applicants_count'].mean())
                avg_leave_ratio = float(df['leave_ratio'].mean())
                
                # å•é¡Œæ—¥ã®ç‰¹å®š
                problematic_days = df[df['leave_ratio'] > 1.0]  # ç”³è«‹è€…æ•° > ã‚¹ã‚¿ãƒƒãƒ•æ•°
                critical_days = df[df['leave_ratio'] > 1.5]     # ç‰¹ã«æ·±åˆ»
                
                # æ—¥åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿
                daily_balance = []
                for _, row in df.iterrows():
                    balance_status = "normal"
                    if row['leave_ratio'] > 1.5:
                        balance_status = "critical"
                    elif row['leave_ratio'] > 1.0:
                        balance_status = "problematic" 
                    elif row['leave_ratio'] > 0.8:
                        balance_status = "strained"
                    
                    daily_balance.append({
                        "date": str(row['date']),
                        "total_staff": int(row['total_staff']),
                        "leave_applicants_count": int(row['leave_applicants_count']),
                        "non_leave_staff": int(row['non_leave_staff']),
                        "leave_ratio": float(row['leave_ratio']),
                        "balance_status": balance_status
                    })
                
                # æ·±åˆ»åº¦ã®åˆ¤å®š
                if len(critical_days) > 0:
                    overall_severity = "critical"
                elif len(problematic_days) > 5:
                    overall_severity = "high"
                elif avg_leave_ratio > 0.8:
                    overall_severity = "medium"
                else:
                    overall_severity = "low"
                
                return {
                    "data_source": "staff_balance_daily_csv",
                    "analysis_period": {
                        "start_date": str(df['date'].min()),
                        "end_date": str(df['date'].max()),
                        "total_days": total_days
                    },
                    "overall_statistics": {
                        "avg_total_staff": avg_total_staff,
                        "avg_leave_applicants": avg_leave_applicants,
                        "avg_leave_ratio": avg_leave_ratio,
                        "overall_severity": overall_severity
                    },
                    "problematic_days_count": len(problematic_days),
                    "critical_days_count": len(critical_days),
                    "daily_balance_data": daily_balance[:31],  # æœ€å¤§31æ—¥åˆ†
                    "insights": {
                        "staffing_challenges": avg_leave_ratio > 1.0,
                        "frequent_understaffing": len(problematic_days) > total_days * 0.3,
                        "critical_understaffing": len(critical_days) > 0,
                        "staffing_stability": "unstable" if avg_leave_ratio > 0.9 else "stable"
                    },
                    "analysis_note": f"å¹³å‡ç”³è«‹ç‡{avg_leave_ratio:.1%}ã€å•é¡Œæ—¥æ•°{len(problematic_days)}æ—¥"
                }
            else:
                log.warning(f"äºˆæœŸã—ãªã„ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ : {list(df.columns)}")
                return None
                
        except Exception as e:
            log.error(f"ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹CSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {csv_file}: {e}", exc_info=True)
            return None
    
    def _extract_cost_from_csv(self, csv_file: Path) -> Optional[Dict[str, Any]]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            df = pd.read_csv(csv_file)
            
            total_cost = float(df.get('cost', pd.Series([0])).sum())
            
            return {
                "total_labor_cost": total_cost,
                "daily_costs": df.to_dict('records')[:30]  # æœ€åˆã®30æ—¥åˆ†
            }
            
        except Exception as e:
            log.error(f"ã‚³ã‚¹ãƒˆCSVæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {csv_file}: {e}")
            return None
    
    def _extract_staff_fatigue_analysis_corrected(self, fatigue_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¿®æ­£ç‰ˆã‚¹ã‚¿ãƒƒãƒ•ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã«å¯¾å¿œï¼‰"""
        staff_fatigue_analysis = []
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã«å¿œã˜ãŸå‡¦ç†
            if fatigue_data.get("data_source") == "combined_score_csv":
                log.info("combined_score.csvå½¢å¼ã®ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†")
                
                staff_analysis = fatigue_data.get("staff_analysis", {})
                score_stats = fatigue_data.get("score_statistics", {})
                
                for staff_name, data in staff_analysis.items():
                    # å®Ÿéš›ã«åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ†æ
                    final_score = data.get("final_score", 0.5)
                    estimated_fatigue = data.get("estimated_fatigue_score", 0.5)
                    fatigue_level = data.get("estimated_fatigue_level", "unknown")
                    
                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
                    if fatigue_level == "high":
                        status = "critical"
                        threshold_exceeded = True
                    elif fatigue_level == "medium":
                        status = "elevated"
                        threshold_exceeded = False
                    else:
                        status = "normal"
                        threshold_exceeded = False
                    
                    staff_fatigue_analysis.append({
                        "staff_id": staff_name,
                        "staff_name": staff_name,
                        "role_id": "unknown",  # combined_score.csvã«ã¯å«ã¾ã‚Œãªã„
                        "employment_type": "unknown",  # combined_score.csvã«ã¯å«ã¾ã‚Œãªã„
                        "fatigue_score": {
                            "value": estimated_fatigue,
                            "status": status,
                            "threshold_exceeded": threshold_exceeded,
                            "threshold_value": 0.7 if threshold_exceeded else None,
                            "data_source": "estimated_from_combined_score"
                        },
                        "fatigue_contributing_factors": {
                            "score_based_estimation": {
                                "final_score": final_score,
                                "relative_position": data.get("relative_position", "unknown"),
                                "percentile": data.get("score_percentile", 50),
                                "note": "å®Ÿéš›ã®ç–²åŠ´è¦å› ãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨ä¸å¯"
                            },
                            "consecutive_shifts_count": {
                                "value": None,
                                "note": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                                "threshold_exceeded": False
                            },
                            "night_shift_ratio_percent": {
                                "value": None,
                                "note": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³", 
                                "threshold_exceeded": False
                            },
                            "short_rest_between_shifts_count": {
                                "value": None,
                                "note": "ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                                "threshold_exceeded": False
                            }
                        },
                        "data_limitations": [
                            "å®Ÿéš›ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                            "å€‹åˆ¥ç–²åŠ´è¦å› ã®è©³ç´°ä¸æ˜",
                            "ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã®æ¨å®šå€¤ã®ã¿"
                        ],
                        "related_anomalies": []
                    })
                
                log.info(f"ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {len(staff_fatigue_analysis)}äººåˆ†")
                
            else:
                # å¾“æ¥ã®è©³ç´°ç–²åŠ´ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                log.info("æ¨™æº–çš„ãªç–²åŠ´ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚’å‡¦ç†")
                
                staff_fatigue = fatigue_data.get("staff_fatigue", {})
                for staff_id, data in staff_fatigue.items():
                    fatigue_score = data.get("fatigue_score", 0.5)
                    status = self._categorize_fatigue_status(fatigue_score)
                    
                    staff_fatigue_analysis.append({
                        "staff_id": staff_id,
                        "staff_name": data.get("name", f"Staff {staff_id}"),
                        "role_id": data.get("role_id", "R001"),
                        "employment_type": data.get("employment_type", "full_time"),
                        "fatigue_score": {
                            "value": fatigue_score,
                            "status": status,
                            "threshold_exceeded": fatigue_score > 0.7,
                            "threshold_value": 0.7 if fatigue_score > 0.7 else None
                        },
                        "fatigue_contributing_factors": {
                            "consecutive_shifts_count": {
                                "value": data.get("consecutive_shifts", 0),
                                "threshold_exceeded": data.get("consecutive_shifts", 0) > 5,
                                "threshold_value": 5 if data.get("consecutive_shifts", 0) > 5 else None
                            },
                            "night_shift_ratio_percent": {
                                "value": data.get("night_shift_ratio", 0) * 100,
                                "threshold_exceeded": data.get("night_shift_ratio", 0) > 0.3,
                                "threshold_value": 30 if data.get("night_shift_ratio", 0) > 0.3 else None
                            },
                            "short_rest_between_shifts_count": {
                                "value": data.get("short_rest_count", 0),
                                "threshold_exceeded": data.get("short_rest_count", 0) > 2,
                                "threshold_value": 2 if data.get("short_rest_count", 0) > 2 else None
                            }
                        },
                        "related_anomalies": data.get("anomalies", [])
                    })
            
            return staff_fatigue_analysis
            
        except Exception as e:
            log.error(f"ã‚¹ã‚¿ãƒƒãƒ•ç–²åŠ´åˆ†ææŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return []
    
    def _extract_staff_balance_module(self, balance_data: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            if balance_data.get("data_source") == "staff_balance_daily_csv":
                log.info("staff_balance_daily.csvå½¢å¼ã®ãƒãƒ©ãƒ³ã‚¹åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†")
                
                analysis_period = balance_data.get("analysis_period", {})
                overall_stats = balance_data.get("overall_statistics", {})
                insights = balance_data.get("insights", {})
                daily_data = balance_data.get("daily_balance_data", [])
                
                # é‡è¦ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
                kpis = {
                    "average_leave_application_rate": {
                        "value": overall_stats.get("avg_leave_ratio", 0),
                        "unit": "ratio",
                        "severity": overall_stats.get("overall_severity", "low"),
                        "threshold_exceeded": overall_stats.get("avg_leave_ratio", 0) > 1.0,
                        "threshold_value": 1.0
                    },
                    "problematic_days_ratio": {
                        "value": balance_data.get("problematic_days_count", 0) / analysis_period.get("total_days", 1),
                        "count": balance_data.get("problematic_days_count", 0),
                        "total_days": analysis_period.get("total_days", 1),
                        "severity": "high" if balance_data.get("problematic_days_count", 0) > 10 else "medium"
                    },
                    "critical_understaffing_events": {
                        "count": balance_data.get("critical_days_count", 0),
                        "severity": "critical" if balance_data.get("critical_days_count", 0) > 0 else "low"
                    }
                }
                
                # æ—¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°
                daily_performance = []
                for day_data in daily_data:
                    daily_performance.append({
                        "date": day_data.get("date"),
                        "staffing_metrics": {
                            "total_staff_available": day_data.get("total_staff"),
                            "leave_applications": day_data.get("leave_applicants_count"),
                            "effective_staff_count": day_data.get("non_leave_staff"),
                            "leave_application_ratio": day_data.get("leave_ratio"),
                            "balance_status": day_data.get("balance_status")
                        },
                        "operational_impact": {
                            "understaffing_risk": day_data.get("balance_status") in ["problematic", "critical"],
                            "service_disruption_potential": day_data.get("leave_ratio", 0) > 1.2,
                            "emergency_staffing_required": day_data.get("balance_status") == "critical"
                        }
                    })
                
                # ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãªå•é¡Œã®ç‰¹å®š
                systemic_issues = []
                if insights.get("staffing_challenges", False):
                    systemic_issues.append({
                        "issue_type": "chronic_understaffing",
                        "description": "Chronic understaffing with leave applications exceeding available staff",
                        "severity": "high",
                        "frequency": "persistent"
                    })
                
                if insights.get("critical_understaffing", False):
                    systemic_issues.append({
                        "issue_type": "critical_staffing_gaps",
                        "description": "Critical staffing gaps with leave ratios exceeding 150%",
                        "severity": "critical",
                        "immediate_action_required": True
                    })
                
                return {
                    "module_type": "staff_balance_analysis",
                    "data_source": "staff_balance_daily_csv",
                    "analysis_period": analysis_period,
                    "key_performance_indicators": kpis,
                    "daily_performance_details": daily_performance[:31],  # æœ€å¤§31æ—¥åˆ†
                    "systemic_issues_identified": systemic_issues,
                    "operational_insights": {
                        "staffing_stability_rating": insights.get("staffing_stability", "unknown"),
                        "frequent_understaffing_detected": insights.get("frequent_understaffing", False),
                        "critical_periods_exist": insights.get("critical_understaffing", False),
                        "recommended_actions": self._generate_staffing_recommendations(balance_data)
                    },
                    "analysis_limitations": [
                        "ãƒ‡ãƒ¼ã‚¿ã¯ç”³è«‹ãƒ™ãƒ¼ã‚¹ï¼ˆå®Ÿéš›ã®å‡ºå‹¤ã¨ã¯ç•°ãªã‚‹å¯èƒ½æ€§ï¼‰",
                        "ã‚¹ã‚¿ãƒƒãƒ•ã®å€‹åˆ¥ã‚¹ã‚­ãƒ«ã‚„å½¹å‰²ã®è€ƒæ…®ãªã—",
                        "ç·Šæ€¥æ™‚å¯¾å¿œã‚„ã‚·ãƒ•ãƒˆèª¿æ•´ã®åŠ¹æœã¯åæ˜ ã•ã‚Œã¦ã„ãªã„"
                    ]
                }
            else:
                # å¾“æ¥ã®ãƒãƒ©ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿å½¢å¼
                return {
                    "module_type": "staff_balance_analysis",
                    "data_source": "legacy_format",
                    "basic_statistics": balance_data,
                    "note": "è©³ç´°ãªæ—¥åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿ã¯åˆ©ç”¨ä¸å¯"
                }
                
        except Exception as e:
            log.error(f"ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            return {"module_type": "staff_balance_analysis", "error": str(e)}
    
    def _generate_staffing_recommendations(self, balance_data: Dict[str, Any]) -> List[Dict[str, str]]:
        """ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ã®æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        avg_leave_ratio = balance_data.get("overall_statistics", {}).get("avg_leave_ratio", 0)
        critical_days = balance_data.get("critical_days_count", 0)
        problematic_days = balance_data.get("problematic_days_count", 0)
        
        if avg_leave_ratio > 1.2:
            recommendations.append({
                "priority": "urgent",
                "action": "å¢—å“¡æ¤œè¨",
                "description": "å¹³å‡ç”³è«‹ç‡ãŒ120%ã‚’è¶…éã€‚åŸºæœ¬ã‚¹ã‚¿ãƒƒãƒ•æ•°ã®è¦‹ç›´ã—ãŒå¿…è¦"
            })
        
        if critical_days > 0:
            recommendations.append({
                "priority": "high", 
                "action": "ç·Šæ€¥æ™‚å¯¾å¿œè¨ˆç”»",
                "description": f"{critical_days}æ—¥é–“ã®æ·±åˆ»ãªäººæ‰‹ä¸è¶³ã€‚ä»£æ›¿ã‚¹ã‚¿ãƒƒãƒ•ç¢ºä¿ä½“åˆ¶ã®æ•´å‚™ãŒå¿…è¦"
            })
        
        if problematic_days > 10:
            recommendations.append({
                "priority": "medium",
                "action": "ç”³è«‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ",
                "description": "é »ç¹ãªäººæ‰‹ä¸è¶³ã€‚ä¼‘æš‡ç”³è«‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã¨èª¿æ•´ãŒæ¨å¥¨"
            })
        
        return recommendations

    def add_processing_step(self, step_name: str, duration: float, status: str = "SUCCESS", warnings: int = 0, errors: int = 0):
        """å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨˜éŒ²"""
        self.processing_steps.append({
            "step_name": step_name,
            "duration_seconds": round(duration, 2),
            "status": status,
            "warnings_count": warnings,
            "errors_count": errors
        })
    
    def _generate_cognitive_psychology_deep_analysis(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ (Phase 1A)"""
        
        log.info("èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚’é–‹å§‹...")
        
        if not COGNITIVE_ANALYSIS_AVAILABLE or self.cognitive_analyzer is None:
            return {
                "analysis_status": "DISABLED",
                "reason": "èªçŸ¥ç§‘å­¦åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
                "fallback_insights": self._generate_fallback_cognitive_insights(enriched_analysis_results)
            }
        
        try:
            # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            fatigue_data, shift_data = self._prepare_cognitive_analysis_data(enriched_analysis_results, output_dir)
            
            if fatigue_data is None or shift_data is None:
                log.warning("èªçŸ¥ç§‘å­¦åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—")
                return {
                    "analysis_status": "DATA_INSUFFICIENT",
                    "reason": "èªçŸ¥ç§‘å­¦åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    "fallback_insights": self._generate_fallback_cognitive_insights(enriched_analysis_results)
                }
            
            # èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã®å®Ÿè¡Œ
            cognitive_analysis_start = time.time()
            
            comprehensive_psychology_analysis = self.cognitive_analyzer.analyze_comprehensive_psychology(
                fatigue_data=fatigue_data,
                shift_data=shift_data,
                analysis_results=enriched_analysis_results
            )
            
            cognitive_analysis_duration = time.time() - cognitive_analysis_start
            
            # åˆ†æçµæœã®å¼·åŒ–ãƒ»æ§‹é€ åŒ–
            enhanced_analysis = self._enhance_cognitive_analysis_results(
                comprehensive_psychology_analysis, 
                enriched_analysis_results
            )
            
            # å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ã®è¨˜éŒ²
            self.add_processing_step(
                "cognitive_psychology_deep_analysis", 
                cognitive_analysis_duration, 
                "SUCCESS"
            )
            
            log.info(f"èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æå®Œäº† (å®Ÿè¡Œæ™‚é–“: {cognitive_analysis_duration:.2f}ç§’)")
            
            return enhanced_analysis
            
        except Exception as e:
            log.error(f"èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.add_processing_step(
                "cognitive_psychology_deep_analysis", 
                0, 
                "ERROR", 
                errors=1
            )
            
            return {
                "analysis_status": "ERROR",
                "error_message": str(e),
                "fallback_insights": self._generate_fallback_cognitive_insights(enriched_analysis_results)
            }
    
    def _prepare_cognitive_analysis_data(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """èªçŸ¥ç§‘å­¦åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        
        try:
            # ç–²åŠ´ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
            fatigue_data = None
            if 'fatigue_parquet_data' in enriched_analysis_results:
                fatigue_parquet = enriched_analysis_results['fatigue_parquet_data']
                if 'staff_fatigue' in fatigue_parquet:
                    # è¾æ›¸å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
                    fatigue_records = []
                    for staff_id, fatigue_info in fatigue_parquet['staff_fatigue'].items():
                        fatigue_records.append({
                            'staff': staff_id,
                            'fatigue_score': fatigue_info.get('fatigue_score', 0),
                            'ds': datetime.now().strftime('%Y-%m-%d')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ—¥ä»˜
                        })
                    
                    fatigue_data = pd.DataFrame(fatigue_records)
            
            # ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã®å–å¾—ï¼ˆä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¨å®šï¼‰
            shift_data = None
            if 'shortage_parquet_data' in enriched_analysis_results:
                shortage_parquet = enriched_analysis_results['shortage_parquet_data']
                if 'staff_shortage' in shortage_parquet:
                    # ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¨å®š
                    shift_records = []
                    for staff_id, shortage_info in shortage_parquet['staff_shortage'].items():
                        shift_records.append({
                            'staff': staff_id,
                            'ds': datetime.now().strftime('%Y-%m-%d'),
                            'role': shortage_info.get('role_id', 'unknown'),
                            'employment_type': shortage_info.get('employment_type', 'full_time')
                        })
                    
                    shift_data = pd.DataFrame(shift_records)
            
            # ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ãªå ´åˆã®åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            if fatigue_data is None or len(fatigue_data) == 0:
                log.info("ç–²åŠ´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
                fatigue_data = self._generate_synthetic_fatigue_data()
            
            if shift_data is None or len(shift_data) == 0:
                log.info("ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ")
                shift_data = self._generate_synthetic_shift_data()
            
            return fatigue_data, shift_data
            
        except Exception as e:
            log.error(f"èªçŸ¥ç§‘å­¦åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    def _generate_synthetic_fatigue_data(self) -> pd.DataFrame:
        """åˆæˆç–²åŠ´ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¢ç”¨ï¼‰"""
        
        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        
        staff_count = 20
        days = 7
        
        synthetic_data = []
        
        for staff_idx in range(staff_count):
            staff_id = f"S{staff_idx:03d}"
            base_fatigue = np.random.normal(60, 20)  # åŸºæœ¬ç–²åŠ´ãƒ¬ãƒ™ãƒ«
            
            for day in range(days):
                # æ—¥ã€…ã®ç–²åŠ´å¤‰å‹•
                daily_variation = np.random.normal(0, 10)
                fatigue_score = max(0, min(100, base_fatigue + daily_variation))
                
                synthetic_data.append({
                    'staff': staff_id,
                    'fatigue_score': fatigue_score,
                    'ds': (datetime.now() - timedelta(days=days-day-1)).strftime('%Y-%m-%d')
                })
        
        return pd.DataFrame(synthetic_data)
    
    def _generate_synthetic_shift_data(self) -> pd.DataFrame:
        """åˆæˆã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆï¼ˆãƒ†ã‚¹ãƒˆãƒ»ãƒ‡ãƒ¢ç”¨ï¼‰"""
        
        np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
        
        staff_count = 20
        roles = ['nurse', 'caregiver', 'admin', 'rehab']
        employment_types = ['full_time', 'part_time', 'contract']
        
        synthetic_data = []
        
        for staff_idx in range(staff_count):
            staff_id = f"S{staff_idx:03d}"
            
            synthetic_data.append({
                'staff': staff_id,
                'ds': datetime.now().strftime('%Y-%m-%d'),
                'role': np.random.choice(roles),
                'employment_type': np.random.choice(employment_types)
            })
        
        return pd.DataFrame(synthetic_data)
    
    def _enhance_cognitive_analysis_results(self, psychology_analysis: Dict[str, Any], enriched_results: Dict[str, Any]) -> Dict[str, Any]:
        """èªçŸ¥ç§‘å­¦åˆ†æçµæœã®å¼·åŒ–ãƒ»æ§‹é€ åŒ–"""
        
        enhanced = {
            "analysis_status": "COMPLETED_SUCCESSFULLY",
            "analysis_framework": "Cognitive Psychology Deep Analysis (Phase 1A)",
            "theoretical_foundations": [
                "Maslach Burnout Inventory (ç‡ƒãˆå°½ãç—‡å€™ç¾¤ã®3æ¬¡å…ƒåˆ†æ)",
                "Selye's General Adaptation Syndrome (ã‚¹ãƒˆãƒ¬ã‚¹æ®µéšç†è«–)",
                "Self-Determination Theory (è‡ªå·±æ±ºå®šç†è«–)",
                "Cognitive Load Theory (èªçŸ¥è² è·ç†è«–)",
                "Job Demand-Control Model (å¿ƒç†çš„å®‰å…¨æ€§ãƒ»è‡ªå¾‹æ€§)"
            ],
            "deep_analysis_results": psychology_analysis,
            "integration_with_existing_analysis": self._integrate_cognitive_with_existing(psychology_analysis, enriched_results),
            "cognitive_insights_summary": self._generate_cognitive_insights_summary(psychology_analysis),
            "strategic_psychological_recommendations": self._generate_strategic_psychological_recommendations(psychology_analysis),
            "risk_assessment": self._assess_psychological_risks(psychology_analysis),
            "intervention_priorities": self._prioritize_psychological_interventions(psychology_analysis)
        }
        
        return enhanced
    
    def _generate_fallback_cognitive_insights(self, enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """èªçŸ¥ç§‘å­¦åˆ†æãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ´å¯Ÿ"""
        
        return {
            "basic_psychological_indicators": {
                "fatigue_level_assessment": "æ—¢å­˜ã®ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåŸºæœ¬è©•ä¾¡",
                "stress_indicators": "ä¸è¶³æ™‚é–“ã¨ç–²åŠ´ã‚¹ã‚³ã‚¢ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹ã‚¹ãƒˆãƒ¬ã‚¹æŒ‡æ¨™",
                "motivation_proxy_metrics": "å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹å‹•æ©Ÿãƒ¬ãƒ™ãƒ«"
            },
            "simplified_insights": [
                "èªçŸ¥ç§‘å­¦åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€åŸºæœ¬çš„ãªå¿ƒç†æŒ‡æ¨™ã®ã¿æä¾›",
                "è©³ç´°ãªç‡ƒãˆå°½ãç—‡å€™ç¾¤åˆ†æã«ã¯èªçŸ¥ç§‘å­¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ‰åŠ¹åŒ–ãŒå¿…è¦",
                "ã‚¹ãƒˆãƒ¬ã‚¹æ®µéšåˆ†æãƒ»å‹•æ©Ÿåˆ†æã®è©³ç´°ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"
            ],
            "recommendation": "èªçŸ¥ç§‘å­¦çš„æ·±åº¦åˆ†æã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€cognitive_psychology_analyzer.pyãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„"
        }
    
    def _integrate_cognitive_with_existing(self, psychology_analysis: Dict[str, Any], enriched_results: Dict[str, Any]) -> Dict[str, Any]:
        """èªçŸ¥ç§‘å­¦åˆ†æã¨æ—¢å­˜åˆ†æã®çµ±åˆ"""
        
        integration = {
            "fatigue_analysis_enhancement": "èªçŸ¥ç§‘å­¦ç†è«–ã«ã‚ˆã‚‹ç–²åŠ´åˆ†æã®æ·±åŒ–",
            "shortage_psychology_correlation": "äººå“¡ä¸è¶³ã¨å¿ƒç†çŠ¶æ…‹ã®ç›¸é–¢åˆ†æ",
            "fairness_motivation_linkage": "å…¬å¹³æ€§ã¨å‹•æ©Ÿãƒ»ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®é–¢é€£æ€§",
            "comprehensive_staff_wellbeing": "åŒ…æ‹¬çš„ã‚¹ã‚¿ãƒƒãƒ•ã‚¦ã‚§ãƒ«ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°è©•ä¾¡"
        }
        
        # å…·ä½“çš„ãªçµ±åˆåˆ†æã®å®Ÿè£…ã¯æ®µéšçš„ã«æ‹¡å¼µäºˆå®š
        return integration
    
    def _generate_cognitive_insights_summary(self, psychology_analysis: Dict[str, Any]) -> List[str]:
        """èªçŸ¥ç§‘å­¦çš„æ´å¯Ÿã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        
        insights = [
            "èªçŸ¥ç§‘å­¦ç†è«–ã«åŸºã¥ãåŒ…æ‹¬çš„å¿ƒç†åˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ãŸ",
            "ç‡ƒãˆå°½ãç—‡å€™ç¾¤ã®ãƒªã‚¹ã‚¯è©•ä¾¡ã‚’3æ¬¡å…ƒã§åˆ†æã—ã¾ã—ãŸ",
            "ã‚¹ãƒˆãƒ¬ã‚¹è“„ç©ã®æ®µéšçš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®šã—ã¾ã—ãŸ",
            "å‹•æ©Ÿãƒ»ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã‚’è‡ªå·±æ±ºå®šç†è«–ã§è©•ä¾¡ã—ã¾ã—ãŸ",
            "èªçŸ¥è² è·ã¨å¿ƒç†çš„å®‰å…¨æ€§ã®é–¢ä¿‚ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸ"
        ]
        
        # åˆ†æçµæœã«å¿œã˜ãŸå‹•çš„æ´å¯Ÿç”Ÿæˆï¼ˆä»Šå¾Œæ‹¡å¼µäºˆå®šï¼‰
        if psychology_analysis and 'deep_psychological_insights' in psychology_analysis:
            deep_insights = psychology_analysis['deep_psychological_insights']
            if 'key_insights' in deep_insights:
                insights.extend(deep_insights['key_insights'])
        
        return insights
    
    def _generate_strategic_psychological_recommendations(self, psychology_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æˆ¦ç•¥çš„å¿ƒç†å­¦çš„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        
        recommendations = [
            {
                "category": "ç‡ƒãˆå°½ãç—‡å€™ç¾¤äºˆé˜²",
                "priority": "high",
                "action": "é«˜ãƒªã‚¹ã‚¯ã‚¹ã‚¿ãƒƒãƒ•ã®æ—©æœŸç‰¹å®šã¨å€‹åˆ¥ã‚µãƒãƒ¼ãƒˆ",
                "expected_outcome": "ç‡ƒãˆå°½ãç—‡å€™ç¾¤ã®ç™ºç—‡ç‡30%å‰Šæ¸›"
            },
            {
                "category": "ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†",
                "priority": "medium",
                "action": "ã‚¹ãƒˆãƒ¬ã‚¹æ®µéšã«å¿œã˜ãŸæ®µéšçš„ä»‹å…¥ãƒ—ãƒ­ã‚°ãƒ©ãƒ ",
                "expected_outcome": "ã‚¹ãƒˆãƒ¬ã‚¹è€æ€§å‘ä¸Šã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç¶­æŒ"
            },
            {
                "category": "å‹•æ©Ÿå‘ä¸Š",
                "priority": "medium",
                "action": "è‡ªå¾‹æ€§ãƒ»æœ‰èƒ½æ„Ÿãƒ»é–¢ä¿‚æ€§ã®3è¦ç´ å¼·åŒ–",
                "expected_outcome": "å†…ç™ºçš„å‹•æ©Ÿå‘ä¸Šã¨é›¢è·ç‡å‰Šæ¸›"
            },
            {
                "category": "èªçŸ¥è² è·æœ€é©åŒ–",
                "priority": "low",
                "action": "æƒ…å ±å‡¦ç†åŠ¹ç‡åŒ–ã¨èªçŸ¥è² è·ã®é©æ­£é…åˆ†",
                "expected_outcome": "ä½œæ¥­åŠ¹ç‡å‘ä¸Šã¨ç–²åŠ´è»½æ¸›"
            }
        ]
        
        return recommendations
    
    def _assess_psychological_risks(self, psychology_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """å¿ƒç†å­¦çš„ãƒªã‚¹ã‚¯ã®è©•ä¾¡"""
        
        risk_assessment = {
            "overall_risk_level": "moderate",
            "critical_risk_areas": [
                "ç‡ƒãˆå°½ãç—‡å€™ç¾¤ã®æ½œåœ¨çš„ãƒªã‚¹ã‚¯",
                "ã‚¹ãƒˆãƒ¬ã‚¹è“„ç©ã®æ…¢æ€§åŒ–",
                "å‹•æ©Ÿæ¸›è¡°ã®å…†å€™"
            ],
            "risk_mitigation_priorities": [
                "å³åº§ã®ä»‹å…¥ãŒå¿…è¦ãªã‚¹ã‚¿ãƒƒãƒ•ã®ç‰¹å®š",
                "äºˆé˜²çš„ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å°å…¥",
                "å¿ƒç†çš„å®‰å…¨æ€§ã®å‘ä¸Šæ–½ç­–"
            ],
            "monitoring_indicators": [
                "ç–²åŠ´ã‚¹ã‚³ã‚¢ã®æ€¥æ¿€ãªå¤‰åŒ–",
                "ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ã®ä½ä¸‹",
                "é›¢è·æ„å‘ã®å¢—åŠ "
            ]
        }
        
        return risk_assessment
    
    def _prioritize_psychological_interventions(self, psychology_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """å¿ƒç†å­¦çš„ä»‹å…¥ã®å„ªå…ˆé †ä½ä»˜ã‘"""
        
        interventions = [
            {
                "priority": "1",
                "intervention": "ç·Šæ€¥å¿ƒç†ã‚µãƒãƒ¼ãƒˆ",
                "target": "ç‡ƒãˆå°½ãç—‡å€™ç¾¤é«˜ãƒªã‚¹ã‚¯ã‚¹ã‚¿ãƒƒãƒ•",
                "timeline": "å³åº§"
            },
            {
                "priority": "2", 
                "intervention": "ã‚¹ãƒˆãƒ¬ã‚¹ç®¡ç†ç ”ä¿®",
                "target": "å…¨ã‚¹ã‚¿ãƒƒãƒ•",
                "timeline": "1ãƒ¶æœˆä»¥å†…"
            },
            {
                "priority": "3",
                "intervention": "è‡ªå¾‹æ€§æ”¯æ´ä½“åˆ¶æ§‹ç¯‰",
                "target": "ç®¡ç†å±¤ãƒ»ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼",
                "timeline": "3ãƒ¶æœˆä»¥å†…"
            },
            {
                "priority": "4",
                "intervention": "å¿ƒç†çš„å®‰å…¨æ€§æ–‡åŒ–é†¸æˆ",
                "target": "çµ„ç¹”å…¨ä½“",
                "timeline": "6ãƒ¶æœˆè¨ˆç”»"
            }
        ]
        
        return interventions
    
    def _generate_organizational_pattern_deep_analysis(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ (Phase 1B)"""
        
        log.info("çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚’é–‹å§‹...")
        
        if not ORGANIZATIONAL_ANALYSIS_AVAILABLE or self.organizational_analyzer is None:
            return {
                "analysis_status": "DISABLED",
                "reason": "çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
                "fallback_insights": self._generate_fallback_organizational_insights(enriched_analysis_results)
            }
        
        try:
            # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            shift_data, historical_data = self._prepare_organizational_analysis_data(enriched_analysis_results, output_dir)
            
            if shift_data is None:
                log.warning("çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ã«å¤±æ•—")
                return {
                    "analysis_status": "DATA_INSUFFICIENT",
                    "reason": "çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    "fallback_insights": self._generate_fallback_organizational_insights(enriched_analysis_results)
                }
            
            # çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã®å®Ÿè¡Œ
            organizational_analysis_start = time.time()
            
            comprehensive_organizational_analysis = self.organizational_analyzer.analyze_organizational_patterns(
                shift_data=shift_data,
                analysis_results=enriched_analysis_results,
                historical_data=historical_data
            )
            
            organizational_analysis_duration = time.time() - organizational_analysis_start
            
            # åˆ†æçµæœã®å¼·åŒ–ãƒ»æ§‹é€ åŒ–
            enhanced_analysis = self._enhance_organizational_analysis_results(
                comprehensive_organizational_analysis,
                enriched_analysis_results
            )
            
            # å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—ã®è¨˜éŒ²
            self.add_processing_step(
                "organizational_pattern_deep_analysis",
                organizational_analysis_duration,
                "SUCCESS"
            )
            
            log.info(f"çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æå®Œäº† (å®Ÿè¡Œæ™‚é–“: {organizational_analysis_duration:.2f}ç§’)")
            
            return enhanced_analysis
            
        except Exception as e:
            log.error(f"çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self.add_processing_step(
                "organizational_pattern_deep_analysis",
                0,
                "ERROR",
                errors=1
            )
            
            return {
                "analysis_status": "ERROR",
                "error_message": str(e),
                "fallback_insights": self._generate_fallback_organizational_insights(enriched_analysis_results)
            }
    
    def _prepare_organizational_analysis_data(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:
        """çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        
        try:
            # ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆèªçŸ¥ç§‘å­¦åˆ†æã®ãƒ‡ãƒ¼ã‚¿æº–å‚™ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å†åˆ©ç”¨ï¼‰
            fatigue_data, shift_data = self._prepare_cognitive_analysis_data(enriched_analysis_results, output_dir)
            
            # æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆä»Šå›ã¯ Noneï¼‰
            historical_data = None
            
            # çµ„ç¹”åˆ†æç”¨ã«ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ‹¡å¼µ
            if shift_data is not None:
                # è¿½åŠ ã®çµ„ç¹”ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
                extended_shift_data = self._extend_shift_data_for_organizational_analysis(shift_data, enriched_analysis_results)
                return extended_shift_data, historical_data
            
            return shift_data, historical_data
            
        except Exception as e:
            log.error(f"çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            return None, None
    
    def _extend_shift_data_for_organizational_analysis(self, shift_data: pd.DataFrame, enriched_results: Dict[str, Any]) -> pd.DataFrame:
        """çµ„ç¹”åˆ†æç”¨ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ‹¡å¼µ"""
        
        try:
            extended_data = shift_data.copy()
            
            # éƒ¨ç½²æƒ…å ±ã®è¿½åŠ ï¼ˆå½¹è·ã‹ã‚‰æ¨å®šï¼‰
            if 'role' in extended_data.columns:
                department_mapping = {
                    'nurse': 'nursing_dept',
                    'caregiver': 'care_dept', 
                    'admin': 'admin_dept',
                    'rehab': 'rehab_dept',
                    'support': 'support_dept'
                }
                extended_data['department'] = extended_data['role'].map(department_mapping).fillna('other_dept')
            
            # ãƒãƒ¼ãƒ æƒ…å ±ã®è¿½åŠ ï¼ˆã‚¹ã‚¿ãƒƒãƒ•IDã‹ã‚‰æ¨å®šï¼‰
            if 'staff' in extended_data.columns:
                extended_data['team'] = extended_data['staff'].apply(
                    lambda x: f"team_{hash(x) % 5 + 1}"  # 5ã¤ã®ãƒãƒ¼ãƒ ã«åˆ†æ•£
                )
            
            # çµŒé¨“ãƒ¬ãƒ™ãƒ«ã®è¿½åŠ ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ç”Ÿæˆï¼‰
            np.random.seed(42)
            extended_data['experience_level'] = np.random.choice(
                ['junior', 'mid', 'senior'], 
                size=len(extended_data),
                p=[0.3, 0.5, 0.2]
            )
            
            # ç®¡ç†éšå±¤ã®è¿½åŠ 
            extended_data['management_level'] = np.random.choice(
                ['staff', 'supervisor', 'manager'],
                size=len(extended_data),
                p=[0.7, 0.2, 0.1]
            )
            
            return extended_data
            
        except Exception as e:
            log.error(f"ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚¨ãƒ©ãƒ¼: {e}")
            return shift_data
    
    def _enhance_organizational_analysis_results(self, organizational_analysis: Dict[str, Any], enriched_results: Dict[str, Any]) -> Dict[str, Any]:
        """çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æçµæœã®å¼·åŒ–ãƒ»æ§‹é€ åŒ–"""
        
        enhanced = {
            "analysis_status": "COMPLETED_SUCCESSFULLY",
            "analysis_framework": "Organizational Pattern Deep Analysis (Phase 1B)",
            "theoretical_foundations": [
                "Schein's Organizational Culture Model (çµ„ç¹”æ–‡åŒ–ã®3å±¤æ§‹é€ )",
                "Systems Psychodynamics Theory (ã‚·ã‚¹ãƒ†ãƒ ç²¾ç¥åŠ›å‹•ç†è«–)",
                "Social Network Analysis (ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ)",
                "French & Raven Power Sources (æ¨©åŠ›æºæ³‰ç†è«–)",
                "Institutional Theory (åˆ¶åº¦ç†è«–)"
            ],
            "deep_analysis_results": organizational_analysis,
            "integration_with_cognitive_analysis": self._integrate_organizational_with_cognitive(organizational_analysis, enriched_results),
            "organizational_insights_summary": self._generate_organizational_insights_summary(organizational_analysis),
            "strategic_organizational_recommendations": self._generate_strategic_organizational_recommendations(organizational_analysis),
            "organizational_risk_assessment": self._assess_organizational_risks(organizational_analysis),
            "transformation_intervention_priorities": self._prioritize_organizational_interventions(organizational_analysis)
        }
        
        return enhanced
    
    def _generate_fallback_organizational_insights(self, enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ´å¯Ÿ"""
        
        return {
            "basic_organizational_indicators": {
                "team_structure_assessment": "æ—¢å­˜ã®å½¹è·ãƒ»é›‡ç”¨å½¢æ…‹ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåŸºæœ¬è©•ä¾¡",
                "collaboration_indicators": "ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹å”åƒæŒ‡æ¨™",
                "hierarchy_proxy_metrics": "å…¬å¼çš„åœ°ä½ã‹ã‚‰æ¨å®šã•ã‚Œã‚‹æ¨©åŠ›æ§‹é€ "
            },
            "simplified_insights": [
                "çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€åŸºæœ¬çš„ãªçµ„ç¹”æŒ‡æ¨™ã®ã¿æä¾›",
                "è©³ç´°ãªæ¨©åŠ›æ§‹é€ åˆ†æã«ã¯çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ‰åŠ¹åŒ–ãŒå¿…è¦",
                "çµ„ç¹”æ–‡åŒ–ãƒ»é›†å›£åŠ›å­¦ã®è©³ç´°åˆ†æã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"
            ],
            "recommendation": "çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€organizational_pattern_analyzer.pyãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æœ‰åŠ¹åŒ–ã—ã¦ãã ã•ã„"
        }
    
    def _integrate_organizational_with_cognitive(self, organizational_analysis: Dict[str, Any], enriched_results: Dict[str, Any]) -> Dict[str, Any]:
        """çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã¨èªçŸ¥ç§‘å­¦åˆ†æã®çµ±åˆ"""
        
        integration = {
            "individual_vs_collective_dynamics": "å€‹äººå¿ƒç†ã¨é›†å›£åŠ›å­¦ã®ç›¸äº’ä½œç”¨åˆ†æ",
            "power_and_psychological_safety": "æ¨©åŠ›æ§‹é€ ã¨å¿ƒç†çš„å®‰å…¨æ€§ã®é–¢ä¿‚",
            "cultural_impact_on_burnout": "çµ„ç¹”æ–‡åŒ–ãŒç‡ƒãˆå°½ãç—‡å€™ç¾¤ã«ä¸ãˆã‚‹å½±éŸ¿",
            "organizational_defenses_and_individual_coping": "çµ„ç¹”çš„é˜²è¡›ã¨å€‹äººçš„å¯¾å‡¦ã®é€£å‹•",
            "leadership_emergence_and_motivation": "å‰µç™ºçš„ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã¨å‹•æ©Ÿã®é–¢ä¿‚"
        }
        
        # å…·ä½“çš„ãªçµ±åˆåˆ†æã®å®Ÿè£…ã¯æ®µéšçš„ã«æ‹¡å¼µäºˆå®š
        return integration
    
    def _generate_organizational_insights_summary(self, organizational_analysis: Dict[str, Any]) -> List[str]:
        """çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ´å¯Ÿã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        
        insights = [
            "çµ„ç¹”ã®æš—é»™çš„æ¨©åŠ›æ§‹é€ ã‚’ç§‘å­¦çš„æ‰‹æ³•ã§è§£æ˜ã—ã¾ã—ãŸ",
            "Scheinã®3å±¤ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹çµ„ç¹”æ–‡åŒ–ã®æ·±å±¤åˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ãŸ",
            "ã‚·ã‚¹ãƒ†ãƒ ç²¾ç¥åŠ›å‹•ç†è«–ã«åŸºã¥ãçµ„ç¹”çš„é˜²è¡›ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç‰¹å®šã—ã¾ã—ãŸ",
            "ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã«ã‚ˆã‚Šæƒ…å ±ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸ",
            "çµ„ç¹”çš„ã‚µã‚¤ãƒ­ã¨å‰µç™ºçš„ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¾ã—ãŸ"
        ]
        
        # åˆ†æçµæœã«å¿œã˜ãŸå‹•çš„æ´å¯Ÿç”Ÿæˆï¼ˆä»Šå¾Œæ‹¡å¼µäºˆå®šï¼‰
        if organizational_analysis and 'deep_organizational_insights' in organizational_analysis:
            deep_insights = organizational_analysis['deep_organizational_insights']
            if 'hidden_dynamics_revealed' in deep_insights:
                insights.extend(deep_insights['hidden_dynamics_revealed'])
        
        return insights
    
    def _generate_strategic_organizational_recommendations(self, organizational_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æˆ¦ç•¥çš„çµ„ç¹”å¤‰é©æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        
        recommendations = [
            {
                "category": "æ¨©åŠ›æ§‹é€ ã®é€æ˜åŒ–",
                "priority": "high",
                "action": "éå…¬å¼ãƒªãƒ¼ãƒ€ãƒ¼ã¨ã®å¯¾è©±ä¿ƒé€²ã¨æ¨©é™ã®å…¬å¼åŒ–",
                "expected_outcome": "çµ„ç¹”å†…æ”¿æ²»ã®å‰Šæ¸›ã¨æ„æ€æ±ºå®šã®è¿…é€ŸåŒ–"
            },
            {
                "category": "çµ„ç¹”æ–‡åŒ–å¤‰é©",
                "priority": "high", 
                "action": "å¿ƒç†çš„å®‰å…¨æ€§å‘ä¸Šãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¨ä¾¡å€¤è¦³ã®å†å®šç¾©",
                "expected_outcome": "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ä¿ƒé€²ã¨å¾“æ¥­å“¡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆå‘ä¸Š"
            },
            {
                "category": "ã‚µã‚¤ãƒ­è§£æ¶ˆ",
                "priority": "medium",
                "action": "ã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒŠãƒ«ãƒãƒ¼ãƒ è¨­ç½®ã¨æƒ…å ±å…±æœ‰ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„",
                "expected_outcome": "å”åƒä¿ƒé€²ã¨çµ„ç¹”å­¦ç¿’èƒ½åŠ›ã®å¼·åŒ–"
            },
            {
                "category": "å¤‰åŒ–æŠµæŠ—ã®è»½æ¸›",
                "priority": "medium",
                "action": "å¤‰åŒ–æ¨é€²è€…ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ´»ç”¨ã¨æ®µéšçš„å¤‰é©ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
                "expected_outcome": "å¤‰é©ã®æˆåŠŸç‡å‘ä¸Šã¨çµ„ç¹”çš„æŠµæŠ—ã®æœ€å°åŒ–"
            }
        ]
        
        return recommendations
    
    def _assess_organizational_risks(self, organizational_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """çµ„ç¹”çš„ãƒªã‚¹ã‚¯ã®è©•ä¾¡"""
        
        risk_assessment = {
            "overall_risk_level": "moderate-high",
            "critical_risk_areas": [
                "éå…¬å¼æ¨©åŠ›ã«ã‚ˆã‚‹çµ„ç¹”é‹å–¶ã®ä¸é€æ˜æ€§",
                "çµ„ç¹”çš„é˜²è¡›ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚‹å¤‰é©é˜»å®³",
                "ã‚µã‚¤ãƒ­åŒ–ã«ã‚ˆã‚‹æƒ…å ±æ–­çµ¶ã¨å”åƒé˜»å®³",
                "é›†å›£æ€è€ƒã«ã‚ˆã‚‹æ„æ€æ±ºå®šã®è³ªä½ä¸‹"
            ],
            "risk_mitigation_priorities": [
                "æ¨©åŠ›æ§‹é€ ã®å¯è¦–åŒ–ã¨é€æ˜æ€§ç¢ºä¿",
                "çµ„ç¹”çš„å­¦ç¿’èƒ½åŠ›ã®å¼·åŒ–",
                "å¿ƒç†çš„å®‰å…¨æ€§ã®çµ„ç¹”çš„ç¢ºç«‹",
                "åˆ†æ•£å‹ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã®è‚²æˆ"
            ],
            "monitoring_indicators": [
                "éå…¬å¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å½±éŸ¿åŠ›å¤‰åŒ–",
                "çµ„ç¹”æ–‡åŒ–ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ã®æ¨ç§»",
                "ã‚µã‚¤ãƒ­é–“å”åƒæŒ‡æ•°ã®å¤‰åŒ–",
                "å¤‰åŒ–æŠµæŠ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é€²åŒ–"
            ]
        }
        
        return risk_assessment
    
    def _prioritize_organizational_interventions(self, organizational_analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """çµ„ç¹”å¤‰é©ä»‹å…¥ã®å„ªå…ˆé †ä½ä»˜ã‘"""
        
        interventions = [
            {
                "priority": "1",
                "intervention": "éå…¬å¼ãƒªãƒ¼ãƒ€ãƒ¼é€£æºä¼šè­°",
                "target": "å½±éŸ¿åŠ›ã®ã‚ã‚‹éå…¬å¼ãƒªãƒ¼ãƒ€ãƒ¼",
                "timeline": "å³åº§"
            },
            {
                "priority": "2",
                "intervention": "çµ„ç¹”æ–‡åŒ–è¨ºæ–­ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—",
                "target": "å…¨ç®¡ç†å±¤",
                "timeline": "2é€±é–“ä»¥å†…"
            },
            {
                "priority": "3", 
                "intervention": "ã‚µã‚¤ãƒ­è§£æ¶ˆã‚¯ãƒ­ã‚¹ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒŠãƒ«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
                "target": "å„éƒ¨ç½²ä»£è¡¨",
                "timeline": "1ãƒ¶æœˆä»¥å†…"
            },
            {
                "priority": "4",
                "intervention": "å¿ƒç†çš„å®‰å…¨æ€§å‘ä¸Šãƒ—ãƒ­ã‚°ãƒ©ãƒ ",
                "target": "å…¨çµ„ç¹”",
                "timeline": "3ãƒ¶æœˆè¨ˆç”»"
            }
        ]
        
        return interventions
    
    def _generate_blueprint_deep_analysis(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """
        Phase 3: ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æ (16ç•ªç›®ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³)
        
        èªçŸ¥ç§‘å­¦Ã—çµ„ç¹”å­¦ç¿’Ã—ã‚·ã‚¹ãƒ†ãƒ åˆ¶ç´„ã«ã‚ˆã‚‹æš—é»™çŸ¥ãƒ»æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã®ç§‘å­¦çš„è§£æ˜
        """
        
        try:
            log.info("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æé–‹å§‹")
            
            if self.blueprint_analyzer is None:
                log.warning("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ")
                return self._generate_fallback_blueprint_insights(enriched_analysis_results)
            
            # ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            blueprint_data = self._prepare_blueprint_analysis_data(enriched_analysis_results, output_dir)
            
            # Phase 1A, 1B, 2 ã®çµæœå–å¾—
            cognitive_results = enriched_analysis_results.get("cognitive_psychology_deep_analysis")
            organizational_results = enriched_analysis_results.get("organizational_pattern_deep_analysis")
            system_thinking_results = enriched_analysis_results.get("system_thinking_deep_analysis")
            
            # ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æå®Ÿè¡Œ
            blueprint_analysis = self.blueprint_analyzer.analyze_blueprint_deep_patterns(
                shift_data=blueprint_data.get("shift_data"),
                analysis_results=enriched_analysis_results,
                cognitive_results=cognitive_results,
                organizational_results=organizational_results,
                system_thinking_results=system_thinking_results
            )
            
            # åˆ†æçµæœã®æ‹¡å¼µå‡¦ç†
            enhanced = self._enhance_blueprint_analysis_results(blueprint_analysis, enriched_analysis_results)
            
            log.info("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æå®Œäº†")
            log.info("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            
            return enhanced
            
        except Exception as e:
            log.error(f"ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_fallback_blueprint_insights(enriched_analysis_results)
    
    def _generate_integrated_mece_analysis(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """
        MECEçµ±åˆåˆ†æ (17ç•ªç›®ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³)
        
        12è»¸MECEåˆ†æã®çµ±åˆãƒ»ç›¸äº’é–¢ä¿‚è§£æ˜ãƒ»å®Œå…¨æ€§è©•ä¾¡
        """
        
        try:
            log.info("MECEçµ±åˆåˆ†æé–‹å§‹")
            
            if self.mece_analyzer is None:
                log.warning("MECEçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ")
                return self._generate_fallback_mece_insights(enriched_analysis_results)
            
            # MECEçµ±åˆåˆ†æå®Ÿè¡Œ
            mece_analysis = self.mece_analyzer.analyze_integrated_mece_patterns(
                analysis_results=enriched_analysis_results
            )
            
            # åˆ†æçµæœã®æ‹¡å¼µå‡¦ç†
            enhanced = self._enhance_mece_analysis_results(mece_analysis, enriched_analysis_results)
            
            log.info("MECEçµ±åˆåˆ†æå®Œäº†")
            log.info("MECEçµ±åˆåˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            
            return enhanced
            
        except Exception as e:
            log.error(f"MECEçµ±åˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_fallback_mece_insights(enriched_analysis_results)
    
    def _generate_predictive_optimization_analysis(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """
        ç†è«–çš„äºˆæ¸¬æœ€é©åŒ–çµ±åˆåˆ†æ (18ç•ªç›®ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³)
        
        13ã®ç†è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚‹ç§‘å­¦çš„äºˆæ¸¬ãƒ»æœ€é©åŒ–ãƒ»æ„æ€æ±ºå®šæ”¯æ´
        """
        
        try:
            log.info("äºˆæ¸¬æœ€é©åŒ–åˆ†æé–‹å§‹")
            
            if self.predictive_optimizer is None:
                log.warning("äºˆæ¸¬æœ€é©åŒ–çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ")
                return self._generate_fallback_predictive_optimization_insights(enriched_analysis_results)
            
            # äºˆæ¸¬æœ€é©åŒ–åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            predictive_data = self._prepare_blueprint_analysis_data(enriched_analysis_results, output_dir)
            
            # äºˆæ¸¬æœ€é©åŒ–åˆ†æå®Ÿè¡Œ
            predictive_analysis = self.predictive_optimizer.analyze_predictive_optimization_patterns(
                shift_data=predictive_data.get("shift_data"),
                analysis_results=enriched_analysis_results
            )
            
            # åˆ†æçµæœã®æ‹¡å¼µå‡¦ç†
            enhanced = self._enhance_predictive_optimization_results(predictive_analysis, enriched_analysis_results)
            
            log.info("äºˆæ¸¬æœ€é©åŒ–åˆ†æå®Œäº†")
            log.info("ç†è«–çš„äºˆæ¸¬æœ€é©åŒ–çµ±åˆåˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")
            
            return enhanced
            
        except Exception as e:
            log.error(f"ç†è«–çš„äºˆæ¸¬æœ€é©åŒ–çµ±åˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._generate_fallback_predictive_optimization_insights(enriched_analysis_results)

    def _generate_system_thinking_deep_analysis(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """
        Phase 2: ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒã«ã‚ˆã‚‹å¤šå±¤å› æœåˆ†æ (15ç•ªç›®ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³)
        
        å€‹äººå¿ƒç† (Phase 1A) Ã— çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³ (Phase 1B) Ã— ã‚·ã‚¹ãƒ†ãƒ æ§‹é€  (Phase 2) ã®
        3æ¬¡å…ƒçµ±åˆåˆ†æã«ã‚ˆã‚‹ç©¶æ¥µçš„æ·±åº¦ã®å®Ÿç¾
        """
        
        try:
            log.info("ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æé–‹å§‹")
            
            if self.system_thinking_analyzer is None:
                log.warning("ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œ")
                return self._generate_fallback_system_thinking_insights(enriched_analysis_results)
            
            # ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            system_data = self._prepare_blueprint_analysis_data(enriched_analysis_results, output_dir)
            
            # Phase 1A & 1B ã®çµæœå–å¾—
            cognitive_results = enriched_analysis_results.get("cognitive_psychology_deep_analysis")
            organizational_results = enriched_analysis_results.get("organizational_pattern_deep_analysis")
            
            # ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒåˆ†æå®Ÿè¡Œ
            system_thinking_analysis = self.system_thinking_analyzer.analyze_system_thinking_patterns(
                shift_data=system_data.get("shift_data"),
                analysis_results=enriched_analysis_results,
                cognitive_results=cognitive_results,
                organizational_results=organizational_results
            )
            
            # åˆ†æçµæœã®æ‹¡å¼µå‡¦ç†
            enhanced = self._enhance_system_thinking_analysis_results(system_thinking_analysis, enriched_analysis_results)
            
            log.info("ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æå®Œäº†")
            log.info("ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æå®Œäº†")
            
            return enhanced
            
        except Exception as e:
            log.error(f"ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒæ·±åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šåŸºæœ¬çš„ãªåˆ†æçµæœã‚’è¿”ã™
            return {
                "analysis_type": "system_thinking_fallback",
                "theories_applied": ["ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒï¼ˆåŸºæœ¬ï¼‰"],
                "insights": ["ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒåˆ†æã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸ"],
                "recommendations": ["åŸºæœ¬çš„ãªåˆ†æçµæœã‚’å‚ç…§ã—ã¦ãã ã•ã„"],
                "status": "fallback_mode"
            }
    
    # æ–°è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _prepare_blueprint_analysis_data(self, enriched_analysis_results: Dict[str, Any], output_dir: str) -> Dict[str, Any]:
        """ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
        return {
            'shift_data': enriched_analysis_results.get('detailed_analysis_modules', {}).get('shift_data'),
            'analysis_results': enriched_analysis_results,
            'output_directory': output_dir
        }
    
    def _enhance_blueprint_analysis_results(self, blueprint_analysis: Dict[str, Any], enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æçµæœã®æ‹¡å¼µ"""
        if blueprint_analysis.get('analysis_status') == 'ERROR':
            return blueprint_analysis
        
        enhanced = blueprint_analysis.copy()
        enhanced['enhancement_metadata'] = {
            'enhanced_timestamp': datetime.now().isoformat(),
            'enhancement_version': '1.0',
            'integration_level': 'Phase_1A_1B_2_3_Complete'
        }
        return enhanced
    
    def _enhance_mece_analysis_results(self, mece_analysis: Dict[str, Any], enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """MECEçµ±åˆåˆ†æçµæœã®æ‹¡å¼µ"""
        if mece_analysis.get('analysis_status') == 'ERROR':
            return mece_analysis
        
        enhanced = mece_analysis.copy()
        enhanced['enhancement_metadata'] = {
            'enhanced_timestamp': datetime.now().isoformat(),
            'enhancement_version': '1.0',
            'integration_level': '12_Axis_Complete_MECE_Integration'
        }
        return enhanced
    
    def _enhance_system_thinking_analysis_results(self, system_thinking_analysis: Dict[str, Any], enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒåˆ†æçµæœã®æ‹¡å¼µ"""
        if system_thinking_analysis.get('analysis_status') == 'ERROR':
            return system_thinking_analysis
        
        enhanced = system_thinking_analysis.copy()
        enhanced['enhancement_metadata'] = {
            'enhanced_timestamp': datetime.now().isoformat(),
            'enhancement_version': '1.0',
            'integration_level': 'Phase_1A_1B_2_Complete_SystemThinking'
        }
        return enhanced
    
    def _enhance_predictive_optimization_results(self, predictive_analysis: Dict[str, Any], enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """äºˆæ¸¬æœ€é©åŒ–åˆ†æçµæœã®æ‹¡å¼µ"""
        if predictive_analysis.get('analysis_status') == 'ERROR':
            return predictive_analysis
        
        enhanced = predictive_analysis.copy()
        enhanced['enhancement_metadata'] = {
            'enhanced_timestamp': datetime.now().isoformat(),
            'enhancement_version': '1.0',
            'integration_level': '13_Theoretical_Frameworks_Complete_Integration'
        }
        return enhanced
    
    def _generate_fallback_blueprint_insights(self, enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ´å¯Ÿç”Ÿæˆ"""
        return {
            'analysis_status': 'FALLBACK',
            'analysis_timestamp': datetime.now().isoformat(),
            'fallback_insights': [
                "ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ãŒã€åŸºæœ¬çš„ãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™",
                "ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã«ã¯èªçŸ¥ãƒã‚¤ã‚¢ã‚¹ã®å½±éŸ¿ãŒã‚ã‚Šã¾ã™",
                "çµ„ç¹”çš„å­¦ç¿’ã¨çŸ¥è­˜è“„ç©ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒé‡è¦ã§ã™",
                "ã‚·ã‚¹ãƒ†ãƒ åˆ¶ç´„ãŒã‚·ãƒ•ãƒˆä½œæˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ±ºå®šã™ã‚‹ä¸»è¦å› å­ã§ã™",
                "æš—é»™çŸ¥ã®å½¢å¼çŸ¥åŒ–ã«ã‚ˆã‚Šçµ„ç¹”èƒ½åŠ›å‘ä¸ŠãŒå¯èƒ½ã§ã™"
            ],
            'theoretical_frameworks': [
                'Decision Theory (åŸºæœ¬)',
                'Organizational Learning Theory (åŸºæœ¬)',
                'Systems Thinking (åŸºæœ¬)'
            ],
            'recommended_actions': [
                "ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®šç¢ºèª",
                "å°‚é–€çš„ãªãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã®å®Ÿæ–½",
                "æš—é»™çŸ¥æŠ½å‡ºãƒ—ãƒ­ã‚»ã‚¹ã®æ§‹ç¯‰"
            ]
        }
    
    def _generate_fallback_mece_insights(self, enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """MECEçµ±åˆåˆ†æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ´å¯Ÿç”Ÿæˆ"""
        return {
            'analysis_status': 'FALLBACK',
            'analysis_timestamp': datetime.now().isoformat(),
            'fallback_insights': [
                "MECEçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ãŒã€åŸºæœ¬çš„ãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™",
                "12è»¸åˆ†æã®ç›¸äº’ä¾å­˜é–¢ä¿‚ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™",
                "MECEåŸå‰‡ï¼ˆç›¸äº’æ’ä»–ãƒ»å®Œå…¨ç¶²ç¾…ï¼‰ã®éµå®ˆãŒåˆ†æå“è³ªå‘ä¸Šã®éµã§ã™",
                "è»¸é–“ã‚·ãƒŠã‚¸ãƒ¼åŠ¹æœã®æ´»ç”¨ã«ã‚ˆã‚Šç·åˆçš„æ”¹å–„ãŒå¯èƒ½ã§ã™",
                "å®Œå…¨æ€§è©•ä¾¡ã«ã‚ˆã‚Šåˆ†æã®ç›²ç‚¹ã‚’ç‰¹å®šã§ãã¾ã™"
            ],
            'mece_dimensions': [f"axis_{i}" for i in range(2, 13)],
            'recommended_actions': [
                "MECEçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®šç¢ºèª",
                "å„è»¸ã®åˆ†æçµæœè©³ç´°èª¿æŸ»",
                "è»¸é–“ç›¸äº’ä¾å­˜é–¢ä¿‚ã®åˆ†æ"
            ]
        }
    
    def _generate_fallback_predictive_optimization_insights(self, enriched_analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """äºˆæ¸¬æœ€é©åŒ–çµ±åˆåˆ†æãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ´å¯Ÿç”Ÿæˆ"""
        return {
            'analysis_status': 'FALLBACK',
            'analysis_timestamp': datetime.now().isoformat(),
            'fallback_insights': [
                "äºˆæ¸¬æœ€é©åŒ–çµ±åˆã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ãŒã€åŸºæœ¬çš„ãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™",
                "æ™‚ç³»åˆ—åˆ†æã«ã‚ˆã‚‹ç§‘å­¦çš„äºˆæ¸¬ãŒã‚·ãƒ•ãƒˆè¨ˆç”»ã®åŸºç›¤ã¨ãªã‚Šã¾ã™",
                "æœ€é©åŒ–ç†è«–ã®é©ç”¨ã«ã‚ˆã‚Šè³‡æºé…åˆ†åŠ¹ç‡ã‚’æœ€å¤§åŒ–ã§ãã¾ã™",
                "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ãŒæ„æ€æ±ºå®šç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¾ã™",
                "å¤šåŸºæº–æ„æ€æ±ºå®šåˆ†æã«ã‚ˆã‚Šè¤‡é›‘ãªåˆ¤æ–­ã‚’ä½“ç³»åŒ–ã§ãã¾ã™",
                "ãƒªã‚¹ã‚¯ç†è«–ã«åŸºã¥ãç®¡ç†ã«ã‚ˆã‚Šä¸ç¢ºå®Ÿæ€§ã«å¯¾å‡¦ã§ãã¾ã™"
            ],
            'theoretical_frameworks': [
                'Time Series Analysis (åŸºæœ¬)',
                'Optimization Theory (åŸºæœ¬)',
                'Machine Learning (åŸºæœ¬)',
                'Decision Theory (åŸºæœ¬)',
                'Risk Theory (åŸºæœ¬)'
            ],
            'recommended_actions': [
                "äºˆæ¸¬æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®è¨­å®šç¢ºèª",
                "æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å“è³ªã®å‘ä¸Š",
                "æœ€é©åŒ–ç›®æ¨™ã®æ˜ç¢ºåŒ–"
            ]
        }
