#!/usr/bin/env python3
"""
Hierarchical Truth Analyzer
éšå±¤çš„çœŸå®Ÿåˆ†æã‚·ã‚¹ãƒ†ãƒ  - 3æ®µéšæ¤œè¨¼ã«ã‚ˆã‚‹æœ€é«˜ç²¾åº¦åˆ†æ
"""

from __future__ import annotations  # å‹ãƒ’ãƒ³ãƒˆäº’æ›æ€§ã®ãŸã‚ä¿æŒ

# import json  # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã§å°†æ¥ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
import logging
from datetime import datetime  # , timedelta  # timedelta ã¯ç¾åœ¨æœªä½¿ç”¨
from enum import Enum
# from pathlib import Path  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã§å°†æ¥ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
from typing import Any, Dict, List, Optional
# from typing import Tuple  # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

import numpy as np
import pandas as pd  # DataFrameä»¥å¤–ã§ã‚‚åˆ†æå‡¦ç†ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
from pandas import DataFrame

from .enhanced_data_ingestion import QualityAssuredDataset
from .truth_assured_decomposer import DecompositionResult
from .utils import apply_rest_exclusion_filter, log

# Analysis logger
analysis_logger = logging.getLogger('analysis')


class AnalysisConfidenceLevel(Enum):
    """åˆ†æä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«"""
    VERY_HIGH = "very_high"  # 95-100%
    HIGH = "high"            # 85-94%
    MEDIUM = "medium"        # 70-84%
    LOW = "low"              # 50-69%
    VERY_LOW = "very_low"    # 0-49%


class TruthAnalysisResult:
    """çœŸå®Ÿåˆ†æçµæœ"""
    
    def __init__(self, analysis_type: str):
        self.analysis_type = analysis_type
        self.confidence_score: float = 0.0
        self.confidence_level: AnalysisConfidenceLevel = AnalysisConfidenceLevel.VERY_LOW
        
        # åˆ†æçµæœãƒ‡ãƒ¼ã‚¿
        self.shortage_by_time: Dict[str, float] = {}
        self.shortage_by_role: Dict[str, float] = {}
        self.shortage_by_employment: Dict[str, float] = {}
        self.total_shortage_hours: float = 0.0
        
        # æ ¹æ‹ ãƒ‡ãƒ¼ã‚¿
        self.evidence_data: Dict[str, Any] = {}
        self.validation_checks: Dict[str, bool] = {}
        self.truth_indicators: Dict[str, float] = {}
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        self.processing_timestamp: datetime = datetime.now()
        self.data_sources: List[str] = []
        self.processing_notes: List[str] = []
    
    def calculate_confidence_level(self) -> AnalysisConfidenceLevel:
        """ä¿¡é ¼åº¦ãƒ¬ãƒ™ãƒ«è¨ˆç®—"""
        if self.confidence_score >= 95:
            return AnalysisConfidenceLevel.VERY_HIGH
        elif self.confidence_score >= 85:
            return AnalysisConfidenceLevel.HIGH
        elif self.confidence_score >= 70:
            return AnalysisConfidenceLevel.MEDIUM
        elif self.confidence_score >= 50:
            return AnalysisConfidenceLevel.LOW
        else:
            return AnalysisConfidenceLevel.VERY_LOW


class NeedBasedTruthEngine:
    """Need-basedçœŸå®Ÿåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆéšå±¤1: Primary Truth Engineï¼‰"""
    
    def __init__(self):
        self.analysis_precision = "maximum"
        self.truth_verification_level = "strict"
    
    def analyze(self, dataset: QualityAssuredDataset, decomposition: DecompositionResult) -> TruthAnalysisResult:
        """Need-basedçœŸå®Ÿåˆ†æå®Ÿè¡Œ"""
        analysis_logger.info("[PRIMARY_TRUTH] Need-basedçœŸå®Ÿåˆ†æé–‹å§‹")
        
        result = TruthAnalysisResult("need_based_primary")
        
        try:
            # Needåˆ†æçµæœã®æ¤œè¨¼ã¨ç²¾ç·»åŒ–
            if decomposition.need_analysis:
                result = self._analyze_need_based_truth(dataset, decomposition, result)
            else:
                result = self._fallback_to_direct_analysis(dataset, result)
            
            # çœŸå®Ÿæ€§æ¤œè¨¼
            result = self._verify_truth_indicators(dataset, result)
            
            # ä¿¡é ¼åº¦è¨ˆç®—
            result.confidence_score = self._calculate_need_based_confidence(dataset, decomposition, result)
            result.confidence_level = result.calculate_confidence_level()
            
            analysis_logger.info(f"[PRIMARY_TRUTH] å®Œäº†: ä¿¡é ¼åº¦{result.confidence_score:.1f}%")
            
            return result
            
        except Exception as e:
            log.error(f"Need-basedåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            result.processing_notes.append(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return result
    
    def _analyze_need_based_truth(
        self, 
        dataset: QualityAssuredDataset, 
        decomposition: DecompositionResult,
        result: TruthAnalysisResult
    ) -> TruthAnalysisResult:
        """Need-basedçœŸå®Ÿåˆ†æå®Ÿè¡Œ"""
        
        need_analysis = decomposition.need_analysis
        
        # å®Ÿéš›ã®ã‚±ã‚¢éœ€è¦vsç¾åœ¨ã®é…ç½®ã®è©³ç´°æ¯”è¼ƒ
        result.evidence_data["care_demands"] = need_analysis.care_demands_by_time
        result.evidence_data["care_demands_by_role"] = need_analysis.care_demands_by_role
        
        # ç¾åœ¨ã®é…ç½®çŠ¶æ³åˆ†æ
        current_staffing = self._analyze_current_staffing(dataset.data)
        result.evidence_data["current_staffing"] = current_staffing
        
        # ä¸è¶³æ™‚é–“è¨ˆç®—ï¼ˆé«˜ç²¾åº¦ï¼‰
        result.shortage_by_time = self._calculate_precise_time_shortages(
            need_analysis.care_demands_by_time, 
            current_staffing["time_distribution"]
        )
        
        result.shortage_by_role = self._calculate_precise_role_shortages(
            need_analysis.care_demands_by_role,
            current_staffing["role_distribution"]
        )
        
        # ç·ä¸è¶³æ™‚é–“
        result.total_shortage_hours = sum(result.shortage_by_time.values())
        
        # çœŸå®Ÿæ€§æŒ‡æ¨™ã®è¨ˆç®—
        result.truth_indicators = self._calculate_truth_indicators(need_analysis, current_staffing)
        
        result.data_sources.append("need_file_direct")
        result.processing_notes.append("Need Fileç›´æ¥åˆ†æã«ã‚ˆã‚‹é«˜ç²¾åº¦è¨ˆç®—")
        
        return result
    
    def _analyze_current_staffing(self, data: DataFrame) -> Dict[str, Any]:
        """ç¾åœ¨ã®ã‚¹ã‚¿ãƒƒãƒ•é…ç½®çŠ¶æ³åˆ†æ"""
        staffing_analysis = {
            "time_distribution": {},
            "role_distribution": {},
            "employment_distribution": {},
            "total_staff_hours": 0.0,
            "coverage_gaps": []
        }
        
        try:
            # æ™‚é–“åˆ¥é…ç½®åˆ†æ
            time_columns = [col for col in data.columns if self._is_time_column(col)]
            for col in time_columns:
                # ä¼‘æš‡é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é©ç”¨
                filtered_data = apply_rest_exclusion_filter(data, f"staffing_analysis_{col}")
                staff_count = filtered_data[col].notna().sum()
                staffing_analysis["time_distribution"][str(col)] = staff_count
            
            # è·ç¨®åˆ¥é…ç½®åˆ†æ
            if 'role' in data.columns:
                filtered_data = apply_rest_exclusion_filter(data, "role_analysis")
                role_counts = filtered_data[filtered_data['role'].notna()]['role'].value_counts()
                staffing_analysis["role_distribution"] = role_counts.to_dict()
            
            # é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æ
            if 'employment' in data.columns:
                filtered_data = apply_rest_exclusion_filter(data, "employment_analysis")
                employment_counts = filtered_data[filtered_data['employment'].notna()]['employment'].value_counts()
                staffing_analysis["employment_distribution"] = employment_counts.to_dict()
            
            # ç·ã‚¹ã‚¿ãƒƒãƒ•æ™‚é–“è¨ˆç®—
            staffing_analysis["total_staff_hours"] = sum(staffing_analysis["time_distribution"].values())
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
            staffing_analysis["coverage_gaps"] = self._detect_coverage_gaps(staffing_analysis["time_distribution"])
            
            return staffing_analysis
            
        except Exception as e:
            log.error(f"ç¾åœ¨é…ç½®åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return staffing_analysis
    
    def _calculate_precise_time_shortages(
        self, 
        care_demands: Dict[str, float], 
        current_staffing: Dict[str, float]
    ) -> Dict[str, float]:
        """æ™‚é–“åˆ¥ç²¾å¯†ä¸è¶³è¨ˆç®—"""
        shortages = {}
        
        for time_slot, demand in care_demands.items():
            current = current_staffing.get(time_slot, 0.0)
            shortage = max(0.0, demand - current)
            shortages[time_slot] = shortage
        
        return shortages
    
    def _calculate_precise_role_shortages(
        self,
        role_demands: Dict[str, float],
        current_roles: Dict[str, float] 
    ) -> Dict[str, float]:
        """è·ç¨®åˆ¥ç²¾å¯†ä¸è¶³è¨ˆç®—"""
        shortages = {}
        
        for role, demand in role_demands.items():
            current = current_roles.get(role, 0.0)
            shortage = max(0.0, demand - current)
            shortages[role] = shortage
        
        return shortages
    
    def _calculate_truth_indicators(
        self, 
        need_analysis, 
        current_staffing: Dict[str, Any]
    ) -> Dict[str, float]:
        """çœŸå®Ÿæ€§æŒ‡æ¨™è¨ˆç®—"""
        indicators = {}
        
        try:
            # éœ€è¦é©åˆåº¦ï¼ˆå®Ÿéš›ã®éœ€è¦ã«ã©ã‚Œã ã‘é©åˆã—ã¦ã„ã‚‹ã‹ï¼‰
            total_demand = need_analysis.total_care_hours
            total_current = current_staffing["total_staff_hours"]
            
            if total_demand > 0:
                indicators["demand_alignment"] = min(100.0, (total_current / total_demand) * 100)
            else:
                indicators["demand_alignment"] = 0.0
            
            # ã‚±ã‚¢å“è³ªæŒ‡æ¨™ï¼ˆãƒ”ãƒ¼ã‚¯æ™‚é–“ã®å……è¶³åº¦ï¼‰
            peak_coverage = 0.0
            if need_analysis.peak_hours:
                peak_demand = sum(need_analysis.care_demands_by_time.get(h, 0) for h in need_analysis.peak_hours)
                peak_current = sum(current_staffing["time_distribution"].get(h, 0) for h in need_analysis.peak_hours)
                
                if peak_demand > 0:
                    peak_coverage = min(100.0, (peak_current / peak_demand) * 100)
            
            indicators["peak_hour_coverage"] = peak_coverage
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æŒ‡æ¨™
            indicators["data_completeness"] = need_analysis.data_completeness
            indicators["confidence_baseline"] = need_analysis.confidence_score
            
            return indicators
            
        except Exception as e:
            log.error(f"çœŸå®Ÿæ€§æŒ‡æ¨™è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return indicators
    
    def _verify_truth_indicators(self, dataset: QualityAssuredDataset, result: TruthAnalysisResult) -> TruthAnalysisResult:
        """çœŸå®Ÿæ€§æŒ‡æ¨™ã®æ¤œè¨¼"""
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å“è³ªã¨ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            result.validation_checks["data_quality_consistent"] = (
                result.truth_indicators.get("data_completeness", 0) >= 70.0
            )
            
            # éœ€è¦é©åˆæ€§ãƒã‚§ãƒƒã‚¯
            result.validation_checks["demand_realistic"] = (
                0 <= result.truth_indicators.get("demand_alignment", 0) <= 150.0  # 150%ä»¥ä¸‹ã¯ç¾å®Ÿçš„
            )
            
            # ãƒ”ãƒ¼ã‚¯æ™‚ã‚«ãƒãƒ¬ãƒƒã‚¸å¦¥å½“æ€§
            result.validation_checks["peak_coverage_adequate"] = (
                result.truth_indicators.get("peak_hour_coverage", 0) >= 50.0
            )
            
            # ç·ä¸è¶³æ™‚é–“ã®å¦¥å½“æ€§
            result.validation_checks["shortage_reasonable"] = (
                result.total_shortage_hours <= dataset.data.shape[0] * 24  # éç¾å®Ÿçš„ã§ãªã„ç¯„å›²
            )
            
            return result
            
        except Exception as e:
            log.error(f"çœŸå®Ÿæ€§æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return result
    
    def _calculate_need_based_confidence(
        self, 
        dataset: QualityAssuredDataset, 
        decomposition: DecompositionResult, 
        result: TruthAnalysisResult
    ) -> float:
        """Need-basedä¿¡é ¼åº¦è¨ˆç®—"""
        
        try:
            # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦ï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªï¼‰
            base_confidence = dataset.quality_result.overall_score
            
            # Needåˆ†æå“è³ªãƒœãƒ¼ãƒŠã‚¹
            need_quality_bonus = 0.0
            if decomposition.need_analysis:
                need_quality_bonus = decomposition.need_analysis.confidence_score * 0.3
            
            # æ¤œè¨¼ãƒã‚§ãƒƒã‚¯é€šéç‡
            validation_passed = sum(1 for v in result.validation_checks.values() if v)
            validation_total = len(result.validation_checks)
            validation_ratio = validation_passed / validation_total if validation_total > 0 else 0.0
            validation_bonus = validation_ratio * 15.0
            
            # çœŸå®Ÿæ€§æŒ‡æ¨™ãƒœãƒ¼ãƒŠã‚¹
            truth_indicators_avg = np.mean(list(result.truth_indicators.values())) if result.truth_indicators else 0.0
            truth_bonus = truth_indicators_avg * 0.1
            
            # ç·åˆä¿¡é ¼åº¦è¨ˆç®—
            confidence = min(100.0, base_confidence + need_quality_bonus + validation_bonus + truth_bonus)
            
            return confidence
            
        except Exception as e:
            log.error(f"Need-basedä¿¡é ¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def _fallback_to_direct_analysis(self, dataset: QualityAssuredDataset, result: TruthAnalysisResult) -> TruthAnalysisResult:
        """Need Fileãªã—ã®å ´åˆã®ç›´æ¥åˆ†æ"""
        analysis_logger.warning("[PRIMARY_TRUTH] Need Fileæœªæ¤œå‡ºã€ç›´æ¥åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        
        # ç¾åœ¨ã®é…ç½®ã‹ã‚‰éœ€è¦ã‚’æ¨å®š
        current_staffing = self._analyze_current_staffing(dataset.data)
        
        # æ¨å®šéœ€è¦ï¼ˆç¾åœ¨é…ç½®ã®1.2å€ã‚’é©æ­£ã¨ã™ã‚‹ï¼‰
        estimated_demands = {}
        for time_slot, current in current_staffing["time_distribution"].items():
            estimated_demands[time_slot] = current * 1.2
        
        # ä¸è¶³è¨ˆç®—
        result.shortage_by_time = self._calculate_precise_time_shortages(
            estimated_demands, 
            current_staffing["time_distribution"]
        )
        
        result.total_shortage_hours = sum(result.shortage_by_time.values())
        
        # ä¿¡é ¼åº¦ã¯ä½ä¸‹
        result.truth_indicators["estimation_based"] = True
        result.processing_notes.append("Need Fileæœªæ¤œå‡ºã®ãŸã‚æ¨å®šå€¤ã«ã‚ˆã‚‹åˆ†æ")
        
        return result
    
    def _detect_coverage_gaps(self, time_distribution: Dict[str, float]) -> List[str]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º"""
        gaps = []
        
        for time_slot, staff_count in time_distribution.items():
            if staff_count == 0:
                gaps.append(time_slot)
        
        return gaps
    
    def _is_time_column(self, col: Any) -> bool:
        """æ™‚é–“åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(':' in col_str and any(c.isdigit() for c in col_str))


class PatternBasedValidationEngine:
    """Pattern-basedæ¤œè¨¼åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆéšå±¤2: Validation Engineï¼‰"""
    
    def __init__(self):
        self.validation_methods = ["time_series", "statistical", "trend_analysis"]
    
    def validate(self, primary_result: TruthAnalysisResult, dataset: QualityAssuredDataset) -> TruthAnalysisResult:
        """Pattern-basedæ¤œè¨¼åˆ†æ"""
        analysis_logger.info("[VALIDATION_ENGINE] Pattern-basedæ¤œè¨¼é–‹å§‹")
        
        validation_result = TruthAnalysisResult("pattern_based_validation")
        
        try:
            # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼
            validation_result = self._time_series_validation(primary_result, dataset, validation_result)
            
            # çµ±è¨ˆçš„å¦¥å½“æ€§æ¤œè¨¼
            validation_result = self._statistical_validation(primary_result, dataset, validation_result)
            
            # ãƒˆãƒ¬ãƒ³ãƒ‰æ•´åˆæ€§æ¤œè¨¼
            validation_result = self._trend_validation(primary_result, dataset, validation_result)
            
            # æ¤œè¨¼ä¿¡é ¼åº¦è¨ˆç®—
            validation_result.confidence_score = self._calculate_validation_confidence(primary_result, validation_result)
            validation_result.confidence_level = validation_result.calculate_confidence_level()
            
            analysis_logger.info(f"[VALIDATION_ENGINE] å®Œäº†: ä¿¡é ¼åº¦{validation_result.confidence_score:.1f}%")
            
            return validation_result
            
        except Exception as e:
            log.error(f"Pattern-basedæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            validation_result.processing_notes.append(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return validation_result
    
    def _time_series_validation(
        self, 
        primary_result: TruthAnalysisResult, 
        dataset: QualityAssuredDataset,
        validation_result: TruthAnalysisResult
    ) -> TruthAnalysisResult:
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œè¨¼"""
        
        try:
            # æ™‚é–“åˆ¥ä¸è¶³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¦¥å½“æ€§æ¤œè¨¼
            time_shortages = primary_result.shortage_by_time
            
            if time_shortages:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
                shortage_values = list(time_shortages.values())
                
                # æ¥µç«¯ãªå€¤ã®æ¤œå‡º
                shortage_mean = np.mean(shortage_values)
                shortage_std = np.std(shortage_values)
                
                extreme_values = [v for v in shortage_values if abs(v - shortage_mean) > 3 * shortage_std]
                validation_result.validation_checks["no_extreme_outliers"] = len(extreme_values) == 0
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ»‘ã‚‰ã‹ã•æ¤œè¨¼
                if len(shortage_values) > 2:
                    smoothness_score = self._calculate_pattern_smoothness(shortage_values)
                    validation_result.truth_indicators["pattern_smoothness"] = smoothness_score
                    validation_result.validation_checks["pattern_smooth"] = smoothness_score > 0.6
                
                validation_result.evidence_data["time_series_validation"] = {
                    "mean_shortage": shortage_mean,
                    "std_shortage": shortage_std,
                    "extreme_values_count": len(extreme_values),
                    "pattern_consistency": "consistent" if len(extreme_values) == 0 else "inconsistent"
                }
            
            return validation_result
            
        except Exception as e:
            log.error(f"æ™‚ç³»åˆ—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return validation_result
    
    def _statistical_validation(
        self,
        primary_result: TruthAnalysisResult,
        dataset: QualityAssuredDataset,
        validation_result: TruthAnalysisResult
    ) -> TruthAnalysisResult:
        """çµ±è¨ˆçš„å¦¥å½“æ€§æ¤œè¨¼"""
        
        try:
            # ä¸è¶³æ™‚é–“ã®çµ±è¨ˆçš„åˆ†å¸ƒæ¤œè¨¼
            total_shortage = primary_result.total_shortage_hours
            
            # ç¾å®Ÿçš„ç¯„å›²å†…ã‹ãƒã‚§ãƒƒã‚¯
            data_size = dataset.data.shape[0]
            max_reasonable_shortage = data_size * 8  # 1äººã‚ãŸã‚Šæœ€å¤§8æ™‚é–“ä¸è¶³ã¨ä»®å®š
            
            validation_result.validation_checks["shortage_within_reasonable_range"] = (
                0 <= total_shortage <= max_reasonable_shortage
            )
            
            # è·ç¨®é–“ãƒãƒ©ãƒ³ã‚¹æ¤œè¨¼
            role_shortages = primary_result.shortage_by_role
            if role_shortages:
                role_values = list(role_shortages.values())
                if len(role_values) > 1:
                    # ã‚¸ãƒ‹ä¿‚æ•°ã«ã‚ˆã‚‹ä¸å¹³ç­‰åº¦æ¸¬å®š
                    gini_coefficient = self._calculate_gini_coefficient(role_values)
                    validation_result.truth_indicators["role_balance_gini"] = gini_coefficient
                    validation_result.validation_checks["role_balance_reasonable"] = gini_coefficient < 0.8
            
            # ãƒ‡ãƒ¼ã‚¿ã¨ã®æ•´åˆæ€§æ¤œè¨¼
            if 'role' in dataset.data.columns:
                actual_role_distribution = dataset.data['role'].value_counts(normalize=True)
                shortage_distribution = {k: v/sum(role_shortages.values()) for k, v in role_shortages.items() if sum(role_shortages.values()) > 0}
                
                # åˆ†å¸ƒã®é¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆKLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹çš„ãªè€ƒãˆæ–¹ï¼‰
                distribution_similarity = self._calculate_distribution_similarity(
                    actual_role_distribution.to_dict(), 
                    shortage_distribution
                )
                validation_result.truth_indicators["distribution_similarity"] = distribution_similarity
                validation_result.validation_checks["distribution_consistent"] = distribution_similarity > 0.7
            
            return validation_result
            
        except Exception as e:
            log.error(f"çµ±è¨ˆçš„æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return validation_result
    
    def _trend_validation(
        self,
        primary_result: TruthAnalysisResult,
        dataset: QualityAssuredDataset,
        validation_result: TruthAnalysisResult
    ) -> TruthAnalysisResult:
        """ãƒˆãƒ¬ãƒ³ãƒ‰æ•´åˆæ€§æ¤œè¨¼"""
        
        try:
            # æ™‚é–“å¸¯åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰ã®å¦¥å½“æ€§
            time_shortages = primary_result.shortage_by_time
            
            if len(time_shortages) >= 3:
                # ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯ã®å¦¥å½“æ€§æ¤œè¨¼
                sorted_times = sorted(time_shortages.items(), key=lambda x: x[1], reverse=True)
                peak_times = [t[0] for t in sorted_times[:3]]  # ä¸Šä½3æ™‚é–“å¸¯
                
                # ä¸€èˆ¬çš„ãªãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã®æ•´åˆæ€§
                expected_peak_patterns = ["07:00", "08:00", "17:00", "18:00", "22:00", "23:00"]
                peak_pattern_match = any(
                    any(expected in peak_time for expected in expected_peak_patterns)
                    for peak_time in peak_times
                )
                
                validation_result.validation_checks["peak_pattern_realistic"] = peak_pattern_match
                validation_result.evidence_data["detected_peak_times"] = peak_times
            
            # é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰æ¤œè¨¼ï¼ˆéå»ãƒ‡ãƒ¼ã‚¿ã¨ã®æ¯”è¼ƒï¼‰
            # æ³¨: å®Ÿè£…ã§ã¯éå»ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ç¾åœ¨ã¯åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯ã®ã¿
            validation_result.validation_checks["trend_analysis_completed"] = True
            
            return validation_result
            
        except Exception as e:
            log.error(f"ãƒˆãƒ¬ãƒ³ãƒ‰æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")  
            return validation_result
    
    def _calculate_pattern_smoothness(self, values: List[float]) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ»‘ã‚‰ã‹ã•è¨ˆç®—"""
        if len(values) < 3:
            return 1.0
        
        # éš£æ¥ã™ã‚‹å€¤ã®å·®ã®åˆ†æ•£ã‚’åŸºæº–ã«ã—ãŸæ»‘ã‚‰ã‹ã•ã‚¹ã‚³ã‚¢
        differences = [abs(values[i+1] - values[i]) for i in range(len(values)-1)]
        if not differences:
            return 1.0
        
        diff_std = np.std(differences)
        diff_mean = np.mean(differences)
        
        # æ¨™æº–åå·®ãŒå¹³å‡ã«å¯¾ã—ã¦å°ã•ã„ã»ã©æ»‘ã‚‰ã‹
        smoothness = 1.0 / (1.0 + diff_std / max(diff_mean, 0.01))
        return min(1.0, smoothness)
    
    def _calculate_gini_coefficient(self, values: List[float]) -> float:
        """ã‚¸ãƒ‹ä¿‚æ•°è¨ˆç®—"""
        if not values or len(values) < 2:
            return 0.0
        
        values = sorted([v for v in values if v >= 0])
        n = len(values)
        
        if sum(values) == 0:
            return 0.0
        
        cumulative = np.cumsum(values)
        return (n + 1 - 2 * sum((n + 1 - i) * v for i, v in enumerate(values, 1))) / (n * sum(values))
    
    def _calculate_distribution_similarity(self, dist1: Dict[str, float], dist2: Dict[str, float]) -> float:
        """åˆ†å¸ƒã®é¡ä¼¼æ€§è¨ˆç®—"""
        try:
            all_keys = set(dist1.keys()) | set(dist2.keys())
            
            similarity_sum = 0.0
            for key in all_keys:
                val1 = dist1.get(key, 0.0)
                val2 = dist2.get(key, 0.0)
                similarity_sum += min(val1, val2)
            
            return similarity_sum
            
        except Exception as e:
            log.error(f"åˆ†å¸ƒé¡ä¼¼æ€§è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def _calculate_validation_confidence(
        self, 
        primary_result: TruthAnalysisResult, 
        validation_result: TruthAnalysisResult
    ) -> float:
        """æ¤œè¨¼ä¿¡é ¼åº¦è¨ˆç®—"""
        
        try:
            # ãƒ—ãƒ©ã‚¤ãƒãƒªçµæœã®ä¿¡é ¼åº¦ã‚’ãƒ™ãƒ¼ã‚¹
            base_confidence = primary_result.confidence_score
            
            # æ¤œè¨¼ãƒã‚§ãƒƒã‚¯é€šéç‡
            checks_passed = sum(1 for v in validation_result.validation_checks.values() if v)
            total_checks = len(validation_result.validation_checks)
            check_ratio = checks_passed / total_checks if total_checks > 0 else 0.0
            
            # æ¤œè¨¼ã«ã‚ˆã‚‹ä¿¡é ¼åº¦èª¿æ•´
            if check_ratio >= 0.8:
                confidence_adjustment = 5.0  # ãƒœãƒ¼ãƒŠã‚¹
            elif check_ratio >= 0.6:
                confidence_adjustment = 0.0  # å¤‰æ›´ãªã—
            else:
                confidence_adjustment = -10.0  # ãƒšãƒŠãƒ«ãƒ†ã‚£
            
            # çœŸå®Ÿæ€§æŒ‡æ¨™ã«ã‚ˆã‚‹èª¿æ•´
            truth_indicators_avg = np.mean(list(validation_result.truth_indicators.values())) if validation_result.truth_indicators else 0.0
            truth_adjustment = truth_indicators_avg * 0.1
            
            final_confidence = min(100.0, max(0.0, base_confidence + confidence_adjustment + truth_adjustment))
            
            return final_confidence
            
        except Exception as e:
            log.error(f"æ¤œè¨¼ä¿¡é ¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0


class ProportionalFallbackEngine:
    """Proportionalè£œå®Œåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆéšå±¤3: Fallback Engineï¼‰"""
    
    def __init__(self):
        self.fallback_mode = "proportional_estimation"
    
    def supplement(self, dataset: QualityAssuredDataset, primary_confidence: float) -> TruthAnalysisResult:
        """æŒ‰åˆ†è£œå®Œåˆ†æ"""
        analysis_logger.info("[FALLBACK_ENGINE] Proportionalè£œå®Œåˆ†æé–‹å§‹")
        
        result = TruthAnalysisResult("proportional_fallback")
        
        try:
            # æŒ‰åˆ†æ–¹å¼ã«ã‚ˆã‚‹åŸºæœ¬åˆ†æ
            result = self._proportional_shortage_analysis(dataset, result)
            
            # è£œå®Œä¿¡é ¼åº¦è¨ˆç®—
            result.confidence_score = self._calculate_fallback_confidence(primary_confidence, result)
            result.confidence_level = result.calculate_confidence_level()
            
            result.processing_notes.append("ãƒ‡ãƒ¼ã‚¿å“è³ªä¸è¶³ã®ãŸã‚æŒ‰åˆ†æ–¹å¼ã«ã‚ˆã‚‹è£œå®Œåˆ†æ")
            analysis_logger.info(f"[FALLBACK_ENGINE] å®Œäº†: ä¿¡é ¼åº¦{result.confidence_score:.1f}%")
            
            return result
            
        except Exception as e:
            log.error(f"æŒ‰åˆ†è£œå®Œåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            result.processing_notes.append(f"è£œå®Œåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return result
    
    def _proportional_shortage_analysis(self, dataset: QualityAssuredDataset, result: TruthAnalysisResult) -> TruthAnalysisResult:
        """æŒ‰åˆ†æ–¹å¼ä¸è¶³åˆ†æ"""
        
        try:
            data = dataset.data
            filtered_data = apply_rest_exclusion_filter(data, "proportional_analysis")
            
            if filtered_data.empty:
                return result
            
            # å…¨ä½“ã®æ¨å®šä¸è¶³æ™‚é–“ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
            total_records = len(filtered_data)
            estimated_total_shortage = total_records * 0.5  # 1ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚ãŸã‚Š0.5æ™‚é–“ä¸è¶³ã¨ä»®å®š
            
            # è·ç¨®åˆ¥æŒ‰åˆ†
            if 'role' in filtered_data.columns:
                role_counts = filtered_data['role'].value_counts()
                for role, count in role_counts.items():
                    proportion = count / total_records
                    result.shortage_by_role[role] = estimated_total_shortage * proportion
            
            # é›‡ç”¨å½¢æ…‹åˆ¥æŒ‰åˆ†
            if 'employment' in filtered_data.columns:
                employment_counts = filtered_data['employment'].value_counts()
                for employment, count in employment_counts.items():
                    proportion = count / total_records
                    result.shortage_by_employment[employment] = estimated_total_shortage * proportion
            
            # æ™‚é–“åˆ¥æŒ‰åˆ†ï¼ˆæ™‚é–“åˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆï¼‰
            time_columns = [col for col in data.columns if self._is_time_column(col)]
            if time_columns:
                time_slot_shortage = estimated_total_shortage / len(time_columns)
                for col in time_columns:
                    result.shortage_by_time[str(col)] = time_slot_shortage
            
            result.total_shortage_hours = estimated_total_shortage
            
            # è£œå®Œç‰¹æœ‰ã®çœŸå®Ÿæ€§æŒ‡æ¨™
            result.truth_indicators["proportional_estimation"] = True
            result.truth_indicators["data_driven_confidence"] = min(100.0, total_records * 2)  # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã«åŸºã¥ãä¿¡é ¼åº¦
            
            return result
            
        except Exception as e:
            log.error(f"æŒ‰åˆ†åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return result
    
    def _calculate_fallback_confidence(self, primary_confidence: float, result: TruthAnalysisResult) -> float:
        """è£œå®Œä¿¡é ¼åº¦è¨ˆç®—"""
        
        # æŒ‰åˆ†æ–¹å¼ã¯åŸºæœ¬çš„ã«ä½ä¿¡é ¼åº¦
        base_fallback_confidence = 45.0
        
        # ãƒ‡ãƒ¼ã‚¿é‡ã«ã‚ˆã‚‹èª¿æ•´
        data_driven_confidence = result.truth_indicators.get("data_driven_confidence", 0.0)
        data_adjustment = min(15.0, data_driven_confidence * 0.1)
        
        # ãƒ—ãƒ©ã‚¤ãƒãƒªåˆ†æã®å¤±æ•—åº¦ã«ã‚ˆã‚‹èª¿æ•´
        primary_failure_penalty = max(0.0, (70.0 - primary_confidence) * 0.1)
        
        final_confidence = max(10.0, base_fallback_confidence + data_adjustment - primary_failure_penalty)
        
        return min(60.0, final_confidence)  # æŒ‰åˆ†æ–¹å¼ã®æœ€å¤§ä¿¡é ¼åº¦ã¯60%
    
    def _is_time_column(self, col: Any) -> bool:
        """æ™‚é–“åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(':' in col_str and any(c.isdigit() for c in col_str))


class ComprehensiveTruthResult:
    """ç·åˆçœŸå®Ÿåˆ†æçµæœ"""
    
    def __init__(self):
        self.primary_result: Optional[TruthAnalysisResult] = None
        self.validation_result: Optional[TruthAnalysisResult] = None
        self.fallback_result: Optional[TruthAnalysisResult] = None
        
        self.final_analysis: TruthAnalysisResult = TruthAnalysisResult("comprehensive_truth")
        self.analysis_method_used: str = "unknown"
        self.confidence_level: AnalysisConfidenceLevel = AnalysisConfidenceLevel.VERY_LOW
        
        self.recommendation: str = ""
        self.analysis_summary: Dict[str, Any] = {}
        self.processing_metadata: Dict[str, Any] = {}
    
    def generate_comprehensive_report(self) -> str:
        """ç·åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
ğŸ¯ éšå±¤çš„çœŸå®Ÿåˆ†æ - ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
{'='*70}
ğŸ“Š æœ€çµ‚ä¿¡é ¼åº¦: {self.final_analysis.confidence_score:.1f}% ({self.confidence_level.value})
ğŸ” æ¡ç”¨åˆ†ææ‰‹æ³•: {self.analysis_method_used}

ğŸ“‹ åˆ†æçµæœã‚µãƒãƒªãƒ¼:
â”œâ”€ ç·ä¸è¶³æ™‚é–“: {self.final_analysis.total_shortage_hours:.1f}æ™‚é–“
â”œâ”€ è·ç¨®åˆ¥ä¸è¶³: {len(self.final_analysis.shortage_by_role)}è·ç¨®
â”œâ”€ æ™‚é–“åˆ¥ä¸è¶³: {len(self.final_analysis.shortage_by_time)}æ™‚é–“å¸¯
â””â”€ é›‡ç”¨å½¢æ…‹åˆ¥ä¸è¶³: {len(self.final_analysis.shortage_by_employment)}å½¢æ…‹

ğŸ† å„éšå±¤ã®è©•ä¾¡:
â”œâ”€ éšå±¤1 (Need-based): {self.primary_result.confidence_score:.1f}% if self.primary_result else 'N/A'
â”œâ”€ éšå±¤2 (Pattern-based): {self.validation_result.confidence_score:.1f}% if self.validation_result else 'N/A'
â””â”€ éšå±¤3 (Proportional): {self.fallback_result.confidence_score:.1f}% if self.fallback_result else 'N/A'

ğŸ’¡ æ¨å¥¨äº‹é …: {self.recommendation}

ğŸ”§ å‡¦ç†ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿:
â””â”€ åˆ†æå®Œäº†æ™‚åˆ»: {self.final_analysis.processing_timestamp.strftime('%Y-%m-%d %H:%M:%S')}
"""
        return report


class HierarchicalTruthAnalyzer:
    """éšå±¤çš„çœŸå®Ÿåˆ†æã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.primary_engine = NeedBasedTruthEngine()
        self.validation_engine = PatternBasedValidationEngine()
        self.fallback_engine = ProportionalFallbackEngine()
        
        self.analysis_threshold = {
            "primary_minimum": 70.0,
            "validation_minimum": 60.0,
            "fallback_acceptable": 40.0
        }
    
    def analyze_with_confidence(
        self, 
        dataset: QualityAssuredDataset, 
        decomposition: DecompositionResult
    ) -> ComprehensiveTruthResult:
        """ä¿¡é ¼åº¦ä»˜ãéšå±¤åˆ†æ"""
        analysis_logger.info("[HIERARCHICAL_TRUTH] éšå±¤çš„çœŸå®Ÿåˆ†æé–‹å§‹")
        
        comprehensive_result = ComprehensiveTruthResult()
        
        try:
            # éšå±¤1: Need-basedçœŸå®Ÿåˆ†æ
            comprehensive_result.primary_result = self.primary_engine.analyze(dataset, decomposition)
            
            # éšå±¤2: Pattern-basedæ¤œè¨¼ï¼ˆãƒ—ãƒ©ã‚¤ãƒãƒªãŒä¸€å®šä»¥ä¸Šã®å ´åˆï¼‰
            if comprehensive_result.primary_result.confidence_score >= self.analysis_threshold["validation_minimum"]:
                comprehensive_result.validation_result = self.validation_engine.validate(
                    comprehensive_result.primary_result, dataset
                )
            
            # éšå±¤3: Proportionalè£œå®Œï¼ˆå¿…è¦ãªå ´åˆã®ã¿ï¼‰
            if comprehensive_result.primary_result.confidence_score < self.analysis_threshold["primary_minimum"]:
                comprehensive_result.fallback_result = self.fallback_engine.supplement(
                    dataset, comprehensive_result.primary_result.confidence_score
                )
            
            # æœ€çµ‚çµæœçµ±åˆ
            comprehensive_result = self._integrate_analysis_results(comprehensive_result)
            
            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            comprehensive_result.recommendation = self._generate_recommendation(comprehensive_result)
            
            analysis_logger.info(f"[HIERARCHICAL_TRUTH] å®Œäº†: æœ€çµ‚ä¿¡é ¼åº¦{comprehensive_result.final_analysis.confidence_score:.1f}%")
            analysis_logger.info(comprehensive_result.generate_comprehensive_report())
            
            return comprehensive_result
            
        except Exception as e:
            log.error(f"éšå±¤çš„çœŸå®Ÿåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            comprehensive_result.final_analysis.processing_notes.append(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return comprehensive_result
    
    def _integrate_analysis_results(self, comprehensive_result: ComprehensiveTruthResult) -> ComprehensiveTruthResult:
        """åˆ†æçµæœçµ±åˆ"""
        
        try:
            primary = comprehensive_result.primary_result
            validation = comprehensive_result.validation_result
            fallback = comprehensive_result.fallback_result
            
            # æœ€é«˜ä¿¡é ¼åº¦ã®åˆ†æçµæœã‚’æ¡ç”¨
            candidates = []
            if primary:
                candidates.append(("primary", primary))
            if validation:
                candidates.append(("validation", validation))  
            if fallback:
                candidates.append(("fallback", fallback))
            
            if not candidates:
                comprehensive_result.analysis_method_used = "none"
                return comprehensive_result
            
            # ä¿¡é ¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            candidates.sort(key=lambda x: x[1].confidence_score, reverse=True)
            best_method, best_result = candidates[0]
            
            # æœ€è‰¯çµæœã‚’æœ€çµ‚çµæœã¨ã—ã¦æ¡ç”¨
            comprehensive_result.final_analysis = best_result
            comprehensive_result.analysis_method_used = best_method
            comprehensive_result.confidence_level = best_result.confidence_level
            
            # åˆ†æã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            comprehensive_result.analysis_summary = {
                "methods_evaluated": len(candidates),
                "selected_method": best_method,
                "confidence_scores": {method: result.confidence_score for method, result in candidates},
                "validation_performed": validation is not None,
                "fallback_required": fallback is not None
            }
            
            return comprehensive_result
            
        except Exception as e:
            log.error(f"çµæœçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            return comprehensive_result
    
    def _generate_recommendation(self, comprehensive_result: ComprehensiveTruthResult) -> str:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        
        final_confidence = comprehensive_result.final_analysis.confidence_score
        method_used = comprehensive_result.analysis_method_used
        
        if final_confidence >= 90:
            return f"ä¿¡é ¼åº¦{final_confidence:.1f}%ã§é«˜ç²¾åº¦åˆ†æå®Œäº†ã€‚{method_used}æ‰‹æ³•ã«ã‚ˆã‚‹çµæœã‚’æ„æ€æ±ºå®šã«ä½¿ç”¨å¯èƒ½ã€‚"
        elif final_confidence >= 75:
            return f"ä¿¡é ¼åº¦{final_confidence:.1f}%ã§è‰¯å¥½ãªåˆ†æçµæœã€‚{method_used}æ‰‹æ³•ã«ã‚ˆã‚‹çµæœã¯ä¿¡é ¼æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
        elif final_confidence >= 60:
            return f"ä¿¡é ¼åº¦{final_confidence:.1f}%ã§ä¸­ç¨‹åº¦ã®åˆ†æçµæœã€‚{method_used}æ‰‹æ³•ã«ã‚ˆã‚‹çµæœã¯å‚è€ƒã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã€‚"
        elif final_confidence >= 40:
            return f"ä¿¡é ¼åº¦{final_confidence:.1f}%ã§ä½å“è³ªãªåˆ†æçµæœã€‚ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        else:
            return f"ä¿¡é ¼åº¦{final_confidence:.1f}%ã§åˆ†æçµæœã®ä¿¡é ¼æ€§ãŒä¸ååˆ†ã€‚ãƒ‡ãƒ¼ã‚¿ã®å†åé›†ãƒ»ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãŒå¿…è¦ã§ã™ã€‚"


# ä¾¿åˆ©é–¢æ•°
def analyze_with_hierarchical_truth(
    dataset: QualityAssuredDataset, 
    decomposition: DecompositionResult
) -> ComprehensiveTruthResult:
    """éšå±¤çš„çœŸå®Ÿåˆ†æï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    analyzer = HierarchicalTruthAnalyzer()
    return analyzer.analyze_with_confidence(dataset, decomposition)


# Export
__all__ = [
    "TruthAnalysisResult",
    "ComprehensiveTruthResult", 
    "HierarchicalTruthAnalyzer",
    "analyze_with_hierarchical_truth"
]