#!/usr/bin/env python3
"""
Truth-Assured Data Decomposer
çœŸå®Ÿæ€§ä¿è¨¼ãƒ‡ãƒ¼ã‚¿åˆ†è§£ã‚·ã‚¹ãƒ†ãƒ  - Need Fileå„ªå…ˆã®é«˜ç²¾åº¦åˆ†è§£
"""

from __future__ import annotations  # å‹ãƒ’ãƒ³ãƒˆäº’æ›æ€§ã®ãŸã‚ä¿æŒ

# import json  # JSONå‡ºåŠ›ã§å°†æ¥ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
import logging
from datetime import datetime  # , timedelta  # timedelta ã¯ç¾åœ¨æœªä½¿ç”¨
from pathlib import Path
from typing import Any, Dict, List, Optional
# from typing import Tuple  # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

import numpy as np
import pandas as pd
from pandas import DataFrame

# from .constants import DEFAULT_SLOT_MINUTES  # ç¾åœ¨æœªä½¿ç”¨ã ãŒè¨­å®šç³»ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
from .enhanced_data_ingestion import QualityAssuredDataset
from .utils import log

# Analysis logger
analysis_logger = logging.getLogger('analysis')


class NeedFileAnalysisResult:
    """Need Fileåˆ†æçµæœ"""
    
    def __init__(self):
        self.care_demands_by_time: Dict[str, float] = {}
        self.care_demands_by_role: Dict[str, float] = {}
        self.care_demands_by_day: Dict[str, float] = {}
        self.total_care_hours: float = 0.0
        self.peak_hours: List[str] = []
        self.low_hours: List[str] = []
        self.confidence_score: float = 0.0
        self.data_completeness: float = 0.0
        self.detected_patterns: List[Dict[str, Any]] = []


class StaffConstraintLearning:
    """ã‚¹ã‚¿ãƒƒãƒ•åˆ¶ç´„å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.learned_constraints: Dict[str, Dict[str, Any]] = {}
        self.constraint_confidence: Dict[str, float] = {}
        self.learning_metadata: Dict[str, Any] = {}
    
    def learn_from_actual_patterns(self, schedule_data: DataFrame) -> Dict[str, Dict[str, Any]]:
        """å®Ÿéš›ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰åˆ¶ç´„ã‚’å­¦ç¿’"""
        analysis_logger.info("[CONSTRAINT_LEARNING] åˆ¶ç´„å­¦ç¿’é–‹å§‹")
        
        try:
            constraints = {}
            
            if 'staff' not in schedule_data.columns:
                return constraints
            
            # å„ã‚¹ã‚¿ãƒƒãƒ•ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            for staff_name in schedule_data['staff'].unique():
                if pd.isna(staff_name) or self._is_rest_marker(staff_name):
                    continue
                
                staff_data = schedule_data[schedule_data['staff'] == staff_name]
                staff_constraints = self._analyze_staff_patterns(staff_name, staff_data)
                
                if staff_constraints:
                    constraints[staff_name] = staff_constraints
                    analysis_logger.debug(f"[CONSTRAINT] {staff_name}: {len(staff_constraints)}å€‹ã®åˆ¶ç´„ã‚’å­¦ç¿’")
            
            self.learned_constraints = constraints
            analysis_logger.info(f"[CONSTRAINT_LEARNING] å®Œäº†: {len(constraints)}åã®åˆ¶ç´„ã‚’å­¦ç¿’")
            
            return constraints
            
        except Exception as e:
            log.error(f"åˆ¶ç´„å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _analyze_staff_patterns(self, staff_name: str, staff_data: DataFrame) -> Dict[str, Any]:
        """å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        constraints = {}
        
        try:
            # å‹¤å‹™å¯èƒ½æ™‚é–“å¸¯åˆ†æ
            if 'time_slot' in staff_data.columns:
                working_hours = staff_data['time_slot'].unique()
                constraints['preferred_hours'] = list(working_hours)
                
                # å¤œå‹¤å¯¾å¿œå¯å¦åˆ¤å®š
                night_hours = [h for h in working_hours if self._is_night_hour(h)]
                constraints['night_shift_capable'] = len(night_hours) > 0
            
            # å‹¤å‹™é »åº¦åˆ†æ
            if 'date' in staff_data.columns or 'ds' in staff_data.columns:
                date_col = 'date' if 'date' in staff_data.columns else 'ds'
                working_dates = pd.to_datetime(staff_data[date_col]).dt.date.unique()
                
                # é€±å‹¤å‹™æ—¥æ•°æ¨å®š
                date_range = max(working_dates) - min(working_dates)
                weeks = max(1, date_range.days / 7)
                avg_days_per_week = len(working_dates) / weeks
                constraints['avg_working_days_per_week'] = round(avg_days_per_week, 1)
            
            # è·ç¨®æƒ…å ±
            if 'role' in staff_data.columns:
                roles = staff_data['role'].unique()
                constraints['primary_role'] = roles[0] if len(roles) > 0 else None
                constraints['multi_role_capable'] = len(roles) > 1
            
            # é›‡ç”¨å½¢æ…‹æƒ…å ±
            if 'employment' in staff_data.columns:
                employment_types = staff_data['employment'].unique()
                constraints['employment_type'] = employment_types[0] if len(employment_types) > 0 else None
            
            # é€£ç¶šå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            consecutive_work_days = self._analyze_consecutive_patterns(staff_data)
            if consecutive_work_days:
                constraints['max_consecutive_days'] = consecutive_work_days
            
            return constraints
            
        except Exception as e:
            log.error(f"ã‚¹ã‚¿ãƒƒãƒ•ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼ ({staff_name}): {e}")
            return {}
    
    def _is_rest_marker(self, value: Any) -> bool:
        """ä¼‘æš‡ãƒãƒ¼ã‚«ãƒ¼åˆ¤å®š"""
        if pd.isna(value):
            return True
        
        rest_patterns = ['Ã—', 'X', 'x', 'ä¼‘', 'ä¼‘ã¿', 'ä¼‘æš‡', 'æ¬ ', 'æ¬ å‹¤', 'OFF', 'off', 'Off', '-', 'âˆ’', 'â€•']
        return str(value).strip() in rest_patterns
    
    def _is_night_hour(self, hour_str: str) -> bool:
        """å¤œå‹¤æ™‚é–“åˆ¤å®š"""
        try:
            if ':' in str(hour_str):
                hour = int(str(hour_str).split(':')[0])
                return hour >= 22 or hour <= 6
            return False
        except:
            return False
    
    def _analyze_consecutive_patterns(self, staff_data: DataFrame) -> Optional[int]:
        """é€£ç¶šå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            if 'date' not in staff_data.columns and 'ds' not in staff_data.columns:
                return None
            
            date_col = 'date' if 'date' in staff_data.columns else 'ds'
            working_dates = pd.to_datetime(staff_data[date_col]).dt.date
            working_dates_sorted = sorted(working_dates.unique())
            
            max_consecutive = 1
            current_consecutive = 1
            
            for i in range(1, len(working_dates_sorted)):
                if (working_dates_sorted[i] - working_dates_sorted[i-1]).days == 1:
                    current_consecutive += 1
                    max_consecutive = max(max_consecutive, current_consecutive)
                else:
                    current_consecutive = 1
            
            return max_consecutive if max_consecutive > 1 else None
            
        except Exception as e:
            log.error(f"é€£ç¶šå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None


class ContextualPatternDetector:
    """æ–‡è„ˆçš„ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.detected_patterns: List[Dict[str, Any]] = []
        self.facility_characteristics: Dict[str, Any] = {}
    
    def detect_facility_patterns(self, dataset: QualityAssuredDataset) -> Dict[str, Any]:
        """æ–½è¨­ç‰¹æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        analysis_logger.info("[PATTERN_DETECTION] æ–½è¨­ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºé–‹å§‹")
        
        try:
            patterns = {}
            data = dataset.data
            
            # æ–½è¨­è¦æ¨¡æ¨å®š
            patterns['facility_scale'] = self._estimate_facility_scale(data)
            
            # å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            patterns['shift_patterns'] = self._analyze_shift_patterns(data)
            
            # è·ç¨®æ§‹æˆåˆ†æ
            patterns['role_composition'] = self._analyze_role_composition(data)
            
            # æ™‚é–“å¸¯åˆ¥éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³
            patterns['time_demand_patterns'] = self._analyze_time_demand_patterns(data)
            
            # æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
            patterns['weekday_patterns'] = self._analyze_weekday_patterns(data)
            
            self.facility_characteristics = patterns
            analysis_logger.info("[PATTERN_DETECTION] å®Œäº†")
            
            return patterns
            
        except Exception as e:
            log.error(f"æ–½è¨­ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def _estimate_facility_scale(self, data: DataFrame) -> Dict[str, Any]:
        """æ–½è¨­è¦æ¨¡æ¨å®š"""
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•æ•°ã‚«ã‚¦ãƒ³ãƒˆ
            if 'staff' in data.columns:
                unique_staff = data[data['staff'].notna()]['staff'].unique()
                valid_staff = [s for s in unique_staff if not self._is_rest_marker(s)]
                staff_count = len(valid_staff)
            else:
                staff_count = 0
            
            # æ–½è¨­è¦æ¨¡åˆ†é¡
            if staff_count <= 10:
                scale_category = "small"  # å°è¦æ¨¡æ–½è¨­
            elif staff_count <= 30:
                scale_category = "medium"  # ä¸­è¦æ¨¡æ–½è¨­
            else:
                scale_category = "large"  # å¤§è¦æ¨¡æ–½è¨­
            
            return {
                "staff_count": staff_count,
                "scale_category": scale_category,
                "confidence": 0.9 if staff_count > 0 else 0.3
            }
            
        except Exception as e:
            log.error(f"æ–½è¨­è¦æ¨¡æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            return {"staff_count": 0, "scale_category": "unknown", "confidence": 0.0}
    
    def _analyze_shift_patterns(self, data: DataFrame) -> Dict[str, Any]:
        """å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            patterns = {"detected_shifts": [], "coverage_24h": False}
            
            # æ™‚é–“åˆ—ã®æ¤œå‡º
            time_columns = [col for col in data.columns if self._is_time_column(col)]
            
            if time_columns:
                # 24æ™‚é–“ã‚«ãƒãƒ¼åˆ¤å®š
                hours = set()
                for col in time_columns:
                    hour = self._extract_hour_from_column(col)
                    if hour is not None:
                        hours.add(hour)
                
                patterns["coverage_24h"] = len(hours) >= 16  # 16æ™‚é–“ä»¥ä¸Šãªã‚‰24hæ–½è¨­ã¨ã¿ãªã™
                patterns["covered_hours"] = len(hours)
                
                # ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¨å®š
                if any(h >= 22 or h <= 6 for h in hours):
                    patterns["detected_shifts"].append("night_shift")
                if any(6 <= h <= 10 for h in hours):
                    patterns["detected_shifts"].append("morning_shift")
                if any(10 <= h <= 18 for h in hours):
                    patterns["detected_shifts"].append("day_shift")
                if any(18 <= h <= 22 for h in hours):
                    patterns["detected_shifts"].append("evening_shift")
            
            return patterns
            
        except Exception as e:
            log.error(f"å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"detected_shifts": [], "coverage_24h": False}
    
    def _analyze_role_composition(self, data: DataFrame) -> Dict[str, Any]:
        """è·ç¨®æ§‹æˆåˆ†æ"""
        try:
            if 'role' not in data.columns:
                return {"roles": [], "primary_role": None}
            
            role_counts = data[data['role'].notna()]['role'].value_counts()
            total_role_records = role_counts.sum()
            
            role_composition = {}
            for role, count in role_counts.items():
                role_composition[role] = {
                    "count": count,
                    "percentage": (count / total_role_records * 100) if total_role_records > 0 else 0
                }
            
            primary_role = role_counts.index[0] if not role_counts.empty else None
            
            return {
                "roles": list(role_counts.index),
                "primary_role": primary_role,
                "composition": role_composition,
                "diversity_score": len(role_counts) / max(1, total_role_records) * 100
            }
            
        except Exception as e:
            log.error(f"è·ç¨®æ§‹æˆåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"roles": [], "primary_role": None}
    
    def _analyze_time_demand_patterns(self, data: DataFrame) -> Dict[str, Any]:
        """æ™‚é–“å¸¯åˆ¥éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            time_patterns = {"peak_hours": [], "low_hours": [], "demand_curve": {}}
            
            # æ™‚é–“åˆ—ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•é…ç½®æ•°ã‚’é›†è¨ˆ
            time_columns = [col for col in data.columns if self._is_time_column(col)]
            
            for col in time_columns:
                hour = self._extract_hour_from_column(col)
                if hour is not None:
                    # ãã®æ™‚é–“å¸¯ã®å®Ÿéš›ã®ã‚¹ã‚¿ãƒƒãƒ•é…ç½®æ•°
                    staff_count = data[col].notna().sum()
                    time_patterns["demand_curve"][f"{hour:02d}:00"] = staff_count
            
            if time_patterns["demand_curve"]:
                # ãƒ”ãƒ¼ã‚¯æ™‚é–“ã¨é–‘æ•£æ™‚é–“ã®ç‰¹å®š
                sorted_hours = sorted(time_patterns["demand_curve"].items(), key=lambda x: x[1])
                
                # ä¸Šä½25%ã‚’ãƒ”ãƒ¼ã‚¯ã€ä¸‹ä½25%ã‚’é–‘æ•£ã¨ã™ã‚‹
                total_hours = len(sorted_hours)
                peak_threshold = int(total_hours * 0.75)
                low_threshold = int(total_hours * 0.25)
                
                time_patterns["peak_hours"] = [h[0] for h in sorted_hours[peak_threshold:]]
                time_patterns["low_hours"] = [h[0] for h in sorted_hours[:low_threshold]]
            
            return time_patterns
            
        except Exception as e:
            log.error(f"æ™‚é–“å¸¯åˆ¥éœ€è¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"peak_hours": [], "low_hours": [], "demand_curve": {}}
    
    def _analyze_weekday_patterns(self, data: DataFrame) -> Dict[str, Any]:
        """æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        try:
            weekday_patterns = {"weekday_demand": {}, "weekend_different": False}
            
            # æ—¥ä»˜åˆ—ã®æ¤œå‡º
            date_columns = [col for col in data.columns if self._is_date_column(col)]
            
            for col in date_columns:
                date_obj = self._parse_date_column(col)
                if date_obj:
                    weekday = date_obj.strftime("%A")
                    staff_on_day = data[col].notna().sum()
                    
                    if weekday not in weekday_patterns["weekday_demand"]:
                        weekday_patterns["weekday_demand"][weekday] = []
                    weekday_patterns["weekday_demand"][weekday].append(staff_on_day)
            
            # å¹³æ—¥ã¨é€±æœ«ã®å·®ç•°åˆ†æ
            if weekday_patterns["weekday_demand"]:
                weekday_avg = np.mean([np.mean(v) for k, v in weekday_patterns["weekday_demand"].items() 
                                     if k not in ["Saturday", "Sunday"]])
                weekend_avg = np.mean([np.mean(v) for k, v in weekday_patterns["weekday_demand"].items() 
                                      if k in ["Saturday", "Sunday"]])
                
                if abs(weekday_avg - weekend_avg) > weekday_avg * 0.1:  # 10%ä»¥ä¸Šã®å·®
                    weekday_patterns["weekend_different"] = True
            
            return weekday_patterns
            
        except Exception as e:
            log.error(f"æ›œæ—¥åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"weekday_demand": {}, "weekend_different": False}
    
    def _is_rest_marker(self, value: Any) -> bool:
        """ä¼‘æš‡ãƒãƒ¼ã‚«ãƒ¼åˆ¤å®š"""
        if pd.isna(value):
            return True
        rest_patterns = ['Ã—', 'X', 'x', 'ä¼‘', 'ä¼‘ã¿', 'ä¼‘æš‡', 'æ¬ ', 'æ¬ å‹¤', 'OFF', 'off', 'Off', '-', 'âˆ’', 'â€•']
        return str(value).strip() in rest_patterns
    
    def _is_time_column(self, col: Any) -> bool:
        """æ™‚é–“åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(':' in col_str and any(c.isdigit() for c in col_str))
    
    def _is_date_column(self, col: Any) -> bool:
        """æ—¥ä»˜åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(re.search(r'\d{1,2}[/-]\d{1,2}', col_str))
    
    def _extract_hour_from_column(self, col: Any) -> Optional[int]:
        """åˆ—åã‹ã‚‰æ™‚é–“ã‚’æŠ½å‡º"""
        try:
            import re
            col_str = str(col)
            match = re.search(r'(\d{1,2}):', col_str)
            if match:
                return int(match.group(1))
            return None
        except:
            return None
    
    def _parse_date_column(self, col: Any) -> Optional[datetime]:
        """æ—¥ä»˜åˆ—ãƒ‘ãƒ¼ã‚¹"""
        try:
            import re
            col_str = str(col)
            match = re.search(r'(\d{1,2})[/-](\d{1,2})', col_str)
            if match:
                month, day = map(int, match.groups())
                year = datetime.now().year
                return datetime(year, month, day)
            return None
        except:
            return None


class AnomalyDetector:
    """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.detected_anomalies: List[Dict[str, Any]] = []
    
    def detect_anomalies(self, dataset: QualityAssuredDataset) -> List[Dict[str, Any]]:
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        analysis_logger.info("[ANOMALY_DETECTION] ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºé–‹å§‹")
        
        anomalies = []
        data = dataset.data
        
        try:
            # 1. æ¥µç«¯ãªåã‚Šã®æ¤œå‡º
            anomalies.extend(self._detect_extreme_imbalance(data))
            
            # 2. æ™‚é–“è»¸ã®ä¸é€£ç¶šæ€§æ¤œå‡º
            anomalies.extend(self._detect_time_discontinuity(data))
            
            # 3. ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ç•°å¸¸æ¤œå‡º
            anomalies.extend(self._detect_staffing_anomalies(data))
            
            # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªç•°å¸¸æ¤œå‡º
            anomalies.extend(self._detect_data_quality_issues(data))
            
            self.detected_anomalies = anomalies
            analysis_logger.info(f"[ANOMALY_DETECTION] å®Œäº†: {len(anomalies)}ä»¶ã®ç•°å¸¸ã‚’æ¤œå‡º")
            
            return anomalies
            
        except Exception as e:
            log.error(f"ç•°å¸¸æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _detect_extreme_imbalance(self, data: DataFrame) -> List[Dict[str, Any]]:
        """æ¥µç«¯ãªåã‚Šæ¤œå‡º"""
        anomalies = []
        
        try:
            # è·ç¨®åˆ¥åã‚Šæ¤œå‡º
            if 'role' in data.columns:
                role_counts = data[data['role'].notna()]['role'].value_counts()
                if not role_counts.empty:
                    max_count = role_counts.max()
                    min_count = role_counts.min()
                    
                    if max_count > min_count * 10:  # 10å€ä»¥ä¸Šã®å·®
                        anomalies.append({
                            "type": "extreme_role_imbalance",
                            "severity": "warning",
                            "description": f"è·ç¨®é–“ã®æ¥µç«¯ãªåã‚Š: æœ€å¤§{max_count}äºº vs æœ€å°{min_count}äºº",
                            "affected_roles": list(role_counts.index)
                        })
            
            return anomalies
            
        except Exception as e:
            log.error(f"æ¥µç«¯åã‚Šæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _detect_time_discontinuity(self, data: DataFrame) -> List[Dict[str, Any]]:
        """æ™‚é–“è»¸ä¸é€£ç¶šæ€§æ¤œå‡º"""
        anomalies = []
        
        try:
            # æ™‚é–“åˆ—ã®é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
            time_columns = [col for col in data.columns if self._is_time_column(col)]
            
            if len(time_columns) > 1:
                hours = []
                for col in time_columns:
                    hour = self._extract_hour_from_column(col)
                    if hour is not None:
                        hours.append(hour)
                
                hours.sort()
                gaps = []
                for i in range(1, len(hours)):
                    gap = hours[i] - hours[i-1]
                    if gap > 2:  # 2æ™‚é–“ä»¥ä¸Šã®ç©ºç™½
                        gaps.append((hours[i-1], hours[i]))
                
                if gaps:
                    anomalies.append({
                        "type": "time_discontinuity",
                        "severity": "warning", 
                        "description": f"æ™‚é–“è»¸ã«{len(gaps)}ç®‡æ‰€ã®ä¸é€£ç¶šã‚’æ¤œå‡º",
                        "gaps": gaps
                    })
            
            return anomalies
            
        except Exception as e:
            log.error(f"æ™‚é–“è»¸ä¸é€£ç¶šæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _detect_staffing_anomalies(self, data: DataFrame) -> List[Dict[str, Any]]:
        """ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ç•°å¸¸æ¤œå‡º"""
        anomalies = []
        
        try:
            if 'staff' in data.columns:
                # å˜ä¸€ã‚¹ã‚¿ãƒƒãƒ•ã«ã‚ˆã‚‹æ¥µç«¯ãªé›†ä¸­
                staff_workload = data[data['staff'].notna()]['staff'].value_counts()
                
                if not staff_workload.empty:
                    avg_workload = staff_workload.mean()
                    max_workload = staff_workload.max()
                    
                    if max_workload > avg_workload * 3:  # å¹³å‡ã®3å€ä»¥ä¸Š
                        overworked_staff = staff_workload[staff_workload > avg_workload * 2].index.tolist()
                        anomalies.append({
                            "type": "extreme_workload_concentration",
                            "severity": "warning",
                            "description": f"{len(overworked_staff)}åã®ã‚¹ã‚¿ãƒƒãƒ•ã«æ¥­å‹™ãŒé›†ä¸­",
                            "affected_staff": overworked_staff[:5]  # æœ€å¤§5åè¡¨ç¤º
                        })
            
            return anomalies
            
        except Exception as e:
            log.error(f"ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ç•°å¸¸æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _detect_data_quality_issues(self, data: DataFrame) -> List[Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œæ¤œå‡º"""
        anomalies = []
        
        try:
            # æ¥µç«¯ãªæ¬ æç‡
            total_cells = data.size
            missing_cells = data.isna().sum().sum()
            missing_rate = missing_cells / total_cells if total_cells > 0 else 0
            
            if missing_rate > 0.5:  # 50%ä»¥ä¸Šæ¬ æ
                anomalies.append({
                    "type": "high_missing_rate",
                    "severity": "error",
                    "description": f"ãƒ‡ãƒ¼ã‚¿æ¬ æç‡ãŒ{missing_rate:.1%}ã¨é«˜ã„",
                    "missing_rate": missing_rate
                })
            
            return anomalies
            
        except Exception as e:
            log.error(f"ãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _is_time_column(self, col: Any) -> bool:
        """æ™‚é–“åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(':' in col_str and any(c.isdigit() for c in col_str))
    
    def _extract_hour_from_column(self, col: Any) -> Optional[int]:
        """åˆ—åã‹ã‚‰æ™‚é–“æŠ½å‡º"""
        try:
            import re
            col_str = str(col)
            match = re.search(r'(\d{1,2}):', col_str)
            if match:
                return int(match.group(1))
            return None
        except:
            return None


class DecompositionResult:
    """åˆ†è§£çµæœã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.need_analysis: Optional[NeedFileAnalysisResult] = None
        self.learned_constraints: Dict[str, Dict[str, Any]] = {}
        self.facility_patterns: Dict[str, Any] = {}
        self.detected_anomalies: List[Dict[str, Any]] = []
        self.decomposition_metadata: Dict[str, Any] = {}
        self.confidence_score: float = 0.0
        self.processing_timestamp: datetime = datetime.now()
    
    def get_summary_report(self) -> str:
        """åˆ†è§£çµæœã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ"""
        report = f"""
ğŸ”§ ãƒ‡ãƒ¼ã‚¿åˆ†è§£çµæœã‚µãƒãƒªãƒ¼
{'='*60}
â° å‡¦ç†æ™‚åˆ»: {self.processing_timestamp.strftime('%Y-%m-%d %H:%M:%S')}
ğŸ¯ ä¿¡é ¼åº¦: {self.confidence_score:.1f}%

ğŸ“Š Needåˆ†æçµæœ:
â”œâ”€ ç·ã‚±ã‚¢æ™‚é–“: {self.need_analysis.total_care_hours:.1f}æ™‚é–“ (æ¨å®š)
â”œâ”€ ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯: {', '.join(self.need_analysis.peak_hours) if self.need_analysis else 'N/A'}
â””â”€ ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§: {self.need_analysis.data_completeness:.1f}% (æ¨å®š)

ğŸ§  å­¦ç¿’ã—ãŸåˆ¶ç´„: {len(self.learned_constraints)}ååˆ†
ğŸ“ˆ æ–½è¨­ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(self.facility_patterns)}ç¨®é¡æ¤œå‡º
âš ï¸  æ¤œå‡ºã—ãŸç•°å¸¸: {len(self.detected_anomalies)}ä»¶

ğŸ¥ æ–½è¨­ç‰¹æ€§:
â”œâ”€ è¦æ¨¡: {self.facility_patterns.get('facility_scale', {}).get('scale_category', 'unknown')}
â”œâ”€ 24æ™‚é–“é‹å–¶: {'Yes' if self.facility_patterns.get('shift_patterns', {}).get('coverage_24h', False) else 'No'}
â””â”€ ä¸»è¦è·ç¨®: {self.facility_patterns.get('role_composition', {}).get('primary_role', 'unknown')}
"""
        return report


class TruthAssuredDecomposer:
    """çœŸå®Ÿæ€§ä¿è¨¼ãƒ‡ãƒ¼ã‚¿åˆ†è§£ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.constraint_learner = StaffConstraintLearning() 
        self.pattern_detector = ContextualPatternDetector()
        self.anomaly_detector = AnomalyDetector()
    
    def decompose_with_truth_priority(self, dataset: QualityAssuredDataset) -> DecompositionResult:
        """Truthå„ªå…ˆãƒ‡ãƒ¼ã‚¿åˆ†è§£"""
        analysis_logger.info("[DECOMPOSITION] çœŸå®Ÿæ€§ä¿è¨¼ãƒ‡ãƒ¼ã‚¿åˆ†è§£é–‹å§‹")
        
        result = DecompositionResult()
        
        try:
            # Step 1: Need Fileå„ªå…ˆåˆ†æ
            result.need_analysis = self._perform_need_analysis(dataset)
            
            # Step 2: åˆ¶ç´„å­¦ç¿’
            result.learned_constraints = self.constraint_learner.learn_from_actual_patterns(dataset.data)
            
            # Step 3: æ–½è¨­ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
            result.facility_patterns = self.pattern_detector.detect_facility_patterns(dataset)
            
            # Step 4: ç•°å¸¸æ¤œå‡º
            result.detected_anomalies = self.anomaly_detector.detect_anomalies(dataset)
            
            # Step 5: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            result.decomposition_metadata = self._generate_metadata(dataset, result)
            
            # Step 6: ä¿¡é ¼åº¦è¨ˆç®—
            result.confidence_score = self._calculate_confidence(dataset, result) 
            
            analysis_logger.info(f"[DECOMPOSITION] å®Œäº†: ä¿¡é ¼åº¦{result.confidence_score:.1f}%")
            analysis_logger.info(result.get_summary_report())
            
            return result
            
        except Exception as e:
            log.error(f"ãƒ‡ãƒ¼ã‚¿åˆ†è§£ã‚¨ãƒ©ãƒ¼: {e}")
            result.decomposition_metadata["error"] = str(e)
            return result
    
    def _perform_need_analysis(self, dataset: QualityAssuredDataset) -> NeedFileAnalysisResult:
        """Need Fileåˆ†æå®Ÿè¡Œ"""
        analysis_logger.info("[NEED_ANALYSIS] Need Fileåˆ†æé–‹å§‹")
        
        need_result = NeedFileAnalysisResult()
        
        try:
            data = dataset.data
            
            # Need Fileã‚·ãƒ¼ãƒˆã®æ¤œå‡º
            if dataset.schema.get("has_need_file", False):
                # Need Fileå°‚ç”¨åˆ†æ
                need_result = self._analyze_need_file_data(dataset)
            else:
                # é€šå¸¸ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Needæ¨å®š
                need_result = self._estimate_needs_from_schedule(data)
            
            analysis_logger.info(f"[NEED_ANALYSIS] å®Œäº†: ä¿¡é ¼åº¦{need_result.confidence_score:.1f}%")
            
            return need_result
            
        except Exception as e:
            log.error(f"Needåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return need_result
    
    def _analyze_need_file_data(self, dataset: QualityAssuredDataset) -> NeedFileAnalysisResult:
        """å°‚ç”¨Need Fileãƒ‡ãƒ¼ã‚¿åˆ†æ"""
        result = NeedFileAnalysisResult()
        
        try:
            # Need Fileã‚’ç›´æ¥èª­ã¿è¾¼ã¿
            excel_path = Path(dataset.lineage.source_file)
            
            # Needã‚·ãƒ¼ãƒˆã‚’ç‰¹å®š
            excel_file = pd.ExcelFile(excel_path)
            need_sheet = None
            for sheet in excel_file.sheet_names:
                if "Need" in sheet or "need" in sheet:
                    need_sheet = sheet
                    break
            
            if need_sheet:
                need_data = pd.read_excel(excel_path, sheet_name=need_sheet)
                
                # æ™‚é–“åˆ¥éœ€è¦åˆ†æ
                time_columns = [col for col in need_data.columns if self._is_time_column(col)]
                for col in time_columns:
                    total_need = need_data[col].sum() if col in need_data.columns else 0
                    result.care_demands_by_time[str(col)] = total_need
                
                # è·ç¨®åˆ¥éœ€è¦åˆ†æ
                if 'role' in need_data.columns:
                    role_needs = need_data.groupby('role').sum().sum(axis=1)
                    result.care_demands_by_role = role_needs.to_dict()
                
                # ç·ã‚±ã‚¢æ™‚é–“è¨ˆç®—
                result.total_care_hours = sum(result.care_demands_by_time.values())
                
                # ãƒ”ãƒ¼ã‚¯ãƒ»é–‘æ•£æ™‚é–“å¸¯ç‰¹å®š
                if result.care_demands_by_time:
                    sorted_times = sorted(result.care_demands_by_time.items(), key=lambda x: x[1])
                    total_times = len(sorted_times)
                    
                    peak_threshold = int(total_times * 0.8)
                    low_threshold = int(total_times * 0.2)
                    
                    result.peak_hours = [t[0] for t in sorted_times[peak_threshold:]]
                    result.low_hours = [t[0] for t in sorted_times[:low_threshold]]
                
                result.confidence_score = 95.0  # Need Fileç›´æ¥åˆ†æã¯é«˜ä¿¡é ¼åº¦
                result.data_completeness = (need_data.notna().sum().sum() / need_data.size) * 100
                
            return result
            
        except Exception as e:
            log.error(f"Need Fileåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            result.confidence_score = 0.0
            return result
    
    def _estimate_needs_from_schedule(self, data: DataFrame) -> NeedFileAnalysisResult:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰Needæ¨å®š"""
        result = NeedFileAnalysisResult()
        
        try:
            # æ™‚é–“åˆ¥ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ã‹ã‚‰éœ€è¦æ¨å®š
            time_columns = [col for col in data.columns if self._is_time_column(col)]
            
            for col in time_columns:
                staff_count = data[col].notna().sum()
                # å®Ÿéš›ã®é…ç½®äººæ•°ã‹ã‚‰éœ€è¦ã‚’æ¨å®šï¼ˆ1.2å€ã‚’é©æ­£éœ€è¦ã¨ã™ã‚‹ï¼‰
                estimated_need = staff_count * 1.2
                result.care_demands_by_time[str(col)] = estimated_need
            
            # è·ç¨®åˆ¥æ¨å®š
            if 'role' in data.columns:
                role_distribution = data[data['role'].notna()]['role'].value_counts()
                total_estimated_need = sum(result.care_demands_by_time.values())
                
                for role, count in role_distribution.items():
                    proportion = count / role_distribution.sum()
                    result.care_demands_by_role[role] = total_estimated_need * proportion
            
            result.total_care_hours = sum(result.care_demands_by_time.values())
            result.confidence_score = 60.0  # æ¨å®šå€¤ãªã®ã§ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦
            result.data_completeness = (data.notna().sum().sum() / data.size) * 100
            
            return result
            
        except Exception as e:
            log.error(f"Needæ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            result.confidence_score = 0.0
            return result
    
    def _generate_metadata(self, dataset: QualityAssuredDataset, result: DecompositionResult) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        return {
            "decomposition_timestamp": datetime.now().isoformat(),
            "source_quality_score": dataset.quality_result.overall_score,
            "source_recommended_method": dataset.quality_result.recommended_analysis_method,
            "constraints_learned": len(result.learned_constraints),
            "patterns_detected": len(result.facility_patterns),
            "anomalies_found": len(result.detected_anomalies),
            "processing_mode": "need_priority" if dataset.schema.get("has_need_file") else "schedule_estimation"
        }
    
    def _calculate_confidence(self, dataset: QualityAssuredDataset, result: DecompositionResult) -> float:
        """ç·åˆä¿¡é ¼åº¦è¨ˆç®—"""
        try:
            base_score = dataset.quality_result.overall_score
            
            # Needåˆ†æã®ä¿¡é ¼åº¦
            need_confidence = result.need_analysis.confidence_score if result.need_analysis else 0.0
            
            # åˆ¶ç´„å­¦ç¿’ã®å……å®Ÿåº¦
            constraint_completeness = min(100.0, len(result.learned_constraints) * 10)
            
            # ç•°å¸¸æ¤œå‡ºã«ã‚ˆã‚‹æ¸›ç‚¹
            anomaly_penalty = min(20.0, len(result.detected_anomalies) * 5)
            
            # é‡ã¿ä»˜ãå¹³å‡
            confidence = (
                base_score * 0.4 +
                need_confidence * 0.4 +
                constraint_completeness * 0.2 -
                anomaly_penalty
            )
            
            return max(0.0, min(100.0, confidence))
            
        except Exception as e:
            log.error(f"ä¿¡é ¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0
    
    def _is_time_column(self, col: Any) -> bool:
        """æ™‚é–“åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(':' in col_str and any(c.isdigit() for c in col_str))


# ä¾¿åˆ©é–¢æ•°
def decompose_with_truth_assurance(dataset: QualityAssuredDataset) -> DecompositionResult:
    """çœŸå®Ÿæ€§ä¿è¨¼ãƒ‡ãƒ¼ã‚¿åˆ†è§£ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    decomposer = TruthAssuredDecomposer()
    return decomposer.decompose_with_truth_priority(dataset)


# Export
__all__ = [
    "DecompositionResult",
    "TruthAssuredDecomposer", 
    "decompose_with_truth_assurance"
]