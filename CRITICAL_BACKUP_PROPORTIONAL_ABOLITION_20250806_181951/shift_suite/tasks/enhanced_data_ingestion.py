#!/usr/bin/env python3
"""
Enhanced Data Ingestion System
çµ±åˆãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ã‚·ã‚¹ãƒ†ãƒ  - Truth-Driven Analysisã®åŸºç›¤
"""

from __future__ import annotations  # å‹ãƒ’ãƒ³ãƒˆäº’æ›æ€§ã®ãŸã‚ä¿æŒ

import json
import logging
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional
# from typing import Tuple  # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

import numpy as np  # æ•°å€¤è¨ˆç®—ã§ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ä¿æŒ
import pandas as pd
from pandas import DataFrame

from .constants import (
    # DEFAULT_SLOT_MINUTES,  # ç¾åœ¨æœªä½¿ç”¨ã ãŒå°†æ¥çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹å¯èƒ½æ€§
    QUALITY_PREVIEW_ROWS, QUALITY_SCORE_MAX, 
    QUALITY_PARTIAL_EXCEL_SCORE, QUALITY_LARGE_FILE_SIZE, QUALITY_LARGE_FILE_SCORE,
    QUALITY_SINGLE_SHEET_SCORE, QUALITY_STAFF_MISSING_SCORE, QUALITY_WEIGHTS
)
from .utils import log  # , write_meta  # write_meta ã¯ç¾åœ¨æœªä½¿ç”¨

# Analysis logger
analysis_logger = logging.getLogger('analysis')


class QualityAssessmentResult:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡çµæœ"""
    
    def __init__(self):
        self.overall_score: float = 0.0
        self.file_format_score: float = 0.0
        self.structure_score: float = 0.0
        self.date_range_score: float = 0.0
        self.staff_integrity_score: float = 0.0
        self.shift_code_score: float = 0.0
        self.completeness_score: float = 0.0
        
        self.issues: List[str] = []
        self.warnings: List[str] = []
        self.recommendations: List[str] = []
        
        self.detected_slot_interval: Optional[int] = None
        self.detected_date_range: Optional[Tuple[datetime, datetime]] = None
        self.staff_count: int = 0
        self.duplicate_staff_names: List[str] = []
        self.missing_dates: List[str] = []
        
        self.recommended_analysis_method: str = "need_based"
        self.confidence_level: str = "high"


class DataLineageTracker:
    """ãƒ‡ãƒ¼ã‚¿ç³»è­œè¿½è·¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.tracking_id: str = ""
        self.source_file: str = ""
        self.ingestion_timestamp: datetime = datetime.now()
        self.processing_steps: List[Dict[str, Any]] = []
        self.metadata: Dict[str, Any] = {}
    
    def track_step(self, step_name: str, details: Dict[str, Any]) -> None:
        """å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨˜éŒ²"""
        self.processing_steps.append({
            "step": step_name,
            "timestamp": datetime.now().isoformat(),
            "details": details
        })
        analysis_logger.info(f"[LINEAGE] {step_name}: {details}")
    
    def generate_lineage_report(self) -> Dict[str, Any]:
        """ç³»è­œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        return {
            "tracking_id": self.tracking_id,
            "source_file": self.source_file,
            "ingestion_timestamp": self.ingestion_timestamp.isoformat(),
            "processing_steps": self.processing_steps,
            "metadata": self.metadata
        }


class DynamicSchemaInferrer:
    """å‹•çš„ã‚¹ã‚­ãƒ¼ãƒæ¨è«–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.inferred_schema: Dict[str, Any] = {}
        self.confidence_scores: Dict[str, float] = {}
    
    def infer_structure(self, excel_path: Path) -> Dict[str, Any]:
        """Excelæ§‹é€ ã®è‡ªå‹•æ¨è«–"""
        try:
            # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§æ§‹é€ åˆ†æ
            excel_file = pd.ExcelFile(excel_path)
            sheets_info = {}
            
            for sheet_name in excel_file.sheet_names:
                try:
                    df = pd.read_excel(excel_path, sheet_name=sheet_name, nrows=QUALITY_PREVIEW_ROWS)
                    sheets_info[sheet_name] = {
                        "columns": list(df.columns),
                        "shape": df.shape,
                        "detected_type": self._detect_sheet_type(df, sheet_name)
                    }
                except Exception as e:
                    log.warning(f"ã‚·ãƒ¼ãƒˆ'{sheet_name}'ã®è§£æã§ã‚¨ãƒ©ãƒ¼: {e}")
                    sheets_info[sheet_name] = {"error": str(e)}
            
            # ã‚¹ã‚­ãƒ¼ãƒæ¨è«–
            self.inferred_schema = {
                "file_type": "excel",
                "sheets": sheets_info,
                "primary_data_sheet": self._identify_primary_sheet(sheets_info),
                "has_need_file": "Need" in [s.upper() for s in excel_file.sheet_names],
                "has_master_sheet": any("å‹¤å‹™åŒºåˆ†" in s or "ãƒã‚¹ã‚¿ãƒ¼" in s for s in excel_file.sheet_names)
            }
            
            return self.inferred_schema
            
        except Exception as e:
            log.error(f"ã‚¹ã‚­ãƒ¼ãƒæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}
    
    def _detect_sheet_type(self, df: DataFrame, sheet_name: str) -> str:
        """ã‚·ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã®æ¤œå‡º"""
        sheet_name_upper = sheet_name.upper()
        
        if "NEED" in sheet_name_upper:
            return "need_file"
        elif "å‹¤å‹™åŒºåˆ†" in sheet_name or "ãƒã‚¹ã‚¿ãƒ¼" in sheet_name:
            return "master_data"
        elif any(col for col in df.columns if str(col).replace("/", "").replace("-", "").isdigit()):
            return "daily_schedule"
        else:
            return "unknown"
    
    def _identify_primary_sheet(self, sheets_info: Dict) -> Optional[str]:
        """ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã®ç‰¹å®š"""
        # Need fileãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯æœ€å„ªå…ˆ
        for sheet_name, info in sheets_info.items():
            if info.get("detected_type") == "need_file":
                return sheet_name
        
        # æ¬¡ã«å‹¤å‹™è¡¨å½¢å¼ã®ã‚·ãƒ¼ãƒˆ
        for sheet_name, info in sheets_info.items():
            if info.get("detected_type") == "daily_schedule":
                return sheet_name
        
        return None


class DataQualityChecker:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.slot_detector = SlotIntervalDetector()
        self.date_validator = DateRangeValidator()
        self.staff_validator = StaffIntegrityValidator()
    
    def evaluate(self, excel_path: Path) -> QualityAssessmentResult:
        """ç·åˆçš„ãªå“è³ªè©•ä¾¡"""
        result = QualityAssessmentResult()
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒã‚§ãƒƒã‚¯
            result.file_format_score = self._check_file_format(excel_path)
            
            # Excelèª­ã¿è¾¼ã¿
            excel_file = pd.ExcelFile(excel_path)
            
            # æ§‹é€ åˆ†æ
            result.structure_score = self._analyze_structure(excel_file)
            
            # ä¸»è¦ã‚·ãƒ¼ãƒˆç‰¹å®š
            primary_sheet = self._find_primary_sheet(excel_file)
            if not primary_sheet:
                result.issues.append("ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆãŒç‰¹å®šã§ãã¾ã›ã‚“")
                return result
            
            # ä¸»è¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            df = pd.read_excel(excel_path, sheet_name=primary_sheet)
            
            # å„ç¨®å“è³ªãƒã‚§ãƒƒã‚¯
            result.date_range_score, result.detected_date_range, result.missing_dates = self._check_date_range(df)
            result.staff_integrity_score, result.staff_count, result.duplicate_staff_names = self._check_staff_integrity(df)
            result.shift_code_score, result.detected_slot_interval = self._check_shift_codes(df)
            result.completeness_score = self._check_completeness(df)
            
            # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
            result.overall_score = self._calculate_overall_score(result)
            
            # æ¨å¥¨åˆ†ææ‰‹æ³•æ±ºå®š
            result.recommended_analysis_method, result.confidence_level = self._recommend_analysis_method(result)
            
            # æ”¹å–„ææ¡ˆç”Ÿæˆ
            result.recommendations = self._generate_recommendations(result)
            
            analysis_logger.info(f"[QUALITY] å“è³ªè©•ä¾¡å®Œäº†: {result.overall_score:.1f}/100ç‚¹")
            
        except Exception as e:
            log.error(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            result.issues.append(f"å“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
        
        return result
    
    def _check_file_format(self, excel_path: Path) -> float:
        """ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒã‚§ãƒƒã‚¯"""
        try:
            if not excel_path.exists():
                return 0.0
            if excel_path.suffix.lower() not in ['.xlsx', '.xls']:
                return QUALITY_PARTIAL_EXCEL_SCORE
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            file_size = excel_path.stat().st_size
            if file_size == 0:
                return 0.0
            elif file_size > QUALITY_LARGE_FILE_SIZE:
                return QUALITY_LARGE_FILE_SCORE
            
            return QUALITY_SCORE_MAX
        except Exception:
            return 0.0
    
    def _analyze_structure(self, excel_file: pd.ExcelFile) -> float:
        """æ§‹é€ åˆ†æ"""
        try:
            sheet_count = len(excel_file.sheet_names)
            if sheet_count == 0:
                return 0.0
            elif sheet_count >= 2:  # è¤‡æ•°ã‚·ãƒ¼ãƒˆã‚ã‚‹å ´åˆã¯è‰¯ã„
                return QUALITY_SCORE_MAX
            else:
                return QUALITY_SINGLE_SHEET_SCORE  # å˜ä¸€ã‚·ãƒ¼ãƒˆã§ã‚‚ä½¿ç”¨å¯èƒ½
        except Exception:
            return 0.0
    
    def _find_primary_sheet(self, excel_file: pd.ExcelFile) -> Optional[str]:
        """ä¸»è¦ã‚·ãƒ¼ãƒˆç‰¹å®š"""
        # Need fileã‚’æœ€å„ªå…ˆ
        for sheet_name in excel_file.sheet_names:
            if "Need" in sheet_name or "need" in sheet_name:
                return sheet_name
        
        # æœ€åˆã®ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨
        return excel_file.sheet_names[0] if excel_file.sheet_names else None
    
    def _check_date_range(self, df: DataFrame) -> Tuple[float, Optional[Tuple[datetime, datetime]], List[str]]:
        """æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯"""
        try:
            date_columns = []
            dates = []
            
            for col in df.columns:
                if self._is_date_column(col):
                    date_columns.append(col)
                    parsed_date = self._parse_date_column(col)
                    if parsed_date:
                        dates.append(parsed_date)
            
            if not dates:
                return 0.0, None, ["æ—¥ä»˜åˆ—ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ"]
            
            dates.sort()
            date_range = (dates[0], dates[-1])
            
            # é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯
            expected_dates = []
            current_date = dates[0]
            while current_date <= dates[-1]:
                expected_dates.append(current_date)
                current_date += timedelta(days=1)
            
            missing_dates = []
            for expected in expected_dates:
                if expected not in dates:
                    missing_dates.append(expected.strftime("%Y-%m-%d"))
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            completeness_ratio = (len(dates) - len(missing_dates)) / len(expected_dates)
            score = completeness_ratio * 100
            
            return score, date_range, missing_dates
            
        except Exception as e:
            log.error(f"æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯ã§ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0, None, [f"æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}"]
    
    def _check_staff_integrity(self, df: DataFrame) -> Tuple[float, int, List[str]]:
        """ã‚¹ã‚¿ãƒƒãƒ•ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            if 'staff' not in df.columns:
                return QUALITY_STAFF_MISSING_SCORE, 0, ["staffåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"]
            
            staff_series = df['staff'].dropna()
            unique_staff = staff_series.unique()
            staff_count = len(unique_staff)
            
            # é‡è¤‡åæ¤œå‡º
            staff_counts = staff_series.value_counts()
            duplicates = staff_counts[staff_counts > 1].index.tolist()
            
            # ç„¡åŠ¹ãªåå‰æ¤œå‡º
            invalid_patterns = ['Ã—', 'X', 'x', 'ä¼‘', 'æ¬ ', 'OFF', '-', 'âˆ’', 'â€•']
            valid_staff = []
            for staff in unique_staff:
                if pd.notna(staff) and str(staff).strip() not in invalid_patterns:
                    valid_staff.append(staff)
            
            valid_ratio = len(valid_staff) / len(unique_staff) if unique_staff.size > 0 else 0
            score = valid_ratio * 100
            
            return score, len(valid_staff), duplicates
            
        except Exception as e:
            return 0.0, 0, [f"ã‚¹ã‚¿ãƒƒãƒ•æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}"]
    
    def _check_shift_codes(self, df: DataFrame) -> Tuple[float, Optional[int]]:
        """å‹¤å‹™ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯"""
        try:
            # æ™‚é–“ã‚‰ã—ã„åˆ—ã‚’æ¢ã™
            time_columns = []
            for col in df.columns:
                col_str = str(col)
                if ':' in col_str or any(t in col_str for t in ['æ™‚', 'H', 'h']):
                    time_columns.append(col)
            
            if not time_columns:
                return 50.0, None
            
            # ã‚¹ãƒ­ãƒƒãƒˆé–“éš”æ¤œå‡º
            detected_interval = self.slot_detector.detect_interval(time_columns)
            
            score = 90.0 if detected_interval else 70.0
            return score, detected_interval
            
        except Exception as e:
            return 0.0, None
    
    def _check_completeness(self, df: DataFrame) -> float:
        """ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯"""
        try:
            total_cells = df.size
            non_null_cells = df.count().sum()
            completeness_ratio = non_null_cells / total_cells if total_cells > 0 else 0
            return completeness_ratio * 100
        except Exception:
            return 0.0
    
    def _calculate_overall_score(self, result: QualityAssessmentResult) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        weighted_score = (
            result.file_format_score * QUALITY_WEIGHTS['file_format'] +
            result.structure_score * QUALITY_WEIGHTS['structure'] +
            result.date_range_score * QUALITY_WEIGHTS['date_range'] +
            result.staff_integrity_score * QUALITY_WEIGHTS['staff_integrity'] +
            result.shift_code_score * QUALITY_WEIGHTS['shift_code'] +
            result.completeness_score * QUALITY_WEIGHTS['completeness']
        )
        
        return round(weighted_score, 1)
    
    def _recommend_analysis_method(self, result: QualityAssessmentResult) -> Tuple[str, str]:
        """æ¨å¥¨åˆ†ææ‰‹æ³•æ±ºå®š"""
        if result.overall_score >= 85:
            return "need_based", "high"
        elif result.overall_score >= 70:
            return "time_axis", "medium"
        else:
            return "proportional", "low"
    
    def _generate_recommendations(self, result: QualityAssessmentResult) -> List[str]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        recommendations = []
        
        if result.overall_score < 70:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿å“è³ªãŒä½ã„ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„")
        
        if result.duplicate_staff_names:
            recommendations.append(f"é‡è¤‡ã™ã‚‹ã‚¹ã‚¿ãƒƒãƒ•åãŒã‚ã‚Šã¾ã™: {', '.join(result.duplicate_staff_names[:3])}")
        
        if result.missing_dates:
            recommendations.append(f"æ¬ ææ—¥ä»˜ãŒã‚ã‚Šã¾ã™: {len(result.missing_dates)}æ—¥")
        
        if result.detected_slot_interval:
            recommendations.append(f"æ¤œå‡ºã•ã‚ŒãŸã‚¹ãƒ­ãƒƒãƒˆé–“éš”: {result.detected_slot_interval}åˆ†")
        
        return recommendations
    
    def _is_date_column(self, col: Any) -> bool:
        """æ—¥ä»˜åˆ—åˆ¤å®š"""
        col_str = str(col)
        return bool(re.search(r'\d{1,2}[/-]\d{1,2}', col_str))
    
    def _parse_date_column(self, col: Any) -> Optional[datetime]:
        """æ—¥ä»˜åˆ—ãƒ‘ãƒ¼ã‚¹"""
        col_str = str(col)
        match = re.search(r'(\d{1,2})[/-](\d{1,2})', col_str)
        if match:
            month, day = map(int, match.groups())
            try:
                year = datetime.now().year
                return datetime(year, month, day)
            except ValueError:
                return None
        return None


class SlotIntervalDetector:
    """ã‚¹ãƒ­ãƒƒãƒˆé–“éš”æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """
    
    def detect_interval(self, time_columns: List[str]) -> Optional[int]:
        """æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆé–“éš”ã‚’è‡ªå‹•æ¤œå‡º"""
        try:
            times = []
            for col in time_columns:
                time_match = re.search(r'(\d{1,2}):(\d{2})', str(col))
                if time_match:
                    hour, minute = map(int, time_match.groups())
                    times.append(hour * 60 + minute)
            
            if len(times) < 2:
                return None
            
            times.sort()
            intervals = [times[i+1] - times[i] for i in range(len(times)-1)]
            
            # æœ€ã‚‚é »å‡ºã™ã‚‹é–“éš”
            from collections import Counter
            interval_counts = Counter(intervals)
            most_common_interval = interval_counts.most_common(1)[0][0]
            
            # 15, 30, 60åˆ†ã®ã„ãšã‚Œã‹ã«æ­£è¦åŒ–
            if most_common_interval <= 20:
                return 15
            elif most_common_interval <= 45:
                return 30
            else:
                return 60
                
        except Exception as e:
            log.error(f"ã‚¹ãƒ­ãƒƒãƒˆé–“éš”æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None


class DateRangeValidator:
    """æ—¥ä»˜ç¯„å›²ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼"""
    
    def validate_continuity(self, dates: List[datetime]) -> Tuple[bool, List[datetime]]:
        """æ—¥ä»˜ã®é€£ç¶šæ€§ã‚’æ¤œè¨¼"""
        if not dates:
            return False, []
        
        dates.sort()
        missing_dates = []
        
        current_date = dates[0]
        for expected_date in pd.date_range(dates[0], dates[-1], freq='D'):
            if expected_date.to_pydatetime() not in dates:
                missing_dates.append(expected_date.to_pydatetime())
        
        is_continuous = len(missing_dates) == 0
        return is_continuous, missing_dates


class StaffIntegrityValidator:
    """ã‚¹ã‚¿ãƒƒãƒ•ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.rest_patterns = [
            'Ã—', 'X', 'x', 'ä¼‘', 'ä¼‘ã¿', 'ä¼‘æš‡', 'æ¬ ', 'æ¬ å‹¤',
            'OFF', 'off', 'Off', '-', 'âˆ’', 'â€•', 'nan', 'NaN', 'null',
            'æœ‰', 'æœ‰ä¼‘', 'ç‰¹', 'ç‰¹ä¼‘', 'ä»£', 'ä»£ä¼‘', 'æŒ¯', 'æŒ¯ä¼‘'
        ]
    
    def validate_staff_names(self, staff_series: pd.Series) -> Dict[str, Any]:
        """ã‚¹ã‚¿ãƒƒãƒ•åã®å¦¥å½“æ€§æ¤œè¨¼"""
        validation_result = {
            "total_entries": len(staff_series),
            "unique_staff": 0,
            "duplicate_names": [],
            "invalid_entries": [],
            "rest_entries": 0,
            "valid_staff_ratio": 0.0
        }
        
        # éNULLå€¤ã®ã¿å‡¦ç†
        staff_clean = staff_series.dropna()
        
        # é‡è¤‡æ¤œå‡º
        staff_counts = staff_clean.value_counts()
        duplicates = staff_counts[staff_counts > 1].index.tolist()
        validation_result["duplicate_names"] = duplicates
        
        # ä¼‘æš‡ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å¤–
        valid_staff = []
        rest_count = 0
        
        for staff in staff_clean:
            staff_str = str(staff).strip()
            if staff_str in self.rest_patterns:
                rest_count += 1
            elif staff_str and staff_str not in ['', ' ', 'ã€€']:
                valid_staff.append(staff)
        
        validation_result["unique_staff"] = len(set(valid_staff))
        validation_result["rest_entries"] = rest_count
        validation_result["valid_staff_ratio"] = len(valid_staff) / len(staff_clean) if len(staff_clean) > 0 else 0.0
        
        return validation_result


class QualityAssuredDataset:
    """å“è³ªä¿è¨¼ä»˜ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(
        self, 
        data: DataFrame, 
        quality_result: QualityAssessmentResult,
        schema: Dict[str, Any],
        lineage: DataLineageTracker
    ):
        self.data = data
        self.quality_result = quality_result
        self.schema = schema
        self.lineage = lineage
        self.metadata = self._generate_metadata()
    
    def _generate_metadata(self) -> Dict[str, Any]:
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        return {
            "processing_timestamp": datetime.now().isoformat(),
            "data_shape": self.data.shape,
            "quality_score": self.quality_result.overall_score,
            "recommended_method": self.quality_result.recommended_analysis_method,
            "confidence_level": self.quality_result.confidence_level,
            "detected_slot_interval": self.quality_result.detected_slot_interval,
            "staff_count": self.quality_result.staff_count
        }
    
    def get_quality_report(self) -> str:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
ğŸ” ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
{'='*50}
ğŸ“Š ç·åˆã‚¹ã‚³ã‚¢: {self.quality_result.overall_score:.1f}/100ç‚¹

ğŸ“‹ è©³ç´°è©•ä¾¡:
â”œâ”€ ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {self.quality_result.file_format_score:.1f}ç‚¹
â”œâ”€ ãƒ‡ãƒ¼ã‚¿æ§‹é€ : {self.quality_result.structure_score:.1f}ç‚¹  
â”œâ”€ æ—¥ä»˜ç¯„å›²: {self.quality_result.date_range_score:.1f}ç‚¹
â”œâ”€ ã‚¹ã‚¿ãƒƒãƒ•æ•´åˆæ€§: {self.quality_result.staff_integrity_score:.1f}ç‚¹
â”œâ”€ å‹¤å‹™ã‚³ãƒ¼ãƒ‰: {self.quality_result.shift_code_score:.1f}ç‚¹
â””â”€ ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§: {self.quality_result.completeness_score:.1f}ç‚¹

ğŸ¯ æ¨å¥¨åˆ†ææ‰‹æ³•: {self.quality_result.recommended_analysis_method}
ğŸ“ˆ ä¿¡é ¼åº¦: {self.quality_result.confidence_level}

âš ï¸  æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ: {len(self.quality_result.issues)}ä»¶
ğŸ’¡ æ”¹å–„ææ¡ˆ: {len(self.quality_result.recommendations)}ä»¶
"""
        return report


class TruthAssuredDataIngestion:
    """Truth-Drivençµ±åˆãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.quality_checker = DataQualityChecker()
        self.schema_inferrer = DynamicSchemaInferrer()
        self.lineage_tracker = DataLineageTracker()
    
    def ingest_with_quality_assurance(self, excel_path: Path) -> QualityAssuredDataset:
        """å“è³ªä¿è¨¼ä»˜ããƒ‡ãƒ¼ã‚¿å…¥ç¨¿"""
        log.info(f"[INGESTION] çµ±åˆãƒ‡ãƒ¼ã‚¿å…¥ç¨¿é–‹å§‹: {excel_path.name}")
        
        # ç³»è­œè¿½è·¡åˆæœŸåŒ–
        self.lineage_tracker.tracking_id = f"INGEST_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.lineage_tracker.source_file = str(excel_path)
        
        try:
            # Step 1: å“è³ªè©•ä¾¡
            self.lineage_tracker.track_step("quality_assessment", {"stage": "start"})
            quality_result = self.quality_checker.evaluate(excel_path)
            self.lineage_tracker.track_step("quality_assessment", {
                "score": quality_result.overall_score,
                "recommended_method": quality_result.recommended_analysis_method
            })
            
            # Step 2: ã‚¹ã‚­ãƒ¼ãƒæ¨è«–
            self.lineage_tracker.track_step("schema_inference", {"stage": "start"})
            schema = self.schema_inferrer.infer_structure(excel_path)
            self.lineage_tracker.track_step("schema_inference", {"schema": schema})
            
            # Step 3: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
            self.lineage_tracker.track_step("data_loading", {"stage": "start"})
            primary_sheet = schema.get("primary_data_sheet")
            
            if not primary_sheet:
                raise ValueError("ä¸»è¦ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆãŒç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸ")
            
            data = pd.read_excel(excel_path, sheet_name=primary_sheet)
            self.lineage_tracker.track_step("data_loading", {
                "sheet": primary_sheet,
                "shape": data.shape
            })
            
            # Step 4: å“è³ªä¿è¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            dataset = QualityAssuredDataset(data, quality_result, schema, self.lineage_tracker)
            
            # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
            log.info(f"[INGESTION] å…¥ç¨¿å®Œäº†: ã‚¹ã‚³ã‚¢{quality_result.overall_score:.1f}ç‚¹")
            analysis_logger.info(dataset.get_quality_report())
            
            return dataset
            
        except Exception as e:
            log.error(f"[INGESTION] ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ã‚¨ãƒ©ãƒ¼: {e}")
            self.lineage_tracker.track_step("error", {"error": str(e)})
            raise
    
    def save_ingestion_report(self, dataset: QualityAssuredDataset, output_dir: Path) -> Path:
        """å…¥ç¨¿ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        report_path = output_dir / f"ingestion_report_{dataset.lineage.tracking_id}.json"
        
        report_data = {
            "ingestion_summary": dataset.metadata,
            "quality_assessment": {
                "overall_score": dataset.quality_result.overall_score,
                "detailed_scores": {
                    "file_format": dataset.quality_result.file_format_score,
                    "structure": dataset.quality_result.structure_score,
                    "date_range": dataset.quality_result.date_range_score,
                    "staff_integrity": dataset.quality_result.staff_integrity_score,
                    "shift_code": dataset.quality_result.shift_code_score,
                    "completeness": dataset.quality_result.completeness_score
                },
                "issues": dataset.quality_result.issues,
                "warnings": dataset.quality_result.warnings,
                "recommendations": dataset.quality_result.recommendations
            },
            "schema_inference": dataset.schema,
            "data_lineage": dataset.lineage.generate_lineage_report()
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
        
        log.info(f"[INGESTION] ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        return report_path


# ä¾¿åˆ©é–¢æ•°
def ingest_excel_with_quality_assurance(excel_path: Path) -> QualityAssuredDataset:
    """å“è³ªä¿è¨¼ä»˜ãExcelå…¥ç¨¿ï¼ˆä¾¿åˆ©é–¢æ•°ï¼‰"""
    ingestion_system = TruthAssuredDataIngestion()
    return ingestion_system.ingest_with_quality_assurance(excel_path)


# Export
__all__ = [
    "QualityAssessmentResult",
    "QualityAssuredDataset", 
    "TruthAssuredDataIngestion",
    "ingest_excel_with_quality_assurance"
]