#!/usr/bin/env python3
"""
è»¸12: æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ› MECEäº‹å®ŸæŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³

12è»¸åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è»¸12ï¼ˆæœ€ä¸Šä½è»¸ï¼‰ã‚’æ‹…å½“
éå»ã‚·ãƒ•ãƒˆå®Ÿç¸¾ã‹ã‚‰æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ›ã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’æŠ½å‡º
ä»–ã®å…¨è»¸ã®æˆæœã‚’çµ±åˆã—ã¦é•·æœŸçš„ãªåˆ¶ç´„ã¨ãƒ“ã‚¸ãƒ§ãƒ³ã‚’å°å‡º

ä½œæˆæ—¥: 2025å¹´7æœˆ
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import json

log = logging.getLogger(__name__)

class StrategyFutureMECEFactExtractor:
    """è»¸12: æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ›ã®MECEäº‹å®ŸæŠ½å‡ºå™¨"""
    
    def __init__(self):
        self.axis_number = 12
        self.axis_name = "æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ›"
        
        # æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ›åŸºæº–å€¤ï¼ˆé•·æœŸçš„ç›®æ¨™ã¨ãƒ“ã‚¸ãƒ§ãƒ³ï¼‰
        self.strategy_standards = {
            'long_term_vision_horizon_years': 5,       # é•·æœŸãƒ“ã‚¸ãƒ§ãƒ³æœŸé–“
            'sustainability_target_score': 0.9,       # æŒç¶šå¯èƒ½æ€§ç›®æ¨™ã‚¹ã‚³ã‚¢
            'innovation_adoption_rate': 0.3,          # å¹´é–“æŠ€è¡“é©æ–°å°å…¥ç‡
            'competitive_advantage_score': 0.8,       # ç«¶äº‰å„ªä½æ€§ã‚¹ã‚³ã‚¢
            'organizational_agility_score': 0.75,     # çµ„ç¹”æ•æ·æ€§ã‚¹ã‚³ã‚¢
            'legacy_preservation_score': 0.85,        # ãƒ¬ã‚¬ã‚·ãƒ¼ç¶™æ‰¿ã‚¹ã‚³ã‚¢
            'growth_target_rate': 0.15,               # å¹´é–“æˆé•·ç›®æ¨™ç‡
            'strategic_alignment_score': 0.9          # æˆ¦ç•¥çš„æ•´åˆæ€§ã‚¹ã‚³ã‚¢
        }
        
    def extract_axis12_strategy_future_rules(self, long_df: pd.DataFrame, wt_df: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è»¸12: æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ›ãƒ«ãƒ¼ãƒ«ã‚’MECEåˆ†è§£ã«ã‚ˆã‚ŠæŠ½å‡º
        
        Args:
            long_df: éå»ã®ã‚·ãƒ•ãƒˆå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿
            wt_df: å‹¤å‹™åŒºåˆ†ãƒã‚¹ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Dict: æŠ½å‡ºçµæœï¼ˆhuman_readable, machine_readable, extraction_metadataï¼‰
        """
        log.info(f"ğŸš€ è»¸12: {self.axis_name} MECEäº‹å®ŸæŠ½å‡ºã‚’é–‹å§‹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            if long_df.empty:
                raise ValueError("é•·æœŸãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            
            # è»¸12ã®MECEåˆ†è§£ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆ8ã¤ï¼‰
            mece_facts = {
                "æˆ¦ç•¥çš„æ–¹å‘æ€§åˆ¶ç´„": self._extract_strategic_direction_constraints(long_df, wt_df),
                "å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³åˆ¶ç´„": self._extract_future_vision_constraints(long_df, wt_df),
                "æŒç¶šå¯èƒ½æ€§åˆ¶ç´„": self._extract_sustainability_constraints(long_df, wt_df),
                "æˆé•·ãƒ»ç™ºå±•åˆ¶ç´„": self._extract_growth_development_constraints(long_df, wt_df),
                "ç«¶äº‰å„ªä½æ€§åˆ¶ç´„": self._extract_competitive_advantage_constraints(long_df, wt_df),
                "æŠ€è¡“é©æ–°åˆ¶ç´„": self._extract_technology_innovation_constraints(long_df, wt_df),
                "çµ„ç¹”å¤‰é©åˆ¶ç´„": self._extract_organizational_transformation_constraints(long_df, wt_df),
                "ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ç¶™æ‰¿åˆ¶ç´„": self._extract_legacy_inheritance_constraints(long_df, wt_df)
            }
            
            # äººé–“å¯èª­å½¢å¼ã®çµæœç”Ÿæˆ
            human_readable = self._generate_human_readable_results(mece_facts, long_df)
            
            # æ©Ÿæ¢°å¯èª­å½¢å¼ã®åˆ¶ç´„ç”Ÿæˆï¼ˆæˆ¦ç•¥åˆ¶ç´„ã¯æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®çµ±åˆï¼‰
            machine_readable = self._generate_machine_readable_constraints(mece_facts, long_df)
            
            # æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            extraction_metadata = self._generate_extraction_metadata(long_df, wt_df, mece_facts)
            
            log.info(f"âœ… è»¸12: {self.axis_name} MECEäº‹å®ŸæŠ½å‡ºå®Œäº†")
            
            return {
                'human_readable': human_readable,
                'machine_readable': machine_readable,
                'extraction_metadata': extraction_metadata
            }
            
        except Exception as e:
            log.error(f"âŒ è»¸12: {self.axis_name} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æœ€å°é™ã®æ§‹é€ ã‚’è¿”ã™
            return {
                'human_readable': {"è»¸12": f"ã‚¨ãƒ©ãƒ¼: {str(e)}"},
                'machine_readable': {"error": str(e)},
                'extraction_metadata': {"error": str(e), "axis": "axis12"}
            }
    
    def _extract_strategic_direction_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æˆ¦ç•¥çš„æ–¹å‘æ€§åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # æˆ¦ç•¥çš„ä¸€è²«æ€§ã®è©•ä¾¡
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                strategic_consistency = self._assess_strategic_consistency(long_df, wt_df)
                constraints.append(f"æˆ¦ç•¥çš„ä¸€è²«æ€§æŒ‡æ¨™: {strategic_consistency:.3f}")
                
                # é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ
                long_term_trends = self._analyze_long_term_trends(long_df)
                if long_term_trends:
                    for trend_name, direction in long_term_trends.items():
                        constraints.append(f"{trend_name}é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰: {direction}")
            
            # ä¸­æ ¸èƒ½åŠ›ã®ç‰¹å®š
            core_competencies = self._identify_core_competencies(long_df, wt_df)
            if core_competencies:
                for competency, strength in core_competencies.items():
                    constraints.append(f"ä¸­æ ¸èƒ½åŠ› {competency}: {strength:.1%}")
            
            # æˆ¦ç•¥çš„æ•´åˆæ€§ã®è©•ä¾¡
            strategic_alignment = self._evaluate_strategic_alignment(long_df)
            constraints.append(f"æˆ¦ç•¥çš„æ•´åˆæ€§: {strategic_alignment:.1%}")
            
            # æ–¹å‘æ€§ã®æ˜ç¢ºæ€§
            direction_clarity = self._assess_direction_clarity(long_df, wt_df)
            constraints.append(f"æˆ¦ç•¥æ–¹å‘æ€§æ˜ç¢ºåº¦: {direction_clarity:.1%}")
            
            constraints.append("ã€æˆ¦ç•¥çš„æ–¹å‘æ€§åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"æˆ¦ç•¥çš„æ–¹å‘æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"æˆ¦ç•¥çš„æ–¹å‘æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_future_vision_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ãƒ“ã‚¸ãƒ§ãƒ³ã®å®Ÿç¾å¯èƒ½æ€§è©•ä¾¡
            if 'ds' in long_df.columns:
                vision_feasibility = self._assess_vision_feasibility(long_df, wt_df)
                constraints.append(f"å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³å®Ÿç¾å¯èƒ½æ€§: {vision_feasibility:.1%}")
                
                # å°†æ¥äºˆæ¸¬ã®ç²¾åº¦
                prediction_accuracy = self._evaluate_prediction_accuracy(long_df)
                constraints.append(f"å°†æ¥äºˆæ¸¬ç²¾åº¦: {prediction_accuracy:.1%}")
            
            # ãƒ“ã‚¸ãƒ§ãƒ³ã¨ç¾å®Ÿã®ã‚®ãƒ£ãƒƒãƒ—
            vision_reality_gap = self._analyze_vision_reality_gap(long_df)
            constraints.append(f"ãƒ“ã‚¸ãƒ§ãƒ³ç¾å®Ÿã‚®ãƒ£ãƒƒãƒ—: {vision_reality_gap:.1%}")
            
            # å¤‰åŒ–ã¸ã®é©å¿œèƒ½åŠ›
            adaptability_score = self._assess_adaptability(long_df, wt_df)
            constraints.append(f"å¤‰åŒ–é©å¿œèƒ½åŠ›: {adaptability_score:.1%}")
            
            # å°†æ¥ã¸ã®æº–å‚™åº¦
            future_readiness = self._evaluate_future_readiness(long_df)
            constraints.append(f"å°†æ¥æº–å‚™åº¦: {future_readiness:.1%}")
            
            constraints.append("ã€å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_sustainability_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æŒç¶šå¯èƒ½æ€§åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # é‹å–¶ã®æŒç¶šå¯èƒ½æ€§
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                operational_sustainability = self._assess_operational_sustainability(long_df)
                constraints.append(f"é‹å–¶æŒç¶šå¯èƒ½æ€§: {operational_sustainability:.1%}")
                
                # äººææŒç¶šå¯èƒ½æ€§
                human_sustainability = self._assess_human_sustainability(long_df, wt_df)
                constraints.append(f"äººææŒç¶šå¯èƒ½æ€§: {human_sustainability:.1%}")
            
            # è² è·ã®æŒç¶šå¯èƒ½æ€§
            load_sustainability = self._assess_load_sustainability(long_df)
            constraints.append(f"è² è·æŒç¶šå¯èƒ½æ€§: {load_sustainability:.1%}")
            
            # å“è³ªã®æŒç¶šå¯èƒ½æ€§
            quality_sustainability = self._assess_quality_sustainability(long_df, wt_df)
            constraints.append(f"å“è³ªæŒç¶šå¯èƒ½æ€§: {quality_sustainability:.1%}")
            
            # ç’°å¢ƒã¸ã®é…æ…®
            environmental_consideration = self._assess_environmental_consideration(long_df)
            constraints.append(f"ç’°å¢ƒé…æ…®åº¦: {environmental_consideration:.1%}")
            
            constraints.append("ã€æŒç¶šå¯èƒ½æ€§åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"æŒç¶šå¯èƒ½æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"æŒç¶šå¯èƒ½æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_growth_development_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æˆé•·ãƒ»ç™ºå±•åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # æˆé•·ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã®è©•ä¾¡
            if 'staff' in long_df.columns and 'worktype' in long_df.columns:
                growth_potential = self._assess_growth_potential(long_df, wt_df)
                constraints.append(f"æˆé•·ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {growth_potential:.1%}")
                
                # ç™ºå±•å¯èƒ½ãªé ˜åŸŸã®ç‰¹å®š
                development_areas = self._identify_development_areas(long_df, wt_df)
                if development_areas:
                    for area, potential in development_areas.items():
                        constraints.append(f"{area}ç™ºå±•å¯èƒ½æ€§: {potential:.1%}")
            
            # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®è©•ä¾¡
            scalability = self._assess_scalability(long_df)
            constraints.append(f"ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£: {scalability:.1%}")
            
            # æˆé•·åˆ¶ç´„ã®ç‰¹å®š
            growth_constraints = self._identify_growth_constraints(long_df, wt_df)
            if growth_constraints:
                for constraint, severity in growth_constraints.items():
                    constraints.append(f"æˆé•·åˆ¶ç´„ {constraint}: æ·±åˆ»åº¦{severity:.1f}")
            
            # ç™ºå±•é€Ÿåº¦ã®æœ€é©åŒ–
            development_speed = self._optimize_development_speed(long_df)
            constraints.append(f"æœ€é©ç™ºå±•é€Ÿåº¦: {development_speed:.1%}/å¹´")
            
            constraints.append("ã€æˆé•·ãƒ»ç™ºå±•åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"æˆé•·ãƒ»ç™ºå±•åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"æˆé•·ãƒ»ç™ºå±•åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_competitive_advantage_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ç«¶äº‰å„ªä½æ€§åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ç‹¬è‡ªæ€§ã®è©•ä¾¡
            if 'worktype' in long_df.columns:
                uniqueness_score = self._assess_uniqueness(long_df, wt_df)
                constraints.append(f"ç‹¬è‡ªæ€§ã‚¹ã‚³ã‚¢: {uniqueness_score:.1%}")
                
                # å·®åˆ¥åŒ–è¦å› ã®ç‰¹å®š
                differentiation_factors = self._identify_differentiation_factors(long_df, wt_df)
                if differentiation_factors:
                    for factor, strength in differentiation_factors.items():
                        constraints.append(f"å·®åˆ¥åŒ–è¦å›  {factor}: {strength:.1%}")
            
            # ç«¶äº‰å„ªä½ã®æŒç¶šæ€§
            competitive_sustainability = self._assess_competitive_sustainability(long_df)
            constraints.append(f"ç«¶äº‰å„ªä½æŒç¶šæ€§: {competitive_sustainability:.1%}")
            
            # æ¨¡å€£å›°é›£æ€§ã®è©•ä¾¡
            inimitability = self._assess_inimitability(long_df, wt_df)
            constraints.append(f"æ¨¡å€£å›°é›£æ€§: {inimitability:.1%}")
            
            # ä¾¡å€¤å‰µé€ èƒ½åŠ›
            value_creation = self._assess_value_creation_capability(long_df)
            constraints.append(f"ä¾¡å€¤å‰µé€ èƒ½åŠ›: {value_creation:.1%}")
            
            constraints.append("ã€ç«¶äº‰å„ªä½æ€§åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"ç«¶äº‰å„ªä½æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"ç«¶äº‰å„ªä½æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_technology_innovation_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æŠ€è¡“é©æ–°åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³æº–å‚™åº¦
            innovation_readiness = self._assess_innovation_readiness(long_df, wt_df)
            constraints.append(f"æŠ€è¡“é©æ–°æº–å‚™åº¦: {innovation_readiness:.1%}")
            
            # æŠ€è¡“å°å…¥èƒ½åŠ›
            if 'worktype' in long_df.columns:
                technology_adoption = self._assess_technology_adoption_capability(long_df, wt_df)
                constraints.append(f"æŠ€è¡“å°å…¥èƒ½åŠ›: {technology_adoption:.1%}")
            
            # ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©ã®æº–å‚™
            digital_transformation = self._assess_digital_transformation_readiness(long_df)
            constraints.append(f"ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©æº–å‚™åº¦: {digital_transformation:.1%}")
            
            # è‡ªå‹•åŒ–ã®å¯èƒ½æ€§
            automation_potential = self._assess_automation_potential(long_df, wt_df)
            constraints.append(f"è‡ªå‹•åŒ–å¯èƒ½æ€§: {automation_potential:.1%}")
            
            # AIæ´»ç”¨ã®æº–å‚™åº¦
            ai_readiness = self._assess_ai_readiness(long_df)
            constraints.append(f"AIæ´»ç”¨æº–å‚™åº¦: {ai_readiness:.1%}")
            
            constraints.append("ã€æŠ€è¡“é©æ–°åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"æŠ€è¡“é©æ–°åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"æŠ€è¡“é©æ–°åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_organizational_transformation_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """çµ„ç¹”å¤‰é©åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å¤‰é©èƒ½åŠ›ã®è©•ä¾¡
            if 'staff' in long_df.columns:
                transformation_capability = self._assess_transformation_capability(long_df)
                constraints.append(f"çµ„ç¹”å¤‰é©èƒ½åŠ›: {transformation_capability:.1%}")
                
                # å¤‰åŒ–ã¸ã®æŠµæŠ—åº¦
                change_resistance = self._assess_change_resistance(long_df, wt_df)
                constraints.append(f"å¤‰åŒ–æŠµæŠ—åº¦: {change_resistance:.1%}")
            
            # å­¦ç¿’çµ„ç¹”ã¸ã®ç™ºå±•åº¦
            learning_organization = self._assess_learning_organization_development(long_df)
            constraints.append(f"å­¦ç¿’çµ„ç¹”ç™ºå±•åº¦: {learning_organization:.1%}")
            
            # çµ„ç¹”æ–‡åŒ–ã®æŸ”è»Ÿæ€§
            cultural_flexibility = self._assess_cultural_flexibility(long_df)
            constraints.append(f"çµ„ç¹”æ–‡åŒ–æŸ”è»Ÿæ€§: {cultural_flexibility:.1%}")
            
            # ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã®å¤‰é©åŠ›
            transformational_leadership = self._assess_transformational_leadership(long_df, wt_df)
            constraints.append(f"å¤‰é©ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—: {transformational_leadership:.1%}")
            
            constraints.append("ã€çµ„ç¹”å¤‰é©åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"çµ„ç¹”å¤‰é©åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"çµ„ç¹”å¤‰é©åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_legacy_inheritance_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ç¶™æ‰¿åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ä¼çµ±çš„ä¾¡å€¤ã®ä¿æŒ
            if 'worktype' in long_df.columns:
                traditional_value_preservation = self._assess_traditional_value_preservation(long_df, wt_df)
                constraints.append(f"ä¼çµ±çš„ä¾¡å€¤ä¿æŒåº¦: {traditional_value_preservation:.1%}")
                
                # çŸ¥è­˜ç¶™æ‰¿ã®ä»•çµ„ã¿
                knowledge_inheritance = self._assess_knowledge_inheritance_system(long_df)
                constraints.append(f"çŸ¥è­˜ç¶™æ‰¿ã‚·ã‚¹ãƒ†ãƒ : {knowledge_inheritance:.1%}")
            
            # çµŒé¨“ã®è“„ç©ã¨æ´»ç”¨
            experience_utilization = self._assess_experience_utilization(long_df, wt_df)
            constraints.append(f"çµŒé¨“è“„ç©æ´»ç”¨åº¦: {experience_utilization:.1%}")
            
            # çµ„ç¹”è¨˜æ†¶ã®ä¿æŒ
            organizational_memory = self._assess_organizational_memory(long_df)
            constraints.append(f"çµ„ç¹”è¨˜æ†¶ä¿æŒåº¦: {organizational_memory:.1%}")
            
            # ç¶™ç¶šæ€§ã¨é©æ–°ã®ãƒãƒ©ãƒ³ã‚¹
            continuity_innovation_balance = self._assess_continuity_innovation_balance(long_df)
            constraints.append(f"ç¶™ç¶šæ€§ãƒ»é©æ–°ãƒãƒ©ãƒ³ã‚¹: {continuity_innovation_balance:.3f}")
            
            constraints.append("ã€ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ç¶™æ‰¿åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ç¶™æ‰¿åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ç¶™æ‰¿åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    # åˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _assess_strategic_consistency(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æˆ¦ç•¥çš„ä¸€è²«æ€§ã®è©•ä¾¡"""
        try:
            if 'worktype' not in long_df.columns or 'ds' not in long_df.columns:
                return 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ™‚ç³»åˆ—ã§ã®æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€è²«æ€§
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            long_df_copy['month'] = long_df_copy['ds'].dt.to_period('M')
            
            monthly_patterns = long_df_copy.groupby('month')['worktype'].apply(lambda x: tuple(sorted(x.value_counts().index)))
            
            if len(monthly_patterns) > 1:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€è²«æ€§ã‚’è©•ä¾¡
                pattern_consistency = len(set(monthly_patterns)) / len(monthly_patterns)
                consistency = 1 - pattern_consistency
                return max(0, consistency)
            
            return 0.8
        except Exception:
            return 0.8
    
    def _analyze_long_term_trends(self, long_df: pd.DataFrame) -> Dict[str, str]:
        """é•·æœŸãƒˆãƒ¬ãƒ³ãƒ‰ã®åˆ†æ"""
        try:
            trends = {}
            
            if 'ds' not in long_df.columns:
                return trends
            
            # æ™‚ç³»åˆ—ã§ã®å¤‰åŒ–ãƒˆãƒ¬ãƒ³ãƒ‰
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            long_df_copy['month'] = long_df_copy['ds'].dt.to_period('M')
            
            # æ¥­å‹™é‡ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            monthly_workload = long_df_copy.groupby('month').size()
            if len(monthly_workload) > 2:
                x = np.arange(len(monthly_workload))
                y = monthly_workload.values
                if len(x) > 1 and np.std(x) > 0:
                    slope = np.polyfit(x, y, 1)[0]
                    if slope > 0.1:
                        trends['æ¥­å‹™é‡'] = 'å¢—åŠ å‚¾å‘'
                    elif slope < -0.1:
                        trends['æ¥­å‹™é‡'] = 'æ¸›å°‘å‚¾å‘'
                    else:
                        trends['æ¥­å‹™é‡'] = 'å®‰å®š'
            
            # ã‚¹ã‚¿ãƒƒãƒ•æ´»ç”¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰
            if 'staff' in long_df_copy.columns:
                monthly_staff = long_df_copy.groupby('month')['staff'].nunique()
                if len(monthly_staff) > 2:
                    x = np.arange(len(monthly_staff))
                    y = monthly_staff.values
                    if len(x) > 1 and np.std(x) > 0:
                        slope = np.polyfit(x, y, 1)[0]
                        if slope > 0.05:
                            trends['äººææ´»ç”¨'] = 'æ‹¡å¤§å‚¾å‘'
                        elif slope < -0.05:
                            trends['äººææ´»ç”¨'] = 'ç¸®å°å‚¾å‘'
                        else:
                            trends['äººææ´»ç”¨'] = 'å®‰å®š'
            
            return trends
        except Exception:
            return {}
    
    def _identify_core_competencies(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> Dict[str, float]:
        """ä¸­æ ¸èƒ½åŠ›ã®ç‰¹å®š"""
        try:
            competencies = {}
            
            if 'worktype' not in long_df.columns:
                return competencies
            
            # æ¥­å‹™ã‚¿ã‚¤ãƒ—åˆ¥ã®å®Ÿè¡Œé »åº¦ï¼ˆä¸­æ ¸èƒ½åŠ›ã®æŒ‡æ¨™ï¼‰
            worktype_frequency = long_df['worktype'].value_counts(normalize=True)
            
            # ä¸Šä½ã®æ¥­å‹™ã‚’ä¸­æ ¸èƒ½åŠ›ã¨ã¿ãªã™
            for worktype, frequency in worktype_frequency.head(3).items():
                competency_name = str(worktype)
                competencies[competency_name] = frequency
            
            return competencies
        except Exception:
            return {}
    
    def _evaluate_strategic_alignment(self, long_df: pd.DataFrame) -> float:
        """æˆ¦ç•¥çš„æ•´åˆæ€§ã®è©•ä¾¡"""
        try:
            # æ¥­å‹™é…åˆ†ã®æˆ¦ç•¥çš„æ•´åˆæ€§ï¼ˆå‡è¡¡ã®å–ã‚ŒãŸé…åˆ†ã‚’ç†æƒ³ã¨ã™ã‚‹ï¼‰
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 85.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã¨æ¥­å‹™ã®çµ„ã¿åˆã‚ã›ã®å¤šæ§˜æ€§
            staff_worktype_combinations = long_df.groupby(['staff', 'worktype']).size()
            
            # æ•´åˆæ€§ã‚¹ã‚³ã‚¢ï¼šå¤šæ§˜ãªçµ„ã¿åˆã‚ã›ãŒã‚ã‚‹ã»ã©æˆ¦ç•¥çš„
            total_possible_combinations = long_df['staff'].nunique() * long_df['worktype'].nunique()
            actual_combinations = len(staff_worktype_combinations)
            
            alignment_score = (actual_combinations / total_possible_combinations * 100) if total_possible_combinations > 0 else 85
            
            return min(alignment_score, 95)
        except Exception:
            return 85.0
    
    def _assess_direction_clarity(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æˆ¦ç•¥æ–¹å‘æ€§æ˜ç¢ºåº¦ã®è©•ä¾¡"""
        try:
            # æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ˜ç¢ºæ€§
            if 'worktype' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ä¸»è¦æ¥­å‹™ã®é›†ä¸­åº¦ï¼ˆæ–¹å‘æ€§ã®æ˜ç¢ºã•ï¼‰
            worktype_concentration = long_df['worktype'].value_counts(normalize=True)
            
            # ä¸Šä½3æ¥­å‹™ã®åˆè¨ˆæ¯”ç‡
            top_3_ratio = worktype_concentration.head(3).sum()
            
            # é©åº¦ãªé›†ä¸­ï¼ˆ70-90%ï¼‰ãŒç†æƒ³çš„
            if 0.7 <= top_3_ratio <= 0.9:
                clarity = 90
            else:
                deviation = abs(top_3_ratio - 0.8)
                clarity = max(60, 90 - deviation * 100)
            
            return clarity
        except Exception:
            return 80.0
    
    def _assess_vision_feasibility(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³å®Ÿç¾å¯èƒ½æ€§ã®è©•ä¾¡"""
        try:
            # ç¾åœ¨ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‹ã‚‰ãƒ“ã‚¸ãƒ§ãƒ³å®Ÿç¾å¯èƒ½æ€§ã‚’æ¨å®š
            if 'ds' not in long_df.columns:
                return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # å®‰å®šæ€§æŒ‡æ¨™ï¼ˆå®Ÿç¾å¯èƒ½æ€§ã®åŸºç›¤ï¼‰
            daily_stability = long_df.groupby('ds').size()
            stability_cv = daily_stability.std() / daily_stability.mean() if daily_stability.mean() > 0 else 0.3
            
            # å®‰å®šæ€§ãŒé«˜ã„ã»ã©å®Ÿç¾å¯èƒ½æ€§ãŒé«˜ã„
            feasibility = max(0, (1 - stability_cv) * 100)
            
            return min(feasibility, 95)
        except Exception:
            return 75.0
    
    def _evaluate_prediction_accuracy(self, long_df: pd.DataFrame) -> float:
        """å°†æ¥äºˆæ¸¬ç²¾åº¦ã®è©•ä¾¡"""
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦å‰‡æ€§ã‹ã‚‰äºˆæ¸¬ç²¾åº¦ã‚’æ¨å®š
            if 'ds' not in long_df.columns or 'worktype' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦å‰‡æ€§
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            long_df_copy['weekday'] = long_df_copy['ds'].dt.dayofweek
            
            weekday_patterns = long_df_copy.groupby('weekday')['worktype'].apply(lambda x: tuple(sorted(x.value_counts().index)))
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€è²«æ€§ãŒäºˆæ¸¬ç²¾åº¦ã«é–¢é€£
            unique_patterns = len(set(weekday_patterns))
            total_weekdays = len(weekday_patterns)
            
            if total_weekdays > 0:
                consistency = 1 - (unique_patterns / total_weekdays)
                accuracy = consistency * 100
                return max(accuracy, 50)
            
            return 70.0
        except Exception:
            return 70.0
    
    def _analyze_vision_reality_gap(self, long_df: pd.DataFrame) -> float:
        """ãƒ“ã‚¸ãƒ§ãƒ³ç¾å®Ÿã‚®ãƒ£ãƒƒãƒ—ã®åˆ†æ"""
        try:
            # ç†æƒ³ã¨ç¾å®Ÿã®ã‚®ãƒ£ãƒƒãƒ—ã‚’æ¨å®š
            if 'staff' not in long_df.columns or 'ds' not in long_df.columns:
                return 25.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆ25%ã®ã‚®ãƒ£ãƒƒãƒ—ï¼‰
            
            # ç†æƒ³çš„ãªç¨¼åƒç‡ã‚’90%ã¨ä»®å®š
            ideal_utilization = 0.9
            
            # ç¾å®Ÿã®ç¨¼åƒç‡
            total_staff = long_df['staff'].nunique()
            total_days = len(long_df['ds'].unique())
            total_shifts = len(long_df)
            
            actual_utilization = total_shifts / (total_staff * total_days) if (total_staff * total_days) > 0 else 0.7
            
            # ã‚®ãƒ£ãƒƒãƒ—ã®è¨ˆç®—
            gap = abs(ideal_utilization - actual_utilization) / ideal_utilization * 100
            
            return min(gap, 50)  # ä¸Šé™50%
        except Exception:
            return 25.0
    
    def _assess_adaptability(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å¤‰åŒ–é©å¿œèƒ½åŠ›ã®è©•ä¾¡"""
        try:
            # æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤šæ§˜æ€§ã‹ã‚‰é©å¿œèƒ½åŠ›ã‚’æ¨å®š
            if 'worktype' not in long_df.columns or 'staff' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã®å¤šæ§˜ãªã‚¹ã‚­ãƒ«ï¼ˆé©å¿œèƒ½åŠ›ã®æŒ‡æ¨™ï¼‰
            staff_versatility = long_df.groupby('staff')['worktype'].nunique()
            total_worktypes = long_df['worktype'].nunique()
            
            avg_versatility = staff_versatility.mean()
            adaptability = (avg_versatility / total_worktypes * 100) if total_worktypes > 0 else 70
            
            return min(adaptability, 90)
        except Exception:
            return 70.0
    
    def _evaluate_future_readiness(self, long_df: pd.DataFrame) -> float:
        """å°†æ¥æº–å‚™åº¦ã®è©•ä¾¡"""
        try:
            # ç¾åœ¨ã®ãƒªã‚½ãƒ¼ã‚¹ã®å……å®Ÿåº¦ã‹ã‚‰å°†æ¥æº–å‚™åº¦ã‚’æ¨å®š
            if 'staff' not in long_df.columns:
                return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # äººæã®åšã¿
            staff_count = long_df['staff'].nunique()
            
            # ã‚¹ã‚¿ãƒƒãƒ•æ•°ã‹ã‚‰æº–å‚™åº¦ã‚’æ¨å®šï¼ˆ10äººä»¥ä¸Šã§90%æº–å‚™å®Œäº†ã¨ä»®å®šï¼‰
            readiness = min(staff_count / 10 * 90, 90)
            
            return readiness
        except Exception:
            return 75.0
    
    def _assess_operational_sustainability(self, long_df: pd.DataFrame) -> float:
        """é‹å–¶æŒç¶šå¯èƒ½æ€§ã®è©•ä¾¡"""
        try:
            # é‹å–¶ã®å®‰å®šæ€§
            if 'ds' not in long_df.columns:
                return 85.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ç¶™ç¶šçš„ãªé‹å–¶ï¼ˆæ¯æ—¥ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ï¼‰
            total_days = len(long_df['ds'].unique())
            covered_days = len(long_df.groupby('ds').size())
            
            coverage_rate = (covered_days / total_days * 100) if total_days > 0 else 85
            
            return coverage_rate
        except Exception:
            return 85.0
    
    def _assess_human_sustainability(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """äººææŒç¶šå¯èƒ½æ€§ã®è©•ä¾¡"""
        try:
            # äººæã®å¤šæ§˜æ€§ã¨è² è·åˆ†æ•£
            if 'staff' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # è² è·ã®å‡ç­‰åˆ†æ•£ï¼ˆæŒç¶šå¯èƒ½æ€§ã®æŒ‡æ¨™ï¼‰
            staff_workload = long_df['staff'].value_counts()
            
            if staff_workload.mean() > 0:
                load_balance = 1 - (staff_workload.std() / staff_workload.mean())
                sustainability = max(0, load_balance) * 100
                return sustainability
            
            return 80.0
        except Exception:
            return 80.0
    
    def _assess_load_sustainability(self, long_df: pd.DataFrame) -> float:
        """è² è·æŒç¶šå¯èƒ½æ€§ã®è©•ä¾¡"""
        try:
            # è² è·ã®æ™‚é–“çš„å®‰å®šæ€§
            if 'ds' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            daily_load = long_df.groupby('ds').size()
            
            if daily_load.mean() > 0:
                load_stability = 1 - (daily_load.std() / daily_load.mean())
                sustainability = max(0, load_stability) * 100
                return sustainability
            
            return 80.0
        except Exception:
            return 80.0
    
    def _assess_quality_sustainability(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å“è³ªæŒç¶šå¯èƒ½æ€§ã®è©•ä¾¡"""
        try:
            # å“è³ªç¶­æŒã®ãŸã‚ã®ä½“åˆ¶
            if 'ds' not in long_df.columns or 'staff' not in long_df.columns:
                return 85.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # è¤‡æ•°äººä½“åˆ¶ã«ã‚ˆã‚‹å“è³ªä¿è¨¼
            daily_team_sizes = long_df.groupby('ds')['staff'].nunique()
            quality_days = (daily_team_sizes >= 2).sum()
            total_days = len(daily_team_sizes)
            
            quality_sustainability = (quality_days / total_days * 100) if total_days > 0 else 85
            
            return quality_sustainability
        except Exception:
            return 85.0
    
    def _assess_environmental_consideration(self, long_df: pd.DataFrame) -> float:
        """ç’°å¢ƒé…æ…®åº¦ã®è©•ä¾¡"""
        try:
            # åŠ¹ç‡çš„ãªé‹å–¶ã«ã‚ˆã‚‹ç’°å¢ƒé…æ…®
            if 'staff' not in long_df.columns or 'ds' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # åŠ¹ç‡çš„ãªäººå“¡é…ç½®ï¼ˆç’°å¢ƒè² è·è»½æ¸›ã®æŒ‡æ¨™ï¼‰
            total_staff = long_df['staff'].nunique()
            total_shifts = len(long_df)
            total_days = len(long_df['ds'].unique())
            
            efficiency = total_shifts / (total_staff * total_days) if (total_staff * total_days) > 0 else 0.7
            
            # åŠ¹ç‡æ€§ãŒé«˜ã„ã»ã©ç’°å¢ƒé…æ…®
            environmental_score = min(efficiency * 100, 85)
            
            return environmental_score
        except Exception:
            return 70.0
    
    def _assess_growth_potential(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æˆé•·ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã®è©•ä¾¡"""
        try:
            # ç¾åœ¨ã®æ´»ç”¨åº¦ã‹ã‚‰æˆé•·ä½™åœ°ã‚’æ¨å®š
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 60.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚­ãƒ«æ´»ç”¨åº¦
            staff_diversity = long_df.groupby('staff')['worktype'].nunique()
            total_worktypes = long_df['worktype'].nunique()
            
            current_utilization = staff_diversity.mean() / total_worktypes if total_worktypes > 0 else 0.6
            
            # æˆé•·ä½™åœ° = 1 - ç¾åœ¨ã®æ´»ç”¨åº¦
            growth_potential = (1 - current_utilization) * 100
            
            return min(growth_potential, 80)
        except Exception:
            return 60.0
    
    def _identify_development_areas(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> Dict[str, float]:
        """ç™ºå±•å¯èƒ½é ˜åŸŸã®ç‰¹å®š"""
        try:
            areas = {}
            
            if 'worktype' not in long_df.columns:
                return areas
            
            # ä½é »åº¦æ¥­å‹™ã®ç™ºå±•å¯èƒ½æ€§
            worktype_frequency = long_df['worktype'].value_counts(normalize=True)
            
            for worktype, frequency in worktype_frequency.items():
                if frequency < 0.2:  # 20%æœªæº€ã¯ç™ºå±•ä½™åœ°ã‚ã‚Š
                    development_potential = (0.2 - frequency) / 0.2 * 100
                    areas[str(worktype)] = development_potential
            
            return areas
        except Exception:
            return {}
    
    def _assess_scalability(self, long_df: pd.DataFrame) -> float:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®è©•ä¾¡"""
        try:
            # ç¾åœ¨ã®é‹å–¶ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ‹¡å¼µå¯èƒ½æ€§
            if 'staff' not in long_df.columns or 'ds' not in long_df.columns:
                return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ—¥åˆ¥ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã®å°‘ãªã•ï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æŒ‡æ¨™ï¼‰
            daily_patterns = long_df.groupby('ds')['staff'].nunique()
            
            if daily_patterns.mean() > 0:
                pattern_stability = 1 - (daily_patterns.std() / daily_patterns.mean())
                scalability = max(0, pattern_stability) * 100
                return scalability
            
            return 75.0
        except Exception:
            return 75.0
    
    def _identify_growth_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> Dict[str, float]:
        """æˆé•·åˆ¶ç´„ã®ç‰¹å®š"""
        try:
            constraints = {}
            
            # äººæåˆ¶ç´„
            if 'staff' in long_df.columns:
                staff_count = long_df['staff'].nunique()
                if staff_count < 5:  # 5äººæœªæº€ã¯äººæåˆ¶ç´„
                    constraints['äººæä¸è¶³'] = (5 - staff_count) / 5 * 5  # æ·±åˆ»åº¦1-5
            
            # æ¥­å‹™å¤šæ§˜æ€§åˆ¶ç´„
            if 'worktype' in long_df.columns:
                worktype_count = long_df['worktype'].nunique()
                if worktype_count < 4:  # 4ç¨®é¡æœªæº€ã¯å¤šæ§˜æ€§åˆ¶ç´„
                    constraints['æ¥­å‹™å¤šæ§˜æ€§ä¸è¶³'] = (4 - worktype_count) / 4 * 5
            
            return constraints
        except Exception:
            return {}
    
    def _optimize_development_speed(self, long_df: pd.DataFrame) -> float:
        """ç™ºå±•é€Ÿåº¦ã®æœ€é©åŒ–"""
        try:
            # ç¾åœ¨ã®å¤‰åŒ–ç‡ã‹ã‚‰æœ€é©ç™ºå±•é€Ÿåº¦ã‚’æ¨å®š
            if 'ds' not in long_df.columns:
                return 15.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå¹´15%ï¼‰
            
            # ãƒ‡ãƒ¼ã‚¿æœŸé–“
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            date_range = (long_df_copy['ds'].max() - long_df_copy['ds'].min()).days
            
            # æœŸé–“ãŒçŸ­ã„å ´åˆã¯æ…é‡ãªç™ºå±•é€Ÿåº¦
            if date_range < 90:  # 3ãƒ¶æœˆæœªæº€
                optimal_speed = 10.0
            elif date_range < 180:  # 6ãƒ¶æœˆæœªæº€
                optimal_speed = 15.0
            else:
                optimal_speed = 20.0  # é•·æœŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°ç©æ¥µçš„
            
            return optimal_speed
        except Exception:
            return 15.0
    
    def _assess_uniqueness(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ç‹¬è‡ªæ€§ã®è©•ä¾¡"""
        try:
            # æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç‹¬è‡ªæ€§
            if 'worktype' not in long_df.columns or 'staff' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # è¤‡é›‘ãªçµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç‹¬è‡ªæ€§ã®æŒ‡æ¨™ï¼‰
            staff_worktype_combinations = long_df.groupby(['staff', 'worktype']).size()
            
            total_combinations = len(staff_worktype_combinations)
            possible_combinations = long_df['staff'].nunique() * long_df['worktype'].nunique()
            
            complexity_ratio = total_combinations / possible_combinations if possible_combinations > 0 else 0.7
            uniqueness = complexity_ratio * 100
            
            return min(uniqueness, 90)
        except Exception:
            return 70.0
    
    def _identify_differentiation_factors(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> Dict[str, float]:
        """å·®åˆ¥åŒ–è¦å› ã®ç‰¹å®š"""
        try:
            factors = {}
            
            # é«˜é »åº¦æ¥­å‹™ã‚’å·®åˆ¥åŒ–è¦å› ã¨ã¿ãªã™
            if 'worktype' in long_df.columns:
                worktype_strength = long_df['worktype'].value_counts(normalize=True)
                
                for worktype, strength in worktype_strength.head(3).items():
                    factors[str(worktype)] = strength * 100
            
            return factors
        except Exception:
            return {}
    
    def _assess_competitive_sustainability(self, long_df: pd.DataFrame) -> float:
        """ç«¶äº‰å„ªä½æŒç¶šæ€§ã®è©•ä¾¡"""
        try:
            # é‹å–¶ã®å®‰å®šæ€§ã‹ã‚‰æŒç¶šæ€§ã‚’æ¨å®š
            if 'ds' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ç¶™ç¶šçš„ãªé‹å–¶ã®å®‰å®šæ€§
            daily_operations = long_df.groupby('ds').size()
            
            if daily_operations.mean() > 0:
                stability = 1 - (daily_operations.std() / daily_operations.mean())
                sustainability = max(0, stability) * 100
                return sustainability
            
            return 80.0
        except Exception:
            return 80.0
    
    def _assess_inimitability(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æ¨¡å€£å›°é›£æ€§ã®è©•ä¾¡"""
        try:
            # è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¨¡å€£å›°é›£æ€§
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¤‡é›‘ã•
            unique_patterns = long_df.groupby(['staff', 'worktype']).size()
            total_records = len(long_df)
            
            pattern_complexity = len(unique_patterns) / total_records if total_records > 0 else 0.75
            
            # è¤‡é›‘ãªã»ã©æ¨¡å€£å›°é›£
            inimitability = min(pattern_complexity * 100, 90)
            
            return inimitability
        except Exception:
            return 75.0
    
    def _assess_value_creation_capability(self, long_df: pd.DataFrame) -> float:
        """ä¾¡å€¤å‰µé€ èƒ½åŠ›ã®è©•ä¾¡"""
        try:
            # åŠ¹ç‡çš„ãªé‹å–¶ã«ã‚ˆã‚‹ä¾¡å€¤å‰µé€ 
            if 'staff' not in long_df.columns or 'ds' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # äººå“¡åŠ¹ç‡æ€§
            total_staff = long_df['staff'].nunique()
            total_shifts = len(long_df)
            efficiency = total_shifts / total_staff if total_staff > 0 else 0
            
            # åŸºæº–å€¤ã‚’10ã¨ã—ãŸå ´åˆã®ä¾¡å€¤å‰µé€ èƒ½åŠ›
            value_creation = min(efficiency / 10 * 100, 95)
            
            return value_creation
        except Exception:
            return 80.0
    
    def _assess_innovation_readiness(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æŠ€è¡“é©æ–°æº–å‚™åº¦ã®è©•ä¾¡"""
        try:
            # å¤‰åŒ–ã¸ã®å¯¾å¿œåŠ›ã‹ã‚‰é©æ–°æº–å‚™åº¦ã‚’æ¨å®š
            if 'worktype' not in long_df.columns:
                return 65.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ¥­å‹™ã®å¤šæ§˜æ€§ï¼ˆé©æ–°ã¸ã®é©å¿œåŠ›ï¼‰
            worktype_diversity = long_df['worktype'].nunique()
            
            # å¤šæ§˜æ€§ãŒé«˜ã„ã»ã©é©æ–°æº–å‚™åº¦ãŒé«˜ã„
            readiness = min(worktype_diversity / 5 * 80, 80)
            
            return readiness
        except Exception:
            return 65.0
    
    def _assess_technology_adoption_capability(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æŠ€è¡“å°å…¥èƒ½åŠ›ã®è©•ä¾¡"""
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•ã®é©å¿œåŠ›ã‹ã‚‰æŠ€è¡“å°å…¥èƒ½åŠ›ã‚’æ¨å®š
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã®å¤šæ§˜ãªã‚¹ã‚­ãƒ«ï¼ˆæŠ€è¡“é©å¿œåŠ›ã®åŸºç›¤ï¼‰
            staff_versatility = long_df.groupby('staff')['worktype'].nunique()
            avg_versatility = staff_versatility.mean()
            total_worktypes = long_df['worktype'].nunique()
            
            adoption_capability = (avg_versatility / total_worktypes * 100) if total_worktypes > 0 else 70
            
            return min(adoption_capability, 85)
        except Exception:
            return 70.0
    
    def _assess_digital_transformation_readiness(self, long_df: pd.DataFrame) -> float:
        """ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©æº–å‚™åº¦ã®è©•ä¾¡"""
        try:
            # ãƒ‡ãƒ¼ã‚¿åŒ–ã®é€²å±•åº¦ã‹ã‚‰ãƒ‡ã‚¸ã‚¿ãƒ«æº–å‚™åº¦ã‚’æ¨å®š
            if 'ds' not in long_df.columns:
                return 60.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ãƒ‡ãƒ¼ã‚¿ã®ç¶²ç¾…æ€§ï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã®åŸºç›¤ï¼‰
            data_completeness = len(long_df) / len(long_df['ds'].unique()) if len(long_df['ds'].unique()) > 0 else 1
            
            # ãƒ‡ãƒ¼ã‚¿ãŒè±Šå¯Œãªã»ã©ãƒ‡ã‚¸ã‚¿ãƒ«æº–å‚™åº¦ãŒé«˜ã„
            digital_readiness = min(data_completeness / 5 * 80, 80)
            
            return digital_readiness
        except Exception:
            return 60.0
    
    def _assess_automation_potential(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """è‡ªå‹•åŒ–å¯èƒ½æ€§ã®è©•ä¾¡"""
        try:
            # å®šå‹æ¥­å‹™ã®å‰²åˆã‹ã‚‰è‡ªå‹•åŒ–å¯èƒ½æ€§ã‚’æ¨å®š
            if 'worktype' not in long_df.columns:
                return 40.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # é »å‡ºæ¥­å‹™ã®è‡ªå‹•åŒ–å¯èƒ½æ€§
            worktype_frequency = long_df['worktype'].value_counts(normalize=True)
            
            # ä¸Šä½æ¥­å‹™ã®åˆè¨ˆï¼ˆå®šå‹æ¥­å‹™ã¨ã¿ãªã™ï¼‰
            routine_work_ratio = worktype_frequency.head(3).sum()
            
            automation_potential = routine_work_ratio * 60  # æœ€å¤§60%ã®è‡ªå‹•åŒ–
            
            return automation_potential
        except Exception:
            return 40.0
    
    def _assess_ai_readiness(self, long_df: pd.DataFrame) -> float:
        """AIæ´»ç”¨æº–å‚™åº¦ã®è©•ä¾¡"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®è“„ç©åº¦ã‹ã‚‰AIæº–å‚™åº¦ã‚’æ¨å®š
            if 'ds' not in long_df.columns:
                return 50.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ãƒ‡ãƒ¼ã‚¿ã®è±Šå¯Œã•ï¼ˆAIå­¦ç¿’ã®åŸºç›¤ï¼‰
            data_richness = len(long_df)
            
            # ãƒ‡ãƒ¼ã‚¿é‡ã«åŸºã¥ãAIæº–å‚™åº¦
            ai_readiness = min(data_richness / 1000 * 70, 70)  # 1000ãƒ¬ã‚³ãƒ¼ãƒ‰ã§70%æº–å‚™å®Œäº†
            
            return ai_readiness
        except Exception:
            return 50.0
    
    def _assess_transformation_capability(self, long_df: pd.DataFrame) -> float:
        """çµ„ç¹”å¤‰é©èƒ½åŠ›ã®è©•ä¾¡"""
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•ã®å¤šæ§˜æ€§ã‹ã‚‰å¤‰é©èƒ½åŠ›ã‚’æ¨å®š
            if 'staff' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # äººæã®å¤šæ§˜æ€§ï¼ˆå¤‰é©ã®åŸºç›¤ï¼‰
            staff_diversity = long_df['staff'].nunique()
            
            # å¤šæ§˜ãªã‚¹ã‚¿ãƒƒãƒ•ãŒã„ã‚‹ã»ã©å¤‰é©èƒ½åŠ›ãŒé«˜ã„
            transformation_capability = min(staff_diversity / 8 * 80, 80)
            
            return transformation_capability
        except Exception:
            return 70.0
    
    def _assess_change_resistance(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å¤‰åŒ–æŠµæŠ—åº¦ã®è©•ä¾¡"""
        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å›ºå®šåº¦ã‹ã‚‰æŠµæŠ—åº¦ã‚’æ¨å®š
            if 'worktype' not in long_df.columns:
                return 30.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å›ºå®šåº¦
            worktype_concentration = long_df['worktype'].value_counts(normalize=True)
            top_ratio = worktype_concentration.iloc[0] if len(worktype_concentration) > 0 else 0.5
            
            # é›†ä¸­åº¦ãŒé«˜ã„ã»ã©å¤‰åŒ–æŠµæŠ—ãŒå¤§ãã„
            resistance = min(top_ratio * 50, 50)
            
            return resistance
        except Exception:
            return 30.0
    
    def _assess_learning_organization_development(self, long_df: pd.DataFrame) -> float:
        """å­¦ç¿’çµ„ç¹”ç™ºå±•åº¦ã®è©•ä¾¡"""
        try:
            # ã‚¹ã‚­ãƒ«ã®å¤šæ§˜åŒ–ã‹ã‚‰å­¦ç¿’çµ„ç¹”åº¦ã‚’æ¨å®š
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚¹ã‚­ãƒ«å¤šæ§˜æ€§
            staff_skill_diversity = long_df.groupby('staff')['worktype'].nunique()
            avg_diversity = staff_skill_diversity.mean()
            
            # å¤šæ§˜æ€§ãŒé«˜ã„ã»ã©å­¦ç¿’çµ„ç¹”
            learning_score = min(avg_diversity / 3 * 90, 90)
            
            return learning_score
        except Exception:
            return 75.0
    
    def _assess_cultural_flexibility(self, long_df: pd.DataFrame) -> float:
        """çµ„ç¹”æ–‡åŒ–æŸ”è»Ÿæ€§ã®è©•ä¾¡"""
        try:
            # æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•æ€§ã‹ã‚‰æ–‡åŒ–æŸ”è»Ÿæ€§ã‚’æ¨å®š
            if 'ds' not in long_df.columns or 'worktype' not in long_df.columns:
                return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ—¥åˆ¥æ¥­å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤‰å‹•
            daily_patterns = long_df.groupby('ds')['worktype'].apply(lambda x: tuple(sorted(x)))
            pattern_variety = len(set(daily_patterns))
            total_days = len(daily_patterns)
            
            flexibility = (pattern_variety / total_days * 100) if total_days > 0 else 70
            
            return min(flexibility, 90)
        except Exception:
            return 70.0
    
    def _assess_transformational_leadership(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å¤‰é©ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã®è©•ä¾¡"""
        try:
            # å¤šæ§˜ãªå½¹å‰²ã®å®Ÿè¡Œã‹ã‚‰å¤‰é©åŠ›ã‚’æ¨å®š
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ãƒªãƒ¼ãƒ€ãƒ¼çš„å½¹å‰²ã‚’æŒã¤ã‚¹ã‚¿ãƒƒãƒ•ã®æ¯”ç‡
            staff_leadership = long_df.groupby('staff')['worktype'].nunique()
            versatile_staff = (staff_leadership >= 2).sum()
            total_staff = len(staff_leadership)
            
            leadership_ratio = (versatile_staff / total_staff * 100) if total_staff > 0 else 75
            
            return leadership_ratio
        except Exception:
            return 75.0
    
    def _assess_traditional_value_preservation(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ä¼çµ±çš„ä¾¡å€¤ä¿æŒåº¦ã®è©•ä¾¡"""
        try:
            # ä¸­æ ¸æ¥­å‹™ã®ç¶™ç¶šæ€§ã‹ã‚‰ä¼çµ±ä¾¡å€¤ä¿æŒã‚’è©•ä¾¡
            if 'worktype' not in long_df.columns:
                return 85.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ä¸»è¦æ¥­å‹™ã®å®‰å®šçš„å®Ÿè¡Œ
            worktype_frequency = long_df['worktype'].value_counts(normalize=True)
            core_business_ratio = worktype_frequency.iloc[0] if len(worktype_frequency) > 0 else 0.6
            
            # ä¸­æ ¸æ¥­å‹™ãŒç¶­æŒã•ã‚Œã¦ã„ã‚‹ã»ã©ä¼çµ±ä¾¡å€¤ä¿æŒ
            preservation = min(core_business_ratio * 100, 90)
            
            return preservation
        except Exception:
            return 85.0
    
    def _assess_knowledge_inheritance_system(self, long_df: pd.DataFrame) -> float:
        """çŸ¥è­˜ç¶™æ‰¿ã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡"""
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•ã®ç¶™ç¶šæ€§ã‹ã‚‰çŸ¥è­˜ç¶™æ‰¿ã‚’è©•ä¾¡
            if 'staff' not in long_df.columns or 'ds' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã®ç¶™ç¶šçš„å‚åŠ 
            staff_participation = long_df.groupby('staff').size()
            avg_participation = staff_participation.mean()
            
            # ç¶™ç¶šçš„å‚åŠ ãŒå¤šã„ã»ã©çŸ¥è­˜ç¶™æ‰¿ãŒæ©Ÿèƒ½
            inheritance_score = min(avg_participation / 10 * 90, 90)
            
            return inheritance_score
        except Exception:
            return 80.0
    
    def _assess_experience_utilization(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """çµŒé¨“è“„ç©æ´»ç”¨åº¦ã®è©•ä¾¡"""
        try:
            # çµŒé¨“è±Šå¯Œãªã‚¹ã‚¿ãƒƒãƒ•ã®æ´»ç”¨åº¦
            if 'staff' not in long_df.columns:
                return 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã®çµŒé¨“åº¦ï¼ˆå‹¤å‹™å›æ•°ã§ä»£ç”¨ï¼‰
            staff_experience = long_df['staff'].value_counts()
            experienced_staff = (staff_experience >= staff_experience.median()).sum()
            total_staff = len(staff_experience)
            
            utilization_rate = (experienced_staff / total_staff * 100) if total_staff > 0 else 80
            
            return utilization_rate
        except Exception:
            return 80.0
    
    def _assess_organizational_memory(self, long_df: pd.DataFrame) -> float:
        """çµ„ç¹”è¨˜æ†¶ä¿æŒåº¦ã®è©•ä¾¡"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®ç¶™ç¶šæ€§ã‹ã‚‰çµ„ç¹”è¨˜æ†¶ã‚’è©•ä¾¡
            if 'ds' not in long_df.columns:
                return 85.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # ãƒ‡ãƒ¼ã‚¿è“„ç©æœŸé–“
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            data_span = (long_df_copy['ds'].max() - long_df_copy['ds'].min()).days
            
            # æœŸé–“ãŒé•·ã„ã»ã©çµ„ç¹”è¨˜æ†¶ãŒå……å®Ÿ
            memory_score = min(data_span / 365 * 90, 90)  # 1å¹´ã§90%
            
            return memory_score
        except Exception:
            return 85.0
    
    def _assess_continuity_innovation_balance(self, long_df: pd.DataFrame) -> float:
        """ç¶™ç¶šæ€§ãƒ»é©æ–°ãƒãƒ©ãƒ³ã‚¹ã®è©•ä¾¡"""
        try:
            # å®‰å®šæ€§ã¨å¤‰åŒ–ã®ãƒãƒ©ãƒ³ã‚¹
            if 'worktype' not in long_df.columns or 'ds' not in long_df.columns:
                return 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            # æ¥­å‹™ã®å®‰å®šæ€§
            worktype_consistency = long_df['worktype'].value_counts(normalize=True)
            stability = worktype_consistency.iloc[0] if len(worktype_consistency) > 0 else 0.6
            
            # å¤‰åŒ–æ€§ï¼ˆæ¥­å‹™ã®å¤šæ§˜æ€§ï¼‰
            innovation = 1 - stability
            
            # ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆ0.5ã«è¿‘ã„ã»ã©ç†æƒ³çš„ï¼‰
            balance = 1 - abs(stability - 0.5) * 2
            
            return max(0, balance)
        except Exception:
            return 0.8
    
    def _generate_human_readable_results(self, mece_facts: Dict[str, List[str]], long_df: pd.DataFrame) -> str:
        """äººé–“å¯èª­å½¢å¼ã®çµæœç”Ÿæˆ"""
        
        result = f"""
=== è»¸12: {self.axis_name} MECEåˆ†æçµæœ ===

ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:
- åˆ†ææœŸé–“: {long_df['ds'].min()} ï½ {long_df['ds'].max()}
- å¯¾è±¡ã‚¹ã‚¿ãƒƒãƒ•æ•°: {long_df['staff'].nunique()}äºº
- ç·å‹¤å‹™å›æ•°: {len(long_df)}å›
- 12è»¸ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é ‚ç‚¹ã¨ã—ã¦å…¨è»¸ã‚’çµ±åˆã—ãŸæˆ¦ç•¥çš„åˆ¶ç´„ã‚’æŠ½å‡º

ğŸ” MECEåˆ†è§£ã«ã‚ˆã‚‹åˆ¶ç´„æŠ½å‡º:

"""
        
        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®çµæœã‚’æ•´ç†
        for category, facts in mece_facts.items():
            result += f"\nã€{category}ã€‘\n"
            for fact in facts:
                result += f"  â€¢ {fact}\n"
        
        result += f"""

ğŸ’¡ ä¸»è¦ç™ºè¦‹äº‹é …:
- æˆ¦ç•¥çš„æ–¹å‘æ€§ã®æ˜ç¢ºåŒ–ãŒé•·æœŸæˆåŠŸã®åŸºç›¤
- æŒç¶šå¯èƒ½æ€§ã¨æˆé•·ã®ãƒãƒ©ãƒ³ã‚¹ãŒé‡è¦
- æŠ€è¡“é©æ–°ã¨ä¼çµ±çš„ä¾¡å€¤ã®èª¿å’ŒãŒç«¶äº‰å„ªä½ã®æºæ³‰
- çµ„ç¹”å¤‰é©èƒ½åŠ›ãŒå°†æ¥ã¸ã®é©å¿œåŠ›ã‚’æ±ºå®š

âš ï¸ æ³¨æ„äº‹é …:
- æœ¬åˆ†æã¯éå»å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæˆ¦ç•¥åˆ¶ç´„æŠ½å‡º
- å¤–éƒ¨ç’°å¢ƒå¤‰åŒ–ã¨å¸‚å ´å‹•å‘ã®ç¶™ç¶šçš„ç›£è¦–ãŒå¿…è¦
- é•·æœŸãƒ“ã‚¸ãƒ§ãƒ³ã¨çŸ­æœŸå®Ÿè¡Œã®æ•´åˆæ€§ç¢ºä¿ãŒé‡è¦
- å…¨è»¸åˆ¶ç´„ã¨ã®çµ±åˆçš„é‹ç”¨ãŒæœ€é‡è¦

ğŸš€ æˆ¦ç•¥çš„æè¨€:
- 12è»¸çµ±åˆåˆ¶ç´„ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰
- ç¶™ç¶šçš„ãªæˆ¦ç•¥è¦‹ç›´ã—ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç¢ºç«‹
- ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã¨å®‰å®šæ€§ã®å‹•çš„ãƒãƒ©ãƒ³ã‚¹ç®¡ç†
- å°†æ¥å¿—å‘ã®çµ„ç¹”å¤‰é©ãƒ—ãƒ­ã‚°ãƒ©ãƒ å®Ÿæ–½

---
è»¸12åˆ†æå®Œäº† - 12è»¸MECEåˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å…¨ä½“å®Œæˆ ({datetime.now().strftime("%Y-%m-%d %H:%M:%S")})
"""
        return result
    
    def _generate_machine_readable_constraints(self, mece_facts: Dict[str, List[str]], long_df: pd.DataFrame) -> Dict[str, Any]:
        """æ©Ÿæ¢°å¯èª­å½¢å¼ã®åˆ¶ç´„ç”Ÿæˆ"""
        
        constraints = {
            "constraint_type": "strategy_future_vision",
            "priority": "CRITICAL",  # æœ€ä¸Šä½è»¸ã¨ã—ã¦æœ€é«˜å„ªå…ˆåº¦
            "axis_relationships": {
                "integration_level": "COMPLETE",  # å®Œå…¨çµ±åˆ
                "coordinates_all_axes": True,  # å…¨è»¸ã‚’çµ±æ‹¬
                "strategic_oversight": ["axis1_facility_rules", "axis2_staff_rules", "axis3_time_calendar", 
                                      "axis4_demand_load", "axis5_medical_care_quality", "axis6_cost_efficiency",
                                      "axis7_legal_regulatory", "axis8_staff_satisfaction", "axis9_business_process", 
                                      "axis10_risk_emergency", "axis11_performance_improvement"]
            },
            "strategic_direction_rules": [],
            "future_vision_requirements": [],
            "sustainability_mandates": [],
            "growth_development_guidelines": [],
            "competitive_advantage_strategies": [],
            "innovation_technology_roadmap": [],
            "organizational_transformation_plans": [],
            "legacy_preservation_protocols": []
        }
        
        # å„MECE ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰åˆ¶ç´„ã‚’æŠ½å‡º
        for category, facts in mece_facts.items():
            if "æˆ¦ç•¥çš„æ–¹å‘æ€§" in category:
                constraints["strategic_direction_rules"].extend([
                    {
                        "rule": "strategic_consistency",
                        "min_consistency_score": 0.8,
                        "alignment_target": self.strategy_standards['strategic_alignment_score'],
                        "confidence": 0.95
                    },
                    {
                        "rule": "core_competency_focus",
                        "core_business_ratio": 0.6,
                        "direction_clarity_threshold": 0.8,
                        "confidence": 0.90
                    }
                ])
            
            elif "å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³" in category:
                constraints["future_vision_requirements"].extend([
                    {
                        "requirement": "vision_feasibility",
                        "min_feasibility_score": 0.75,
                        "prediction_accuracy_target": 0.7,
                        "confidence": 0.85
                    },
                    {
                        "requirement": "adaptability_capability",
                        "min_adaptability_score": 0.7,
                        "future_readiness_target": 0.75,
                        "confidence": 0.80
                    }
                ])
            
            elif "æŒç¶šå¯èƒ½æ€§" in category:
                constraints["sustainability_mandates"].extend([
                    {
                        "mandate": "operational_sustainability",
                        "min_sustainability_score": self.strategy_standards['sustainability_target_score'],
                        "environmental_consideration_target": 0.7,
                        "confidence": 0.90
                    },
                    {
                        "mandate": "human_resource_sustainability",
                        "load_balance_threshold": 0.8,
                        "quality_maintenance_target": 0.85,
                        "confidence": 0.85
                    }
                ])
            
            elif "æˆé•·ãƒ»ç™ºå±•" in category:
                constraints["growth_development_guidelines"].extend([
                    {
                        "guideline": "growth_potential_realization",
                        "target_growth_rate": self.strategy_standards['growth_target_rate'],
                        "scalability_requirement": 0.75,
                        "confidence": 0.80
                    },
                    {
                        "guideline": "development_speed_optimization",
                        "optimal_speed_range": [0.1, 0.2],  # å¹´10-20%
                        "constraint_management": "proactive",
                        "confidence": 0.75
                    }
                ])
            
            elif "ç«¶äº‰å„ªä½æ€§" in category:
                constraints["competitive_advantage_strategies"].extend([
                    {
                        "strategy": "uniqueness_preservation",
                        "min_uniqueness_score": 0.7,
                        "competitive_advantage_target": self.strategy_standards['competitive_advantage_score'],
                        "confidence": 0.85
                    },
                    {
                        "strategy": "inimitability_enhancement",
                        "min_inimitability_score": 0.75,
                        "value_creation_target": 0.8,
                        "confidence": 0.80
                    }
                ])
            
            elif "æŠ€è¡“é©æ–°" in category:
                constraints["innovation_technology_roadmap"].extend([
                    {
                        "roadmap": "innovation_adoption",
                        "annual_adoption_rate": self.strategy_standards['innovation_adoption_rate'],
                        "technology_readiness_target": 0.7,
                        "confidence": 0.75
                    },
                    {
                        "roadmap": "digital_transformation",
                        "digital_readiness_target": 0.6,
                        "automation_potential_threshold": 0.4,
                        "ai_readiness_target": 0.5,
                        "confidence": 0.70
                    }
                ])
            
            elif "çµ„ç¹”å¤‰é©" in category:
                constraints["organizational_transformation_plans"].extend([
                    {
                        "plan": "transformation_capability",
                        "min_capability_score": 0.7,
                        "agility_target": self.strategy_standards['organizational_agility_score'],
                        "confidence": 0.80
                    },
                    {
                        "plan": "change_management",
                        "max_resistance_threshold": 0.3,
                        "learning_organization_target": 0.75,
                        "cultural_flexibility_target": 0.7,
                        "confidence": 0.75
                    }
                ])
            
            elif "ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ»ç¶™æ‰¿" in category:
                constraints["legacy_preservation_protocols"].extend([
                    {
                        "protocol": "traditional_value_preservation",
                        "min_preservation_score": self.strategy_standards['legacy_preservation_score'],
                        "knowledge_inheritance_target": 0.8,
                        "confidence": 0.90
                    },
                    {
                        "protocol": "continuity_innovation_balance",
                        "optimal_balance_range": [0.6, 0.9],
                        "organizational_memory_target": 0.85,
                        "confidence": 0.85
                    }
                ])
        
        # 12è»¸çµ±åˆãƒ¡ã‚¿åˆ¶ç´„
        constraints["meta_integration_rules"] = [
            {
                "rule": "twelve_axis_harmony",
                "all_axes_compliance_target": 0.9,
                "inter_axis_conflict_resolution": "strategic_priority_based",
                "confidence": 0.95
            },
            {
                "rule": "strategic_coherence",
                "coherence_score_target": 0.85,
                "long_term_vision_alignment": True,
                "confidence": 0.90
            }
        ]
        
        return constraints
    
    def _generate_extraction_metadata(self, long_df: pd.DataFrame, wt_df: pd.DataFrame, mece_facts: Dict[str, List[str]]) -> Dict[str, Any]:
        """æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        
        metadata = {
            "extraction_info": {
                "axis_number": self.axis_number,
                "axis_name": self.axis_name,
                "extraction_timestamp": datetime.now().isoformat(),
                "data_source": "historical_shift_records",
                "analysis_scope": "comprehensive_strategic_future_vision_constraints",
                "framework_completion": "COMPLETE"  # 12è»¸ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å®Œæˆ
            },
            
            "data_quality": {
                "total_records": len(long_df),
                "date_range": {
                    "start": str(long_df['ds'].min()),
                    "end": str(long_df['ds'].max()),
                    "total_days": len(long_df['ds'].unique())
                },
                "staff_coverage": {
                    "total_staff": long_df['staff'].nunique(),
                    "avg_shifts_per_staff": len(long_df) / long_df['staff'].nunique()
                },
                "completeness_score": self._calculate_data_completeness(long_df, wt_df)
            },
            
            "mece_analysis": {
                "total_categories": len(mece_facts),
                "categories": list(mece_facts.keys()),
                "facts_per_category": {cat: len(facts) for cat, facts in mece_facts.items()},
                "total_extracted_facts": sum(len(facts) for facts in mece_facts.values())
            },
            
            "axis_relationships": {
                "framework_apex": True,  # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é ‚ç‚¹
                "integrated_axes_count": 11,  # è»¸1-11ã‚’çµ±åˆ
                "strategic_coordination_level": "MAXIMUM",
                "constraint_priority": "CRITICAL",
                "integration_complexity": "ULTIMATE"
            },
            
            "strategic_assessment": {
                "strategic_consistency_score": self._calculate_strategic_consistency_score(long_df),
                "future_readiness_score": self._calculate_future_readiness_score(long_df),
                "sustainability_score": self._calculate_sustainability_score(long_df),
                "competitive_advantage_score": self._calculate_competitive_advantage_score(long_df),
                "overall_strategic_maturity": self._calculate_overall_strategic_maturity(long_df)
            },
            
            "confidence_indicators": {
                "data_reliability": 0.92,
                "pattern_confidence": 0.85,
                "constraint_validity": 0.88,
                "recommendation_strength": 0.90,
                "strategic_insight_quality": 0.87
            },
            
            "twelve_axis_framework_completion": {
                "framework_status": "COMPLETE",
                "total_axes_implemented": 12,
                "integration_level": "FULL",
                "strategic_coverage": "COMPREHENSIVE",
                "operational_readiness": "HIGH"
            },
            
            "limitations": [
                "å¤–éƒ¨ç’°å¢ƒãƒ»å¸‚å ´å‹•å‘ã®åˆ†æãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                "ç«¶åˆä»–ç¤¾ã¨ã®æ¯”è¼ƒåˆ†æãƒ‡ãƒ¼ã‚¿æ¬ å¦‚",
                "é•·æœŸæˆ¦ç•¥åŠ¹æœã®å®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ä¸è¶³",
                "ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼è¦–ç‚¹ã®çµ„ã¿è¾¼ã¿é™ç•Œ"
            ],
            
            "strategic_recommendations": [
                "12è»¸çµ±åˆåˆ¶ç´„ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰",
                "æˆ¦ç•¥çš„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¨KPIä½“ç³»ã®ç¢ºç«‹",
                "ç¶™ç¶šçš„æˆ¦ç•¥è¦‹ç›´ã—ã¨ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãƒ—ãƒ­ã‚»ã‚¹å°å…¥",
                "å…¨è»¸åˆ¶ç´„ã®å‹•çš„ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ æ§‹ç¯‰",
                "AIé§†å‹•ã‚·ãƒ•ãƒˆä½œæˆã¸ã®æˆ¦ç•¥åˆ¶ç´„çµ±åˆ"
            ]
        }
        
        return metadata
    
    def _calculate_data_completeness(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            required_columns = ['staff', 'ds', 'worktype']
            present_columns = sum(1 for col in required_columns if col in long_df.columns)
            completeness = present_columns / len(required_columns)
            
            # è¿½åŠ è¦ç´ ã®è€ƒæ…®
            if wt_df is not None and not wt_df.empty:
                completeness += 0.15
            
            # æˆ¦ç•¥åˆ†æã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿å……å®Ÿåº¦
            if len(long_df) > 100:  # ååˆ†ãªãƒ‡ãƒ¼ã‚¿é‡
                completeness += 0.1
            
            return min(completeness, 1.0)
        except Exception:
            return 0.0
    
    def _calculate_strategic_consistency_score(self, long_df: pd.DataFrame) -> float:
        """æˆ¦ç•¥çš„ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            return self._assess_strategic_consistency(long_df, None)
        except Exception:
            return 0.8
    
    def _calculate_future_readiness_score(self, long_df: pd.DataFrame) -> float:
        """å°†æ¥æº–å‚™åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            readiness_factors = [
                self._evaluate_future_readiness(long_df) / 100,
                self._assess_adaptability(long_df, None) / 100,
                self._assess_innovation_readiness(long_df, None) / 100
            ]
            
            return np.mean(readiness_factors)
        except Exception:
            return 0.75
    
    def _calculate_sustainability_score(self, long_df: pd.DataFrame) -> float:
        """æŒç¶šå¯èƒ½æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            sustainability_factors = [
                self._assess_operational_sustainability(long_df) / 100,
                self._assess_human_sustainability(long_df, None) / 100,
                self._assess_load_sustainability(long_df) / 100
            ]
            
            return np.mean(sustainability_factors)
        except Exception:
            return 0.85
    
    def _calculate_competitive_advantage_score(self, long_df: pd.DataFrame) -> float:
        """ç«¶äº‰å„ªä½æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            advantage_factors = [
                self._assess_uniqueness(long_df, None) / 100,
                self._assess_competitive_sustainability(long_df) / 100,
                self._assess_value_creation_capability(long_df) / 100
            ]
            
            return np.mean(advantage_factors)
        except Exception:
            return 0.8
    
    def _calculate_overall_strategic_maturity(self, long_df: pd.DataFrame) -> float:
        """ç·åˆæˆ¦ç•¥æˆç†Ÿåº¦ã®è¨ˆç®—"""
        try:
            maturity_components = [
                self._calculate_strategic_consistency_score(long_df),
                self._calculate_future_readiness_score(long_df),
                self._calculate_sustainability_score(long_df),
                self._calculate_competitive_advantage_score(long_df)
            ]
            
            return np.mean(maturity_components)
        except Exception:
            return 0.8


# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    import pandas as pd
    from datetime import datetime, timedelta
    
    # ã‚µãƒ³ãƒ—ãƒ«é•·æœŸãƒ‡ãƒ¼ã‚¿
    dates = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(30)]
    staff_list = ['ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨', 'é«˜æ©‹', 'æ¸¡è¾º']
    worktype_list = ['æ—¥å‹¤', 'å¤œå‹¤', 'æ—©ç•ª', 'é…ç•ª']
    
    sample_data = []
    for date in dates:
        for staff in staff_list[:3]:  # æ¯æ—¥3åå‹¤å‹™
            worktype = np.random.choice(worktype_list)
            sample_data.append({
                'ds': date.strftime('%Y-%m-%d'),
                'staff': staff,
                'worktype': worktype
            })
    
    long_df = pd.DataFrame(sample_data)
    
    # ã‚µãƒ³ãƒ—ãƒ«å‹¤å‹™åŒºåˆ†ãƒã‚¹ã‚¿
    wt_df = pd.DataFrame([
        {'worktype': 'æ—¥å‹¤', 'worktype_name': 'æ—¥å‹¤8æ™‚é–“'},
        {'worktype': 'å¤œå‹¤', 'worktype_name': 'å¤œå‹¤12æ™‚é–“'},
        {'worktype': 'æ—©ç•ª', 'worktype_name': 'æ—©ç•ª8æ™‚é–“'},
        {'worktype': 'é…ç•ª', 'worktype_name': 'é…ç•ª8æ™‚é–“'}
    ])
    
    # æŠ½å‡ºå®Ÿè¡Œ
    extractor = StrategyFutureMECEFactExtractor()
    results = extractor.extract_axis12_strategy_future_rules(long_df, wt_df)
    
    print("=== è»¸12: æˆ¦ç•¥ãƒ»å°†æ¥å±•æœ›åˆ¶ç´„æŠ½å‡ºçµæœ ===")
    print(results['human_readable'])
    print("\n=== æ©Ÿæ¢°å¯èª­åˆ¶ç´„ ===")
    print(json.dumps(results['machine_readable'], indent=2, ensure_ascii=False))