# shift_suite / tasks / heatmap.py
# v1.8.1 (æ—¥æ›œæ—¥Needè¨ˆç®—ä¿®æ­£ç‰ˆ)
from __future__ import annotations

import datetime as dt
from datetime import time
from pathlib import Path
from typing import List, Set

import numpy as np
import openpyxl
import pandas as pd
import logging
from openpyxl.formatting.rule import ColorScaleRule
from openpyxl.styles import PatternFill
from openpyxl.utils import get_column_letter

from .constants import SUMMARY5, DEFAULT_SLOT_MINUTES
from shift_suite.i18n import translate as _

# 'log' ã¨ã„ã†åå‰ã§ãƒ­ã‚¬ãƒ¼ã‚’å–å¾— (utils.pyã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã‚‹logã¨åŒã˜)
from .utils import (
    _parse_as_date,
    derive_max_staff,
    gen_labels,
    log,
    safe_sheet,
    save_df_xlsx,
    write_meta,
    validate_need_calculation,
)

analysis_logger = logging.getLogger('analysis')


def apply_business_hours_constraint(need_df: pd.DataFrame, business_start: time = time(8, 0), business_end: time = time(17, 30)) -> pd.DataFrame:
    """
    å–¶æ¥­æ™‚é–“åˆ¶ç´„ã‚’é©ç”¨ï¼ˆç¾å®Ÿæ€§ç¢ºä¿ï¼‰
    
    Args:
        need_df: éœ€è¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        business_start: å–¶æ¥­é–‹å§‹æ™‚åˆ»ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 8:00ï¼‰
        business_end: å–¶æ¥­çµ‚äº†æ™‚åˆ»ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 17:30ï¼‰
        
    Returns:
        å–¶æ¥­æ™‚é–“å¤–ã®éœ€è¦ã‚’0ã«è¨­å®šã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    
    filtered_df = need_df.copy()
    slots_filtered = 0
    total_slots = 0
    original_total = need_df.sum().sum()
    
    for time_slot in need_df.index:
        total_slots += 1
        try:
            # æ™‚é–“æ–‡å­—åˆ—ã‚’è§£æ (ä¾‹: "08:00", "17:30")
            hour, minute = map(int, str(time_slot).split(':'))
            slot_time = time(hour, minute)
            
            # å–¶æ¥­æ™‚é–“å¤–ã¯éœ€è¦ã‚’0ã«è¨­å®š
            if not (business_start <= slot_time <= business_end):
                filtered_df.loc[time_slot] = 0
                slots_filtered += 1
                
        except (ValueError, AttributeError):
            # è§£æã§ããªã„å ´åˆã¯ä¿æŒ
            log.debug(f"[REALISTIC] æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆè§£æã‚¨ãƒ©ãƒ¼: {time_slot}")
    
    filtered_total = filtered_df.sum().sum()
    reduction_ratio = (original_total - filtered_total) / original_total if original_total > 0 else 0
    
    log.info(f"[REALISTIC] å–¶æ¥­æ™‚é–“åˆ¶ç´„é©ç”¨å®Œäº†:")
    log.info(f"[REALISTIC]   ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã‚¹ãƒ­ãƒƒãƒˆ: {slots_filtered}/{total_slots} ({slots_filtered/total_slots*100:.1f}%)")
    log.info(f"[REALISTIC]   éœ€è¦å‰Šæ¸›: {original_total:.1f} â†’ {filtered_total:.1f} (-{reduction_ratio*100:.1f}%)")
    
    return filtered_df


# é€šå¸¸å‹¤å‹™ã®åˆ¤å®šç”¨å®šæ•°


def calculate_integrated_monthly_pattern_need(
    actual_staff_by_slot_and_date: pd.DataFrame,
    ref_start_date: dt.date,
    ref_end_date: dt.date,
    statistic_method: str,
    remove_outliers: bool,
    iqr_multiplier: float = 1.5,
    slot_minutes_for_empty: int = DEFAULT_SLOT_MINUTES,
    *,
    holidays: set[dt.date] | None = None,
    adjustment_factor: float = 1.0,
    include_zero_days: bool = True,
    all_dates_in_period: list[dt.date] | None = None,
    business_hours_only: bool = True,  # ğŸ”§ REALISTIC FIX: å–¶æ¥­æ™‚é–“åˆ¶ç´„ã‚ªãƒ—ã‚·ãƒ§ãƒ³
) -> pd.DataFrame:
    """
    æœˆæ¬¡åŸºæº–å€¤çµ±åˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆé©æ–°çš„æ‰‹æ³•ï¼‰
    
    1. å„æœˆã‹ã‚‰æ›œæ—¥Ã—æ™‚é–“å¸¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
    2. æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ±è¨ˆçš„ã«çµ±åˆ
    3. çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ï¼ˆæœŸé–“ä¾å­˜æ€§å®Œå…¨è§£æ±ºï¼‰
    
    æœŸé–“ã«é–¢ä¿‚ãªãä¸€è²«ã—ãŸçµæœã‚’ä¿è¨¼ã™ã‚‹æ•°å­¦çš„è§£æ±ºç­–
    """
    log.info(f"[INTEGRATED_PATTERN] æœˆæ¬¡çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ–¹å¼é–‹å§‹: {ref_start_date} - {ref_end_date}")
    
    # æœŸé–“ã‚’æœˆåˆ¥ã«åˆ†å‰²
    monthly_data = {}
    current_date = ref_start_date
    
    while current_date <= ref_end_date:
        month_key = current_date.strftime('%Y-%m')
        if month_key not in monthly_data:
            monthly_data[month_key] = {
                'dates': [],
                'start_date': current_date,
                'end_date': current_date
            }
        
        monthly_data[month_key]['dates'].append(current_date)
        monthly_data[month_key]['end_date'] = current_date
        current_date += dt.timedelta(days=1)
    
    log.info(f"[INTEGRATED_PATTERN] æ¤œå‡ºæœˆæ•°: {len(monthly_data)}")
    
    # Step 1: å„æœˆã®åŸºæº–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
    monthly_patterns = []
    
    for month_key, month_info in monthly_data.items():
        log.info(f"[INTEGRATED_PATTERN] === {month_key} ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ ===")
        
        # è©²å½“æœˆã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        month_columns = []
        for date_obj in month_info['dates']:
            for col in actual_staff_by_slot_and_date.columns:
                if isinstance(col, dt.date) and col == date_obj:
                    month_columns.append(col)
                elif isinstance(col, str):
                    try:
                        col_date = pd.to_datetime(col).date()
                        if col_date == date_obj:
                            month_columns.append(col)
                    except:
                        continue
        
        if not month_columns:
            log.warning(f"[INTEGRATED_PATTERN] {month_key}: ãƒ‡ãƒ¼ã‚¿ãªã—ã€ã‚¹ã‚­ãƒƒãƒ—")
            continue
        
        # æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆï¼ˆçµ±è¨ˆå‡¦ç†æœ€å°é™ï¼‰
        month_pattern = create_monthly_dow_pattern(
            actual_staff_by_slot_and_date[month_columns],
            month_info['dates'],
            slot_minutes_for_empty,
            holidays or set(),
            include_zero_days
        )
        
        if not month_pattern.empty:
            monthly_patterns.append(month_pattern)
            log.info(f"[INTEGRATED_PATTERN] {month_key}: ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆå®Œäº†")
        else:
            log.warning(f"[INTEGRATED_PATTERN] {month_key}: ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆå¤±æ•—")
    
    if not monthly_patterns:
        log.error("[INTEGRATED_PATTERN] æœ‰åŠ¹ãªæœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãªã—")
        time_index_labels = pd.Index(gen_labels(slot_minutes_for_empty), name="time")
        return pd.DataFrame(0, index=time_index_labels, columns=range(7))
    
    log.info(f"[INTEGRATED_PATTERN] æœ‰åŠ¹æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(monthly_patterns)}")
    
    # Step 2: çµ±è¨ˆçš„çµ±åˆï¼ˆå›ºå®šã‚µãƒ³ãƒ—ãƒ«æ•°ã«ã‚ˆã‚‹æœŸé–“ä¾å­˜æ€§è§£æ±ºï¼‰
    integrated_pattern = create_integrated_pattern(monthly_patterns, statistic_method)
    
    # ğŸ”§ CRITICAL FIX: å–¶æ¥­æ™‚é–“åˆ¶ç´„ã®é©ç”¨ï¼ˆç¾å®Ÿæ€§ç¢ºä¿ï¼‰
    if business_hours_only:
        log.info("[REALISTIC] å–¶æ¥­æ™‚é–“åˆ¶ç´„ã‚’é©ç”¨ä¸­...")
        integrated_pattern = apply_business_hours_constraint(
            integrated_pattern, 
            business_start=dt.time(8, 0),
            business_end=dt.time(17, 30)
        )
        
        # åŠ¹æœã‚’ãƒ­ã‚°å‡ºåŠ›
        total_before = None  # åˆ¶ç´„å‰ã®å€¤ã¯æ—¢ã«è¨ˆç®—æ¸ˆã¿
        total_after = integrated_pattern.sum().sum()
        log.info(f"[REALISTIC] å–¶æ¥­æ™‚é–“åˆ¶ç´„é©ç”¨å¾Œ: ç·éœ€è¦ = {total_after:.1f}äººãƒ»ã‚¹ãƒ­ãƒƒãƒˆ")
    
    log.info(f"[INTEGRATED_PATTERN] çµ±åˆå®Œäº† (çµ±è¨ˆæ‰‹æ³•: {statistic_method})")
    log.info(f"[INTEGRATED_PATTERN] æœŸé–“ä¾å­˜æ€§è§£æ±º: ã‚µãƒ³ãƒ—ãƒ«æ•°å›ºå®š = {len(monthly_patterns)}")
    
    return integrated_pattern


def create_monthly_dow_pattern(
    month_data: pd.DataFrame,
    month_dates: list[dt.date],
    slot_minutes: int,
    holidays: set[dt.date],
    include_zero_days: bool
) -> pd.DataFrame:
    """
    å˜æœˆã‹ã‚‰æ›œæ—¥Ã—æ™‚é–“å¸¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ
    çµ±è¨ˆå‡¦ç†ã‚’æœ€å°é™ã«æŠ‘åˆ¶
    """
    time_index_labels = pd.Index(gen_labels(slot_minutes), name="time")
    pattern = pd.DataFrame(0.0, index=time_index_labels, columns=range(7))
    
    # å„æ›œæ—¥Ã—æ™‚é–“å¸¯ã®ä»£è¡¨å€¤ã‚’è¨ˆç®—
    for dow in range(7):  # 0=æœˆæ›œ ï½ 6=æ—¥æ›œ
        # è©²å½“æ›œæ—¥ã®æ—¥ä»˜ã‚’æŠ½å‡º
        dow_dates = [d for d in month_dates if d.weekday() == dow and d not in holidays]
        
        if not dow_dates:
            log.info(f"[PATTERN_CREATE] æ›œæ—¥{dow}: è©²å½“æ—¥ãªã—")
            continue
        
        # è©²å½“æ›œæ—¥ã®ãƒ‡ãƒ¼ã‚¿åˆ—ã‚’æŠ½å‡º
        dow_columns = []
        for col in month_data.columns:
            if isinstance(col, dt.date) and col.weekday() == dow and col not in holidays:
                dow_columns.append(col)
            elif isinstance(col, str):
                try:
                    col_date = pd.to_datetime(col).date()
                    if col_date.weekday() == dow and col_date not in holidays:
                        dow_columns.append(col)
                except:
                    continue
        
        if not dow_columns:
            continue
        
        dow_data = month_data[dow_columns]
        
        # å„æ™‚é–“å¸¯ã®ä»£è¡¨å€¤ã‚’è¨ˆç®—
        for time_slot in time_index_labels:
            if time_slot not in dow_data.index:
                continue
            
            values = dow_data.loc[time_slot].values
            values = [float(v) for v in values if not pd.isna(v)]
            
            if not values:
                representative_value = 0.0
            elif len(values) == 1:
                representative_value = values[0]
            elif len(values) <= 3:
                # å°‘æ•°ãƒ‡ãƒ¼ã‚¿: å¹³å‡å€¤ä½¿ç”¨
                representative_value = np.mean(values)
            else:
                # ååˆ†ãªãƒ‡ãƒ¼ã‚¿: ä¸­å¤®å€¤ä½¿ç”¨ï¼ˆå¤–ã‚Œå€¤ã«é ‘å¥ï¼‰
                representative_value = np.median(values)
            
            pattern.loc[time_slot, dow] = max(0, representative_value)
    
    return pattern


def create_integrated_pattern(monthly_patterns: list[pd.DataFrame], statistic_method: str) -> pd.DataFrame:
    """
    æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’çµ±è¨ˆçš„ã«çµ±åˆ
    å›ºå®šã‚µãƒ³ãƒ—ãƒ«æ•°ã«ã‚ˆã‚‹æœŸé–“ä¾å­˜æ€§ã®å®Œå…¨è§£æ±º
    
    ğŸ”§ é‡è¦ä¿®æ­£: Needå€¤ã®ç•°å¸¸æ”¾å¤§å•é¡Œã®æ ¹æœ¬è§£æ±º
    çµ±è¨ˆå€¤ã‚’ãã®ã¾ã¾ä½¿ç”¨ã›ãšã€ç¾å®Ÿçš„ãªç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    """
    if not monthly_patterns:
        raise ValueError("æœˆæ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç©ºã§ã™")
    
    # çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆæœŸåŒ–
    base_pattern = monthly_patterns[0].copy()
    integrated = base_pattern.copy()
    integrated.iloc[:, :] = 0.0
    
    log.info(f"[PATTERN_INTEGRATION] çµ±åˆå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {len(monthly_patterns)} (å›ºå®š)")
    
    # ğŸ”§ ä¿®æ­£: å„æœˆã®æœ€å¤§å€¤ã‚’å–å¾—ã—ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŸºæº–ã‚’è¨­å®š
    max_values_per_month = [pattern.max().max() for pattern in monthly_patterns if not pattern.empty]
    realistic_max_staff = np.median(max_values_per_month) if max_values_per_month else 10
    log.info(f"[PATTERN_INTEGRATION] ç¾å®Ÿçš„æœ€å¤§ã‚¹ã‚¿ãƒƒãƒ•æ•°åŸºæº–: {realistic_max_staff}")
    
    # å„ã‚»ãƒ«ï¼ˆæ™‚é–“å¸¯Ã—æ›œæ—¥ï¼‰ã”ã¨ã«çµ±è¨ˆå‡¦ç†
    for time_slot in base_pattern.index:
        for dow in base_pattern.columns:
            # å„æœˆã‹ã‚‰ã®å€¤ã‚’åé›†
            values = []
            for pattern in monthly_patterns:
                if time_slot in pattern.index and dow in pattern.columns:
                    value = pattern.loc[time_slot, dow]
                    if not pd.isna(value):
                        values.append(float(value))
            
            if not values:
                integrated_value = 0.0
            elif len(values) == 1:
                integrated_value = values[0]
            else:
                # çµ±è¨ˆæ‰‹æ³•é©ç”¨ï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°å›ºå®šï¼‰
                if statistic_method == "ä¸­å¤®å€¤":
                    raw_value = np.median(values)
                elif statistic_method == "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«":
                    raw_value = np.percentile(values, 25)
                elif statistic_method == "75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«":
                    raw_value = np.percentile(values, 75)
                else:  # å¹³å‡å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
                    raw_value = np.mean(values)
                
                # ğŸ”§ é‡è¦ä¿®æ­£: ç¾å®Ÿçš„ç¯„å›²ã¸ã®åˆ¶é™
                integrated_value = min(raw_value, realistic_max_staff * 1.2)  # 20%ãƒãƒ¼ã‚¸ãƒ³
            
            integrated.loc[time_slot, dow] = max(0, round(integrated_value))
    
    log.info(f"[PATTERN_INTEGRATION] çµ±åˆå®Œäº† (æ‰‹æ³•: {statistic_method})")
    
    # æ¤œè¨¼ãƒ­ã‚°
    total_need = integrated.sum().sum()
    log.info(f"[PATTERN_INTEGRATION] çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³ç·Need: {total_need}")
    
    # ğŸ”§ ä¿®æ­£: ç•°å¸¸å€¤æ¤œå‡ºã¨ã‚¢ãƒ©ãƒ¼ãƒˆ
    if total_need > realistic_max_staff * 48 * 7 * 0.8:  # é€±å…¨ä½“ã®80%ã‚’ä¸Šé™ã¨ã—ã¦è­¦å‘Š
        log.warning(f"[PATTERN_INTEGRATION] âš ï¸ ç•°å¸¸ã«é«˜ã„Needå€¤ã‚’æ¤œå‡º: {total_need} (åŸºæº–å€¤: {realistic_max_staff * 48 * 7 * 0.8})")
        log.warning(f"[PATTERN_INTEGRATION] Needå€¤ã‚’ç¾å®Ÿçš„ç¯„å›²ã«åˆ¶é™ã—ã¾ã™")
        # å…¨ä½“ã‚’ç¾å®Ÿçš„ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        scale_factor = (realistic_max_staff * 48 * 7 * 0.5) / total_need
        integrated = integrated * scale_factor
        integrated = integrated.round().astype(int)
        total_need = integrated.sum().sum()
        log.info(f"[PATTERN_INTEGRATION] ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¾Œç·Need: {total_need}")
    
    # å„æ›œæ—¥ã®ã‚µãƒãƒªãƒ¼
    for dow in range(7):
        dow_total = integrated.iloc[:, dow].sum()
        dow_name = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"][dow]
        log.info(f"[PATTERN_INTEGRATION] {dow_name}æ›œæ—¥åˆè¨ˆNeed: {dow_total}")
    
    return integrated

def create_timestamped_heatmap_log(heatmap_results: dict, output_dir: Path) -> Path:
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    import datetime as dt
    
    timestamp = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥%Hæ™‚%Måˆ†")
    log_filename = f"{timestamp}_ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ­ã‚°.txt"
    log_filepath = output_dir / log_filename
    
    try:
        with open(log_filepath, 'w', encoding='utf-8') as f:
            f.write(f"=== ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆçµæœãƒ¬ãƒãƒ¼ãƒˆ ===\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {timestamp}\n")
            f.write(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}\n")
            f.write("=" * 50 + "\n\n")
            
            # 1. å…¨ä½“çµ±è¨ˆ
            f.write("ã€1. å…¨ä½“çµ±è¨ˆã€‘\n")
            overall_stats = heatmap_results.get('overall_stats', {})
            f.write(f"  å¯¾è±¡æœŸé–“: {overall_stats.get('start_date', 'N/A')} ï½ {overall_stats.get('end_date', 'N/A')}\n")
            f.write(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {overall_stats.get('total_records', 0):,}ä»¶\n")
            f.write(f"  å‹¤å‹™ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {overall_stats.get('work_records', 0):,}ä»¶\n")
            f.write(f"  ä¼‘æš‡ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {overall_stats.get('leave_records', 0):,}ä»¶\n")
            f.write(f"  æ¨å®šä¼‘æ¥­æ—¥æ•°: {overall_stats.get('estimated_holidays', 0)}æ—¥\n")
            f.write(f"  ã‚¹ãƒ­ãƒƒãƒˆé–“éš”: {overall_stats.get('slot_minutes', 0)}åˆ†\n\n")
            
            # 2. è·ç¨®åˆ¥çµ±è¨ˆ
            f.write("ã€2. è·ç¨®åˆ¥çµ±è¨ˆã€‘\n")
            role_stats = heatmap_results.get('role_stats', [])
            if role_stats:
                f.write("  è·ç¨®å             | ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ | Needè¨ˆç®— | ãƒ‡ãƒ¼ã‚¿è¡Œæ•°\n")
                f.write("  " + "-" * 50 + "\n")
                for role in role_stats:
                    role_name = str(role.get('role', 'N/A'))[:15].ljust(15)
                    file_status = "âœ“" if role.get('file_created', False) else "âœ—"
                    need_status = "âœ“" if role.get('need_calculated', False) else "âœ—"
                    data_rows = role.get('data_rows', 0)
                    f.write(f"  {role_name} |      {file_status}       |    {need_status}     | {data_rows:8d}\n")
            else:
                f.write("  è·ç¨®ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 3. é›‡ç”¨å½¢æ…‹åˆ¥çµ±è¨ˆ
            f.write("ã€3. é›‡ç”¨å½¢æ…‹åˆ¥çµ±è¨ˆã€‘\n")
            emp_stats = heatmap_results.get('employment_stats', [])
            if emp_stats:
                f.write("  é›‡ç”¨å½¢æ…‹           | ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ | Needè¨ˆç®— | ãƒ‡ãƒ¼ã‚¿è¡Œæ•°\n")
                f.write("  " + "-" * 50 + "\n")
                for emp in emp_stats:
                    emp_name = str(emp.get('employment', 'N/A'))[:15].ljust(15)
                    file_status = "âœ“" if emp.get('file_created', False) else "âœ—"
                    need_status = "âœ“" if emp.get('need_calculated', False) else "âœ—"
                    data_rows = emp.get('data_rows', 0)
                    f.write(f"  {emp_name} |      {file_status}       |    {need_status}     | {data_rows:8d}\n")
            else:
                f.write("  é›‡ç”¨å½¢æ…‹ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 4. Needè¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            f.write("ã€4. Needè¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘\n")
            need_params = heatmap_results.get('need_calculation_params', {})
            f.write(f"  çµ±è¨ˆæ‰‹æ³•: {need_params.get('statistic_method', 'N/A')}\n")
            f.write(f"  å‚ç…§æœŸé–“: {need_params.get('ref_start_date', 'N/A')} ï½ {need_params.get('ref_end_date', 'N/A')}\n")
            f.write(f"  å¤–ã‚Œå€¤é™¤å»: {need_params.get('remove_outliers', False)}\n")
            f.write(f"  IQRä¹—æ•°: {need_params.get('iqr_multiplier', 'N/A')}\n")
            f.write(f"  ä¼‘æ¥­æ—¥å«ã‚€: {need_params.get('include_zero_days', True)}\n")
            f.write(f"  èª¿æ•´ä¿‚æ•°: {need_params.get('adjustment_factor', 1.0)}\n\n")
            
            # 5. ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§
            f.write("ã€5. ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã€‘\n")
            generated_files = heatmap_results.get('generated_files', [])
            if generated_files:
                for file_info in generated_files:
                    f.write(f"  âœ“ {file_info}\n")
            else:
                f.write("  ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãªã—\n")
            f.write("\n")
            
            # 6. è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼
            warnings = heatmap_results.get('warnings', [])
            errors = heatmap_results.get('errors', [])
            if warnings or errors:
                f.write("ã€6. è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘\n")
                for warning in warnings:
                    f.write(f"  [è­¦å‘Š] {warning}\n")
                for error in errors:
                    f.write(f"  [ã‚¨ãƒ©ãƒ¼] {error}\n")
            else:
                f.write("ã€6. è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘\n  ãªã—\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†\n")
            
        log.info(f"[heatmap] ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {log_filepath}")
        return log_filepath
        
    except Exception as e:
        log.error(f"[heatmap] ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

# æ–°è¦è¿½åŠ : é€šå¸¸å‹¤å‹™ã®åˆ¤å®šç”¨å®šæ•°
DEFAULT_HOLIDAY_TYPE = "é€šå¸¸å‹¤å‹™"

STAFF_ALIASES = ["staff", "æ°å", "åå‰", "å¾“æ¥­å“¡"]
ROLE_ALIASES = ["role", "è·ç¨®", "å½¹è·", "éƒ¨ç½²"]


def _resolve(df: pd.DataFrame, prefer: str, aliases: list[str], new: str) -> str:
    if prefer in df.columns:
        df.rename(columns={prefer: new}, inplace=True)
        return new
    for alias_name in aliases:
        if alias_name in df.columns:
            df.rename(columns={alias_name: new}, inplace=True)
            return new
    raise KeyError(
        f"åˆ— '{prefer}' ã¾ãŸã¯ãã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ {aliases} ãŒDataFrameã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
    )


def _apply_conditional_formatting_to_worksheet(
    worksheet: openpyxl.worksheet.worksheet.Worksheet, df_data_columns: pd.Index
):
    log.debug(f"[heatmap._apply_cf] æ›¸å¼è¨­å®šå¯¾è±¡ã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ: '{worksheet.title}'")
    log.debug(
        f"[heatmap._apply_cf] æ›¸å¼è¨­å®šã®åŸºæº–ã¨ãªã‚‹ãƒ‡ãƒ¼ã‚¿åˆ— (df_data_columns): {df_data_columns.tolist() if isinstance(df_data_columns, pd.Index) else df_data_columns}"
    )
    if worksheet.max_row <= 1:
        return
    if (
        df_data_columns.empty
        if isinstance(df_data_columns, pd.Index)
        else not df_data_columns
    ):
        return
    first_data_col_excel_idx = 2
    last_data_col_excel_idx = (
        first_data_col_excel_idx + (len(df_data_columns) - 1)
        if (isinstance(df_data_columns, pd.Index) and not df_data_columns.empty)
        or (not isinstance(df_data_columns, pd.Index) and df_data_columns)
        else first_data_col_excel_idx - 1
    )
    if last_data_col_excel_idx < first_data_col_excel_idx:
        return
    range_start_cell = "B2"
    range_end_col_letter = get_column_letter(last_data_col_excel_idx)
    range_end_row_num = worksheet.max_row
    data_range_string = f"{range_start_cell}:{range_end_col_letter}{range_end_row_num}"
    log.info(
        f"[heatmap._apply_cf] ã‚·ãƒ¼ãƒˆ '{worksheet.title}' ã«æ¡ä»¶ä»˜ãæ›¸å¼ã‚’é©ç”¨ã—ã¾ã™ã€‚ç¯„å›²: {data_range_string}"
    )
    try:
        color_scale_rule = ColorScaleRule(
            start_type="min",
            start_color="FFFFE0",
            mid_type="percentile",
            mid_value=50,
            mid_color="FFA500",
            end_type="max",
            end_color="FF0000",
        )
        worksheet.conditional_formatting.add(data_range_string, color_scale_rule)
    except Exception as e:
        log.error(f"[heatmap._apply_cf] æ¡ä»¶ä»˜ãæ›¸å¼é©ç”¨ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)


def _apply_holiday_column_styling(
    worksheet: openpyxl.worksheet.worksheet.Worksheet,
    date_columns_in_excel: pd.Index,
    estimated_holidays: Set[dt.date],
    utils_parse_as_date_func,
):
    if not estimated_holidays or date_columns_in_excel.empty:
        return
    holiday_fill = PatternFill(
        start_color="D3D3D3", end_color="D3D3D3", fill_type="solid"
    )
    first_data_col_excel_letter_idx = 2
    for i, col_name_excel_str in enumerate(date_columns_in_excel):
        current_col_date = utils_parse_as_date_func(str(col_name_excel_str))
        if current_col_date and current_col_date in estimated_holidays:
            target_excel_col_idx = first_data_col_excel_letter_idx + i
            col_letter = get_column_letter(target_excel_col_idx)
            for row_idx in range(1, worksheet.max_row + 1):
                worksheet[f"{col_letter}{row_idx}"].fill = holiday_fill


def calculate_monthly_baseline_need(
    actual_staff_by_slot_and_date: pd.DataFrame,
    ref_start_date: dt.date,
    ref_end_date: dt.date,
    statistic_method: str,
    remove_outliers: bool,
    iqr_multiplier: float = 1.5,
    slot_minutes_for_empty: int = DEFAULT_SLOT_MINUTES,
    *,
    holidays: set[dt.date] | None = None,
    adjustment_factor: float = 1.0,
    include_zero_days: bool = True,
    all_dates_in_period: list[dt.date] | None = None,
) -> pd.DataFrame:
    """
    ãƒ¬ã‚¬ã‚·ãƒ¼æœˆå˜ä½åŸºæº–å€¤æ–¹å¼ â†’ æ–°çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ–¹å¼ã¸ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
    æœŸé–“ä¾å­˜æ€§å•é¡Œã®æ ¹æœ¬è§£æ±º
    """
    log.info(f"[LEGACY_REDIRECT] calculate_monthly_baseline_need â†’ calculate_integrated_monthly_pattern_need")
    log.info(f"[LEGACY_REDIRECT] æœŸé–“ä¾å­˜æ€§å•é¡Œè§£æ±ºã®ãŸã‚æ–°æ–¹å¼ã«è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ")
    
    # æ–°çµ±åˆãƒ‘ã‚¿ãƒ¼ãƒ³æ–¹å¼ã«å®Œå…¨åˆ‡ã‚Šæ›¿ãˆ
    return calculate_integrated_monthly_pattern_need(
        actual_staff_by_slot_and_date,
        ref_start_date,
        ref_end_date,
        statistic_method,
        remove_outliers,
        iqr_multiplier,
        slot_minutes_for_empty,
        holidays=holidays,
        adjustment_factor=adjustment_factor,
        include_zero_days=include_zero_days,
        all_dates_in_period=all_dates_in_period,
    )


def calculate_pattern_based_need(
    actual_staff_by_slot_and_date: pd.DataFrame,
    ref_start_date: dt.date,
    ref_end_date: dt.date,
    statistic_method: str,
    remove_outliers: bool,
    iqr_multiplier: float = 1.5,
    slot_minutes_for_empty: int = DEFAULT_SLOT_MINUTES,
    *,
    holidays: set[dt.date] | None = None,
    adjustment_factor: float = 1.0,
    include_zero_days: bool = True,
    all_dates_in_period: list[dt.date] | None = None,
) -> pd.DataFrame:
    # ä¿®æ­£ç®‡æ‰€: logger.info -> log.info ãªã©ã€ãƒ­ã‚¬ãƒ¼åã‚’ 'log' ã«çµ±ä¸€
    log.info(
        f"[heatmap.calculate_pattern_based_need] å‚ç…§æœŸé–“: {ref_start_date} - {ref_end_date}, æ‰‹æ³•: {statistic_method}, å¤–ã‚Œå€¤é™¤å»: {remove_outliers}"
    )

    time_index_labels = pd.Index(gen_labels(slot_minutes_for_empty), name="time")
    default_dow_need_df = pd.DataFrame(
        0, index=time_index_labels, columns=range(7)
    )  # æœˆæ›œ0 - æ—¥æ›œ6

    if actual_staff_by_slot_and_date.empty:
        log.warning(
            "[heatmap.calculate_pattern_based_need] å…¥åŠ›å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®0 needã‚’è¿”ã—ã¾ã™ã€‚"
        )
        return default_dow_need_df

    # actual_staff_by_slot_and_date ã®åˆ—åãŒæ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªãƒ»å¤‰æ›
    # å‘¼ã³å‡ºã—å…ƒ(build_heatmap)ã§åˆ—åã‚’dt.dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›æ¸ˆã¿ã®ã‚‚ã®ã‚’æ¸¡ã™ã‚ˆã†ã«ä¿®æ­£
    df_for_calc = actual_staff_by_slot_and_date.copy()

    holidays_set = set(holidays or [])

    if include_zero_days:
        log.info("[NEED_FIX] include_zero_days=True â†’ ä¼‘æ¥­æ—¥é™¤å¤–ãªã—")

    # é‡è¦ãªä¿®æ­£ï¼šå…¨æœŸé–“ã®æ—¥ä»˜ã‚’è€ƒæ…®ã™ã‚‹
    if all_dates_in_period and include_zero_days:
        all_dates_in_ref = [
            d for d in all_dates_in_period
            if isinstance(d, dt.date) and ref_start_date <= d <= ref_end_date and d not in holidays_set
        ]

        # å®Ÿç¸¾ãŒãªã„æ—¥ä»˜ã‚’0ã§åŸ‹ã‚ã‚‹
        for date in all_dates_in_ref:
            if date not in df_for_calc.columns:
                df_for_calc[date] = 0

        log.info(f"[NEED_FIX] å…¨æœŸé–“ã®æ—¥ä»˜ã‚’è€ƒæ…®: å…ƒã®åˆ—æ•°={len(actual_staff_by_slot_and_date.columns)}, è£œå®Œå¾Œ={len(df_for_calc.columns)}")

    # å‚ç…§æœŸé–“ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (åˆ—åãŒdt.dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’å‰æ)
    cols_to_process_dow = [
        col_date
        for col_date in df_for_calc.columns
        if (
            isinstance(col_date, dt.date)
            and ref_start_date <= col_date <= ref_end_date
            and col_date not in holidays_set
        )
    ]

    if not cols_to_process_dow:
        log.warning(
            f"[heatmap.calculate_pattern_based_need] å‚ç…§æœŸé–“ ({ref_start_date} - {ref_end_date}) ã«è©²å½“ã™ã‚‹å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        )
        return default_dow_need_df

    filtered_slot_df_dow = df_for_calc[cols_to_process_dow]
    dow_need_df_calculated = pd.DataFrame(
        index=filtered_slot_df_dow.index, columns=range(7), dtype=float
    )

    for day_of_week_idx in range(7):
        dow_cols_to_agg = [
            col_dt
            for col_dt in filtered_slot_df_dow.columns
            if col_dt.weekday() == day_of_week_idx
        ]

        # ãƒ‡ãƒãƒƒã‚°: æ›œæ—¥åãƒãƒƒãƒ”ãƒ³ã‚°
        dow_names = {0: "æœˆæ›œæ—¥", 1: "ç«æ›œæ—¥", 2: "æ°´æ›œæ—¥", 3: "æœ¨æ›œæ—¥", 4: "é‡‘æ›œæ—¥", 5: "åœŸæ›œæ—¥", 6: "æ—¥æ›œæ—¥"}
        dow_name = dow_names.get(day_of_week_idx, f"æ›œæ—¥{day_of_week_idx}")
        log.info(f"[NEED_DEBUG] === {dow_name} ({day_of_week_idx}) å‡¦ç†é–‹å§‹ ===")
        log.info(f"[NEED_DEBUG] å¯¾è±¡æ—¥ä»˜æ•°: {len(dow_cols_to_agg)}")

        if not dow_cols_to_agg:
            log.warning(f"[NEED_DEBUG] {dow_name}: å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãªã—")
            dow_need_df_calculated[day_of_week_idx] = 0
            continue

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›ï¼ˆå…¨æ›œæ—¥ï¼‰
        log.info(
            f"[NEED_DEBUG] å¯¾è±¡æ—¥ä»˜ä¾‹: {[d.strftime('%Y-%m-%d') for d in dow_cols_to_agg[:3]]}{'...' if len(dow_cols_to_agg) > 3 else ''}"
        )

        data_for_dow_calc = filtered_slot_df_dow[dow_cols_to_agg]

        is_significant_holiday = False
        if not data_for_dow_calc.empty:
            avg_staff_per_day_overall = filtered_slot_df_dow.sum().mean()
            avg_staff_per_day_dow = data_for_dow_calc.sum().mean()
            analysis_logger.info(
                f"æ›œæ—¥ '{dow_name}'({day_of_week_idx}) ã®å¿…è¦äººæ•°è¨ˆç®—: "
                f"æ›œæ—¥åˆ¥å¹³å‡å‹¤å‹™äººæ•° = {avg_staff_per_day_dow:.2f}, "
                f"å…¨ä½“å¹³å‡å‹¤å‹™äººæ•° = {avg_staff_per_day_overall:.2f}, "
                f"é©ç”¨ä¸­ã®çµ±è¨ˆæ‰‹æ³• = '{statistic_method}'"
            )
            # æ—¥æ›œæ—¥ã¯å¼·åˆ¶çš„ã«ç‰¹æ®Šå‡¦ç†å¯¾è±¡ã¨ã™ã‚‹
            if day_of_week_idx == 6:  # æ—¥æ›œæ—¥
                is_significant_holiday = True
                analysis_logger.info("[SUNDAY_FORCE] æ—¥æ›œæ—¥ã®ãŸã‚å¼·åˆ¶çš„ã«ç‰¹æ®Šå‡¦ç†ã‚’é©ç”¨ã—ã¾ã™")
            elif avg_staff_per_day_dow < (avg_staff_per_day_overall * 0.25):
                analysis_logger.warning(
                    f"æ›œæ—¥ '{dow_name}'({day_of_week_idx}) ã¯å‹¤å‹™å®Ÿç¸¾ãŒè‘—ã—ãå°‘ãªã„ãŸã‚ã€"
                    f"å¿…è¦äººæ•°ãŒå®Ÿæ…‹ã¨ä¹–é›¢ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                )
                is_significant_holiday = True

        # æ—¥æ¯ã®åˆè¨ˆäººæ•°ã‚’è¨ˆç®—
        daily_totals = data_for_dow_calc.sum()
        log.info(f"  å„æ—¥ã®ç·å‹¤å‹™äººæ•°: {daily_totals.values.tolist()}")
        log.info(f"  æ—¥å¹³å‡ç·å‹¤å‹™äººæ•°: {daily_totals.mean():.2f}")

        # æ™‚é–“å¸¯åˆ¥ã®è©³ç´°ï¼ˆç‰¹ã«æ—¥æ›œæ—¥ã¯è©³ç´°ã«ï¼‰
        if day_of_week_idx == 6:
            log.info("[SUNDAY_DEBUG] ========== æ—¥æ›œæ—¥ã®è©³ç´°åˆ†æ ==========")
            log.info("[SUNDAY_DEBUG] å¯¾è±¡æœŸé–“ã®å…¨æ—¥æ›œæ—¥:")
            for d in dow_cols_to_agg:
                daily_sum = data_for_dow_calc[d].sum()
                log.info(f"[SUNDAY_DEBUG]   {d.strftime('%Y-%m-%d')}: {daily_sum}å")

            log.info("[SUNDAY_DEBUG] ä»£è¡¨çš„ãªæ™‚é–“å¸¯ã®å€¤:")
            sample_times = ["09:00", "12:00", "15:00", "18:00"]
            for time_slot in sample_times:
                if time_slot in data_for_dow_calc.index:
                    values = data_for_dow_calc.loc[time_slot].values.tolist()
                    log.info(f"[SUNDAY_DEBUG]   {time_slot}: {values}")
        # â–¼â–¼â–¼ ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ â–¼â–¼â–¼
        # çµ±è¨ˆæ‰‹æ³•ã‚’æ±ºå®šã™ã‚‹
        # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã®çµ±è¨ˆæ‰‹æ³•èª¿æ•´
        if is_significant_holiday:
            current_statistic_method = "ä¸­å¤®å€¤"
            analysis_logger.info(
                f" -> æ›œæ—¥ '{dow_name}' ã¯å®Ÿç¸¾åƒ…å°‘ã®ãŸã‚ã€çµ±è¨ˆæ‰‹æ³•ã‚’ã€Œ{current_statistic_method}ã€ã«è‡ªå‹•èª¿æ•´ã—ã¾ã—ãŸã€‚"
            )
        else:
            current_statistic_method = statistic_method

        for time_slot_val, row_series_data in data_for_dow_calc.iterrows():
            if include_zero_days:
                values_at_slot_current = [0.0 if pd.isna(v) else float(v) for v in row_series_data]
            else:
                values_at_slot_current = row_series_data.dropna().astype(float).tolist()
            analysis_logger.info(
                f"[DEBUG_NEED_DETAIL] å‡¦ç†ä¸­ã®æ™‚é–“å¸¯: {time_slot_val} ({dow_name}), å…ƒãƒ‡ãƒ¼ã‚¿ ({len(values_at_slot_current)}ç‚¹): {values_at_slot_current}"
            )

            if not values_at_slot_current:
                dow_need_df_calculated.loc[time_slot_val, day_of_week_idx] = 0
                continue
            values_for_stat_calc = values_at_slot_current
            if day_of_week_idx == 6 and time_slot_val in ["09:00", "12:00", "15:00"]:
                log.info(f"[SUNDAY_DETAIL] {time_slot_val} æ™‚é–“å¸¯:")
                log.info(f"[SUNDAY_DETAIL]   å…ƒãƒ‡ãƒ¼ã‚¿: {values_at_slot_current}")
            # çµ±è¨ˆå€¤ã®è¨ˆç®—å‰ã«ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
            if day_of_week_idx == 6 or (day_of_week_idx == 1 and time_slot_val == "09:00"):
                log.info(f"\n  [çµ±è¨ˆè¨ˆç®—ãƒ‡ãƒãƒƒã‚°] {dow_name} {time_slot_val}")
                log.info(f"    å…ƒãƒ‡ãƒ¼ã‚¿: {values_at_slot_current}")
                log.info(f"    ãƒ‡ãƒ¼ã‚¿æ•°: {len(values_at_slot_current)}")

            if remove_outliers and len(values_at_slot_current) >= 4:
                q1_val = np.percentile(values_at_slot_current, 25)
                q3_val = np.percentile(values_at_slot_current, 75)
                iqr_val = q3_val - q1_val
                lower_bound_val = q1_val - iqr_multiplier * iqr_val
                upper_bound_val = q3_val + iqr_multiplier * iqr_val
                values_filtered_outlier = [
                    x_val
                    for x_val in values_at_slot_current
                    if lower_bound_val <= x_val <= upper_bound_val
                ]
                # ãƒ‡ãƒãƒƒã‚°: å¤–ã‚Œå€¤é™¤å»ã®è©³ç´°
                if day_of_week_idx == 6 and time_slot_val in ["09:00", "12:00", "15:00"]:
                    log.info(f"[SUNDAY_DETAIL]   å¤–ã‚Œå€¤é™¤å»å¾Œ: {values_filtered_outlier}")

                analysis_logger.info(
                    f"[DEBUG_NEED_DETAIL] å¤–ã‚Œå€¤é™¤å»å®Ÿè¡Œå‰ (Q1:{q1_val:.1f}, Q3:{q3_val:.1f}, IQR:{iqr_val:.1f}), ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ ({len(values_filtered_outlier)}ç‚¹): {values_filtered_outlier}"
                )

                if not values_filtered_outlier:
                    log.debug(
                        f"  æ›œæ—¥ {day_of_week_idx}, æ™‚é–“å¸¯ {time_slot_val}: å¤–ã‚Œå€¤é™¤å»å¾Œãƒ‡ãƒ¼ã‚¿ãªã—ã€‚å…ƒã®ãƒªã‚¹ãƒˆã§è¨ˆç®—ã—ã¾ã™ã€‚"
                    )
                else:
                    values_for_stat_calc = values_filtered_outlier
            need_calculated_val = 0.0
            if values_for_stat_calc:
                # æ±ºå®šã•ã‚ŒãŸçµ±è¨ˆæ‰‹æ³•ã«åŸºã¥ã„ã¦è¨ˆç®—
                if current_statistic_method == "10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«":
                    need_calculated_val = np.percentile(values_for_stat_calc, 10)
                elif current_statistic_method == "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«":
                    need_calculated_val = np.percentile(values_for_stat_calc, 25)
                elif current_statistic_method == "ä¸­å¤®å€¤":
                    need_calculated_val = np.median(values_for_stat_calc)
                elif current_statistic_method == "75ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«":
                    need_calculated_val = np.percentile(values_for_stat_calc, 75)
                elif current_statistic_method == "90ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«":
                    need_calculated_val = np.percentile(values_for_stat_calc, 90)
                else:  # å¹³å‡å€¤
                    need_calculated_val = np.mean(values_for_stat_calc)
            analysis_logger.info(
                f"[DEBUG_NEED_DETAIL] çµ±è¨ˆæ‰‹æ³•({current_statistic_method})é©ç”¨å¾Œã®Needä»®å€¤: {need_calculated_val:.2f}"
            )

            # ãƒ‡ãƒ¼ã‚¿ã®ä¸­å¤®å€¤ãŒå°ã•ã„å ´åˆã¯Needã‚’ä¸Šé™2.0ã«åˆ¶é™
            if values_at_slot_current and np.median(values_at_slot_current) < 2.0:
                need_calculated_val = min(need_calculated_val, 2.0)
                analysis_logger.info(
                    f"  [NEED_CAP] æ›œæ—¥ {day_of_week_idx}, æ™‚é–“å¸¯ {time_slot_val}: "
                    f"å®Ÿç¸¾ä¸­å¤®å€¤ãŒ2æœªæº€ã®ãŸã‚Needã‚’ {need_calculated_val:.1f} ã«åˆ¶é™ã—ã¾ã—ãŸã€‚"
                )
                analysis_logger.info(
                    f"[DEBUG_NEED_DETAIL] Needä¸Šé™é©ç”¨åˆ¤å®š: å…ƒãƒ‡ãƒ¼ã‚¿ä¸­å¤®å€¤={np.median(values_at_slot_current):.1f}ã€‚åˆ¶é™å¾ŒNeed={need_calculated_val:.2f}"
                )

            # èª¿æ•´ä¿‚æ•°ã®é©ç”¨
            need_calculated_val *= adjustment_factor
            
            # å®Ÿãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã®ç‰¹æ®Šå‡¦ç†
            if is_significant_holiday:
                # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„å ´åˆã¯ã€å®Ÿéš›ã®æœ€å¤§å€¤ã‚’ä¸Šé™ã¨ã—ã¦è¨­å®š
                max_actual_val = max(values_at_slot_current) if values_at_slot_current else 0
                if need_calculated_val > max_actual_val * 1.5:  # å®Ÿéš›ã®æœ€å¤§å€¤ã®1.5å€ã‚’ä¸Šé™
                    original_need = need_calculated_val
                    need_calculated_val = max_actual_val * 1.5
                    log.info(f"[STATS_FIX] {dow_name} {time_slot_val}: Needå€¤ã‚’ {original_need:.2f} â†’ {need_calculated_val:.2f} ã«åˆ¶é™ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿è€ƒæ…®ï¼‰")
                
                # ã•ã‚‰ã«ã€0ãŒå¤šã„ãƒ‡ãƒ¼ã‚¿ã§ã¯0ã«ã‚ˆã‚Šè¿‘ã„å€¤ã«èª¿æ•´
                zero_ratio = values_at_slot_current.count(0) / len(values_at_slot_current) if values_at_slot_current else 1
                if zero_ratio > 0.5:  # 50%ä»¥ä¸ŠãŒ0ã®å ´åˆ
                    need_calculated_val *= (1 - zero_ratio * 0.5)  # 0ã®æ¯”ç‡ã«å¿œã˜ã¦æ¸›ç®—
                    log.info(f"[STATS_FIX] {dow_name} {time_slot_val}: 0ãƒ‡ãƒ¼ã‚¿æ¯”ç‡{zero_ratio:.2f}ã«ã‚ˆã‚Šèª¿æ•´ â†’ {need_calculated_val:.2f}")
            
            final_need = round(need_calculated_val) if not pd.isna(need_calculated_val) else 0
            dow_need_df_calculated.loc[time_slot_val, day_of_week_idx] = final_need
            log.debug(
                f"  æ›œæ—¥ {day_of_week_idx}, æ™‚é–“å¸¯ {time_slot_val}: å…ƒãƒ‡ãƒ¼ã‚¿é•· {len(row_series_data.dropna())} -> å¤–ã‚Œå€¤é™¤å»å¾Œ {len(values_for_stat_calc)} -> Need {dow_need_df_calculated.loc[time_slot_val, day_of_week_idx]}"
            )

    # å…¨æ›œæ—¥ã®è¨ˆç®—å®Œäº†å¾Œã€ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
    log.info("[NEED_DEBUG] ========== Needè¨ˆç®—å®Œäº†ã‚µãƒãƒªãƒ¼ ==========")
    for dow_idx in range(7):
        dow_name = dow_names.get(dow_idx, f"æ›œæ—¥{dow_idx}")
        total_need = dow_need_df_calculated[dow_idx].sum()
        max_need = dow_need_df_calculated[dow_idx].max()
        avg_need = dow_need_df_calculated[dow_idx].mean()
        log.info(f"[NEED_DEBUG] {dow_name}: åˆè¨ˆ={total_need:.0f}, æœ€å¤§={max_need:.0f}, å¹³å‡={avg_need:.2f}")

    # ç‰¹ã«æ—¥æ›œæ—¥ã®è©³ç´°
    if 6 in dow_need_df_calculated.columns:
        sunday_data = dow_need_df_calculated[6]
        log.info("\n[æ—¥æ›œæ—¥ã®æ™‚é–“å¸¯åˆ¥Needå€¤]")
        for time_slot, need_val in sunday_data.items():
            if need_val > 0:
                log.info(f"  {time_slot}: {need_val}")

    log.info("[heatmap.calculate_pattern_based_need] æ›œæ—¥åˆ¥ãƒ»æ™‚é–“å¸¯åˆ¥needã®ç®—å‡ºå®Œäº†ã€‚")
    return dow_need_df_calculated.fillna(0).astype(int)


def _filter_work_records(long_df: pd.DataFrame) -> pd.DataFrame:
    """
    æ–°è¦è¿½åŠ : é€šå¸¸å‹¤å‹™ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹
    ä¼‘æš‡ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼ˆholiday_type != "é€šå¸¸å‹¤å‹™"ï¼‰ã‚’é™¤å¤–ã—ã€
    å®Ÿéš›ã«å‹¤å‹™æ™‚é–“ãŒã‚ã‚‹ï¼ˆparsed_slots_count > 0ï¼‰ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã‚’è¿”ã™
    """
    if long_df.empty:
        return long_df

    # é€šå¸¸å‹¤å‹™ä¸”ã¤å‹¤å‹™æ™‚é–“ãŒã‚ã‚‹ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿æŠ½å‡º
    work_records = long_df[
        (long_df.get("holiday_type", DEFAULT_HOLIDAY_TYPE) == DEFAULT_HOLIDAY_TYPE)
        & (long_df.get("parsed_slots_count", 0) > 0)
    ].copy()

    original_count = len(long_df)
    work_count = len(work_records)
    leave_count = original_count - work_count

    log.info(
        f"[heatmap._filter_work_records] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ: å…¨ãƒ¬ã‚³ãƒ¼ãƒ‰={original_count}, å‹¤å‹™ãƒ¬ã‚³ãƒ¼ãƒ‰={work_count}, ä¼‘æš‡ãƒ¬ã‚³ãƒ¼ãƒ‰={leave_count}"
    )

    if not work_records.empty:
        holiday_stats = long_df["holiday_type"].value_counts()
        log.debug(f"[heatmap._filter_work_records] ä¼‘æš‡ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ:\n{holiday_stats}")

    return work_records


def build_heatmap(
    long_df: pd.DataFrame,
    out_dir: str | Path,
    slot_minutes: int = DEFAULT_SLOT_MINUTES,
    *,
    need_calc_method: str | None = None,
    need_stat_method: str | None = None,
    include_zero_days: bool = True,
    need_manual_values: dict | None = None,
    upper_calc_method: str | None = None,
    upper_calc_param: dict | None = None,
    # legacy parameters kept for backward compatibility
    ref_start_date_for_need: dt.date | None = None,
    ref_end_date_for_need: dt.date | None = None,
    need_statistic_method: str | None = None,
    need_remove_outliers: bool | None = None,
    need_iqr_multiplier: float | None = 1.5,
    need_adjustment_factor: float = 1.0,
    min_method: str = "p25",
    max_method: str = "p75",
    holidays: set[dt.date] | None = None,
) -> None:
    holidays_set = set(holidays or [])

    if long_df.empty:
        log.warning("[heatmap.build_heatmap] å…¥åŠ›DataFrame (long_df) ãŒç©ºã§ã™ã€‚")
        return
    required_long_df_cols = {
        "ds",
        "staff",
        "role",
        "code",
        "holiday_type",
        "parsed_slots_count",
    }
    if not required_long_df_cols.issubset(long_df.columns):
        missing_cols = required_long_df_cols - set(long_df.columns)
        log.error(f"[heatmap.build_heatmap] long_dfã«å¿…è¦ãªåˆ— {missing_cols} ãŒä¸è¶³ã€‚")
        out_dir_path = Path(out_dir)
        out_dir_path.mkdir(parents=True, exist_ok=True)
        write_meta(
            out_dir_path / "heatmap.meta.json",
            slot=slot_minutes,
            roles=[],
            dates=[],
            summary_columns=SUMMARY5,
            estimated_holidays=[d.isoformat() for d in sorted(list(holidays or set()))],
        )
        return

    # é‡è¦: ä¼‘æš‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã®çµ±è¨ˆã‚’å…ˆã«åé›†
    leave_stats = {}
    if not long_df.empty and "holiday_type" in long_df.columns:
        holiday_type_stats = long_df["holiday_type"].value_counts()
        leave_stats = {
            "total_records": len(long_df),
            "leave_records": len(
                long_df[long_df["holiday_type"] != DEFAULT_HOLIDAY_TYPE]
            ),
            "holiday_type_breakdown": holiday_type_stats.to_dict(),
        }
        log.info(f"[heatmap.build_heatmap] ä¼‘æš‡çµ±è¨ˆ: {leave_stats}")
    estimated_holidays_set: Set[dt.date] = set()
    all_dates_in_period_list: List[dt.date] = []
    if (
        not long_df.empty
        and "ds" in long_df.columns
        and "parsed_slots_count" in long_df.columns
    ):
        long_df_for_holiday_check = long_df.copy()
        if not pd.api.types.is_datetime64_any_dtype(long_df_for_holiday_check["ds"]):
            long_df_for_holiday_check["ds"] = pd.to_datetime(
                long_df_for_holiday_check["ds"], errors="coerce"
            )
        valid_ds_long_df = long_df_for_holiday_check.dropna(subset=["ds"])
        if not valid_ds_long_df.empty:
            min_date_val = valid_ds_long_df["ds"].dt.date.min()
            max_date_val = valid_ds_long_df["ds"].dt.date.max()
            if (
                pd.NaT not in [min_date_val, max_date_val]
                and isinstance(min_date_val, dt.date)
                and isinstance(max_date_val, dt.date)
            ):
                # ğŸ”§ CRITICAL FIX: ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“ã‚’å„ªå…ˆã—ã€Excelãƒ•ã‚¡ã‚¤ãƒ«å…¨æœŸé–“ã‚’ç„¡è¦–
                # ref_start_date_for_need ã¨ ref_end_date_for_need ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
                if ref_start_date_for_need and ref_end_date_for_need:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“å†…ã§ã®ã¿ã‚¹ã‚­ãƒ£ãƒ³
                    actual_start = max(min_date_val, ref_start_date_for_need)
                    actual_end = min(max_date_val, ref_end_date_for_need)
                    log.info(f"[PERIOD_FIX] ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“å„ªå…ˆ: {ref_start_date_for_need} - {ref_end_date_for_need}")
                    log.info(f"[PERIOD_FIX] ãƒ‡ãƒ¼ã‚¿æœŸé–“åˆ¶é™å¾Œ: {actual_start} - {actual_end}")
                else:
                    # ãƒ¬ã‚¬ã‚·ãƒ¼: å…¨ãƒ‡ãƒ¼ã‚¿æœŸé–“ï¼ˆéæ¨å¥¨ï¼‰
                    actual_start = min_date_val
                    actual_end = max_date_val
                    log.warning(f"[PERIOD_FIX] ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šæœŸé–“ãªã—ã€å…¨ãƒ‡ãƒ¼ã‚¿æœŸé–“ä½¿ç”¨: {min_date_val} - {max_date_val}")
                
                current_scan_date = actual_start
                while current_scan_date <= actual_end:
                    all_dates_in_period_list.append(current_scan_date)
                    current_scan_date += dt.timedelta(days=1)
            else:
                log.warning("[heatmap.build_heatmap] æœ‰åŠ¹ãªæ—¥ä»˜ç¯„å›²ã‚’æ±ºå®šã§ãã¾ã›ã‚“ã€‚")
            if all_dates_in_period_list:
                for current_date_val_iter in all_dates_in_period_list:
                    df_for_current_date_iter = valid_ds_long_df[
                        valid_ds_long_df["ds"].dt.date == current_date_val_iter
                    ]
                    if df_for_current_date_iter.empty:
                        estimated_holidays_set.add(current_date_val_iter)
                        log.debug(
                            f"æ–½è¨­ä¼‘æ¥­æ—¥(æ¨å®š): {current_date_val_iter} (å‹¤å‹™è¨˜éŒ²ãªã—)"
                        )
                        continue

                    # ä¿®æ­£: é€šå¸¸å‹¤å‹™ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã§åˆ¤å®š
                    work_records_today = _filter_work_records(df_for_current_date_iter)
                    if work_records_today.empty:
                        estimated_holidays_set.add(current_date_val_iter)
                        log.debug(
                            f"æ–½è¨­ä¼‘æ¥­æ—¥(æ¨å®š): {current_date_val_iter} (é€šå¸¸å‹¤å‹™ãªã—)"
                        )
            if estimated_holidays_set:
                log.info(
                    f"[heatmap.build_heatmap] æ¨å®šã•ã‚ŒãŸä¼‘æ¥­æ—¥ ({len(estimated_holidays_set)}æ—¥): {sorted(list(estimated_holidays_set))}"
                )
            else:
                log.info("[heatmap.build_heatmap] æ¨å®šã•ã‚Œã‚‹ä¼‘æ¥­æ—¥ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            log.warning(
                "[heatmap.build_heatmap] 'ds'åˆ—ã«æœ‰åŠ¹ãªæ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€ä¼‘æ¥­æ—¥ã‚’æ¨å®šã§ãã¾ã›ã‚“ã€‚"
            )
    else:
        log.warning(
            "[heatmap.build_heatmap] long_dfãŒç©ºã‹ã€å¿…è¦ãªåˆ—ãŒãªã„ãŸã‚ã€ä¼‘æ¥­æ—¥ã‚’æ¨å®šã§ãã¾ã›ã‚“ã€‚"
        )

    # é‡è¦: é€šå¸¸å‹¤å‹™ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿ã§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
    df_for_heatmap_actuals = _filter_work_records(long_df)
    out_dir_path = Path(out_dir)
    out_dir_path.mkdir(parents=True, exist_ok=True)
    all_date_labels_in_period_str: List[str] = (
        sorted([d.strftime("%Y-%m-%d") for d in all_dates_in_period_list])
        if all_dates_in_period_list
        else []
    )
    time_index_labels = pd.Index(gen_labels(slot_minutes), name="time")

    if df_for_heatmap_actuals.empty and not all_date_labels_in_period_str:
        log.warning(
            "[heatmap.build_heatmap] æœ‰åŠ¹ãªå‹¤å‹™ãƒ‡ãƒ¼ã‚¿ã‚‚æ—¥ä»˜ç¯„å›²ã‚‚ãªã„ãŸã‚ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ç©ºã«ãªã‚Šã¾ã™ã€‚"
        )
        empty_pivot = pd.DataFrame(index=time_index_labels)
        for col_name_ep_loop in SUMMARY5:
            empty_pivot[col_name_ep_loop] = 0
        fp_all_empty_path = out_dir_path / "heat_ALL.parquet"
        try:
            empty_pivot.to_parquet(fp_all_empty_path)
        except Exception as e_empty_write:
            log.error(f"ç©ºã®heat_ALL.parquetã®æ›¸ãè¾¼ã¿ã«å¤±æ•—: {e_empty_write}")
        all_unique_roles_val = (
            sorted(list(set(long_df["role"]))) if "role" in long_df.columns else []
        )
        write_meta(
            out_dir_path / "heatmap.meta.json",
            slot=slot_minutes,
            roles=all_unique_roles_val,
            dates=[],
            summary_columns=SUMMARY5,
            estimated_holidays=[d.isoformat() for d in sorted(list(holidays or set()))],
            leave_statistics=leave_stats,  # ä¼‘æš‡çµ±è¨ˆã‚’è¿½åŠ 
        )
        return

    df_for_heatmap_actuals["time"] = pd.to_datetime(
        df_for_heatmap_actuals["ds"], errors="coerce"
    ).dt.strftime("%H:%M")
    df_for_heatmap_actuals["date_lbl"] = pd.to_datetime(
        df_for_heatmap_actuals["ds"], errors="coerce"
    ).dt.strftime("%Y-%m-%d")
    df_for_heatmap_actuals.dropna(
        subset=["time", "date_lbl", "staff", "role"], inplace=True
    )

    staff_col_name = "staff"
    role_col_name = "role"
    log.info("[heatmap.build_heatmap] å…¨ä½“ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆé–‹å§‹ã€‚")

    pivot_data_all_actual_staff = pd.DataFrame(index=time_index_labels)
    if not df_for_heatmap_actuals.empty:
        if len(df_for_heatmap_actuals) > 50000:
            log.info(
                "[heatmap.build_heatmap] Large dataset detected, using chunked processing"
            )
            chunk_size = 10000
            pivot_chunks = []

            for i in range(0, len(df_for_heatmap_actuals), chunk_size):
                chunk = df_for_heatmap_actuals.iloc[i : i + chunk_size]
                chunk_pivot = chunk.drop_duplicates(
                    subset=["date_lbl", "time", staff_col_name]
                ).pivot_table(
                    index="time",
                    columns="date_lbl",
                    values=staff_col_name,
                    aggfunc="nunique",
                    fill_value=0,
                )
                pivot_chunks.append(chunk_pivot)

            if pivot_chunks:
                pivot_data_all_actual_staff = pd.concat(pivot_chunks, axis=1)
                pivot_data_all_actual_staff = pivot_data_all_actual_staff.groupby(
                    level=0, axis=1
                ).sum()
                pivot_data_all_actual_staff = pivot_data_all_actual_staff.reindex(
                    index=time_index_labels, fill_value=0
                )
        else:
            pivot_data_all_actual_staff = (
                df_for_heatmap_actuals.drop_duplicates(
                    subset=["date_lbl", "time", staff_col_name]
                )
                .pivot_table(
                    index="time",
                    columns="date_lbl",
                    values=staff_col_name,
                    aggfunc="nunique",
                    fill_value=0,
                )
                .reindex(index=time_index_labels, fill_value=0)
            )

    # Ensure all dates in the period are present as columns, filling missing ones with 0
    pivot_data_all_actual_staff = pivot_data_all_actual_staff.reindex(
        columns=all_date_labels_in_period_str, fill_value=0
    )

    actual_staff_for_need_input = pivot_data_all_actual_staff.copy()
    if not actual_staff_for_need_input.empty:
        new_column_map_for_need_input = {}
        for col_str_need in actual_staff_for_need_input.columns:
            dt_obj_need = _parse_as_date(str(col_str_need))
            if dt_obj_need:
                # parsed_date_columns_for_need_input.append(dt_obj_need) # ã“ã‚Œã¯ä¸è¦
                new_column_map_for_need_input[col_str_need] = dt_obj_need
            else:
                log.debug(
                    f"Needè¨ˆç®—ç”¨å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®åˆ—å'{col_str_need}'ã‚’æ—¥ä»˜ã«ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                )
        if new_column_map_for_need_input:
            # renameã™ã‚‹å‰ã«ã€ã‚­ãƒ¼(å…ƒã®åˆ—å)ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
            valid_keys_for_rename = [
                k
                for k in new_column_map_for_need_input.keys()
                if k in actual_staff_for_need_input.columns
            ]
            actual_staff_for_need_input = actual_staff_for_need_input[
                valid_keys_for_rename
            ].rename(columns=new_column_map_for_need_input)
        else:
            actual_staff_for_need_input = pd.DataFrame(index=time_index_labels)

    # app.pyã‹ã‚‰æ¸¡ã•ã‚Œã‚‹æ–°ã—ã„å¼•æ•°(need_stat_method)ã‚’å„ªå…ˆã—ã€
    # legacyå¼•æ•°(need_statistic_method)ãŒã‚ã‚Œã°ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
    final_statistic_method = (
        need_stat_method if need_stat_method is not None else need_statistic_method
    )

    if include_zero_days:
        log.info("[NEED_FIX] include_zero_days=True â†’ æ¨å®šä¼‘æ¥­æ—¥ã‚’ç„¡è¦–")
        final_holidays_to_use = holidays_set
    else:
        final_holidays_to_use = holidays_set.union(estimated_holidays_set)

    # ğŸ¯ æœˆå˜ä½åŸºæº–å€¤æ–¹å¼ã®é©ç”¨
    if ref_end_date_for_need and ref_start_date_for_need:
        period_days = (ref_end_date_for_need - ref_start_date_for_need).days + 1
        if period_days > 60:  # 2ãƒ¶æœˆä»¥ä¸Šã®å ´åˆã¯æœˆå˜ä½åŸºæº–å€¤æ–¹å¼ã‚’é©ç”¨
            log.info(f"[MONTHLY_BASELINE] åˆ†ææœŸé–“{period_days}æ—¥ â†’ æœˆå˜ä½åŸºæº–å€¤æ–¹å¼ã‚’é©ç”¨")
            overall_dow_need_pattern_df = calculate_monthly_baseline_need(
                actual_staff_for_need_input,
                ref_start_date_for_need,
                ref_end_date_for_need,
                final_statistic_method,
                need_remove_outliers,
                need_iqr_multiplier,
                slot_minutes_for_empty=slot_minutes,
                holidays=final_holidays_to_use,
                adjustment_factor=need_adjustment_factor,
                include_zero_days=include_zero_days,
                all_dates_in_period=all_dates_in_period_list,
            )
        else:
            # å¾“æ¥æ–¹å¼ï¼ˆçŸ­æœŸé–“ï¼‰
            overall_dow_need_pattern_df = calculate_pattern_based_need(
                actual_staff_for_need_input,
                ref_start_date_for_need,
                ref_end_date_for_need,
                final_statistic_method,
                need_remove_outliers,
                need_iqr_multiplier,
                slot_minutes_for_empty=slot_minutes,
                holidays=final_holidays_to_use,
                adjustment_factor=need_adjustment_factor,
                include_zero_days=include_zero_days,
                all_dates_in_period=all_dates_in_period_list,
            )
    else:
        overall_dow_need_pattern_df = calculate_pattern_based_need(
            actual_staff_for_need_input,
            ref_start_date_for_need,
            ref_end_date_for_need,
            final_statistic_method,
            need_remove_outliers,
            need_iqr_multiplier,
            slot_minutes_for_empty=slot_minutes,
            holidays=final_holidays_to_use,
            adjustment_factor=need_adjustment_factor,
            include_zero_days=include_zero_days,
            all_dates_in_period=all_dates_in_period_list,
        )

    pivot_data_all_final = pd.DataFrame(
        index=time_index_labels, columns=all_date_labels_in_period_str, dtype=float
    ).fillna(0)
    need_all_final_for_summary = pd.DataFrame(
        index=time_index_labels, columns=all_date_labels_in_period_str, dtype=float
    ).fillna(0)

    for date_str_col_map in all_date_labels_in_period_str:
        if date_str_col_map in pivot_data_all_actual_staff.columns:
            pivot_data_all_final[date_str_col_map] = pivot_data_all_actual_staff[
                date_str_col_map
            ]
        current_date_obj_map = dt.datetime.strptime(date_str_col_map, "%Y-%m-%d").date()
        if current_date_obj_map.weekday() == 6:
            log.info(f"[SUNDAY_APPLY] æ—¥æ›œæ—¥ {date_str_col_map} ã®Needé©ç”¨:")
            log.info(f"[SUNDAY_APPLY]   ä¼‘æ¥­æ—¥åˆ¤å®š: {current_date_obj_map in holidays_set}")
        if current_date_obj_map in holidays_set:
            if current_date_obj_map.weekday() == 6:
                log.info("[SUNDAY_APPLY]   â†’ ä¼‘æ¥­æ—¥ã®ãŸã‚Need=0ã«è¨­å®š")
            need_all_final_for_summary[date_str_col_map] = 0
        else:
            day_of_week_map = current_date_obj_map.weekday()
            if day_of_week_map in overall_dow_need_pattern_df.columns:
                need_all_final_for_summary[date_str_col_map] = overall_dow_need_pattern_df[day_of_week_map]
                if current_date_obj_map.weekday() == 6:
                    need_values = overall_dow_need_pattern_df[day_of_week_map]
                    log.info(f"[SUNDAY_APPLY]   â†’ Needå€¤é©ç”¨: åˆè¨ˆ={need_values.sum():.0f}")
                    log.info(f"[SUNDAY_APPLY]   â†’ Needå€¤è©³ç´°ï¼ˆæœ€åˆ5ã¤ï¼‰: {need_values.head().tolist()}")

            else:
                need_all_final_for_summary[date_str_col_map] = 0
                log.warning(
                    f"æ›œæ—¥ {day_of_week_map} ã®needãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ ({date_str_col_map})ã€‚Needã¯0ã¨ã—ã¾ã™ã€‚"
                )

    # è©³ç´°ãªNeedãƒ‡ãƒ¼ã‚¿ã‚’Parquetãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    need_all_final_for_summary.to_parquet(
        out_dir_path / "need_per_date_slot.parquet"
    )
    log.info("Need per date/slot data saved to need_per_date_slot.parquet.")

    upper_s_representative = (
        derive_max_staff(pivot_data_all_actual_staff, max_method)
        if not pivot_data_all_actual_staff.empty
        else pd.Series(0, index=time_index_labels)
    )

    total_lack_per_time = pd.Series(0, index=time_index_labels, dtype=float)
    total_excess_per_time = pd.Series(0, index=time_index_labels, dtype=float)
    working_day_count = 0

    for date_col in need_all_final_for_summary.columns:
        if date_col in pivot_data_all_final.columns:
            daily_staff = pivot_data_all_final[date_col]
            daily_need = need_all_final_for_summary[date_col]

            date_obj = dt.datetime.strptime(date_col, "%Y-%m-%d").date()
            if date_obj not in holidays_set:
                working_day_count += 1
                daily_lack = (daily_need - daily_staff).clip(lower=0)
                daily_excess = (daily_staff - upper_s_representative).clip(lower=0)

                total_lack_per_time += daily_lack
                total_excess_per_time += daily_excess

    avg_need_series = need_all_final_for_summary.mean(axis=1).round()
    avg_staff_series = (
        pivot_data_all_final.drop(columns=SUMMARY5, errors="ignore")
        .mean(axis=1)
        .round()
        if not pivot_data_all_final.empty
        else pd.Series(0, index=time_index_labels)
    )
    avg_lack_series = (total_lack_per_time / max(working_day_count, 1)).round()
    avg_excess_series = (total_excess_per_time / max(working_day_count, 1)).round()

    pivot_to_excel_all = pivot_data_all_final.copy()
    for col_name_summary_loop, series_data_summary_loop in zip(
        SUMMARY5,
        [
            avg_need_series,
            upper_s_representative,
            avg_staff_series,
            avg_lack_series,
            avg_excess_series,
        ],
        strict=True,
    ):
        pivot_to_excel_all[col_name_summary_loop] = series_data_summary_loop

    analysis_logger.info(
        f"[DEBUG_HEATMAP_FINAL_COLS] heat_ALL.parquetã«ä¿å­˜ã•ã‚Œã‚‹æœ€çµ‚åˆ—: {pivot_to_excel_all.columns.tolist()}"
    )

    fp_all_path = out_dir_path / "heat_ALL.parquet"
    try:
        pivot_to_excel_all.to_parquet(fp_all_path)
        log.info(
            "[heatmap.build_heatmap] å…¨ä½“ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— (heat_ALL.parquet) ä½œæˆå®Œäº†ã€‚"
        )
    except Exception as e_write_all:
        log.error(
            f"[heatmap.build_heatmap] heat_ALL.parquet ä½œæˆã‚¨ãƒ©ãƒ¼: {e_write_all}",
            exc_info=True,
        )

    fp_all_xlsx_path = out_dir_path / "heat_ALL.xlsx"
    try:
        save_df_xlsx(pivot_to_excel_all, fp_all_xlsx_path, sheet_name="heat_ALL")
    except Exception as e_xlsx_all:
        log.error(
            f"[heatmap.build_heatmap] heat_ALL.xlsx ä½œæˆã‚¨ãƒ©ãƒ¼: {e_xlsx_all}",
            exc_info=True,
        )

    try:
        log.info(f"{fp_all_xlsx_path.name} ã«æ›¸å¼ã‚’è¨­å®šã—ã¾ã™ã€‚")
        wb = openpyxl.load_workbook(fp_all_xlsx_path)
        ws = wb.active

        data_columns = pivot_to_excel_all.columns.drop(SUMMARY5, errors="ignore")

        _apply_conditional_formatting_to_worksheet(ws, data_columns)
        _apply_holiday_column_styling(ws, data_columns, holidays_set, _parse_as_date)

        wb.save(fp_all_xlsx_path)
        log.info(f"æ›¸å¼è¨­å®šã‚’ {fp_all_xlsx_path.name} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        log.error(f"{fp_all_xlsx_path.name} ã¸ã®æ›¸å¼è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    unique_roles_list_final_loop = sorted(
        list(set(df_for_heatmap_actuals[role_col_name]))
    )
    log.info(
        f"[heatmap.build_heatmap] è·ç¨®åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆé–‹å§‹ã€‚å¯¾è±¡: {unique_roles_list_final_loop}"
    )
    for role_item_final_loop in unique_roles_list_final_loop:
        role_safe_name_final_loop = safe_sheet(str(role_item_final_loop))
        log.debug(f"è·ç¨® '{role_item_final_loop}' é–‹å§‹...")
        df_role_subset = df_for_heatmap_actuals[
            df_for_heatmap_actuals[role_col_name] == role_item_final_loop
        ]
        pivot_data_role_actual = pd.DataFrame(index=time_index_labels)
        if not df_role_subset.empty:
            pivot_data_role_actual = (
                df_role_subset.drop_duplicates(
                    subset=["date_lbl", "time", staff_col_name]
                )
                .pivot_table(
                    index="time",
                    columns="date_lbl",
                    values=staff_col_name,
                    aggfunc="nunique",
                    fill_value=0,
                )
                .reindex(index=time_index_labels, fill_value=0)
            )
        pivot_data_role_final = pivot_data_role_actual.reindex(
            columns=all_date_labels_in_period_str, fill_value=0
        )
        if not pivot_data_role_final.columns.empty:
            try:
                cols_to_sort_r = pd.Series(pivot_data_role_final.columns).astype(str)
                valid_date_cols_r = cols_to_sort_r[
                    cols_to_sort_r.str.match(r"^\d{4}-\d{2}-\d{2}$")
                ]
                if not valid_date_cols_r.empty:
                    sorted_cols_r = sorted(
                        valid_date_cols_r,
                        key=lambda d: dt.datetime.strptime(d, "%Y-%m-%d").date(),
                    )
                    other_cols_r = [
                        c
                        for c in pivot_data_role_final.columns
                        if c not in valid_date_cols_r.tolist()
                    ]
                    pivot_data_role_final = pivot_data_role_final[
                        sorted_cols_r + other_cols_r
                    ]
            except Exception as e_sort_r:
                log.warning(f"è·ç¨® '{role_item_final_loop}' æ—¥ä»˜ã‚½ãƒ¼ãƒˆå¤±æ•—: {e_sort_r}")

        actual_staff_for_role_need_input = pivot_data_role_actual.copy()
        if not actual_staff_for_role_need_input.empty:
            new_column_map_for_role_need = {}
            for col_str_role in actual_staff_for_role_need_input.columns:
                dt_obj_role = _parse_as_date(str(col_str_role))
                if dt_obj_role:
                    new_column_map_for_role_need[col_str_role] = dt_obj_role

            if new_column_map_for_role_need:
                valid_keys_for_role_rename = [
                    k
                    for k in new_column_map_for_role_need.keys()
                    if k in actual_staff_for_role_need_input.columns
                ]
                actual_staff_for_role_need_input = actual_staff_for_role_need_input[
                    valid_keys_for_role_rename
                ].rename(columns=new_column_map_for_role_need)
            else:
                actual_staff_for_role_need_input = pd.DataFrame(index=time_index_labels)

        # é‡è¦ãªä¿®æ­£ï¼šè·ç¨®åˆ¥ã§ã‚‚å…¨æœŸé–“ã®æ—¥ä»˜ã‚’è£œå®Œ
        if include_zero_days and all_dates_in_period_list:
            for date in all_dates_in_period_list:
                if ref_start_date_for_need <= date <= ref_end_date_for_need and date not in final_holidays_to_use:
                    if date not in actual_staff_for_role_need_input.columns:
                        actual_staff_for_role_need_input[date] = 0

        dow_need_pattern_role_df = calculate_pattern_based_need(
            actual_staff_for_role_need_input,
            ref_start_date_for_need,
            ref_end_date_for_need,
            final_statistic_method,
            need_remove_outliers,
            need_iqr_multiplier,
            slot_minutes_for_empty=slot_minutes,
            holidays=final_holidays_to_use,
            adjustment_factor=need_adjustment_factor,
            include_zero_days=include_zero_days,
            all_dates_in_period=all_dates_in_period_list,
        )

        need_df_role_final = pd.DataFrame(index=time_index_labels, columns=pivot_data_role_final.columns, dtype=float).fillna(0)
        for date_str_col_map in pivot_data_role_final.columns:
            current_date_obj_map = dt.datetime.strptime(date_str_col_map, "%Y-%m-%d").date()
            if current_date_obj_map in holidays_set:
                need_df_role_final[date_str_col_map] = 0
            else:
                day_of_week_map = current_date_obj_map.weekday()
                if day_of_week_map in dow_need_pattern_role_df.columns:
                    need_df_role_final[date_str_col_map] = dow_need_pattern_role_df[day_of_week_map]
                else:
                    need_df_role_final[date_str_col_map] = 0

        # è·ç¨®åˆ¥ã®è©³ç´°Needãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        need_df_role_final.to_parquet(
            out_dir_path / f"need_per_date_slot_role_{role_safe_name_final_loop}.parquet"
        )
        log.info(f"Role-specific need data saved to need_per_date_slot_role_{role_safe_name_final_loop}.parquet")

        need_r_series = need_df_role_final.mean(axis=1).round()

        if upper_calc_method == _("ä¸‹é™å€¤(Need) + å›ºå®šå€¤"):
            fixed_val = (upper_calc_param or {}).get("fixed_value", 0)
            upper_r_series = need_r_series + fixed_val
        elif upper_calc_method == _("ä¸‹é™å€¤(Need) * å›ºå®šä¿‚æ•°"):
            factor = (upper_calc_param or {}).get("factor", 1.0)
            upper_r_series = (need_r_series * factor).apply(np.ceil)
        elif upper_calc_method == _("éå»å®Ÿç¸¾ã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«"):
            pct = (upper_calc_param or {}).get("percentile", 90) / 100
            upper_r_series = pivot_data_role_actual.quantile(pct, axis=1).round()
        else:
            upper_r_series = (
                derive_max_staff(pivot_data_role_actual, max_method)
                if not pivot_data_role_actual.empty
                else pd.Series(0, index=time_index_labels)
            )

        upper_r_series = np.maximum(upper_r_series, need_r_series)
        staff_r_series = (
            pivot_data_role_final.drop(columns=SUMMARY5, errors="ignore")
            .sum(axis=1)
            .round()
        )
        lack_r_series = (need_r_series - staff_r_series).clip(lower=0)
        excess_r_series = (staff_r_series - upper_r_series).clip(lower=0)

        pivot_to_excel_role = pivot_data_role_final.copy()
        for col, data in zip(
            SUMMARY5,
            [
                need_r_series,
                upper_r_series,
                staff_r_series,
                lack_r_series,
                excess_r_series,
            ],
            strict=True,
        ):
            pivot_to_excel_role[col] = data

        fp_role = out_dir_path / f"heat_{role_safe_name_final_loop}.parquet"
        try:
            pivot_to_excel_role.to_parquet(fp_role)
            log.info(f"è·ç¨® '{role_item_final_loop}' ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆå®Œäº†ã€‚")
        except Exception as e_role_write:
            log.error(
                f"heat_{role_safe_name_final_loop}.parquet ä½œæˆã‚¨ãƒ©ãƒ¼: {e_role_write}",
                exc_info=True,
            )

        fp_role_xlsx = out_dir_path / f"heat_{role_safe_name_final_loop}.xlsx"
        try:
            save_df_xlsx(
                pivot_to_excel_role,
                fp_role_xlsx,
                sheet_name=f"heat_{role_safe_name_final_loop}",
            )
        except Exception as e_role_xlsx:
            log.error(
                f"heat_{role_safe_name_final_loop}.xlsx ä½œæˆã‚¨ãƒ©ãƒ¼: {e_role_xlsx}",
                exc_info=True,
            )

        try:
            log.info(f"{fp_role_xlsx.name} ã«æ›¸å¼ã‚’è¨­å®šã—ã¾ã™ã€‚")
            wb = openpyxl.load_workbook(fp_role_xlsx)
            ws = wb.active
            data_columns = pivot_to_excel_role.columns.drop(SUMMARY5, errors="ignore")
            _apply_conditional_formatting_to_worksheet(ws, data_columns)
            _apply_holiday_column_styling(
                ws, data_columns, holidays_set, _parse_as_date
            )
            wb.save(fp_role_xlsx)
        except Exception as e:
            log.error(f"{fp_role_xlsx.name} ã¸ã®æ›¸å¼è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    # â”€â”€ Employment heatmaps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    employment_col_name = "employment"
    unique_employments_list_final_loop = (
        sorted(list(set(df_for_heatmap_actuals[employment_col_name])))
        if employment_col_name in df_for_heatmap_actuals.columns
        else []
    )
    log.info(
        f"[heatmap.build_heatmap] é›‡ç”¨å½¢æ…‹åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆé–‹å§‹ã€‚å¯¾è±¡: {unique_employments_list_final_loop}"
    )
    for emp_item_final_loop in unique_employments_list_final_loop:
        emp_safe_name_final_loop = safe_sheet(str(emp_item_final_loop))
        log.debug(f"é›‡ç”¨å½¢æ…‹ '{emp_item_final_loop}' é–‹å§‹...")
        df_emp_subset = df_for_heatmap_actuals[
            df_for_heatmap_actuals[employment_col_name] == emp_item_final_loop
        ]
        pivot_data_emp_actual = pd.DataFrame(index=time_index_labels)
        if not df_emp_subset.empty:
            pivot_data_emp_actual = (
                df_emp_subset.drop_duplicates(
                    subset=["date_lbl", "time", staff_col_name]
                )
                .pivot_table(
                    index="time",
                    columns="date_lbl",
                    values=staff_col_name,
                    aggfunc="nunique",
                    fill_value=0,
                )
                .reindex(index=time_index_labels, fill_value=0)
            )
        pivot_data_emp_final = pivot_data_emp_actual.reindex(
            columns=all_date_labels_in_period_str, fill_value=0
        )
        if not pivot_data_emp_final.columns.empty:
            try:
                cols_to_sort_e = pd.Series(pivot_data_emp_final.columns).astype(str)
                valid_date_cols_e = cols_to_sort_e[
                    cols_to_sort_e.str.match(r"^\d{4}-\d{2}-\d{2}$")
                ]
                if not valid_date_cols_e.empty:
                    sorted_cols_e = sorted(
                        valid_date_cols_e,
                        key=lambda d: dt.datetime.strptime(d, "%Y-%m-%d").date(),
                    )
                    other_cols_e = [
                        c
                        for c in pivot_data_emp_final.columns
                        if c not in valid_date_cols_e.tolist()
                    ]
                    pivot_data_emp_final = pivot_data_emp_final[
                        sorted_cols_e + other_cols_e
                    ]
            except Exception as e_sort_e:
                log.warning(
                    f"é›‡ç”¨å½¢æ…‹ '{emp_item_final_loop}' æ—¥ä»˜ã‚½ãƒ¼ãƒˆå¤±æ•—: {e_sort_e}"
                )

        actual_staff_for_emp_need_input = pivot_data_emp_actual.copy()
        if not actual_staff_for_emp_need_input.empty:
            new_column_map_for_emp_need = {}
            for col_str_emp in actual_staff_for_emp_need_input.columns:
                dt_obj_emp = _parse_as_date(str(col_str_emp))
                if dt_obj_emp:
                    new_column_map_for_emp_need[col_str_emp] = dt_obj_emp

            if new_column_map_for_emp_need:
                valid_keys_for_emp_rename = [
                    k
                    for k in new_column_map_for_emp_need.keys()
                    if k in actual_staff_for_emp_need_input.columns
                ]
                actual_staff_for_emp_need_input = actual_staff_for_emp_need_input[
                    valid_keys_for_emp_rename
                ].rename(columns=new_column_map_for_emp_need)
            else:
                actual_staff_for_emp_need_input = pd.DataFrame(index=time_index_labels)

        # é‡è¦ãªä¿®æ­£ï¼šé›‡ç”¨å½¢æ…‹åˆ¥ã§ã‚‚å…¨æœŸé–“ã®æ—¥ä»˜ã‚’è£œå®Œ
        if include_zero_days and all_dates_in_period_list:
            for date in all_dates_in_period_list:
                if ref_start_date_for_need <= date <= ref_end_date_for_need and date not in final_holidays_to_use:
                    if date not in actual_staff_for_emp_need_input.columns:
                        actual_staff_for_emp_need_input[date] = 0

        dow_need_pattern_emp_df = calculate_pattern_based_need(
            actual_staff_for_emp_need_input,
            ref_start_date_for_need,
            ref_end_date_for_need,
            final_statistic_method,
            need_remove_outliers,
            need_iqr_multiplier,
            slot_minutes_for_empty=slot_minutes,
            holidays=final_holidays_to_use,
            adjustment_factor=need_adjustment_factor,
            include_zero_days=include_zero_days,
            all_dates_in_period=all_dates_in_period_list,
        )

        need_df_emp_final = pd.DataFrame(index=time_index_labels, columns=pivot_data_emp_final.columns, dtype=float).fillna(0)
        for date_str_col_map in pivot_data_emp_final.columns:
            current_date_obj_map = dt.datetime.strptime(date_str_col_map, "%Y-%m-%d").date()
            if current_date_obj_map in holidays_set:
                need_df_emp_final[date_str_col_map] = 0
            else:
                day_of_week_map = current_date_obj_map.weekday()
                if day_of_week_map in dow_need_pattern_emp_df.columns:
                    need_df_emp_final[date_str_col_map] = dow_need_pattern_emp_df[day_of_week_map]
                else:
                    need_df_emp_final[date_str_col_map] = 0

        # é›‡ç”¨å½¢æ…‹åˆ¥ã®è©³ç´°Needãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        need_df_emp_final.to_parquet(
            out_dir_path / f"need_per_date_slot_emp_{emp_safe_name_final_loop}.parquet"
        )
        log.info(f"Employment-specific need data saved to need_per_date_slot_emp_{emp_safe_name_final_loop}.parquet")

        need_e_series = need_df_emp_final.mean(axis=1).round()
        upper_e_series = (
            derive_max_staff(pivot_data_emp_actual, max_method)
            if not pivot_data_emp_actual.empty
            else pd.Series(0, index=time_index_labels)
        )
        staff_e_series = (
            pivot_data_emp_final.drop(columns=SUMMARY5, errors="ignore")
            .sum(axis=1)
            .round()
        )
        lack_e_series = (need_e_series - staff_e_series).clip(lower=0)
        excess_e_series = (staff_e_series - upper_e_series).clip(lower=0)

        pivot_to_excel_emp = pivot_data_emp_final.copy()
        for col, data in zip(
            SUMMARY5,
            [
                need_e_series,
                upper_e_series,
                staff_e_series,
                lack_e_series,
                excess_e_series,
            ],
            strict=True,
        ):
            pivot_to_excel_emp[col] = data

        fp_emp = out_dir_path / f"heat_emp_{emp_safe_name_final_loop}.parquet"
        try:
            pivot_to_excel_emp.to_parquet(fp_emp)
            log.info(f"é›‡ç”¨å½¢æ…‹ '{emp_item_final_loop}' ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆå®Œäº†ã€‚")
        except Exception as e_emp_write:
            log.error(
                f"heat_emp_{emp_safe_name_final_loop}.parquet ä½œæˆã‚¨ãƒ©ãƒ¼: {e_emp_write}",
                exc_info=True,
            )

        fp_emp_xlsx = out_dir_path / f"heat_emp_{emp_safe_name_final_loop}.xlsx"
        try:
            save_df_xlsx(
                pivot_to_excel_emp,
                fp_emp_xlsx,
                sheet_name=f"heat_emp_{emp_safe_name_final_loop}",
            )
        except Exception as e_emp_xlsx:
            log.error(
                f"heat_emp_{emp_safe_name_final_loop}.xlsx ä½œæˆã‚¨ãƒ©ãƒ¼: {e_emp_xlsx}",
                exc_info=True,
            )

        try:
            log.info(f"{fp_emp_xlsx.name} ã«æ›¸å¼ã‚’è¨­å®šã—ã¾ã™ã€‚")
            wb = openpyxl.load_workbook(fp_emp_xlsx)
            ws = wb.active
            data_columns = pivot_to_excel_emp.columns.drop(SUMMARY5, errors="ignore")
            _apply_conditional_formatting_to_worksheet(ws, data_columns)
            _apply_holiday_column_styling(
                ws, data_columns, holidays_set, _parse_as_date
            )
            wb.save(fp_emp_xlsx)
        except Exception as e:
            log.error(f"{fp_emp_xlsx.name} ã¸ã®æ›¸å¼è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

    all_unique_roles_from_orig_long_df_meta = (
        sorted(list(set(long_df["role"]))) if "role" in long_df.columns else []
    )
    all_unique_employments_from_orig_long_df_meta = (
        sorted(list(set(long_df["employment"])))
        if "employment" in long_df.columns
        else []
    )
    dow_need_pattern_output = (
        overall_dow_need_pattern_df.reset_index()
        .rename(columns={"time": "time"})
        .to_dict(orient="records")
        if not overall_dow_need_pattern_df.empty
        else []
    )  #  indexåå¤‰æ›´

    write_meta(
        out_dir_path / "heatmap.meta.json",
        slot=slot_minutes,
        roles=all_unique_roles_from_orig_long_df_meta,
        dates=all_date_labels_in_period_str,
        summary_columns=SUMMARY5,
        estimated_holidays=[d.isoformat() for d in sorted(list(holidays or set()))],
        employments=all_unique_employments_from_orig_long_df_meta,
        dow_need_pattern=dow_need_pattern_output,
        need_calculation_params={
            "ref_start_date": ref_start_date_for_need.isoformat(),
            "ref_end_date": ref_end_date_for_need.isoformat(),
            "statistic_method": final_statistic_method,
            "remove_outliers": need_remove_outliers,
            "iqr_multiplier": need_iqr_multiplier if need_remove_outliers else None,
        },
        leave_statistics=leave_stats,  # ä¼‘æš‡çµ±è¨ˆã‚’ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«è¿½åŠ 
    )
    validate_need_calculation(need_all_final_for_summary, pivot_data_all_final)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ­ã‚°ã‚’ä½œæˆ
    try:
        # çµ±è¨ˆæƒ…å ±ã‚’åé›†
        work_records_count = len(df_for_heatmap_actuals) if not df_for_heatmap_actuals.empty else 0
        leave_records_count = leave_stats.get('leave_records', 0) if leave_stats else 0
        total_records_count = leave_stats.get('total_records', 0) if leave_stats else 0
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
        generated_files = []
        generated_files.append(f"heat_ALL.parquet ({fp_all_path.stat().st_size} bytes)")
        generated_files.append(f"heat_ALL.xlsx ({fp_all_xlsx_path.stat().st_size} bytes)")
        
        # è·ç¨®åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«
        for role_item in unique_roles_list_final_loop:
            role_safe_name = safe_sheet(str(role_item))
            role_parquet = out_dir_path / f"heat_{role_safe_name}.parquet"
            role_excel = out_dir_path / f"heat_{role_safe_name}.xlsx"
            role_need = out_dir_path / f"need_per_date_slot_role_{role_safe_name}.parquet"
            if role_parquet.exists():
                generated_files.append(f"heat_{role_safe_name}.parquet ({role_parquet.stat().st_size} bytes)")
            if role_excel.exists():
                generated_files.append(f"heat_{role_safe_name}.xlsx ({role_excel.stat().st_size} bytes)")
            if role_need.exists():
                generated_files.append(f"need_per_date_slot_role_{role_safe_name}.parquet ({role_need.stat().st_size} bytes)")
        
        # é›‡ç”¨å½¢æ…‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«
        for emp_item in unique_employments_list_final_loop:
            emp_safe_name = safe_sheet(str(emp_item))
            emp_parquet = out_dir_path / f"heat_emp_{emp_safe_name}.parquet"
            emp_excel = out_dir_path / f"heat_emp_{emp_safe_name}.xlsx"
            emp_need = out_dir_path / f"need_per_date_slot_emp_{emp_safe_name}.parquet"
            if emp_parquet.exists():
                generated_files.append(f"heat_emp_{emp_safe_name}.parquet ({emp_parquet.stat().st_size} bytes)")
            if emp_excel.exists():
                generated_files.append(f"heat_emp_{emp_safe_name}.xlsx ({emp_excel.stat().st_size} bytes)")
            if emp_need.exists():
                generated_files.append(f"need_per_date_slot_emp_{emp_safe_name}.parquet ({emp_need.stat().st_size} bytes)")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        meta_file = out_dir_path / "heatmap.meta.json"
        if meta_file.exists():
            generated_files.append(f"heatmap.meta.json ({meta_file.stat().st_size} bytes)")
        
        # å…¨ä½“needãƒ•ã‚¡ã‚¤ãƒ«
        need_file = out_dir_path / "need_per_date_slot.parquet"
        if need_file.exists():
            generated_files.append(f"need_per_date_slot.parquet ({need_file.stat().st_size} bytes)")
        
        heatmap_results = {
            'overall_stats': {
                'start_date': all_date_labels_in_period_str[0] if all_date_labels_in_period_str else 'N/A',
                'end_date': all_date_labels_in_period_str[-1] if all_date_labels_in_period_str else 'N/A',
                'total_records': total_records_count,
                'work_records': work_records_count,
                'leave_records': leave_records_count,
                'estimated_holidays': len(estimated_holidays_set),
                'slot_minutes': slot_minutes
            },
            'role_stats': [
                {
                    'role': role,
                    'file_created': (out_dir_path / f"heat_{safe_sheet(str(role))}.parquet").exists(),
                    'need_calculated': (out_dir_path / f"need_per_date_slot_role_{safe_sheet(str(role))}.parquet").exists(),
                    'data_rows': len(df_for_heatmap_actuals[df_for_heatmap_actuals['role'] == role]) if not df_for_heatmap_actuals.empty else 0
                }
                for role in unique_roles_list_final_loop
            ],
            'employment_stats': [
                {
                    'employment': emp,
                    'file_created': (out_dir_path / f"heat_emp_{safe_sheet(str(emp))}.parquet").exists(),
                    'need_calculated': (out_dir_path / f"need_per_date_slot_emp_{safe_sheet(str(emp))}.parquet").exists(),
                    'data_rows': len(df_for_heatmap_actuals[df_for_heatmap_actuals.get('employment', pd.Series()) == emp]) if not df_for_heatmap_actuals.empty and 'employment' in df_for_heatmap_actuals.columns else 0
                }
                for emp in unique_employments_list_final_loop
            ],
            'need_calculation_params': {
                'statistic_method': final_statistic_method,
                'ref_start_date': ref_start_date_for_need.isoformat(),
                'ref_end_date': ref_end_date_for_need.isoformat(),
                'remove_outliers': need_remove_outliers,
                'iqr_multiplier': need_iqr_multiplier if need_remove_outliers else None,
                'include_zero_days': include_zero_days,
                'adjustment_factor': need_adjustment_factor
            },
            'generated_files': generated_files,
            'warnings': [],
            'errors': []
        }
        
        create_timestamped_heatmap_log(heatmap_results, out_dir_path)
        
    except Exception as e:
        log.error(f"[heatmap] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    log.info("[heatmap.build_heatmap] ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆå‡¦ç†å®Œäº†ã€‚")
