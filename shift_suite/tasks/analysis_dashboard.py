#!/usr/bin/env python3
"""
analysis_dashboard.py - ç¶²ç¾…çš„åˆ†ææƒ…å ±ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
ä¸è¶³åˆ†æãƒ»æœ€é©åŒ–åˆ†æãƒ»Needåˆ†æã®çµ±åˆè¡¨ç¤ºã‚·ã‚¹ãƒ†ãƒ 
"""

import logging
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

log = logging.getLogger(__name__)

class ComprehensiveAnalysisDashboard:
    """ç¶²ç¾…çš„åˆ†ææƒ…å ±ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰"""
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.analysis_files = {}
        self.meta_info = {}
        self._scan_analysis_files()
    
    def _scan_analysis_files(self) -> None:
        """åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ¤œå‡ºãƒ»åˆ†é¡"""
        file_patterns = {
            # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
            'need_data': ['need_per_date_slot.parquet', 'need_*.parquet'],
            'heat_data': ['heat_*.parquet', 'heat_*.xlsx'],
            'staff_data': ['intermediate_data.parquet', 'long_*.parquet'],
            
            # ä¸è¶³åˆ†æé–¢é€£
            'shortage_time': ['shortage_time.parquet'],
            'shortage_role': ['shortage_role.parquet'],
            'shortage_ratio': ['shortage_ratio.parquet'],
            'shortage_freq': ['shortage_freq.parquet'],
            'shortage_summary': ['shortage_weekday_timeslot_summary.parquet'],
            
            # æœ€é©åŒ–åˆ†æé–¢é€£
            'optimization': ['optimization_score_time.parquet', 'optimal_hire_plan.parquet'],
            'hire_plan': ['hire_plan_*.parquet', 'hiring_recommendations.parquet'],
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            'meta': ['heatmap.meta.json', 'analysis.meta.json', '*.meta.json'],
            
            # ãã®ä»–åˆ†æçµæœ
            'excess_data': ['excess_time.parquet', 'surplus_vs_need_time.parquet'],
            'leave_data': ['leave_analysis.csv', 'shortage_leave.csv'],
            'forecast': ['forecast_*.parquet', 'prediction_*.parquet']
        }
        
        for category, patterns in file_patterns.items():
            self.analysis_files[category] = []
            for pattern in patterns:
                if '*' in pattern:
                    # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æ¤œç´¢
                    matches = list(self.output_dir.glob(pattern))
                    self.analysis_files[category].extend([f.name for f in matches if f.is_file()])
                else:
                    # ç›´æ¥æ¤œç´¢
                    file_path = self.output_dir / pattern
                    if file_path.exists():
                        self.analysis_files[category].append(pattern)
    
    def get_comprehensive_analysis_info(self) -> Dict[str, Any]:
        """ç¶²ç¾…çš„ãªåˆ†ææƒ…å ±ã‚’å–å¾—"""
        info = {
            'scan_timestamp': datetime.now().isoformat(),
            'output_directory': str(self.output_dir),
            'file_inventory': self.analysis_files,
            'data_summaries': {},
            'analysis_status': {},
            'key_metrics': {},
            'recommendations': []
        }
        
        # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
        for category, files in self.analysis_files.items():
            if files:
                info['data_summaries'][category] = self._get_category_summary(category, files)
        
        # åˆ†æçŠ¶æ³ã®åˆ¤å®š
        info['analysis_status'] = self._assess_analysis_completeness()
        
        # ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
        info['key_metrics'] = self._calculate_key_metrics()
        
        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        info['recommendations'] = self._generate_recommendations()
        
        return info
    
    def _get_category_summary(self, category: str, files: List[str]) -> Dict[str, Any]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼"""
        summary = {
            'file_count': len(files),
            'files': files,
            'data_info': {},
            'last_modified': None,
            'size_info': {}
        }
        
        try:
            for file in files:
                file_path = self.output_dir / file
                if not file_path.exists():
                    continue
                    
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
                stat = file_path.stat()
                summary['size_info'][file] = {
                    'size_mb': round(stat.st_size / (1024*1024), 2),
                    'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                }
                
                # ãƒ‡ãƒ¼ã‚¿å†…å®¹ã®æ¦‚è¦ï¼ˆparquetãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
                if file.endswith('.parquet'):
                    try:
                        df = pd.read_parquet(file_path)
                        summary['data_info'][file] = {
                            'rows': len(df),
                            'columns': len(df.columns),
                            'column_names': list(df.columns)[:10],  # æœ€åˆã®10åˆ—ã®ã¿
                            'data_types': df.dtypes.astype(str).to_dict(),
                            'memory_usage_mb': round(df.memory_usage(deep=True).sum() / (1024*1024), 2)
                        }
                        
                        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ
                        numeric_cols = df.select_dtypes(include=[np.number]).columns
                        if len(numeric_cols) > 0:
                            summary['data_info'][file]['numeric_summary'] = {
                                'mean': df[numeric_cols].mean().to_dict(),
                                'sum': df[numeric_cols].sum().to_dict(),
                                'max': df[numeric_cols].max().to_dict()
                            }
                    except Exception as e:
                        log.warning(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file}: {e}")
                        summary['data_info'][file] = {'error': str(e)}
                
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                elif file.endswith('.json'):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            json_data = json.load(f)
                            summary['data_info'][file] = {
                                'keys': list(json_data.keys()) if isinstance(json_data, dict) else 'non_dict',
                                'size': len(json_data) if isinstance(json_data, (dict, list)) else 1
                            }
                    except Exception as e:
                        summary['data_info'][file] = {'error': str(e)}
        
        except Exception as e:
            summary['error'] = str(e)
            log.error(f"ã‚«ãƒ†ã‚´ãƒªã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼ {category}: {e}")
        
        return summary
    
    def _assess_analysis_completeness(self) -> Dict[str, Any]:
        """åˆ†æå®Œäº†çŠ¶æ³ã®è©•ä¾¡"""
        status = {
            'overall_score': 0,
            'category_scores': {},
            'missing_files': [],
            'critical_issues': []
        }
        
        # å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å®šç¾©
        required_files = {
            'need_data': {'weight': 20, 'files': ['need_per_date_slot.parquet']},
            'heat_data': {'weight': 15, 'files': ['heat_ALL.parquet', 'heat_ALL.xlsx']},
            'shortage_time': {'weight': 25, 'files': ['shortage_time.parquet']},
            'shortage_role': {'weight': 15, 'files': ['shortage_role.parquet']},
            'meta': {'weight': 10, 'files': ['heatmap.meta.json']},
            'staff_data': {'weight': 15, 'files': ['intermediate_data.parquet']}
        }
        
        total_weight = sum(req['weight'] for req in required_files.values())
        achieved_weight = 0
        
        for category, req_info in required_files.items():
            category_files = self.analysis_files.get(category, [])
            has_required = any(req_file in category_files for req_file in req_info['files'])
            
            if has_required:
                achieved_weight += req_info['weight']
                status['category_scores'][category] = 'complete'
            else:
                status['category_scores'][category] = 'missing'
                status['missing_files'].extend(req_info['files'])
        
        status['overall_score'] = round((achieved_weight / total_weight) * 100, 1)
        
        # é‡è¦ãªå•é¡Œã®æ¤œå‡º
        if status['overall_score'] < 70:
            status['critical_issues'].append('åˆ†æã®å®Œäº†ç‡ãŒ70%æœªæº€ã§ã™')
        
        if not self.analysis_files.get('need_data'):
            status['critical_issues'].append('NeedåŸºæº–ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
        
        if not self.analysis_files.get('shortage_time'):
            status['critical_issues'].append('æ™‚é–“åˆ¥ä¸è¶³ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“')
        
        return status
    
    def _calculate_key_metrics(self) -> Dict[str, Any]:
        """ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""
        metrics = {
            'total_shortage_hours': 0,
            'avg_daily_shortage': 0,
            'peak_shortage_timeslot': None,
            'role_shortage_distribution': {},
            'shortage_frequency': 0,
            'optimization_score': 0
        }
        
        try:
            # ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—
            shortage_file = self.output_dir / 'shortage_time.parquet'
            if shortage_file.exists():
                shortage_df = pd.read_parquet(shortage_file)
                
                # ç·ä¸è¶³æ™‚é–“ï¼ˆ30åˆ†å˜ä½ â†’ æ™‚é–“æ›ç®—ï¼‰
                metrics['total_shortage_hours'] = round(shortage_df.sum().sum() * 0.5, 1)
                
                # å¹³å‡æ—¥æ¬¡ä¸è¶³
                if len(shortage_df.columns) > 0:
                    metrics['avg_daily_shortage'] = round(shortage_df.sum(axis=0).mean() * 0.5, 1)
                
                # ãƒ”ãƒ¼ã‚¯ä¸è¶³æ™‚é–“å¸¯
                timeslot_totals = shortage_df.sum(axis=1)
                if not timeslot_totals.empty:
                    peak_index = timeslot_totals.idxmax()
                    metrics['peak_shortage_timeslot'] = {
                        'timeslot': peak_index,
                        'shortage_hours': round(timeslot_totals[peak_index] * 0.5, 1)
                    }
                
                # ä¸è¶³ç™ºç”Ÿé »åº¦
                shortage_occurrences = (shortage_df > 0).sum().sum()
                total_slots = shortage_df.shape[0] * shortage_df.shape[1]
                if total_slots > 0:
                    metrics['shortage_frequency'] = round(shortage_occurrences / total_slots * 100, 1)
            
            # è·ç¨®åˆ¥ä¸è¶³åˆ†å¸ƒ
            shortage_role_file = self.output_dir / 'shortage_role.parquet'
            if shortage_role_file.exists():
                role_df = pd.read_parquet(shortage_role_file)
                if 'lack_h' in role_df.columns and 'role' in role_df.columns:
                    metrics['role_shortage_distribution'] = role_df.set_index('role')['lack_h'].to_dict()
            
            # æœ€é©åŒ–ã‚¹ã‚³ã‚¢
            opt_file = self.output_dir / 'optimization_score_time.parquet'
            if opt_file.exists():
                opt_df = pd.read_parquet(opt_file) 
                if not opt_df.empty:
                    metrics['optimization_score'] = round(opt_df.mean().mean(), 2)
        
        except Exception as e:
            log.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            metrics['calculation_error'] = str(e)
        
        return metrics
    
    def _generate_recommendations(self) -> List[Dict[str, str]]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # åˆ†æå®Œäº†çŠ¶æ³ã«åŸºã¥ãæ¨å¥¨
        status = self._assess_analysis_completeness()
        if status['overall_score'] < 100:
            recommendations.append({
                'type': 'completion',
                'priority': 'high',
                'title': 'åˆ†æã®å®Œäº†',
                'description': f"åˆ†æå®Œäº†ç‡: {status['overall_score']}% - ä¸è¶³ã—ã¦ã„ã‚‹åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„"
            })
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãæ¨å¥¨
        metrics = self._calculate_key_metrics()
        if metrics.get('total_shortage_hours', 0) > 100:
            recommendations.append({
                'type': 'optimization',
                'priority': 'critical',
                'title': 'æ·±åˆ»ãªäººå“¡ä¸è¶³',
                'description': f"ç·ä¸è¶³æ™‚é–“: {metrics['total_shortage_hours']}æ™‚é–“ - ç·Šæ€¥ã®äººå“¡è£œå¼·ãŒå¿…è¦ã§ã™"
            })
        
        if metrics.get('shortage_frequency', 0) > 30:
            recommendations.append({
                'type': 'scheduling',
                'priority': 'high', 
                'title': 'é »ç¹ãªä¸è¶³ç™ºç”Ÿ',
                'description': f"ä¸è¶³ç™ºç”Ÿé »åº¦: {metrics['shortage_frequency']}% - ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦‹ç›´ã—ãŒå¿…è¦ã§ã™"
            })
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸è¶³ã«åŸºã¥ãæ¨å¥¨
        if not self.analysis_files.get('need_data'):
            recommendations.append({
                'type': 'data',
                'priority': 'critical',
                'title': 'NeedåŸºæº–ãƒ‡ãƒ¼ã‚¿ä¸è¶³',
                'description': 'need_per_date_slot.parquetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„'
            })
        
        return recommendations
    
    def generate_analysis_report(self) -> str:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        info = self.get_comprehensive_analysis_info()
        
        report = f"""
=== ç¶²ç¾…çš„åˆ†ææƒ…å ±ãƒ¬ãƒãƒ¼ãƒˆ ===
ç”Ÿæˆæ—¥æ™‚: {info['scan_timestamp']}
åˆ†æãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {info['output_directory']}

ã€åˆ†æå®Œäº†çŠ¶æ³ã€‘
å…¨ä½“ã‚¹ã‚³ã‚¢: {info['analysis_status']['overall_score']}%
"""
        
        for category, score in info['analysis_status']['category_scores'].items():
            status_icon = "âœ…" if score == 'complete' else "âŒ"
            report += f"{status_icon} {category}: {score}\n"
        
        if info['analysis_status']['missing_files']:
            report += f"\nä¸è¶³ãƒ•ã‚¡ã‚¤ãƒ«: {', '.join(info['analysis_status']['missing_files'])}\n"
        
        report += f"""
ã€ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘
ç·ä¸è¶³æ™‚é–“: {info['key_metrics'].get('total_shortage_hours', 0)}æ™‚é–“
å¹³å‡æ—¥æ¬¡ä¸è¶³: {info['key_metrics'].get('avg_daily_shortage', 0)}æ™‚é–“
ä¸è¶³ç™ºç”Ÿé »åº¦: {info['key_metrics'].get('shortage_frequency', 0)}%
"""
        
        if info['key_metrics'].get('peak_shortage_timeslot'):
            peak = info['key_metrics']['peak_shortage_timeslot']
            report += f"ãƒ”ãƒ¼ã‚¯ä¸è¶³æ™‚é–“å¸¯: {peak['timeslot']} ({peak['shortage_hours']}æ™‚é–“)\n"
        
        report += "\nã€æ¨å¥¨äº‹é …ã€‘\n"
        for i, rec in enumerate(info['recommendations'], 1):
            priority_icon = {"critical": "ğŸš¨", "high": "âš ï¸", "medium": "â„¹ï¸"}.get(rec['priority'], "ğŸ“Œ")
            report += f"{i}. {priority_icon} {rec['title']}: {rec['description']}\n"
        
        report += f"""
ã€ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã€‘
"""
        for category, files in info['file_inventory'].items():
            if files:
                report += f"{category}: {len(files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«\n"
                for file in files[:3]:  # æœ€åˆã®3ã¤ã®ã¿è¡¨ç¤º
                    report += f"  - {file}\n"
                if len(files) > 3:
                    report += f"  ... ãŠã‚ˆã³{len(files)-3}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«\n"
        
        return report
    
    def export_analysis_info(self, output_file: str = "comprehensive_analysis_info.json") -> Path:
        """åˆ†ææƒ…å ±ã‚’JSONã§å‡ºåŠ›"""
        info = self.get_comprehensive_analysis_info()
        output_path = self.output_dir / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        
        log.info(f"åˆ†ææƒ…å ±ã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_path}")
        return output_path


# ä¾¿åˆ©ãªé–¢æ•°
def get_analysis_dashboard(output_dir: Path) -> ComprehensiveAnalysisDashboard:
    """åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    return ComprehensiveAnalysisDashboard(output_dir)

def quick_analysis_check(output_dir: Path) -> Dict[str, Any]:
    """ã‚¯ã‚¤ãƒƒã‚¯åˆ†æãƒã‚§ãƒƒã‚¯"""
    dashboard = ComprehensiveAnalysisDashboard(output_dir)
    return dashboard.get_comprehensive_analysis_info()

def generate_analysis_report_file(output_dir: Path, report_file: str = "analysis_report.txt") -> Path:
    """åˆ†æãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ"""
    dashboard = ComprehensiveAnalysisDashboard(output_dir)
    report = dashboard.generate_analysis_report()
    
    report_path = output_dir / report_file
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    log.info(f"åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {report_path}")
    return report_path