from __future__ import annotations

import logging
from collections import defaultdict
from itertools import combinations

import numpy as np
import pandas as pd

# --- analysis thresholds ---
SYNERGY_HIGH_THRESHOLD = 1.5
SYNERGY_LOW_THRESHOLD = 0.3
VETERAN_RATIO_THRESHOLD = 0.7
ROLE_RATIO_THRESHOLD = 0.4
MONTH_END_RATIO_THRESHOLD = 1.2
WEEKEND_RATIO_THRESHOLD = 0.7
EARLY_MONTH_RATIO_THRESHOLD = 0.1
ROLE_COMBO_RATIO_THRESHOLD = 0.1
NEW_STAFF_ONLY_RATIO_THRESHOLD = 0.01

log = logging.getLogger(__name__)


def _analyze_skill_synergy(long_df: pd.DataFrame) -> list:
    """ã‚¹ã‚­ãƒ«ç›¸æ€§ãƒãƒˆãƒªã‚¯ã‚¹åˆ†æï¼šèª°ã¨èª°ã‚’çµ„ã¾ã›ã‚‹ã¨ä¸Šæ‰‹ãã„ãã‹"""
    rules = []

    if 'staff' not in long_df.columns:
        return rules

    # åŒæ™‚å‹¤å‹™ã®å®Ÿç¸¾ã‚’é›†è¨ˆ
    daily_staff = long_df[long_df['parsed_slots_count'] > 0].groupby(
        ['ds', 'code']
    )['staff'].apply(list).reset_index()

    # ãƒšã‚¢ã”ã¨ã®å…±åƒå›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    pair_counts = defaultdict(int)
    total_shifts_per_staff = long_df.groupby('staff')['ds'].nunique()

    for _, row in daily_staff.iterrows():
        staff_list = row['staff']
        if len(staff_list) >= 2:
            for pair in combinations(sorted(set(staff_list)), 2):
                pair_counts[pair] += 1

    # æœŸå¾…å€¤ã¨ã®ä¹–é›¢ã‚’è¨ˆç®—
    for (staff1, staff2), actual_count in pair_counts.items():
        if staff1 not in total_shifts_per_staff.index or staff2 not in total_shifts_per_staff.index:
            continue

        # ç‹¬ç«‹ã—ãŸå ´åˆã®æœŸå¾…å…±åƒå›æ•°
        total_days = long_df['ds'].dt.date.nunique()
        prob1 = total_shifts_per_staff[staff1] / total_days
        prob2 = total_shifts_per_staff[staff2] / total_days
        expected_count = prob1 * prob2 * total_days

        if expected_count > 0:
            synergy_score = actual_count / expected_count

            # æœŸå¾…å€¤ã‹ã‚‰å¤§ããä¹–é›¢ã—ã¦ã„ã‚‹çµ„ã¿åˆã‚ã›ã‚’æ¤œå‡º
            if synergy_score > SYNERGY_HIGH_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ã‚¹ã‚­ãƒ«ç›¸æ€§",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff1}ã€ã¨ã€Œ{staff2}ã€ã¯æ„å›³çš„ã«åŒã˜ã‚·ãƒ•ãƒˆã«é…ç½®ã•ã‚Œã‚‹ï¼ˆç›¸æ€§â—ï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(min(synergy_score / 2, 1.0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å®Ÿç¸¾": actual_count, "æœŸå¾…å€¤": round(expected_count, 1)}
                })
            elif synergy_score < SYNERGY_LOW_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ã‚¹ã‚­ãƒ«ç›¸æ€§",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff1}ã€ã¨ã€Œ{staff2}ã€ã¯æ„å›³çš„ã«åˆ¥ã‚·ãƒ•ãƒˆã«é…ç½®ã•ã‚Œã‚‹ï¼ˆç›¸æ€§Ã—ï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - synergy_score, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å®Ÿç¸¾": actual_count, "æœŸå¾…å€¤": round(expected_count, 1)}
                })

    return rules


def _analyze_workload_distribution(long_df: pd.DataFrame) -> list:
    """è² è·ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°æˆ¦ç•¥ï¼šç¹å¿™æ™‚é–“å¸¯ã§ã®äººå“¡é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    rules = []

    if not {'staff', 'role'}.issubset(long_df.columns):
        return rules

    # æ™‚é–“å¸¯åˆ¥ã®å¹³å‡äººå“¡æ•°ã‚’è¨ˆç®—
    time_staff_counts = long_df[long_df['parsed_slots_count'] > 0].groupby(
        long_df['ds'].dt.time
    )['staff'].nunique().reset_index()
    time_staff_counts.columns = ['time', 'avg_staff']

    # ç¹å¿™æ™‚é–“å¸¯ã‚’ç‰¹å®šï¼ˆä¸Šä½25%ï¼‰
    threshold = time_staff_counts['avg_staff'].quantile(0.75)
    busy_times = time_staff_counts[time_staff_counts['avg_staff'] >= threshold]['time'].tolist()

    if not busy_times:
        return rules

    # ç¹å¿™æ™‚é–“å¸¯ã§ã®è·å“¡ã®çµŒé¨“å€¤åˆ†å¸ƒã‚’åˆ†æ
    busy_df = long_df[long_df['ds'].dt.time.isin(busy_times)]

    # å„è·å“¡ã®ç·å‹¤å‹™æ—¥æ•°ï¼ˆçµŒé¨“å€¤ã®ä»£ç†æŒ‡æ¨™ï¼‰
    staff_experience = long_df.groupby('staff')['ds'].nunique().sort_values(ascending=False)
    median_exp = staff_experience.median()

    # ç¹å¿™æ™‚é–“å¸¯ã§ã®ãƒ™ãƒ†ãƒ©ãƒ³é…ç½®ç‡
    busy_staff = busy_df['staff'].unique()
    veteran_staff = staff_experience[staff_experience > median_exp].index
    veteran_ratio = len(set(busy_staff) & set(veteran_staff)) / len(busy_staff) if len(busy_staff) > 0 else 0

    if veteran_ratio > VETERAN_RATIO_THRESHOLD:
        rules.append({
            "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "è² è·åˆ†æ•£æˆ¦ç•¥",
            "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "ç¹å¿™æ™‚é–“å¸¯ã«ã¯å¿…ãšãƒ™ãƒ†ãƒ©ãƒ³è·å“¡ã‚’å„ªå…ˆé…ç½®ã—ã¦ã„ã‚‹",
            "æ³•å‰‡ã®å¼·åº¦": round(veteran_ratio, 2),
            "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"ç¹å¿™æ™‚é–“å¸¯": [str(t) for t in busy_times[:5]], "ãƒ™ãƒ†ãƒ©ãƒ³é…ç½®ç‡": f"{veteran_ratio:.1%}"}
        })

    # å½¹å‰²åˆ¥ã®åˆ†å¸ƒã‚‚åˆ†æ
    for role in busy_df['role'].unique():
        role_ratio = len(busy_df[busy_df['role'] == role]) / len(busy_df)
        if role_ratio > ROLE_RATIO_THRESHOLD:
            rules.append({
                "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "è² è·åˆ†æ•£æˆ¦ç•¥",
                "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ç¹å¿™æ™‚é–“å¸¯ã§ã¯ã€Œ{role}ã€ã®é…ç½®ã‚’é‡è¦–ã—ã¦ã„ã‚‹",
                "æ³•å‰‡ã®å¼·åº¦": round(role_ratio, 2),
                "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å½¹å‰²æ¯”ç‡": f"{role_ratio:.1%}"}
            })

    return rules


def _analyze_personal_consideration(long_df: pd.DataFrame) -> list:
    """å€‹äººäº‹æƒ…é…æ…®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šç‰¹å®šè·å“¡ã¸ã®å®šæœŸçš„ãªé…æ…®"""
    rules = []

    if 'staff' not in long_df.columns:
        return rules

    # å„è·å“¡ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    for staff in long_df['staff'].unique():
        staff_df = long_df[long_df['staff'] == staff].copy()

        # æ›œæ—¥Ã—æ™‚é–“å¸¯ã®å‹¤å‹™é »åº¦ã‚’è¨ˆç®—
        staff_df['dow'] = staff_df['ds'].dt.dayofweek
        staff_df['hour'] = staff_df['ds'].dt.hour

        # ç‰¹å®šã®æ›œæ—¥ãƒ»æ™‚é–“å¸¯ã‚’é¿ã‘ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        dow_hour_counts = staff_df.groupby(['dow', 'hour']).size()
        total_weeks = staff_df['ds'].dt.isocalendar().week.nunique()

        # æœŸå¾…é »åº¦ã¨å®Ÿéš›ã®é »åº¦ã‚’æ¯”è¼ƒ
        for (dow, hour), count in dow_hour_counts.items():
            expected_count = total_weeks * 0.8  # 80%ã®å‡ºç¾ã‚’æœŸå¾…å€¤ã¨ã™ã‚‹

            if count < expected_count * 0.2:  # æœŸå¾…å€¤ã®20%æœªæº€
                dow_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "å€‹äººé…æ…®",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã¯{dow_names[dow]}æ›œæ—¥ã®{hour}æ™‚å°ã‚’ã»ã¼é¿ã‘ã¦ã„ã‚‹ï¼ˆå€‹äººäº‹æƒ…ï¼Ÿï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - (count / expected_count if expected_count > 0 else 0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å‡ºç¾å›æ•°": count, "æœŸå¾…å›æ•°": round(expected_count, 1)}
                })

        # æœˆåˆãƒ»æœˆæœ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚åˆ†æ
        staff_df['day'] = staff_df['ds'].dt.day
        month_pattern = staff_df.groupby(staff_df['day'] <= 5)['ds'].count()

        if len(month_pattern) == 2:
            early_month_ratio = month_pattern[True] / month_pattern.sum()
            if early_month_ratio < EARLY_MONTH_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "å€‹äººé…æ…®",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã¯æœˆåˆï¼ˆ1-5æ—¥ï¼‰ã®å‹¤å‹™ã‚’ã»ã¼é¿ã‘ã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - early_month_ratio, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœˆåˆå‹¤å‹™ç‡": f"{early_month_ratio:.1%}"}
                })

    return rules


def _analyze_rotation_strategy(long_df: pd.DataFrame) -> list:
    """ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥ï¼šå…¬å¹³æ€§ã‚’ä¿ã¤ãŸã‚ã®è¤‡é›‘ãªãƒ«ãƒ¼ãƒ«"""
    rules = []

    if not {'staff', 'code'}.issubset(long_df.columns):
        return rules

    # å„è·å“¡ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é€£ç¶šæ€§ã‚’åˆ†æ
    for staff in long_df['staff'].unique():
        staff_df = long_df[long_df['staff'] == staff].sort_values('ds')

        # é€£ç¶šå‹¤å‹™æ—¥æ•°ã®è¨ˆç®—
        staff_dates = staff_df[staff_df['parsed_slots_count'] > 0]['ds'].dt.date.unique()

        if len(staff_dates) < 2:
            continue

        # é€£ç¶šå‹¤å‹™ã®ã‚«ã‚¦ãƒ³ãƒˆ
        consecutive_counts = []
        current_streak = 1

        for i in range(1, len(staff_dates)):
            if (staff_dates[i] - staff_dates[i-1]).days == 1:
                current_streak += 1
            else:
                if current_streak > 1:
                    consecutive_counts.append(current_streak)
                current_streak = 1

        if current_streak > 1:
            consecutive_counts.append(current_streak)

        # é•·æœŸé€£å‹¤ã®æ¤œå‡º
        if consecutive_counts:
            max_consecutive = max(consecutive_counts)
            avg_consecutive = np.mean(consecutive_counts)

            if max_consecutive <= 3 and avg_consecutive < 2.5:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã®é€£ç¶šå‹¤å‹™ã¯å¿…ãš3æ—¥ä»¥å†…ã«åˆ¶é™ã•ã‚Œã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - (max_consecutive - 3) / 7, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœ€å¤§é€£ç¶š": max_consecutive, "å¹³å‡é€£ç¶š": round(avg_consecutive, 1)}
                })

        # å‹¤å‹™ã‚³ãƒ¼ãƒ‰ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³
        code_sequence = staff_df[staff_df['parsed_slots_count'] > 0]['code'].tolist()

        if len(code_sequence) >= 3:
            # åŒã˜ã‚³ãƒ¼ãƒ‰ã®é€£ç¶šã‚’é¿ã‘ã¦ã„ã‚‹ã‹
            same_code_runs = []
            current_run = 1

            for i in range(1, len(code_sequence)):
                if code_sequence[i] == code_sequence[i-1]:
                    current_run += 1
                else:
                    if current_run > 1:
                        same_code_runs.append(current_run)
                    current_run = 1

            if same_code_runs and max(same_code_runs) <= 2:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{staff}ã€ã¯åŒã˜å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ3æ—¥ä»¥ä¸Šç¶šã‹ãªã„ã‚ˆã†ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": 0.8,
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœ€å¤§é€£ç¶šåŒä¸€å‹¤å‹™": max(same_code_runs)}
                })

    return rules


def _analyze_risk_mitigation(long_df: pd.DataFrame) -> list:
    """ãƒªã‚¹ã‚¯å›é¿ãƒ«ãƒ¼ãƒ«ï¼šãƒˆãƒ©ãƒ–ãƒ«é˜²æ­¢ã®ãŸã‚ã®æš—é»™ã®é…ç½®ãƒ«ãƒ¼ãƒ«"""
    rules = []

    if not {'staff', 'role'}.issubset(long_df.columns):
        return rules

    # å„æ™‚é–“å¸¯ã§ã®æ–°äººæ¯”ç‡ã‚’åˆ†æ
    staff_experience = long_df.groupby('staff')['ds'].nunique()
    experience_threshold = staff_experience.quantile(0.25)  # ä¸‹ä½25%ã‚’æ–°äººã¨ã™ã‚‹
    new_staff = staff_experience[staff_experience <= experience_threshold].index

    # æ™‚é–“å¸¯åˆ¥ã®æ–°äººæ¯”ç‡
    time_groups = long_df[long_df['parsed_slots_count'] > 0].groupby(
        [long_df['ds'].dt.date, long_df['ds'].dt.hour]
    )

    new_staff_only_count = 0
    total_time_slots = 0

    for (date, hour), group in time_groups:
        unique_staff = group['staff'].unique()
        if len(unique_staff) > 1:  # è¤‡æ•°äººå‹¤å‹™ã®å ´åˆã®ã¿
            total_time_slots += 1
            if all(s in new_staff for s in unique_staff):
                new_staff_only_count += 1

    if total_time_slots > 0:
        new_staff_only_ratio = new_staff_only_count / total_time_slots

        if new_staff_only_ratio < NEW_STAFF_ONLY_RATIO_THRESHOLD:
            rules.append({
                "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒªã‚¹ã‚¯å›é¿",
                "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "æ–°äººã ã‘ã®ã‚·ãƒ•ãƒˆã¯çµ¶å¯¾ã«ä½œã‚‰ãªã„ï¼ˆå¿…ãšãƒ™ãƒ†ãƒ©ãƒ³ã‚’1äººã¯é…ç½®ï¼‰",
                "æ³•å‰‡ã®å¼·åº¦": round(1.0 - new_staff_only_ratio, 2),
                "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æ–°äººã®ã¿ã‚·ãƒ•ãƒˆç™ºç”Ÿç‡": f"{new_staff_only_ratio:.1%}"}
            })

    # ç‰¹å®šã®å½¹å‰²ã®çµ„ã¿åˆã‚ã›åˆ†æ
    role_combinations = defaultdict(int)

    for (date, time), group in long_df.groupby([long_df['ds'].dt.date, long_df['ds'].dt.time]):
        roles = group['role'].unique()
        if len(roles) >= 2:
            for combo in combinations(sorted(roles), 2):
                role_combinations[combo] += 1

    # æœŸå¾…å€¤ã¨æ¯”è¼ƒã—ã¦ç•°å¸¸ã«å°‘ãªã„çµ„ã¿åˆã‚ã›ã‚’æ¤œå‡º
    if role_combinations:
        avg_count = np.mean(list(role_combinations.values()))
        for combo, count in role_combinations.items():
            if count < avg_count * ROLE_COMBO_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "ãƒªã‚¹ã‚¯å›é¿",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": f"ã€Œ{combo[0]}ã€ã¨ã€Œ{combo[1]}ã€ã¯åŒæ™‚é…ç½®ã‚’é¿ã‘ã¦ã„ã‚‹ï¼ˆæ¥­å‹™ä¸Šã®ç†ç”±ï¼Ÿï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - (count / avg_count if avg_count > 0 else 0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"å‡ºç¾å›æ•°": count, "å¹³å‡": round(avg_count, 1)}
                })

    return rules


def _analyze_temporal_context(long_df: pd.DataFrame) -> list:
    """æ™‚ç³»åˆ—ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼šæ™‚æœŸã«ã‚ˆã‚‹é…ç½®æˆ¦ç•¥ã®å¤‰åŒ–"""
    rules = []

    if 'ds' not in long_df.columns:
        return rules

    # æœˆåˆ¥ã®å‚¾å‘åˆ†æ
    long_df['month'] = long_df['ds'].dt.month
    long_df['day'] = long_df['ds'].dt.day

    # æœˆåˆãƒ»æœˆä¸­ãƒ»æœˆæœ«ã§ã®äººå“¡é…ç½®ã®é•ã„
    long_df['period'] = pd.cut(long_df['day'], bins=[0, 10, 20, 31], labels=['æœˆåˆ', 'æœˆä¸­', 'æœˆæœ«'])

    period_stats = long_df[long_df['parsed_slots_count'] > 0].groupby(['period']).agg({
        'staff': 'nunique',
        'ds': 'count'
    })

    if len(period_stats) > 1 and 'æœˆæœ«' in period_stats.index:
        period_stats['avg_staff_per_slot'] = period_stats['ds'] / period_stats['staff']

        mean_staff_per_slot = period_stats['avg_staff_per_slot'].mean()
        if mean_staff_per_slot > 0:
            month_end_ratio = period_stats.loc['æœˆæœ«', 'avg_staff_per_slot'] / mean_staff_per_slot
            if month_end_ratio > MONTH_END_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "æ™‚ç³»åˆ—æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "æœˆæœ«ã¯é€šå¸¸ã‚ˆã‚Šæ‰‹åšã„äººå“¡é…ç½®ã‚’è¡Œã£ã¦ã„ã‚‹ï¼ˆç· ã‚ä½œæ¥­å¯¾å¿œï¼Ÿï¼‰",
                    "æ³•å‰‡ã®å¼·åº¦": round(min(month_end_ratio - 1.0, 1.0), 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"æœˆæœ«é…ç½®å€ç‡": f"{month_end_ratio:.2f}å€"}
                })

    # æ›œæ—¥ã«ã‚ˆã‚‹æˆ¦ç•¥ã®é•ã„
    long_df['dow'] = long_df['ds'].dt.dayofweek

    dow_stats = long_df[long_df['parsed_slots_count'] > 0].groupby('dow').agg({
        'staff': 'nunique',
        'code': lambda x: x.value_counts().to_dict()
    })

    # é€±æœ«ã®ç‰¹åˆ¥ãªé…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³
    if 5 in dow_stats.index and 6 in dow_stats.index:  # åœŸæ—¥
        weekend_staff = dow_stats.loc[[5, 6], 'staff'].mean()
        weekday_staff = dow_stats.loc[[0, 1, 2, 3, 4], 'staff'].mean() if len(dow_stats) > 2 else 0

        if weekday_staff > 0:
            weekend_ratio = weekend_staff / weekday_staff
            if weekend_ratio < WEEKEND_RATIO_THRESHOLD:
                rules.append({
                    "æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼": "æ™‚ç³»åˆ—æˆ¦ç•¥",
                    "ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡": "é€±æœ«ã¯å¹³æ—¥ã®70%ä»¥ä¸‹ã®çœåŠ›ä½“åˆ¶ã§é‹å–¶ã—ã¦ã„ã‚‹",
                    "æ³•å‰‡ã®å¼·åº¦": round(1.0 - weekend_ratio, 2),
                    "è©³ç´°ãƒ‡ãƒ¼ã‚¿": {"é€±æœ«/å¹³æ—¥æ¯”": f"{weekend_ratio:.1%}"}
                })

    return rules


def _extract_surprising_insights(rules_df: pd.DataFrame) -> list:
    """æ„å¤–æ€§ã®é«˜ã„ç™ºè¦‹ã‚’ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
    if rules_df.empty:
        return []

    surprising = []

    # å€‹äººé…æ…®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æ„å¤–ãªã‚‚ã®ã‚’æŠ½å‡º
    personal_rules = rules_df[rules_df['æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼'] == 'å€‹äººé…æ…®']
    if not personal_rules.empty:
        top_personal = personal_rules.nlargest(3, 'æ³•å‰‡ã®å¼·åº¦')
        for _, rule in top_personal.iterrows():
            surprising.append({
                "ç™ºè¦‹": rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'],
                "æ„å¤–æ€§": "ç‰¹å®šå€‹äººã¸ã®é…æ…®ãŒæ˜ç¢ºã«ãƒ‡ãƒ¼ã‚¿ã«ç¾ã‚Œã¦ã„ã‚‹"
            })

    # ã‚¹ã‚­ãƒ«ç›¸æ€§ã§ç›¸æ€§ãŒæ‚ªã„ãƒšã‚¢
    bad_synergy = rules_df[
        (rules_df['æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼'] == 'ã‚¹ã‚­ãƒ«ç›¸æ€§') &
        (rules_df['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'].str.contains('åˆ¥ã‚·ãƒ•ãƒˆ'))
    ]
    if not bad_synergy.empty:
        for _, rule in bad_synergy.iterrows():
            surprising.append({
                "ç™ºè¦‹": rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'],
                "æ„å¤–æ€§": "æ„å›³çš„ã«çµ„ã¿åˆã‚ã›ã‚’é¿ã‘ã¦ã„ã‚‹è·å“¡ãƒšã‚¢ã®å­˜åœ¨"
            })

    return surprising


def _generate_deep_insights_summary(rules_df: pd.DataFrame) -> str:
    """ã‚ˆã‚Šæ´å¯Ÿçš„ãªã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
    if rules_df.empty:
        return "åˆ†æå¯èƒ½ãªæ³•å‰‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"

    summary_parts = []

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®æ³•å‰‡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
    category_counts = rules_df['æ³•å‰‡ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼'].value_counts()

    summary_parts.append("## ğŸ” ã‚·ãƒ•ãƒˆä½œæˆã®æ·±å±¤åˆ†æçµæœ\n")
    summary_parts.append(f"åˆè¨ˆ **{len(rules_df)}å€‹** ã®æš—é»™ã®ãƒ«ãƒ¼ãƒ«ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚\n")

    # æœ€ã‚‚å¼·ã„æ³•å‰‡ãƒˆãƒƒãƒ—3
    top_rules = rules_df.nlargest(3, 'æ³•å‰‡ã®å¼·åº¦')
    summary_parts.append("\n### ğŸ“Š æœ€ã‚‚å½±éŸ¿åŠ›ã®å¼·ã„ãƒ«ãƒ¼ãƒ« TOP3\n")
    for i, (_, rule) in enumerate(top_rules.iterrows(), 1):
        summary_parts.append(f"{i}. **{rule['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡']}** (å¼·åº¦: {rule['æ³•å‰‡ã®å¼·åº¦']})")

    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®ç‰¹å¾´
    summary_parts.append("\n### ğŸ¯ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥ã®ç‰¹å¾´\n")

    if 'ã‚¹ã‚­ãƒ«ç›¸æ€§' in category_counts:
        summary_parts.append(f"- **äººé–“é–¢ä¿‚ã¸ã®é…æ…®**: {category_counts['ã‚¹ã‚­ãƒ«ç›¸æ€§']}å€‹ã®ãƒ«ãƒ¼ãƒ«")
    if 'å€‹äººé…æ…®' in category_counts:
        summary_parts.append(f"- **å€‹äººäº‹æƒ…ã¸ã®å¯¾å¿œ**: {category_counts['å€‹äººé…æ…®']}å€‹ã®ãƒ«ãƒ¼ãƒ«")
    if 'ãƒªã‚¹ã‚¯å›é¿' in category_counts:
        summary_parts.append(f"- **ãƒˆãƒ©ãƒ–ãƒ«é˜²æ­¢ç­–**: {category_counts['ãƒªã‚¹ã‚¯å›é¿']}å€‹ã®ãƒ«ãƒ¼ãƒ«")

    # ç·æ‹¬
    summary_parts.append("\n### ğŸ’¡ ç·æ‹¬")
    summary_parts.append("ã“ã®ã‚·ãƒ•ãƒˆã¯ã€è¡¨é¢çš„ãªãƒ«ãƒ¼ãƒ«ã ã‘ã§ãªãã€")
    summary_parts.append("è·å“¡é–“ã®ç›¸æ€§ã€å€‹äººã®äº‹æƒ…ã€ãƒªã‚¹ã‚¯ç®¡ç†ãªã©ã€")
    summary_parts.append("**å¤šæ¬¡å…ƒçš„ãªé…æ…®**ãŒè¤‡é›‘ã«çµ„ã¿åˆã‚ã•ã£ã¦ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚")

    return "\n".join(summary_parts)


def _calculate_fairness_score(daily_shift_df: pd.DataFrame) -> float:
    """ãã®æ—¥ã®ã‚·ãƒ•ãƒˆã®å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    work_hours = daily_shift_df.groupby("staff")["ds"].count()
    if work_hours.empty:
        return 0.0
    return 1.0 - (work_hours.std() / (work_hours.mean() + 1e-6))


def _calculate_cost_score(daily_shift_df: pd.DataFrame, max_staff: int) -> float:
    """ãã®æ—¥ã®ã‚·ãƒ•ãƒˆã®ã‚³ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
    staff_count = daily_shift_df["staff"].nunique()
    if max_staff == 0:
        return 1.0
    return 1.0 - (staff_count / max_staff)


def _calculate_risk_score(daily_shift_df: pd.DataFrame, new_staff: set[str]) -> float:
    """æ–°äººã ã‘ã®ã‚·ãƒ•ãƒˆã‚’é¿ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‹ã§ãƒªã‚¹ã‚¯ã‚’è©•ä¾¡"""
    time_groups = daily_shift_df.groupby(daily_shift_df["ds"].dt.hour)
    new_only = 0
    total = 0
    for _, group in time_groups:
        staff = set(group["staff"].unique())
        if len(staff) > 1:
            total += 1
            if all(s in new_staff for s in staff):
                new_only += 1
    if total == 0:
        return 1.0
    return 1.0 - new_only / total


def _calculate_satisfaction_score(date: pd.Timestamp, long_df: pd.DataFrame) -> float:
    """é€£å‹¤ã®é•·ã•ã‹ã‚‰æº€è¶³åº¦ã‚’æ¨å®š"""
    day_df = long_df[long_df["ds"].dt.date == date.date()]
    staff_list = day_df["staff"].unique()
    long_run = 0
    for staff in staff_list:
        staff_dates = (
            long_df[long_df["staff"] == staff]["ds"].dt.date.drop_duplicates().sort_values()
        )
        indices = [i for i, d in enumerate(staff_dates) if d == date.date()]
        for idx in indices:
            left = idx
            while left > 0 and (staff_dates.iloc[left] - staff_dates.iloc[left - 1]).days == 1:
                left -= 1
            right = idx
            while right < len(staff_dates) - 1 and (
                staff_dates.iloc[right + 1] - staff_dates.iloc[right]
            ).days == 1:
                right += 1
            if right - left + 1 >= 4:
                long_run += 1
                break
    if len(staff_list) == 0:
        return 1.0
    return 1.0 - long_run / len(staff_list)


def create_scored_blueprint(long_df: pd.DataFrame) -> pd.DataFrame:
    """Calculate daily scores representing shift creator preferences."""
    if long_df.empty:
        return pd.DataFrame()

    df = long_df.copy()
    df["ds"] = pd.to_datetime(df["ds"])

    daily_groups = df.groupby(df["ds"].dt.date)
    max_staff = daily_groups["staff"].nunique().max()

    staff_experience = df.groupby("staff")["ds"].nunique()
    threshold = staff_experience.quantile(0.25)
    new_staff = set(staff_experience[staff_experience <= threshold].index)

    scored_days = []
    for date, group in daily_groups:
        scores = {
            "date": pd.to_datetime(date),
            "fairness_score": _calculate_fairness_score(group),
            "cost_score": _calculate_cost_score(group, int(max_staff)),
            "risk_score": _calculate_risk_score(group, new_staff),
            "satisfaction_score": _calculate_satisfaction_score(pd.to_datetime(date), df),
        }
        scored_days.append(scores)

    return pd.DataFrame(scored_days).set_index("date")


def analyze_tradeoffs(scored_blueprint_df: pd.DataFrame) -> dict:
    """ã‚¹ã‚³ã‚¢é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ†æã™ã‚‹"""
    if scored_blueprint_df.empty:
        return {}

    corr = scored_blueprint_df.corr()
    tradeoff_pairs = {}
    for col1 in corr.columns:
        for col2 in corr.columns:
            if col1 != col2 and corr.loc[col1, col2] < -0.5:
                tradeoff_pairs[f"{col1}_vs_{col2}"] = float(corr.loc[col1, col2])

    scatter_cols = {"fairness_score", "cost_score"}
    scatter_data = (
        scored_blueprint_df[list(scatter_cols)].reset_index().to_dict("records")
        if scatter_cols.issubset(scored_blueprint_df.columns)
        else []
    )

    return {
        "correlation_matrix": corr.to_dict(),
        "strongest_tradeoffs": tradeoff_pairs,
        "scatter_data": scatter_data,
    }


def create_staff_level_blueprint(
    long_df: pd.DataFrame, scored_blueprint_df: pd.DataFrame
) -> pd.DataFrame:
    """Aggregate daily scores at the staff level."""
    if long_df.empty or scored_blueprint_df.empty:
        return pd.DataFrame()

    df = long_df.copy()
    df["ds"] = pd.to_datetime(df["ds"])
    df["date"] = df["ds"].dt.date

    staff_by_day = df[["date", "staff"]].drop_duplicates()
    score_df = scored_blueprint_df.reset_index()
    merged = staff_by_day.merge(score_df, on="date", how="left")

    score_cols = [
        "fairness_score",
        "cost_score",
        "risk_score",
        "satisfaction_score",
    ]
    return merged.groupby("staff")[score_cols].mean()


# backward compatibility
def create_blueprint_list(long_df: pd.DataFrame) -> dict:
    scored = create_scored_blueprint(long_df)
    staff_level = create_staff_level_blueprint(long_df, scored)
    results = analyze_tradeoffs(scored)
    results["staff_level_scores"] = staff_level
    return results
