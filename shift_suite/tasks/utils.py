# shift_suite/tasks/utils.py
# v1.2.1 ( _parse_as_date ç§»è¨­)
"""
shift_suite.tasks.utils  v1.2.0 â€“ UTF-8 write_meta ç‰ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
* 2025-05-06
    - write_meta(): Windows ã§ã‚‚æ–‡å­—åŒ–ã‘ã—ãªã„ã‚ˆã† `encoding="utf-8"` ã‚’æ˜ç¤º
      ï¼ˆæ—¢å®š CP932 ã ã¨ â€œâ€“â€ ãªã©ã® Unicode æ–‡å­—åˆ—ã§ UnicodeEncodeErrorï¼‰
    - æ—§ APIãƒ»é–¢æ•°åã¯ä¸€åˆ‡å¤‰æ›´ãªã—
* 2025-05-16
    - build_stats.py ã‹ã‚‰ _parse_as_date ã‚’ç§»è¨­
"""

from __future__ import annotations

import datetime as dt  #  dt ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚æ˜ç¤ºçš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ä»–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã®äº’æ›æ€§ã®ãŸã‚)
import json
import logging
import math
import re
import shutil
import tempfile
import zipfile
from datetime import (
    datetime,
    timedelta,
)  #  dt ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§ã¯ãªã datetime, timedelta ã‚’ç›´æ¥ä½¿ç”¨
from pathlib import Path
from typing import Any, Sequence  #  Any ã‚’è¿½åŠ 

import numpy as np
import pandas as pd
from pandas import DataFrame, Series

from ..logger_config import configure_logging

# è¿½åŠ ç®‡æ‰€: constants ã‹ã‚‰ SUMMARY5 ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ( _parse_as_date ã§ä½¿ç”¨)
from .constants import SUMMARY5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ãƒ­ã‚¬ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
configure_logging()
log = logging.getLogger(__name__)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. Excel æ—¥ä»˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def excel_date(excel_serial: Any) -> dt.date | None:
    """Excel 1900 ã‚·ãƒªã‚¢ãƒ« or pandas.Timestamp ç­‰ â†’ date"""  #  docstringå¤‰æ›´
    if excel_serial in (None, "", np.nan):
        return None
    if isinstance(excel_serial, (datetime, np.datetime64, pd.Timestamp)):
        # pd.Timestamp(...).to_pydatetime() ã¯ datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
        # ãã®æ—¥ä»˜éƒ¨åˆ†ã®ã¿ã‚’å–å¾—ã™ã‚‹ã«ã¯ .date() ã‚’å‘¼ã³å‡ºã™
        return pd.Timestamp(excel_serial).to_pydatetime().date()  #  .date() ã‚’è¿½åŠ 
    try:
        base = datetime(1899, 12, 30)  # Excel èµ·ç‚¹ (1900-01-00)
        return (base + timedelta(days=float(excel_serial))).date()
    except (ValueError, TypeError):
        return None


def to_hhmm(x: Any) -> str | None:
    """8.5 â†’ '08:30' / '23:45' â†’ '23:45' / Excel ã‚·ãƒªã‚¢ãƒ«ãªã©æŸ”è»Ÿå¤‰æ›"""
    if x in (None, "", np.nan):
        return None
    if isinstance(x, (int, float)) and not math.isnan(x):
        h = int(x)
        m = int(round((x - h) * 60))
        return f"{h:02d}:{m:02d}"
    x_str = str(x).strip()  #  å¤‰æ•°åã‚’ x_str ã«å¤‰æ›´
    m = re.match(r"^(\d{1,2}):(\d{1,2})$", x_str)
    if m:
        return f"{int(m.group(1)):02d}:{int(m.group(2)):02d}"
    #  è¿½åŠ : HH:MM:SS å½¢å¼ã‚‚è€ƒæ…® (ç§’ã¯åˆ‡ã‚Šæ¨ã¦)
    m_ss = re.match(r"^(\d{1,2}):(\d{1,2}):(\d{1,2})$", x_str)
    if m_ss:
        return f"{int(m_ss.group(1)):02d}:{int(m_ss.group(2)):02d}"
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ãƒ©ãƒ™ãƒ«ï¼ãƒ•ã‚¡ã‚¤ãƒ«å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gen_labels(slot: int) -> list[str]:
    """00:00-23:59 ã‚’ slot åˆ†åˆ»ã¿ã§ HH:MM ãƒ©ãƒ™ãƒ«åŒ–"""
    t = datetime(2000, 1, 1)
    labels: list[str] = []
    while t.day == 1:  # 24h
        labels.append(t.strftime("%H:%M"))
        t += timedelta(minutes=slot)
    return labels


INVALID_CHARS = r"[\\/*?:\[\]]|\n|\r"


def safe_sheet(name: str, *, for_path: bool = False) -> str:
    """Excel ã‚·ãƒ¼ãƒˆï¼ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã«å±é™ºæ–‡å­—ã‚’ç½®æ›"""
    out = re.sub(INVALID_CHARS, "_", str(name))
    out = out.strip()[:31]  # Excel ã‚·ãƒ¼ãƒˆã¯ 31 æ–‡å­—ä¸Šé™
    if for_path:
        out = out.replace(" ", "_")
    return out or "sheet"


def safe_read_excel(fp: Path | str, **kwargs: Any) -> DataFrame:
    """Read an Excel file with basic error handling.

    Parameters
    ----------
    fp : Path | str
        Excel file path.
    **kwargs : Any
        Options forwarded to ``pd.read_excel``.

    Returns
    -------
    DataFrame
        Loaded DataFrame.

    Raises
    ------
    FileNotFoundError
        If the file does not exist.
    ValueError
        For empty files or other read errors.
    """
    path = Path(fp)
    if not path.exists():
        log.error("Excel file not found: %s", path)
        raise FileNotFoundError(path)
    try:
        return pd.read_excel(path, **kwargs)
    except pd.errors.EmptyDataError as e:
        log.error("Excel file '%s' is empty: %s", path, e)
        raise ValueError(f"Excel file '{path}' is empty") from e
    except Exception as e:  # noqa: BLE001
        log.error("Failed to read Excel file '%s': %s", path, e)
        raise ValueError(f"Failed to read Excel file '{path}': {e}") from e


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. DataFrame ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_df_xlsx(
    df: DataFrame,
    fp: Path | str,
    sheet_name: str | None = None,
    *,
    index: bool = True,
    engine: str = "openpyxl",
) -> Path:
    """
    æ±ç”¨ Excel ä¿å­˜ãƒ©ãƒƒãƒ‘ãƒ¼
    - é•·ã„ãƒ‘ã‚¹ã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±ã§å®‰å…¨ã«ä¿å­˜
    - sheet_name=None â†’ ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®‰å…¨åŒ–
    """
    fp_path = Path(fp)  #  å¤‰æ•°åã‚’ fp_path ã«å¤‰æ›´
    final_sheet_name = sheet_name or safe_sheet(
        fp_path.stem
    )  #  å¤‰æ•°åã‚’ final_sheet_name ã«å¤‰æ›´
    to_excel_kwargs = dict(index=index, sheet_name=final_sheet_name, engine=engine)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        df.to_excel(tmp.name, **to_excel_kwargs)
        tmp.flush()  # Ensure all data is written to disk
        # tmp.close() # Close should happen before move on some OS, but pd might handle it.
        # Explicitly closing here might be safer before shutil.move.
        # However, pandas might need the file open if it's not fully flushed.
        # For safety, ensure pandas writer is closed if used, or rely on NamedTemporaryFile's delete=False behavior.
        # pd.ExcelWriter context manager is better if more control is needed.
    # Ensure the target directory exists
    fp_path.parent.mkdir(parents=True, exist_ok=True)
    shutil.move(tmp.name, fp_path)

    return fp_path


def save_df_parquet(
    df: DataFrame,
    fp: Path | str,
    *,
    index: bool = True,
) -> Path:
    """Save DataFrame to Parquet file."""
    fp_path = Path(fp)
    fp_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(fp_path, index=index)
    return fp_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_meta(target: Path | str, /, **meta) -> Path:
    """
    JSON ãƒ¡ã‚¿æƒ…å ±ã‚’æ›¸ãå‡ºã—ã€‚UTF-8 å›ºå®šã§ UnicodeEncodeError ã‚’é˜²æ­¢ã€‚

    Parameters
    ----------
    target : Path | str
        * ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¸¡ã—ãŸå ´åˆ â†’ `<dir>/meta.json`
        * ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¸¡ã—ãŸå ´åˆ â†’ ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    meta : dict
        æ›¸ãè¾¼ã‚€ã‚­ãƒ¼ï¼å€¤
    """
    target_path = Path(target)  #  å¤‰æ•°åã‚’ target_path ã«å¤‰æ›´
    meta_fp = target_path / "meta.json" if target_path.is_dir() else target_path
    meta_fp.parent.mkdir(parents=True, exist_ok=True)
    # ensure_ascii=False ã¨ indent=2 ã¯JSONã‚’è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ã«è‰¯ã„ç¿’æ…£
    try:
        meta_fp.write_text(
            json.dumps(
                meta, ensure_ascii=False, indent=2, default=str
            ),  #  default=str ã‚’è¿½åŠ  (datetimeç­‰éã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¯¾ç­–)
            encoding="utf-8",
        )
    except Exception as e:
        log.error(f"Failed to write meta file {meta_fp}: {e}")
        # Optionally, re-raise or handle as appropriate
    return meta_fp


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ZIP ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_make_archive(src_dir: Path, dst_zip: Path) -> Path:
    """Windows é•·ãƒ‘ã‚¹å¯¾å¿œä»˜ã shutil.make_archive ç›¸å½“"""
    src_dir_path, dst_zip_path = Path(src_dir), Path(dst_zip)  #  å¤‰æ•°åå¤‰æ›´
    if dst_zip_path.exists():
        dst_zip_path.unlink()

    with zipfile.ZipFile(dst_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for p_item in src_dir_path.rglob("*"):  #  å¤‰æ•°åå¤‰æ›´
            zf.write(p_item, p_item.relative_to(src_dir_path))
    return dst_zip_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ã‚¹ã‚¿ãƒƒãƒ•æ•°é–¾å€¤è¨ˆç®— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def derive_min_staff(heat: DataFrame | Series, method: str = "p25") -> Series:
    # (æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã¾ã¾ã¨ã—ã¾ã™)
    if isinstance(heat, Series):
        if method == "p25":
            return pd.Series([heat.quantile(0.25)]).round()
        if method == "mean-1s":
            return pd.Series([max(0, heat.mean() - heat.std())]).round()
        if method == "mode":
            # mode() can return an empty Series if all values are unique or NaN
            modes = heat.mode()
            return pd.Series([modes.iloc[0] if not modes.empty else np.nan]).round()
        raise ValueError(f"Unknown min_method: {method}")
    else:
        values = heat.select_dtypes(include=np.number)  #  include=np.number ã«å¤‰æ›´
        if values.empty:  # æ•°å€¤åˆ—ãŒãªã„å ´åˆ
            return pd.Series(dtype=float)  # ç©ºã®Seriesã‚’è¿”ã™
        if method == "p25":
            return values.quantile(0.25, axis=1).round()
        if method == "mean-1s":
            return (values.mean(axis=1) - values.std(axis=1)).clip(lower=0).round()
        if method == "mode":
            # DataFrame.mode() can also result in multiple rows if multiple modes exist for a row
            # Taking the first mode found for each row.
            modes_df = values.mode(axis=1)
            return (
                modes_df.iloc[:, 0].round()
                if not modes_df.empty
                else pd.Series(index=values.index, dtype=float).fillna(np.nan)
            )
        raise ValueError(f"Unknown min_method: {method}")  #  max_method -> min_method


def derive_max_staff(heat: DataFrame | Series, method: str = "mean+1s") -> Series:
    # (æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã¾ã¾ã¨ã—ã¾ã™)
    if isinstance(heat, Series):
        if method == "mean+1s":
            std_val = heat.std()
            if pd.isna(std_val):
                std_val = 0
            return pd.Series([heat.mean() + std_val]).round()
        if method == "p75":
            return pd.Series([heat.quantile(0.75)]).round()
        raise ValueError(f"Unknown max_method: {method}")
    else:
        values = heat.select_dtypes(include=np.number)  #  include=np.number ã«å¤‰æ›´
        if values.empty:  # æ•°å€¤åˆ—ãŒãªã„å ´åˆ
            return pd.Series(dtype=float)
        if method == "mean+1s":
            mu = values.mean(axis=1)
            sig = values.std(axis=1).fillna(0)  # NaNâ†’0
            return (mu + sig).round()
        if method == "p75":
            return values.quantile(0.75, axis=1).round()
        raise ValueError(f"Unknown max_method: {method}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. Jain æŒ‡æ•°è¨ˆç®— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def calculate_jain_index(values: pd.Series) -> float:
    """åˆ†å¸ƒã®å…¬å¹³æ€§ã‚’è©•ä¾¡ã™ã‚‹ Jain æŒ‡æ•° (0-1)ã€‚1 ãŒå®Œå…¨å…¬å¹³"""
    # Ensure the series contains numeric data and handle NaNs
    numeric_values = pd.to_numeric(values, errors="coerce").dropna()
    if (
        numeric_values.empty or len(numeric_values) == 0
    ):  #  len(numeric_values) == 0 ã‚‚ãƒã‚§ãƒƒã‚¯
        return 1.0  # Consider what to return for empty or all-NaN series
    if (
        numeric_values < 0
    ).any():  # Jain index is typically for non-negative resource allocation
        log.warning(
            "Jain index calculation: input series contains negative values. Results may be misleading."
        )

    # (sum of all values)^2 / (number of values * sum of squares of all values)
    sum_of_values_sq = (numeric_values.sum()) ** 2
    sum_of_squares = (numeric_values**2).sum()

    if sum_of_squares == 0:  # Avoid division by zero if all values are zero
        return (
            1.0 if numeric_values.nunique() <= 1 else 0.0
        )  # All zero is fair, multiple zeros and some non-zeros is not max fair.
        # If all are zero, it's perfectly fair in one sense.

    return round(sum_of_values_sq / (len(numeric_values) * sum_of_squares), 3)


# è¿½åŠ ç®‡æ‰€: _parse_as_date é–¢æ•°ã®å®šç¾© (build_stats.py ã‹ã‚‰ç§»è¨­)
def _parse_as_date(column_name: Any) -> dt.date | None:
    """åˆ—åã‚’æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ‘ãƒ¼ã‚¹è©¦è¡Œã€‚å¤±æ•—æ™‚ã¯ None"""
    # ğŸ” ã€è¿½åŠ ã€‘ãƒ‘ãƒ¼ã‚¹éç¨‹ã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–ï¼‰
    # log.debug(f"[DATE_PARSE] ãƒ‘ãƒ¼ã‚¹è©¦è¡Œ: '{column_name}' (å‹: {type(column_name)})")

    result: dt.date | None = None
    if isinstance(column_name, (dt.date, dt.datetime, pd.Timestamp)):
        if isinstance(column_name, pd.Timestamp):
            result = column_name.date()
        elif isinstance(column_name, dt.datetime):
            result = column_name.date()
        else:
            result = column_name
    elif isinstance(column_name, str):
        if column_name.lower() in [s.lower() for s in SUMMARY5]:
            result = None
        else:
            m = re.search(r"(\d{4}-\d{1,2}-\d{1,2})", column_name)
            if m:
                try:
                    result = pd.to_datetime(m.group(1), errors="raise").date()
                except (ValueError, TypeError, pd.errors.ParserError):
                    result = None
            if result is None:
                try:
                    result = pd.to_datetime(column_name.split(" ")[0], errors="raise").date()
                except (ValueError, TypeError, pd.errors.ParserError):
                    try:
                        if "." in column_name:
                            excel_serial = float(column_name)
                        else:
                            excel_serial = int(column_name)
                        if 0 < excel_serial < 200000:
                            result = (datetime(1899, 12, 30) + timedelta(days=excel_serial)).date()
                    except ValueError:
                        result = None
    elif isinstance(column_name, (int, float)):
        try:
            if column_name > 0 and column_name < 200000:
                result = (datetime(1899, 12, 30) + timedelta(days=int(column_name))).date()
        except (ValueError, OverflowError, TypeError):
            result = None
    if result and result.weekday() == 6:
        log.debug(f"[DATE_PARSE] æ—¥æ›œæ—¥ã‚’æ¤œå‡º: {column_name} â†’ {result}")
    return result

    if isinstance(column_name, str):
        # SUMMARY5 ã«å«ã¾ã‚Œã‚‹åˆ—åã¯æ—¥ä»˜ã§ã¯ãªã„ã¨æ˜ç¢ºã«åˆ¤å®š
        if column_name.lower() in [s.lower() for s in SUMMARY5]:
            return None

        # ã¾ãšã¯ YYYY-MM-DD ã®ã‚ˆã†ãªéƒ¨åˆ†æ–‡å­—åˆ—ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡ºã—ã¦ã¿ã‚‹
        m = re.search(r"(\d{4}-\d{1,2}-\d{1,2})", column_name)
        if m:
            try:
                return pd.to_datetime(m.group(1), errors="raise").date()
            except (ValueError, TypeError, pd.errors.ParserError):
                pass

        try:
            # "YYYY-MM-DD HH:MM:SS" ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã‚’æƒ³å®šã—ã€ç©ºç™½ã§åˆ†å‰²ã—ã¦æ—¥ä»˜éƒ¨åˆ†ã‚’æŠ½å‡º
            return pd.to_datetime(column_name.split(" ")[0], errors="raise").date()
        except (ValueError, TypeError, pd.errors.ParserError):
            # Excelæ—¥ä»˜ã‚·ãƒªã‚¢ãƒ«å€¤ã®ã‚ˆã†ãªæ–‡å­—åˆ— "45321.0" ã‚„ "45321" ã‚‚ã“ã“ã§å‡¦ç†ã§ãã‚‹ã‹è©¦ã™
            try:
                if "." in column_name:  # "45321.0"
                    excel_serial = float(column_name)
                else:  # "45321"
                    excel_serial = int(column_name)
                if 0 < excel_serial < 200000:  # å¦¥å½“ãªç¯„å›²
                    return (
                        datetime(1899, 12, 30) + timedelta(days=excel_serial)
                    ).date()
            except ValueError:
                pass  # æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ã¸ã®å¤‰æ›å¤±æ•—
        return None  # ä¸Šè¨˜ã§ãƒ‘ãƒ¼ã‚¹ã§ããªã‘ã‚Œã° None

    if isinstance(column_name, (int, float)):  # Excelã‚·ãƒªã‚¢ãƒ«å€¤
        try:
            # Excelã®æ—¥ä»˜ã¯1899-12-30ãŒ0
            # æœ‰åŠ¹ãªExcelæ—¥ä»˜ã‚·ãƒªã‚¢ãƒ«ã®ç¯„å›²ã‚’è€ƒæ…® (ä¾‹: 1 (1900-01-01) ã‹ã‚‰ã€ç¾å®Ÿçš„ãªæœªæ¥ã®æ—¥ä»˜ã¾ã§)
            # Pythonã®intã‚„floatãŒå·¨å¤§ãªå ´åˆã«OverflowErrorã‚’é¿ã‘ã‚‹
            if (
                column_name > 0 and column_name < 200000
            ):  # 200000ã¯ç´„2444å¹´ãªã®ã§ååˆ†ãªã¯ãš
                return (
                    datetime(1899, 12, 30) + timedelta(days=int(column_name))
                ).date()
        except (ValueError, OverflowError, TypeError):  # TypeErrorã‚‚è¿½åŠ 
            return None
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. Date + Weekday Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def date_with_weekday(date_val: Any) -> str:
    """Return ``YYYY-MM-DD(æ›œæ—¥)`` for the given date string."""

    try:
        dt_val = pd.to_datetime(date_val, errors="raise")
    except Exception:  # noqa: BLE001 - parse failure
        return str(date_val)
    weekday_jp = "æœˆç«æ°´æœ¨é‡‘åœŸæ—¥"[dt_val.weekday()]
    return dt_val.strftime("%Y-%m-%d") + f"({weekday_jp})"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. Public Re-export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
__all__: Sequence[str] = [
    "log",
    "excel_date",
    "to_hhmm",
    "gen_labels",
    "safe_sheet",
    "safe_read_excel",
    "save_df_xlsx",
    "save_df_parquet",
    "write_meta",
    "safe_make_archive",
    "derive_min_staff",
    "derive_max_staff",
    "calculate_jain_index",
    "_parse_as_date",  # è¿½åŠ 
    "date_with_weekday",
]
