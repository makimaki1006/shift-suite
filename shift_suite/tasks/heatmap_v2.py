# shift_suite / tasks / heatmap_v2.py
# v2.0.0 (å‹•çš„ãƒ‡ãƒ¼ã‚¿å¯¾å¿œãƒ»æ±ç”¨Needè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ )

from __future__ import annotations

import datetime as dt
from pathlib import Path
from typing import List, Set, Dict, Optional, Tuple
import json

import numpy as np
import openpyxl
import pandas as pd
import logging

from .constants import SUMMARY5
from shift_suite.i18n import translate as _
from .utils import (
    _parse_as_date,
    derive_max_staff,
    gen_labels,
    log,
    safe_sheet,
    save_df_xlsx,
    write_meta,
    validate_need_calculation,
)

# æ–°ã—ã„æ±ç”¨çµ±è¨ˆã‚¨ãƒ³ã‚¸ãƒ³
# from ..core.statistics_engine import AdaptiveStatisticsEngine
# from ..core.data_models import StaffRecord, DailyStaffSummary, NeedCalculationResult

# Temporary replacement classes for missing core modules
class AdaptiveStatisticsEngine:
    def calculate_need(self, historical_data, target_confidence=0.75):
        import numpy as np
        values = np.array(historical_data)
        need_value = float(np.percentile(values, 75)) if len(values) > 0 else 0.0
        confidence = 0.75
        method = "p75_fallback"
        return need_value, confidence, method

class NeedCalculationResult:
    def __init__(self, date, time_slot, need_count, confidence, calculation_method, data_points, raw_data):
        self.date = date
        self.time_slot = time_slot
        self.need_count = need_count
        self.confidence = confidence
        self.calculation_method = calculation_method
        self.data_points = data_points
        self.raw_data = raw_data

analysis_logger = logging.getLogger('analysis')

# é€šå¸¸å‹¤å‹™ã®åˆ¤å®šç”¨å®šæ•°
DEFAULT_HOLIDAY_TYPE = "é€šå¸¸å‹¤å‹™"

STAFF_ALIASES = ["staff", "æ°å", "åå‰", "å¾“æ¥­å“¡"]
ROLE_ALIASES = ["role", "è·ç¨®", "å½¹è·", "éƒ¨ç½²"]


class UniversalNeedCalculator:
    """æ±ç”¨Needè¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆæ›œæ—¥å›ºæœ‰ãƒ­ã‚¸ãƒƒã‚¯ãªã—ï¼‰"""
    
    def __init__(self):
        self.stats_engine = AdaptiveStatisticsEngine()
        self.calculation_log: List[NeedCalculationResult] = []
    
    def calculate_dynamic_need(
        self,
        historical_data_df: pd.DataFrame,
        target_date_columns: List[str],
        confidence_target: float = 0.75
    ) -> pd.DataFrame:
        """
        å‹•çš„ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãNeedè¨ˆç®—
        
        Args:
            historical_data_df: å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ï¼ˆæ™‚é–“å¸¯Ã—æ—¥ä»˜ï¼‰
            target_date_columns: å¯¾è±¡æ—¥ä»˜åˆ—
            confidence_target: ç›®æ¨™ä¿¡é ¼åº¦
            
        Returns:
            Needè¨ˆç®—çµæœDataFrameï¼ˆæ™‚é–“å¸¯Ã—æ—¥ä»˜ï¼‰
        """
        log.info(f"æ±ç”¨Needè¨ˆç®—é–‹å§‹: å¯¾è±¡æ—¥ä»˜æ•°={len(target_date_columns)}")
        
        result_df = pd.DataFrame(
            index=historical_data_df.index,
            columns=target_date_columns,
            dtype=float
        )
        
        # æ™‚é–“å¸¯ã”ã¨ã«å‡¦ç†
        for time_slot in historical_data_df.index:
            log.debug(f"æ™‚é–“å¸¯ {time_slot} ã®å‡¦ç†é–‹å§‹")
            
            # å¯¾è±¡æ—¥ä»˜ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            slot_data = historical_data_df.loc[time_slot, target_date_columns]
            historical_values = slot_data.dropna().tolist()
            
            if not historical_values:
                log.warning(f"æ™‚é–“å¸¯ {time_slot}: ãƒ‡ãƒ¼ã‚¿ãªã—")
                result_df.loc[time_slot] = 0.0
                continue
            
            # æ±ç”¨çµ±è¨ˆã‚¨ãƒ³ã‚¸ãƒ³ã§Needè¨ˆç®—
            need_value, confidence, method = self.stats_engine.calculate_need(
                historical_data=historical_values,
                target_confidence=confidence_target
            )
            
            # çµæœã‚’è¨˜éŒ²
            calculation_result = NeedCalculationResult(
                date=dt.date.today(),  # è¨ˆç®—æ—¥
                time_slot=time_slot,
                need_count=need_value,
                confidence=confidence,
                calculation_method=method,
                data_points=len(historical_values),
                raw_data=historical_values
            )
            self.calculation_log.append(calculation_result)
            
            # å…¨å¯¾è±¡æ—¥ä»˜ã«åŒã˜Needå€¤ã‚’é©ç”¨
            result_df.loc[time_slot] = need_value
            
            log.debug(
                f"æ™‚é–“å¸¯ {time_slot}: Need={need_value:.2f}, "
                f"ä¿¡é ¼åº¦={confidence:.2f}, æ‰‹æ³•={method}"
            )
        
        log.info("æ±ç”¨Needè¨ˆç®—å®Œäº†")
        return result_df.fillna(0.0)
    
    def get_calculation_summary(self) -> Dict:
        """è¨ˆç®—ã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        if not self.calculation_log:
            return {}
        
        total_calculations = len(self.calculation_log)
        avg_confidence = np.mean([r.confidence for r in self.calculation_log])
        method_counts = {}
        
        for result in self.calculation_log:
            method = result.calculation_method
            method_counts[method] = method_counts.get(method, 0) + 1
        
        return {
            "total_calculations": total_calculations,
            "average_confidence": avg_confidence,
            "method_distribution": method_counts,
            "low_confidence_slots": [
                r.time_slot for r in self.calculation_log 
                if r.confidence < 0.6
            ]
        }


def calculate_universal_need(
    df_for_calc: pd.DataFrame,
    date_columns: List[str],
    confidence_target: float = 0.75,
    **kwargs
) -> pd.DataFrame:
    """
    æ±ç”¨Needè¨ˆç®—ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    å¾“æ¥ã®calculate_pattern_based_needã‚’ç½®ãæ›ãˆã‚‹
    æ›œæ—¥å›ºæœ‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Œå…¨ã«æ’é™¤
    """
    log.info("=== æ±ç”¨Needè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ  v2.0.0 ===")
    log.info(f"å¯¾è±¡æ—¥ä»˜æ•°: {len(date_columns)}")
    log.info(f"æ™‚é–“å¸¯æ•°: {len(df_for_calc.index)}")
    
    # æ±ç”¨è¨ˆç®—ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    calculator = UniversalNeedCalculator()
    
    # Needè¨ˆç®—å®Ÿè¡Œ
    need_df = calculator.calculate_dynamic_need(
        historical_data_df=df_for_calc,
        target_date_columns=date_columns,
        confidence_target=confidence_target
    )
    
    # è¨ˆç®—ã‚µãƒãƒªãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    summary = calculator.get_calculation_summary()
    log.info("=== Needè¨ˆç®—ã‚µãƒãƒªãƒ¼ ===")
    log.info(f"ç·è¨ˆç®—æ•°: {summary.get('total_calculations', 0)}")
    log.info(f"å¹³å‡ä¿¡é ¼åº¦: {summary.get('average_confidence', 0):.3f}")
    log.info(f"æ‰‹æ³•åˆ†å¸ƒ: {summary.get('method_distribution', {})}")
    
    low_confidence = summary.get('low_confidence_slots', [])
    if low_confidence:
        log.warning(f"ä½ä¿¡é ¼åº¦æ™‚é–“å¸¯ (<0.6): {low_confidence}")
    
    return need_df


# å¾“æ¥ã®é–¢æ•°ã‚’æ±ç”¨ç‰ˆã§ç½®ãæ›ãˆ
def calculate_pattern_based_need(
    df_for_calc: pd.DataFrame,
    ref_start_date: dt.date,
    ref_end_date: dt.date,
    statistic_method: str = "ä¸­å¤®å€¤",  # ğŸ”§ CRITICAL FIX: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä¸­å¤®å€¤ã«å¤‰æ›´
    **kwargs
) -> pd.DataFrame:
    """
    å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
    å†…éƒ¨ã§ã¯æ±ç”¨Needè¨ˆç®—ã‚’ä½¿ç”¨
    """
    log.info("[COMPATIBILITY] æ±ç”¨Needè¨ˆç®—ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # æ—¥ä»˜åˆ—ã‚’æŠ½å‡º
    date_columns = [
        col for col in df_for_calc.columns
        if isinstance(col, (dt.date, dt.datetime)) or 
        (isinstance(col, str) and _parse_as_date(col))
    ]
    
    # æœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered_columns = []
    for col in date_columns:
        if isinstance(col, str):
            col_date = _parse_as_date(col)
        else:
            col_date = col if isinstance(col, dt.date) else col.date()
        
        if col_date and ref_start_date <= col_date <= ref_end_date:
            filtered_columns.append(col)
    
    log.info(f"å¯¾è±¡æœŸé–“ ({ref_start_date} - {ref_end_date}): {len(filtered_columns)}æ—¥")
    
    # æ±ç”¨Needè¨ˆç®—ã‚’å®Ÿè¡Œ
    return calculate_universal_need(
        df_for_calc=df_for_calc,
        date_columns=filtered_columns,
        confidence_target=0.75
    )