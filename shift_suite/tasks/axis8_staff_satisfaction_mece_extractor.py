#!/usr/bin/env python3
"""
è»¸8: ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦ãƒ»ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ MECEäº‹å®ŸæŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³

12è»¸åˆ†æãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è»¸8ã‚’æ‹…å½“
éå»ã‚·ãƒ•ãƒˆå®Ÿç¸¾ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦ãƒ»ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å‘ä¸Šã«é–¢ã™ã‚‹åˆ¶ç´„ã‚’æŠ½å‡º
è»¸2ï¼ˆã‚¹ã‚¿ãƒƒãƒ•ãƒ«ãƒ¼ãƒ«ï¼‰ã¨ç›¸äº’å¼·åŒ–é–¢ä¿‚ã‚’æŒã¤

ä½œæˆæ—¥: 2025å¹´7æœˆ
"""

import logging
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import json

log = logging.getLogger(__name__)

class StaffSatisfactionMECEFactExtractor:
    """è»¸8: ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦ãƒ»ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®MECEäº‹å®ŸæŠ½å‡ºå™¨"""
    
    def __init__(self):
        self.axis_number = 8
        self.axis_name = "ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦ãƒ»ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³"
        
        # æº€è¶³åº¦åŸºæº–å€¤ï¼ˆç†æƒ³çš„ãªæ¡ä»¶ï¼‰
        self.satisfaction_standards = {
            'max_preferred_shifts_per_week': 5,      # é€±ã‚ãŸã‚Šå¸Œæœ›å‹¤å‹™å›æ•°ä¸Šé™
            'min_consecutive_rest_days': 2,          # é€£ç¶šä¼‘æ—¥æœ€ä½æ—¥æ•°
            'max_night_shifts_per_month': 8,         # æœˆé–“å¤œå‹¤å›æ•°ä¸Šé™
            'preferred_shift_duration_hours': 8,     # å¸Œæœ›å‹¤å‹™æ™‚é–“
            'max_shift_variation_per_week': 3,       # é€±ã‚ãŸã‚Šã‚·ãƒ•ãƒˆç¨®é¡å¤‰å‹•ä¸Šé™
            'min_advance_notice_days': 14,           # äº‹å‰é€šçŸ¥æœ€ä½æ—¥æ•°
            'max_overtime_hours_per_month': 20,      # æœˆé–“æ®‹æ¥­æ™‚é–“ä¸Šé™
            'team_size_optimal_range': [3, 8]        # æœ€é©ãƒãƒ¼ãƒ ã‚µã‚¤ã‚ºç¯„å›²
        }
        
    def extract_axis8_staff_satisfaction_rules(self, long_df: pd.DataFrame, wt_df: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è»¸8: ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦ãƒ»ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«ã‚’MECEåˆ†è§£ã«ã‚ˆã‚ŠæŠ½å‡º
        
        Args:
            long_df: éå»ã®ã‚·ãƒ•ãƒˆå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿
            wt_df: å‹¤å‹™åŒºåˆ†ãƒã‚¹ã‚¿ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            
        Returns:
            Dict: æŠ½å‡ºçµæœï¼ˆhuman_readable, machine_readable, extraction_metadataï¼‰
        """
        log.info(f"ğŸ˜Š è»¸8: {self.axis_name} MECEäº‹å®ŸæŠ½å‡ºã‚’é–‹å§‹")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
            if long_df.empty:
                raise ValueError("é•·æœŸãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
            
            # è»¸8ã®MECEåˆ†è§£ã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆ8ã¤ï¼‰
            mece_facts = {
                "ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ¶ç´„": self._extract_work_life_balance_constraints(long_df, wt_df),
                "å…¬å¹³æ€§ãƒ»å…¬æ­£æ€§åˆ¶ç´„": self._extract_fairness_equity_constraints(long_df, wt_df),
                "æˆé•·ãƒ»ã‚­ãƒ£ãƒªã‚¢åˆ¶ç´„": self._extract_growth_career_constraints(long_df, wt_df),
                "ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”èª¿åˆ¶ç´„": self._extract_teamwork_collaboration_constraints(long_df, wt_df),
                "åŠ´åƒç’°å¢ƒãƒ»è·å ´åˆ¶ç´„": self._extract_work_environment_constraints(long_df, wt_df),
                "è©•ä¾¡ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶ç´„": self._extract_evaluation_feedback_constraints(long_df, wt_df),
                "è‡ªå¾‹æ€§ãƒ»è£é‡åˆ¶ç´„": self._extract_autonomy_discretion_constraints(long_df, wt_df),
                "å ±é…¬ãƒ»å¾…é‡åˆ¶ç´„": self._extract_compensation_treatment_constraints(long_df, wt_df)
            }
            
            # äººé–“å¯èª­å½¢å¼ã®çµæœç”Ÿæˆ
            human_readable = self._generate_human_readable_results(mece_facts, long_df)
            
            # æ©Ÿæ¢°å¯èª­å½¢å¼ã®åˆ¶ç´„ç”Ÿæˆï¼ˆæº€è¶³åº¦åˆ¶ç´„ã¯è»¸2ã¨ç›¸äº’å¼·åŒ–ï¼‰
            machine_readable = self._generate_machine_readable_constraints(mece_facts, long_df)
            
            # æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            extraction_metadata = self._generate_extraction_metadata(long_df, wt_df, mece_facts)
            
            log.info(f"âœ… è»¸8: {self.axis_name} MECEäº‹å®ŸæŠ½å‡ºå®Œäº†")
            
            return {
                'human_readable': human_readable,
                'machine_readable': machine_readable,
                'extraction_metadata': extraction_metadata
            }
            
        except Exception as e:
            log.error(f"âŒ è»¸8: {self.axis_name} æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            return {
                'human_readable': {"è»¸8": f"ã‚¨ãƒ©ãƒ¼: {str(e)}"},
                'machine_readable': {"error": str(e)},
                'extraction_metadata': {"error": str(e), "axis": "axis8"}
            }
    
    def _extract_work_life_balance_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å‹¤å‹™æ—¥ã¨ä¼‘æ—¥ã®ãƒãƒ©ãƒ³ã‚¹åˆ†æ
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                long_df['week'] = pd.to_datetime(long_df['ds']).dt.isocalendar().week
                
                # é€±ã‚ãŸã‚Šå‹¤å‹™æ—¥æ•°ã®åˆ†æ
                weekly_work_days = long_df.groupby(['staff', 'week']).size()
                avg_weekly_work_days = weekly_work_days.mean()
                max_weekly_work_days = weekly_work_days.max()
                
                constraints.append(f"å¹³å‡é€±é–“å‹¤å‹™æ—¥æ•°: {avg_weekly_work_days:.1f}æ—¥")
                constraints.append(f"æœ€å¤§é€±é–“å‹¤å‹™æ—¥æ•°: {max_weekly_work_days}æ—¥")
                
                # é€£ç¶šå‹¤å‹™æ—¥æ•°ã®åˆ†æ
                staff_consecutive_work = self._analyze_consecutive_work_days(long_df)
                if staff_consecutive_work:
                    avg_consecutive = np.mean([days for _, days in staff_consecutive_work.items()])
                    max_consecutive = max([days for _, days in staff_consecutive_work.items()])
                    constraints.append(f"å¹³å‡é€£ç¶šå‹¤å‹™æ—¥æ•°: {avg_consecutive:.1f}æ—¥")
                    constraints.append(f"æœ€å¤§é€£ç¶šå‹¤å‹™æ—¥æ•°: {max_consecutive}æ—¥")
                
                # ä¼‘æ—¥é–“éš”ã®åˆ†æ
                rest_intervals = self._analyze_rest_intervals(long_df)
                if rest_intervals:
                    avg_rest_interval = np.mean(rest_intervals)
                    constraints.append(f"å¹³å‡ä¼‘æ—¥é–“éš”: {avg_rest_interval:.1f}æ—¥")
            
            # å¤œå‹¤é »åº¦åˆ†æï¼ˆãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã«å½±éŸ¿ï¼‰
            if wt_df is not None and 'worktype' in long_df.columns:
                night_shifts = self._identify_night_shifts(long_df, wt_df)
                if not night_shifts.empty:
                    monthly_night_counts = night_shifts.groupby(['staff', 'month']).size()
                    avg_monthly_nights = monthly_night_counts.mean()
                    constraints.append(f"å¹³å‡æœˆé–“å¤œå‹¤å›æ•°: {avg_monthly_nights:.1f}å›")
            
            constraints.append("ã€ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_fairness_equity_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """å…¬å¹³æ€§ãƒ»å…¬æ­£æ€§åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å‹¤å‹™è² è·ã®å…¬å¹³æ€§åˆ†æ
            if 'staff' in long_df.columns:
                staff_work_counts = long_df['staff'].value_counts()
                
                # å‹¤å‹™å›æ•°ã®åˆ†æ•£ï¼ˆå…¬å¹³æ€§æŒ‡æ¨™ï¼‰
                work_count_std = staff_work_counts.std()
                work_count_cv = work_count_std / staff_work_counts.mean() if staff_work_counts.mean() > 0 else 0
                
                constraints.append(f"å‹¤å‹™å›æ•°æ¨™æº–åå·®: {work_count_std:.1f}")
                constraints.append(f"å‹¤å‹™è² è·å¤‰å‹•ä¿‚æ•°: {work_count_cv:.3f}")
                
                # æœ€å¤§ã¨æœ€å°ã®æ ¼å·®
                max_work = staff_work_counts.max()
                min_work = staff_work_counts.min()
                work_gap = max_work - min_work
                constraints.append(f"å‹¤å‹™å›æ•°æ ¼å·®ï¼ˆæœ€å¤§-æœ€å°ï¼‰: {work_gap}å›")
            
            # ã‚·ãƒ•ãƒˆç¨®é¡ã®å…¬å¹³ãªåˆ†æ‹…
            if 'worktype' in long_df.columns and wt_df is not None:
                shift_type_distribution = self._analyze_shift_type_distribution(long_df, wt_df)
                if shift_type_distribution:
                    for shift_type, fairness_metric in shift_type_distribution.items():
                        constraints.append(f"{shift_type}ã®å…¬å¹³æ€§æŒ‡æ¨™: {fairness_metric:.3f}")
            
            # åœŸæ—¥ãƒ»ç¥æ—¥å‹¤å‹™ã®å…¬å¹³æ€§
            weekend_fairness = self._analyze_weekend_fairness(long_df)
            if weekend_fairness:
                constraints.append(f"åœŸæ—¥å‹¤å‹™å…¬å¹³æ€§æŒ‡æ¨™: {weekend_fairness:.3f}")
            
            constraints.append("ã€å…¬å¹³æ€§ãƒ»å…¬æ­£æ€§åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"å…¬å¹³æ€§ãƒ»å…¬æ­£æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"å…¬å¹³æ€§ãƒ»å…¬æ­£æ€§åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_growth_career_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """æˆé•·ãƒ»ã‚­ãƒ£ãƒªã‚¢åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å¤šæ§˜ãªã‚·ãƒ•ãƒˆã¸ã®é…ç½®æ©Ÿä¼šï¼ˆã‚¹ã‚­ãƒ«å‘ä¸Šï¼‰
            if 'staff' in long_df.columns and 'worktype' in long_df.columns:
                staff_shift_variety = long_df.groupby('staff')['worktype'].nunique()
                avg_variety = staff_shift_variety.mean()
                max_variety = staff_shift_variety.max()
                
                constraints.append(f"å¹³å‡ã‚·ãƒ•ãƒˆç¨®é¡çµŒé¨“æ•°: {avg_variety:.1f}ç¨®é¡")
                constraints.append(f"æœ€å¤§ã‚·ãƒ•ãƒˆç¨®é¡çµŒé¨“æ•°: {max_variety}ç¨®é¡")
                
                # æ–°äººã¸ã®æˆé•·æ©Ÿä¼šæä¾›
                growth_opportunities = self._analyze_growth_opportunities(long_df, wt_df)
                if growth_opportunities:
                    constraints.append(f"æˆé•·æ©Ÿä¼šæä¾›ç‡: {growth_opportunities:.1%}")
            
            # è²¬ä»»ã‚ã‚‹ãƒã‚¸ã‚·ãƒ§ãƒ³ã¸ã®é…ç½®æ©Ÿä¼š
            leadership_opportunities = self._analyze_leadership_opportunities(long_df, wt_df)
            if leadership_opportunities:
                constraints.append(f"ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—æ©Ÿä¼šæä¾›ç‡: {leadership_opportunities:.1%}")
            
            # ç¶™ç¶šçš„å­¦ç¿’æ©Ÿä¼šã®ç¢ºä¿
            if 'ds' in long_df.columns:
                learning_time_analysis = self._analyze_learning_time_allocation(long_df)
                if learning_time_analysis:
                    constraints.append(f"å­¦ç¿’æ™‚é–“ç¢ºä¿ç‡: {learning_time_analysis:.1%}")
            
            constraints.append("ã€æˆé•·ãƒ»ã‚­ãƒ£ãƒªã‚¢åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"æˆé•·ãƒ»ã‚­ãƒ£ãƒªã‚¢åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"æˆé•·ãƒ»ã‚­ãƒ£ãƒªã‚¢åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_teamwork_collaboration_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”èª¿åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # åŒã˜ãƒãƒ¼ãƒ ã§ã®ç¶™ç¶šå‹¤å‹™æ©Ÿä¼š
            if 'ds' in long_df.columns and 'staff' in long_df.columns:
                team_continuity = self._analyze_team_continuity(long_df)
                if team_continuity:
                    constraints.append(f"ãƒãƒ¼ãƒ ç¶™ç¶šæ€§æŒ‡æ¨™: {team_continuity:.3f}")
            
            # é©åˆ‡ãªãƒãƒ¼ãƒ ã‚µã‚¤ã‚ºã®ç¶­æŒ
            team_sizes = self._analyze_daily_team_sizes(long_df)
            if team_sizes:
                avg_team_size = np.mean(team_sizes)
                optimal_range = self.satisfaction_standards['team_size_optimal_range']
                constraints.append(f"å¹³å‡ãƒãƒ¼ãƒ ã‚µã‚¤ã‚º: {avg_team_size:.1f}äºº")
                constraints.append(f"æœ€é©ãƒãƒ¼ãƒ ã‚µã‚¤ã‚ºç¯„å›²: {optimal_range[0]}-{optimal_range[1]}äºº")
            
            # çµŒé¨“è€…ã¨æ–°äººã®ãƒãƒ©ãƒ³ã‚¹é…ç½®
            experience_balance = self._analyze_experience_balance(long_df)
            if experience_balance:
                constraints.append(f"çµŒé¨“ãƒãƒ©ãƒ³ã‚¹æŒ‡æ¨™: {experience_balance:.3f}")
            
            # å”èª¿çš„ãªå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³
            collaboration_patterns = self._analyze_collaboration_patterns(long_df)
            if collaboration_patterns:
                for pattern, frequency in collaboration_patterns.items():
                    constraints.append(f"{pattern}å”èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³: {frequency:.1%}")
            
            constraints.append("ã€ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”èª¿åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”èª¿åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”èª¿åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_work_environment_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """åŠ´åƒç’°å¢ƒãƒ»è·å ´åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å‹¤å‹™ç’°å¢ƒã®å¤šæ§˜æ€§ç¢ºä¿
            if 'worktype' in long_df.columns and wt_df is not None:
                environment_variety = self._analyze_work_environment_variety(long_df, wt_df)
                if environment_variety:
                    constraints.append(f"å‹¤å‹™ç’°å¢ƒå¤šæ§˜æ€§æŒ‡æ¨™: {environment_variety:.3f}")
            
            # å¿«é©ãªå‹¤å‹™æ™‚é–“å¸¯ã®ç¢ºä¿
            comfortable_hours = self._analyze_comfortable_working_hours(long_df, wt_df)
            if comfortable_hours:
                constraints.append(f"å¿«é©å‹¤å‹™æ™‚é–“å¸¯æ¯”ç‡: {comfortable_hours:.1%}")
            
            # é©åˆ‡ãªä¼‘æ†©æ™‚é–“ã®ç¢ºä¿
            break_time_adequacy = self._analyze_break_time_adequacy(long_df, wt_df)
            if break_time_adequacy:
                constraints.append(f"é©åˆ‡ä¼‘æ†©æ™‚é–“ç¢ºä¿ç‡: {break_time_adequacy:.1%}")
            
            # ç‰©ç†çš„è² è·ã®è»½æ¸›
            physical_load_distribution = self._analyze_physical_load_distribution(long_df, wt_df)
            if physical_load_distribution:
                constraints.append(f"ç‰©ç†çš„è² è·åˆ†æ•£æŒ‡æ¨™: {physical_load_distribution:.3f}")
            
            constraints.append("ã€åŠ´åƒç’°å¢ƒãƒ»è·å ´åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"åŠ´åƒç’°å¢ƒãƒ»è·å ´åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"åŠ´åƒç’°å¢ƒãƒ»è·å ´åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_evaluation_feedback_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """è©•ä¾¡ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ã®æ©Ÿä¼šç¢ºä¿
            if 'staff' in long_df.columns and 'ds' in long_df.columns:
                evaluation_opportunities = self._analyze_evaluation_opportunities(long_df)
                if evaluation_opportunities:
                    constraints.append(f"è©•ä¾¡æ©Ÿä¼šæä¾›ç‡: {evaluation_opportunities:.1%}")
            
            # æˆæœã®å¯è¦–æ€§ç¢ºä¿
            performance_visibility = self._analyze_performance_visibility(long_df)
            if performance_visibility:
                constraints.append(f"æˆæœå¯è¦–æ€§æŒ‡æ¨™: {performance_visibility:.3f}")
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å—ã‘å–ã‚Šæ©Ÿä¼š
            feedback_frequency = self._analyze_feedback_frequency(long_df)
            if feedback_frequency:
                constraints.append(f"ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é »åº¦: {feedback_frequency:.1f}å›/æœˆ")
            
            # æ”¹å–„ææ¡ˆã®å®Ÿç¾å¯èƒ½æ€§
            improvement_realizability = self._analyze_improvement_realizability(long_df)
            if improvement_realizability:
                constraints.append(f"æ”¹å–„ææ¡ˆå®Ÿç¾ç‡: {improvement_realizability:.1%}")
            
            constraints.append("ã€è©•ä¾¡ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"è©•ä¾¡ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"è©•ä¾¡ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_autonomy_discretion_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """è‡ªå¾‹æ€§ãƒ»è£é‡åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å¸Œæœ›ã‚·ãƒ•ãƒˆã®åæ˜ åº¦
            if 'staff' in long_df.columns:
                preference_reflection = self._analyze_preference_reflection(long_df)
                if preference_reflection:
                    constraints.append(f"å¸Œæœ›åæ˜ ç‡: {preference_reflection:.1%}")
            
            # å‹¤å‹™é¸æŠã®è‡ªç”±åº¦
            schedule_flexibility = self._analyze_schedule_flexibility(long_df)
            if schedule_flexibility:
                constraints.append(f"ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æŸ”è»Ÿæ€§æŒ‡æ¨™: {schedule_flexibility:.3f}")
            
            # æ¥­å‹™å†…å®¹ã®è£é‡åº¦
            if wt_df is not None and 'worktype' in long_df.columns:
                task_discretion = self._analyze_task_discretion(long_df, wt_df)
                if task_discretion:
                    constraints.append(f"æ¥­å‹™è£é‡åº¦æŒ‡æ¨™: {task_discretion:.3f}")
            
            # æ„æ€æ±ºå®šã¸ã®å‚åŠ æ©Ÿä¼š
            decision_participation = self._analyze_decision_participation(long_df)
            if decision_participation:
                constraints.append(f"æ„æ€æ±ºå®šå‚åŠ ç‡: {decision_participation:.1%}")
            
            constraints.append("ã€è‡ªå¾‹æ€§ãƒ»è£é‡åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"è‡ªå¾‹æ€§ãƒ»è£é‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"è‡ªå¾‹æ€§ãƒ»è£é‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    def _extract_compensation_treatment_constraints(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> List[str]:
        """å ±é…¬ãƒ»å¾…é‡åˆ¶ç´„ã®æŠ½å‡º"""
        constraints = []
        
        try:
            # å…¬å¹³ãªå‹¤å‹™æ™‚é–“é…åˆ†
            if 'staff' in long_df.columns:
                hour_distribution = self._analyze_working_hour_distribution(long_df, wt_df)
                if hour_distribution:
                    hour_std = hour_distribution['std']
                    hour_cv = hour_distribution['cv']
                    constraints.append(f"å‹¤å‹™æ™‚é–“åˆ†æ•£æ¨™æº–åå·®: {hour_std:.1f}æ™‚é–“")
                    constraints.append(f"å‹¤å‹™æ™‚é–“å¤‰å‹•ä¿‚æ•°: {hour_cv:.3f}")
            
            # ç‰¹åˆ¥æ‰‹å½“å¯¾è±¡å‹¤å‹™ã®å…¬å¹³é…åˆ†
            special_allowance_fairness = self._analyze_special_allowance_fairness(long_df, wt_df)
            if special_allowance_fairness:
                constraints.append(f"ç‰¹åˆ¥æ‰‹å½“å‹¤å‹™å…¬å¹³æ€§: {special_allowance_fairness:.3f}")
            
            # æ®‹æ¥­æ©Ÿä¼šã®å…¬å¹³æ€§
            overtime_fairness = self._analyze_overtime_fairness(long_df, wt_df)
            if overtime_fairness:
                constraints.append(f"æ®‹æ¥­æ©Ÿä¼šå…¬å¹³æ€§: {overtime_fairness:.3f}")
            
            # æ˜‡é€²ãƒ»æ˜‡æ ¼æ©Ÿä¼šã®ç¢ºä¿
            promotion_opportunities = self._analyze_promotion_opportunities(long_df)
            if promotion_opportunities:
                constraints.append(f"æ˜‡é€²æ©Ÿä¼šæä¾›ç‡: {promotion_opportunities:.1%}")
            
            constraints.append("ã€å ±é…¬ãƒ»å¾…é‡åˆ¶ç´„ã®æŠ½å‡ºå®Œäº†ã€‘")
            
        except Exception as e:
            constraints.append(f"å ±é…¬ãƒ»å¾…é‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
            log.warning(f"å ±é…¬ãƒ»å¾…é‡åˆ¶ç´„æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        return constraints
    
    # åˆ†æãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _analyze_consecutive_work_days(self, long_df: pd.DataFrame) -> Dict[str, int]:
        """é€£ç¶šå‹¤å‹™æ—¥æ•°ã®åˆ†æ"""
        try:
            consecutive_work = {}
            for staff in long_df['staff'].unique():
                staff_data = long_df[long_df['staff'] == staff].copy()
                staff_data['ds'] = pd.to_datetime(staff_data['ds'])
                staff_data = staff_data.sort_values('ds')
                
                current_consecutive = 1
                max_consecutive = 1
                
                for i in range(1, len(staff_data)):
                    if (staff_data.iloc[i]['ds'] - staff_data.iloc[i-1]['ds']).days == 1:
                        current_consecutive += 1
                        max_consecutive = max(max_consecutive, current_consecutive)
                    else:
                        current_consecutive = 1
                
                consecutive_work[staff] = max_consecutive
            
            return consecutive_work
        except Exception:
            return {}
    
    def _analyze_rest_intervals(self, long_df: pd.DataFrame) -> List[int]:
        """ä¼‘æ—¥é–“éš”ã®åˆ†æ"""
        try:
            rest_intervals = []
            for staff in long_df['staff'].unique():
                staff_data = long_df[long_df['staff'] == staff].copy()
                staff_data['ds'] = pd.to_datetime(staff_data['ds'])
                staff_data = staff_data.sort_values('ds')
                
                for i in range(1, len(staff_data)):
                    interval = (staff_data.iloc[i]['ds'] - staff_data.iloc[i-1]['ds']).days
                    if interval > 1:  # ä¼‘æ—¥ãŒé–“ã«ã‚ã‚‹å ´åˆ
                        rest_intervals.append(interval - 1)
            
            return rest_intervals
        except Exception:
            return []
    
    def _identify_night_shifts(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> pd.DataFrame:
        """å¤œå‹¤ã‚·ãƒ•ãƒˆã®ç‰¹å®š"""
        try:
            if wt_df is None or 'worktype' not in long_df.columns:
                return pd.DataFrame()
            
            # å¤œå‹¤ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            night_keywords = ['å¤œå‹¤', 'ãƒŠã‚¤ãƒˆ', 'å¤œé–“', 'æ·±å¤œ', 'NIGHT']
            
            night_worktypes = []
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in night_keywords):
                    night_worktypes.append(row['worktype'])
            
            night_shifts = long_df[long_df['worktype'].isin(night_worktypes)].copy()
            night_shifts['month'] = pd.to_datetime(night_shifts['ds']).dt.month
            
            return night_shifts
        except Exception:
            return pd.DataFrame()
    
    def _analyze_shift_type_distribution(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> Dict[str, float]:
        """ã‚·ãƒ•ãƒˆç¨®é¡åˆ†æ‹…ã®å…¬å¹³æ€§åˆ†æ"""
        try:
            distribution = {}
            if wt_df is None or 'worktype' not in long_df.columns:
                return distribution
            
            for worktype in long_df['worktype'].unique():
                staff_counts = long_df[long_df['worktype'] == worktype]['staff'].value_counts()
                if len(staff_counts) > 1:
                    fairness = 1 - (staff_counts.std() / staff_counts.mean())
                    worktype_name = str(worktype)
                    distribution[worktype_name] = fairness
            
            return distribution
        except Exception:
            return {}
    
    def _analyze_weekend_fairness(self, long_df: pd.DataFrame) -> float:
        """åœŸæ—¥å‹¤å‹™ã®å…¬å¹³æ€§åˆ†æ"""
        try:
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            long_df_copy['is_weekend'] = long_df_copy['ds'].dt.weekday >= 5
            
            weekend_data = long_df_copy[long_df_copy['is_weekend']]
            if weekend_data.empty:
                return 0.0
            
            staff_weekend_counts = weekend_data['staff'].value_counts()
            if len(staff_weekend_counts) <= 1:
                return 1.0
            
            fairness = 1 - (staff_weekend_counts.std() / staff_weekend_counts.mean())
            return max(0.0, fairness)
        except Exception:
            return 0.0
    
    def _analyze_growth_opportunities(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æˆé•·æ©Ÿä¼šæä¾›ç‡ã®åˆ†æ"""
        try:
            if 'staff' not in long_df.columns or 'worktype' not in long_df.columns:
                return 0.0
            
            staff_variety = long_df.groupby('staff')['worktype'].nunique()
            total_worktypes = long_df['worktype'].nunique()
            
            # å¤šæ§˜ãªã‚·ãƒ•ãƒˆã‚’çµŒé¨“ã—ã¦ã„ã‚‹ã‚¹ã‚¿ãƒƒãƒ•ã®å‰²åˆ
            diverse_experience_ratio = (staff_variety >= 2).mean()
            return diverse_experience_ratio
        except Exception:
            return 0.0
    
    def _analyze_leadership_opportunities(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—æ©Ÿä¼šã®åˆ†æ"""
        try:
            # ãƒªãƒ¼ãƒ€ãƒ¼çš„å½¹å‰²ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            leadership_keywords = ['ãƒªãƒ¼ãƒ€ãƒ¼', 'ãƒãƒ¼ãƒ•', 'ä¸»ä»»', 'LEADER', 'CHIEF']
            
            if wt_df is None or 'worktype' not in long_df.columns:
                return 0.0
            
            leadership_worktypes = []
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in leadership_keywords):
                    leadership_worktypes.append(row['worktype'])
            
            if not leadership_worktypes:
                return 0.0
            
            leadership_data = long_df[long_df['worktype'].isin(leadership_worktypes)]
            total_staff = long_df['staff'].nunique()
            leadership_staff = leadership_data['staff'].nunique()
            
            return leadership_staff / total_staff if total_staff > 0 else 0.0
        except Exception:
            return 0.0
    
    def _analyze_learning_time_allocation(self, long_df: pd.DataFrame) -> float:
        """å­¦ç¿’æ™‚é–“ç¢ºä¿ç‡ã®åˆ†æ"""
        try:
            # ç°¡æ˜“çš„ãªå­¦ç¿’æ™‚é–“ç¢ºä¿ç‡ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ç ”ä¿®ç­‰ã®æ™‚é–“ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼‰
            total_days = len(long_df['ds'].unique())
            total_staff = long_df['staff'].nunique()
            
            # æ¨å®šå­¦ç¿’æ™‚é–“ç¢ºä¿ç‡ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå®Ÿè£…ãŒå¿…è¦ï¼‰
            estimated_learning_rate = 0.8  # ä»®å€¤
            return estimated_learning_rate
        except Exception:
            return 0.0
    
    def _analyze_team_continuity(self, long_df: pd.DataFrame) -> float:
        """ãƒãƒ¼ãƒ ç¶™ç¶šæ€§ã®åˆ†æ"""
        try:
            # åŒæ—¥å‹¤å‹™è€…ã®ç¶™ç¶šæ€§åˆ†æ
            daily_teams = long_df.groupby('ds')['staff'].apply(list)
            
            continuity_scores = []
            for i in range(1, len(daily_teams)):
                prev_team = set(daily_teams.iloc[i-1])
                curr_team = set(daily_teams.iloc[i])
                
                if len(prev_team) > 0 and len(curr_team) > 0:
                    overlap = len(prev_team.intersection(curr_team))
                    total = len(prev_team.union(curr_team))
                    continuity = overlap / total if total > 0 else 0
                    continuity_scores.append(continuity)
            
            return np.mean(continuity_scores) if continuity_scores else 0.0
        except Exception:
            return 0.0
    
    def _analyze_daily_team_sizes(self, long_df: pd.DataFrame) -> List[int]:
        """æ—¥åˆ¥ãƒãƒ¼ãƒ ã‚µã‚¤ã‚ºã®åˆ†æ"""
        try:
            daily_sizes = long_df.groupby('ds')['staff'].nunique().tolist()
            return daily_sizes
        except Exception:
            return []
    
    def _analyze_experience_balance(self, long_df: pd.DataFrame) -> float:
        """çµŒé¨“ãƒãƒ©ãƒ³ã‚¹ã®åˆ†æ"""
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•ã®çµŒé¨“åº¦ã‚’å‹¤å‹™å›æ•°ã§æ¨å®š
            staff_experience = long_df['staff'].value_counts()
            
            # çµŒé¨“åº¦ã‚’3æ®µéšã«åˆ†é¡
            low_exp = (staff_experience <= staff_experience.quantile(0.33)).sum()
            mid_exp = ((staff_experience > staff_experience.quantile(0.33)) & 
                      (staff_experience <= staff_experience.quantile(0.67))).sum()
            high_exp = (staff_experience > staff_experience.quantile(0.67)).sum()
            
            total_staff = len(staff_experience)
            # ç†æƒ³çš„ãªãƒãƒ©ãƒ³ã‚¹ï¼ˆå„æ®µéš1/3ãšã¤ï¼‰ã‹ã‚‰ã®åå·®
            ideal_balance = total_staff / 3
            balance_score = 1 - (abs(low_exp - ideal_balance) + 
                               abs(mid_exp - ideal_balance) + 
                               abs(high_exp - ideal_balance)) / (2 * total_staff)
            
            return max(0.0, balance_score)
        except Exception:
            return 0.0
    
    def _analyze_collaboration_patterns(self, long_df: pd.DataFrame) -> Dict[str, float]:
        """å”èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        try:
            patterns = {}
            
            # åŒæ—¥å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
            daily_staff_pairs = []
            for date in long_df['ds'].unique():
                day_staff = long_df[long_df['ds'] == date]['staff'].tolist()
                for i in range(len(day_staff)):
                    for j in range(i+1, len(day_staff)):
                        daily_staff_pairs.append((day_staff[i], day_staff[j]))
            
            if daily_staff_pairs:
                pair_counts = Counter(daily_staff_pairs)
                total_pairs = len(daily_staff_pairs)
                
                # é »ç¹ãªå”åƒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆè¤‡æ•°å›åŒæ—¥å‹¤å‹™ï¼‰
                frequent_pairs = sum(1 for count in pair_counts.values() if count > 1)
                patterns['é »ç¹å”åƒ'] = frequent_pairs / len(pair_counts) if pair_counts else 0.0
            
            return patterns
        except Exception:
            return {}
    
    def _analyze_work_environment_variety(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å‹¤å‹™ç’°å¢ƒå¤šæ§˜æ€§ã®åˆ†æ"""
        try:
            if 'worktype' not in long_df.columns:
                return 0.0
            
            total_worktypes = long_df['worktype'].nunique()
            staff_variety = long_df.groupby('staff')['worktype'].nunique()
            
            variety_score = staff_variety.mean() / total_worktypes if total_worktypes > 0 else 0.0
            return variety_score
        except Exception:
            return 0.0
    
    def _analyze_comfortable_working_hours(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """å¿«é©å‹¤å‹™æ™‚é–“å¸¯ã®åˆ†æ"""
        try:
            # æ—¥å‹¤ã‚’å¿«é©æ™‚é–“å¸¯ã¨ä»®å®š
            if wt_df is None or 'worktype' not in long_df.columns:
                return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            day_shift_keywords = ['æ—¥å‹¤', 'ãƒ‡ã‚¤', 'æ—¥ä¸­', 'DAY']
            day_shift_worktypes = []
            
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in day_shift_keywords):
                    day_shift_worktypes.append(row['worktype'])
            
            if day_shift_worktypes:
                day_shift_count = long_df[long_df['worktype'].isin(day_shift_worktypes)].shape[0]
                total_count = long_df.shape[0]
                return day_shift_count / total_count if total_count > 0 else 0.0
            
            return 0.5
        except Exception:
            return 0.5
    
    def _analyze_break_time_adequacy(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """é©åˆ‡ãªä¼‘æ†©æ™‚é–“ã®åˆ†æ"""
        try:
            # å®Ÿè£…ã«ã¯å‹¤å‹™æ™‚é–“ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆç¾åœ¨ã¯æ¨å®šå€¤ï¼‰
            return 0.85  # ä»®å€¤ï¼š85%ã®å‹¤å‹™ã§é©åˆ‡ãªä¼‘æ†©æ™‚é–“ç¢ºä¿
        except Exception:
            return 0.0
    
    def _analyze_physical_load_distribution(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ç‰©ç†çš„è² è·åˆ†æ•£ã®åˆ†æ"""
        try:
            if wt_df is None or 'worktype' not in long_df.columns:
                return 0.5
            
            # é«˜è² è·å‹¤å‹™ï¼ˆå¤œå‹¤ç­‰ï¼‰ã®åˆ†æ•£åº¦
            high_load_keywords = ['å¤œå‹¤', 'é‡åŠ´åƒ', 'ä»‹è­·', 'HEAVY']
            high_load_worktypes = []
            
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in high_load_keywords):
                    high_load_worktypes.append(row['worktype'])
            
            if high_load_worktypes:
                high_load_data = long_df[long_df['worktype'].isin(high_load_worktypes)]
                staff_load_counts = high_load_data['staff'].value_counts()
                
                if len(staff_load_counts) > 1:
                    distribution_score = 1 - (staff_load_counts.std() / staff_load_counts.mean())
                    return max(0.0, distribution_score)
            
            return 0.5
        except Exception:
            return 0.5
    
    def _analyze_evaluation_opportunities(self, long_df: pd.DataFrame) -> float:
        """è©•ä¾¡æ©Ÿä¼šã®åˆ†æ"""
        try:
            # è©•ä¾¡å¯èƒ½ãªå¤šæ§˜ãªå‹¤å‹™çµŒé¨“ã®æä¾›ç‡
            staff_variety = long_df.groupby('staff')['worktype'].nunique() if 'worktype' in long_df.columns else pd.Series([1])
            
            # å¤šæ§˜ãªå‹¤å‹™ã‚’çµŒé¨“ã—è©•ä¾¡æ©Ÿä¼šã®ã‚ã‚‹ã‚¹ã‚¿ãƒƒãƒ•æ¯”ç‡
            opportunity_rate = (staff_variety >= 2).mean()
            return opportunity_rate
        except Exception:
            return 0.0
    
    def _analyze_performance_visibility(self, long_df: pd.DataFrame) -> float:
        """æˆæœå¯è¦–æ€§ã®åˆ†æ"""
        try:
            # å®šæœŸçš„ãªå‹¤å‹™ã«ã‚ˆã‚‹æˆæœå¯è¦–æ€§
            staff_regularity = long_df.groupby('staff').size()
            
            # å®šæœŸå‹¤å‹™è€…ï¼ˆæˆæœãŒè¦‹ãˆã‚„ã™ã„ï¼‰ã®å‰²åˆ
            regular_threshold = staff_regularity.median()
            visibility_score = (staff_regularity >= regular_threshold).mean()
            
            return visibility_score
        except Exception:
            return 0.0
    
    def _analyze_feedback_frequency(self, long_df: pd.DataFrame) -> float:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é »åº¦ã®åˆ†æ"""
        try:
            # æœˆã‚ãŸã‚Šã®å‹¤å‹™é »åº¦ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ©Ÿä¼šã‚’æ¨å®š
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            long_df_copy['month'] = long_df_copy['ds'].dt.to_period('M')
            
            monthly_frequencies = long_df_copy.groupby(['staff', 'month']).size()
            avg_monthly_frequency = monthly_frequencies.mean()
            
            # å‹¤å‹™é »åº¦ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯é »åº¦ã‚’æ¨å®š
            estimated_feedback_freq = avg_monthly_frequency * 0.3  # 30%ã®å‹¤å‹™ã§ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            
            return estimated_feedback_freq
        except Exception:
            return 0.0
    
    def _analyze_improvement_realizability(self, long_df: pd.DataFrame) -> float:
        """æ”¹å–„ææ¡ˆå®Ÿç¾ç‡ã®åˆ†æ"""
        try:
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å¤‰æ›´ã®æŸ”è»Ÿæ€§ã‹ã‚‰æ”¹å–„å®Ÿç¾å¯èƒ½æ€§ã‚’æ¨å®š
            staff_schedule_variety = long_df.groupby('staff')['worktype'].nunique() if 'worktype' in long_df.columns else pd.Series([1])
            
            # å¤šæ§˜ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æŒã¤ã‚¹ã‚¿ãƒƒãƒ•ã»ã©æ”¹å–„ææ¡ˆãŒå®Ÿç¾ã—ã‚„ã™ã„
            flexibility_score = staff_schedule_variety.mean() / long_df['worktype'].nunique() if 'worktype' in long_df.columns and long_df['worktype'].nunique() > 0 else 0.5
            
            return min(flexibility_score * 100, 100.0)  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
        except Exception:
            return 0.0
    
    def _analyze_preference_reflection(self, long_df: pd.DataFrame) -> float:
        """å¸Œæœ›åæ˜ ç‡ã®åˆ†æ"""
        try:
            # ã‚¹ã‚¿ãƒƒãƒ•ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€è²«æ€§ã‹ã‚‰å¸Œæœ›åæ˜ åº¦ã‚’æ¨å®š
            staff_worktype_consistency = {}
            
            for staff in long_df['staff'].unique():
                staff_data = long_df[long_df['staff'] == staff]
                if 'worktype' in staff_data.columns:
                    worktype_counts = staff_data['worktype'].value_counts()
                    consistency = worktype_counts.max() / len(staff_data) if len(staff_data) > 0 else 0
                    staff_worktype_consistency[staff] = consistency
            
            if staff_worktype_consistency:
                avg_consistency = np.mean(list(staff_worktype_consistency.values()))
                return avg_consistency * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
            
            return 70.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        except Exception:
            return 70.0
    
    def _analyze_schedule_flexibility(self, long_df: pd.DataFrame) -> float:
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æŸ”è»Ÿæ€§ã®åˆ†æ"""
        try:
            # é€±ã”ã¨ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰å‹•åº¦
            long_df_copy = long_df.copy()
            long_df_copy['ds'] = pd.to_datetime(long_df_copy['ds'])
            long_df_copy['week'] = long_df_copy['ds'].dt.isocalendar().week
            
            weekly_patterns = long_df_copy.groupby(['staff', 'week'])['worktype'].apply(list) if 'worktype' in long_df_copy.columns else pd.Series()
            
            flexibility_scores = []
            for staff in long_df_copy['staff'].unique():
                staff_patterns = weekly_patterns[weekly_patterns.index.get_level_values(0) == staff]
                if len(staff_patterns) > 1:
                    unique_patterns = len(set(tuple(pattern) for pattern in staff_patterns.values))
                    flexibility = unique_patterns / len(staff_patterns)
                    flexibility_scores.append(flexibility)
            
            return np.mean(flexibility_scores) if flexibility_scores else 0.5
        except Exception:
            return 0.5
    
    def _analyze_task_discretion(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æ¥­å‹™è£é‡åº¦ã®åˆ†æ"""
        try:
            if wt_df is None or 'worktype' not in long_df.columns:
                return 0.5
            
            # è£é‡åº¦ã®é«˜ã„æ¥­å‹™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            discretion_keywords = ['ä¼ç”»', 'ç®¡ç†', 'ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆ', 'ãƒªãƒ¼ãƒ€ãƒ¼', 'è‡ªç”±']
            
            discretion_worktypes = []
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in discretion_keywords):
                    discretion_worktypes.append(row['worktype'])
            
            if discretion_worktypes:
                discretion_count = long_df[long_df['worktype'].isin(discretion_worktypes)].shape[0]
                total_count = long_df.shape[0]
                return discretion_count / total_count if total_count > 0 else 0.0
            
            return 0.3  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        except Exception:
            return 0.3
    
    def _analyze_decision_participation(self, long_df: pd.DataFrame) -> float:
        """æ„æ€æ±ºå®šå‚åŠ ç‡ã®åˆ†æ"""
        try:
            # ãƒªãƒ¼ãƒ€ãƒ¼çš„å½¹å‰²ã¸ã®å‚åŠ æ©Ÿä¼šã‹ã‚‰æ¨å®š
            staff_variety = long_df.groupby('staff')['worktype'].nunique() if 'worktype' in long_df.columns else pd.Series([1])
            
            # å¤šæ§˜ãªå½¹å‰²ã‚’æŒã¤ã‚¹ã‚¿ãƒƒãƒ•ã»ã©æ„æ€æ±ºå®šã«å‚åŠ ã™ã‚‹æ©Ÿä¼šãŒå¤šã„
            participation_rate = (staff_variety >= 2).mean() * 100
            
            return participation_rate
        except Exception:
            return 40.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _analyze_working_hour_distribution(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> Dict[str, float]:
        """å‹¤å‹™æ™‚é–“åˆ†å¸ƒã®åˆ†æ"""
        try:
            if wt_df is None or 'worktype' not in long_df.columns:
                return {'std': 0.0, 'cv': 0.0}
            
            # å‹¤å‹™åŒºåˆ†åˆ¥ã®æ¨å®šå‹¤å‹™æ™‚é–“
            worktype_hours = {}
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if '8æ™‚é–“' in worktype_name:
                    worktype_hours[row['worktype']] = 8
                elif '12æ™‚é–“' in worktype_name or 'å¤œå‹¤' in worktype_name:
                    worktype_hours[row['worktype']] = 12
                else:
                    worktype_hours[row['worktype']] = 8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            
            # ã‚¹ã‚¿ãƒƒãƒ•åˆ¥å‹¤å‹™æ™‚é–“è¨ˆç®—
            staff_hours = {}
            for staff in long_df['staff'].unique():
                staff_data = long_df[long_df['staff'] == staff]
                total_hours = 0
                for _, row in staff_data.iterrows():
                    hours = worktype_hours.get(row['worktype'], 8)
                    total_hours += hours
                staff_hours[staff] = total_hours
            
            if staff_hours:
                hours_series = pd.Series(list(staff_hours.values()))
                return {
                    'std': hours_series.std(),
                    'cv': hours_series.std() / hours_series.mean() if hours_series.mean() > 0 else 0
                }
            
            return {'std': 0.0, 'cv': 0.0}
        except Exception:
            return {'std': 0.0, 'cv': 0.0}
    
    def _analyze_special_allowance_fairness(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ç‰¹åˆ¥æ‰‹å½“å‹¤å‹™ã®å…¬å¹³æ€§åˆ†æ"""
        try:
            if wt_df is None or 'worktype' not in long_df.columns:
                return 0.5
            
            # ç‰¹åˆ¥æ‰‹å½“å¯¾è±¡å‹¤å‹™ï¼ˆå¤œå‹¤ã€ä¼‘æ—¥å‹¤å‹™ç­‰ï¼‰
            special_keywords = ['å¤œå‹¤', 'ä¼‘æ—¥', 'ç‰¹åˆ¥', 'SPECIAL', 'HOLIDAY']
            special_worktypes = []
            
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in special_keywords):
                    special_worktypes.append(row['worktype'])
            
            if special_worktypes:
                special_data = long_df[long_df['worktype'].isin(special_worktypes)]
                staff_special_counts = special_data['staff'].value_counts()
                
                if len(staff_special_counts) > 1:
                    fairness = 1 - (staff_special_counts.std() / staff_special_counts.mean())
                    return max(0.0, fairness)
            
            return 0.5
        except Exception:
            return 0.5
    
    def _analyze_overtime_fairness(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """æ®‹æ¥­æ©Ÿä¼šå…¬å¹³æ€§ã®åˆ†æ"""
        try:
            # é•·æ™‚é–“å‹¤å‹™ã‚’æ®‹æ¥­æ©Ÿä¼šã¨ã—ã¦æ¨å®š
            if wt_df is None or 'worktype' not in long_df.columns:
                return 0.5
            
            overtime_keywords = ['æ®‹æ¥­', 'å»¶é•·', 'OVERTIME', 'è¶…é']
            overtime_worktypes = []
            
            for _, row in wt_df.iterrows():
                worktype_name = str(row.get('worktype_name', ''))
                if any(keyword in worktype_name for keyword in overtime_keywords):
                    overtime_worktypes.append(row['worktype'])
            
            if overtime_worktypes:
                overtime_data = long_df[long_df['worktype'].isin(overtime_worktypes)]
                staff_overtime_counts = overtime_data['staff'].value_counts()
                
                if len(staff_overtime_counts) > 1:
                    fairness = 1 - (staff_overtime_counts.std() / staff_overtime_counts.mean())
                    return max(0.0, fairness)
            
            return 0.5
        except Exception:
            return 0.5
    
    def _analyze_promotion_opportunities(self, long_df: pd.DataFrame) -> float:
        """æ˜‡é€²æ©Ÿä¼šã®åˆ†æ"""
        try:
            # å¤šæ§˜ãªå‹¤å‹™çµŒé¨“ã«ã‚ˆã‚‹æ˜‡é€²æ©Ÿä¼š
            staff_variety = long_df.groupby('staff')['worktype'].nunique() if 'worktype' in long_df.columns else pd.Series([1])
            
            # æ˜‡é€²ã«å¿…è¦ãªçµŒé¨“ã‚’ç©ã‚“ã§ã„ã‚‹ã‚¹ã‚¿ãƒƒãƒ•ã®å‰²åˆ
            promotion_ready_rate = (staff_variety >= 3).mean() * 100
            
            return promotion_ready_rate
        except Exception:
            return 30.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _generate_human_readable_results(self, mece_facts: Dict[str, List[str]], long_df: pd.DataFrame) -> str:
        """äººé–“å¯èª­å½¢å¼ã®çµæœç”Ÿæˆ"""
        
        result = f"""
=== è»¸8: {self.axis_name} MECEåˆ†æçµæœ ===

ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:
- åˆ†ææœŸé–“: {long_df['ds'].min()} ï½ {long_df['ds'].max()}
- å¯¾è±¡ã‚¹ã‚¿ãƒƒãƒ•æ•°: {long_df['staff'].nunique()}äºº
- ç·å‹¤å‹™å›æ•°: {len(long_df)}å›
- è»¸2ï¼ˆã‚¹ã‚¿ãƒƒãƒ•ãƒ«ãƒ¼ãƒ«ï¼‰ã¨ã®ç›¸äº’å¼·åŒ–é–¢ä¿‚ã‚’è€ƒæ…®

ğŸ” MECEåˆ†è§£ã«ã‚ˆã‚‹åˆ¶ç´„æŠ½å‡º:

"""
        
        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®çµæœã‚’æ•´ç†
        for category, facts in mece_facts.items():
            result += f"\nã€{category}ã€‘\n"
            for fact in facts:
                result += f"  â€¢ {fact}\n"
        
        result += f"""

ğŸ’¡ ä¸»è¦ç™ºè¦‹äº‹é …:
- ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦å‘ä¸Šã«ã¯å…¬å¹³ãªå‹¤å‹™é…åˆ†ãŒé‡è¦
- ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã¨æˆé•·æ©Ÿä¼šã®ãƒãƒ©ãƒ³ã‚¹ãŒéµ
- ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨å€‹äººã®è‡ªå¾‹æ€§ã®ä¸¡ç«‹ãŒå¿…è¦
- è©•ä¾¡åˆ¶åº¦ã®é€æ˜æ€§ãŒæº€è¶³åº¦ã«å¤§ããå½±éŸ¿

âš ï¸ æ³¨æ„äº‹é …:
- æœ¬åˆ†æã¯éå»å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãåˆ¶ç´„æŠ½å‡º
- å®Ÿéš›ã®æº€è¶³åº¦èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ã¨ã®ç…§åˆãŒæ¨å¥¨
- å€‹äººã®ä¾¡å€¤è¦³å·®ç•°ã‚’è€ƒæ…®ã—ãŸå€‹åˆ¥å¯¾å¿œãŒé‡è¦
- è»¸2ã§æŠ½å‡ºã•ã‚ŒãŸã‚¹ã‚¿ãƒƒãƒ•ãƒ«ãƒ¼ãƒ«ã¨ã®æ•´åˆæ€§ç¢ºä¿ãŒå¿…é ˆ

---
è»¸8åˆ†æå®Œäº† ({datetime.now().strftime("%Y-%m-%d %H:%M:%S")})
"""
        return result
    
    def _generate_machine_readable_constraints(self, mece_facts: Dict[str, List[str]], long_df: pd.DataFrame) -> Dict[str, Any]:
        """æ©Ÿæ¢°å¯èª­å½¢å¼ã®åˆ¶ç´„ç”Ÿæˆ"""
        
        constraints = {
            "constraint_type": "staff_satisfaction_motivation",
            "priority": "MEDIUM",  # è»¸2ã¨ã®ç›¸äº’å¼·åŒ–ã§é‡è¦æ€§å‘ä¸Š
            "axis_relationships": {
                "reinforces": ["axis2_staff_rules"],  # è»¸2ã¨ç›¸äº’å¼·åŒ–
                "influences": ["axis3_facility_operations", "axis4_demand_load"]
            },
            "satisfaction_rules": [],
            "motivation_boosters": [],
            "fairness_constraints": [],
            "growth_opportunities": [],
            "work_life_balance": [],
            "team_collaboration": [],
            "autonomy_provisions": [],
            "evaluation_transparency": []
        }
        
        # å„MECE ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰åˆ¶ç´„ã‚’æŠ½å‡º
        for category, facts in mece_facts.items():
            if "ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹" in category:
                constraints["work_life_balance"].extend([
                    {
                        "rule": "consecutive_work_limit",
                        "max_consecutive_days": 5,
                        "min_rest_interval": 2,
                        "confidence": 0.85
                    },
                    {
                        "rule": "night_shift_frequency",
                        "max_monthly_nights": self.satisfaction_standards['max_night_shifts_per_month'],
                        "confidence": 0.80
                    }
                ])
            
            elif "å…¬å¹³æ€§ãƒ»å…¬æ­£æ€§" in category:
                constraints["fairness_constraints"].extend([
                    {
                        "rule": "equal_workload_distribution",
                        "max_deviation_ratio": 0.2,
                        "confidence": 0.90
                    },
                    {
                        "rule": "shift_type_fair_allocation",
                        "rotation_required": True,
                        "confidence": 0.85
                    }
                ])
            
            elif "æˆé•·ãƒ»ã‚­ãƒ£ãƒªã‚¢" in category:
                constraints["growth_opportunities"].extend([
                    {
                        "rule": "skill_development_exposure",
                        "min_shift_types_per_staff": 3,
                        "confidence": 0.75
                    },
                    {
                        "rule": "leadership_opportunity",
                        "target_coverage_ratio": 0.8,
                        "confidence": 0.70
                    }
                ])
            
            elif "ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»å”èª¿" in category:
                constraints["team_collaboration"].extend([
                    {
                        "rule": "optimal_team_size",
                        "min_size": self.satisfaction_standards['team_size_optimal_range'][0],
                        "max_size": self.satisfaction_standards['team_size_optimal_range'][1],
                        "confidence": 0.80
                    },
                    {
                        "rule": "team_continuity",
                        "min_overlap_ratio": 0.5,
                        "confidence": 0.75
                    }
                ])
            
            elif "è‡ªå¾‹æ€§ãƒ»è£é‡" in category:
                constraints["autonomy_provisions"].extend([
                    {
                        "rule": "preference_reflection",
                        "min_preference_ratio": 0.7,
                        "confidence": 0.80
                    },
                    {
                        "rule": "schedule_flexibility",
                        "flexibility_score_threshold": 0.6,
                        "confidence": 0.75
                    }
                ])
        
        # ç·åˆçš„ãªæº€è¶³åº¦ãƒ«ãƒ¼ãƒ«
        constraints["satisfaction_rules"] = [
            {
                "rule": "comprehensive_satisfaction",
                "work_life_weight": 0.3,
                "fairness_weight": 0.25,
                "growth_weight": 0.2,
                "team_weight": 0.15,
                "autonomy_weight": 0.1,
                "min_total_score": 0.75,
                "confidence": 0.85
            }
        ]
        
        # ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å‘ä¸Šè¦ç´ 
        constraints["motivation_boosters"] = [
            {
                "type": "recognition_opportunities",
                "frequency": "monthly",
                "coverage_target": 0.9,
                "confidence": 0.70
            },
            {
                "type": "skill_advancement",
                "quarterly_development_sessions": 2,
                "confidence": 0.75
            }
        ]
        
        return constraints
    
    def _generate_extraction_metadata(self, long_df: pd.DataFrame, wt_df: pd.DataFrame, mece_facts: Dict[str, List[str]]) -> Dict[str, Any]:
        """æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        
        metadata = {
            "extraction_info": {
                "axis_number": self.axis_number,
                "axis_name": self.axis_name,
                "extraction_timestamp": datetime.now().isoformat(),
                "data_source": "historical_shift_records",
                "analysis_scope": "staff_satisfaction_motivation_constraints"
            },
            
            "data_quality": {
                "total_records": len(long_df),
                "date_range": {
                    "start": str(long_df['ds'].min()),
                    "end": str(long_df['ds'].max()),
                    "total_days": len(long_df['ds'].unique())
                },
                "staff_coverage": {
                    "total_staff": long_df['staff'].nunique(),
                    "avg_shifts_per_staff": len(long_df) / long_df['staff'].nunique()
                },
                "completeness_score": self._calculate_data_completeness(long_df, wt_df)
            },
            
            "mece_analysis": {
                "total_categories": len(mece_facts),
                "categories": list(mece_facts.keys()),
                "facts_per_category": {cat: len(facts) for cat, facts in mece_facts.items()},
                "total_extracted_facts": sum(len(facts) for facts in mece_facts.values())
            },
            
            "axis_relationships": {
                "primary_reinforcement": "axis2_staff_rules",
                "secondary_influences": ["axis3_facility_operations", "axis4_demand_load"],
                "constraint_priority": "MEDIUM",
                "integration_complexity": "MODERATE"
            },
            
            "satisfaction_metrics": {
                "work_life_balance_score": self._calculate_work_life_balance_score(long_df),
                "fairness_index": self._calculate_fairness_index(long_df),
                "growth_opportunity_ratio": self._calculate_growth_opportunity_ratio(long_df),
                "team_cohesion_score": self._calculate_team_cohesion_score(long_df)
            },
            
            "confidence_indicators": {
                "data_reliability": 0.85,
                "pattern_confidence": 0.80,
                "constraint_validity": 0.82,
                "recommendation_strength": 0.78
            },
            
            "limitations": [
                "å®Ÿéš›ã®æº€è¶³åº¦èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³",
                "å€‹äººã®ä¾¡å€¤è¦³å·®ç•°ã‚’æ•°å€¤åŒ–å›°é›£",
                "é•·æœŸçš„ãªæº€è¶³åº¦å¤‰åŒ–ã®è¿½è·¡ãŒå¿…è¦",
                "å¤–éƒ¨è¦å› ï¼ˆçµ¦ä¸ã€ç¦åˆ©åšç”Ÿç­‰ï¼‰ã®å½±éŸ¿æœªè€ƒæ…®"
            ],
            
            "recommendations": [
                "å®šæœŸçš„ãªæº€è¶³åº¦èª¿æŸ»ã®å®Ÿæ–½",
                "å€‹åˆ¥ãƒ’ã‚¢ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹è©³ç´°ãƒ‹ãƒ¼ã‚ºæŠŠæ¡",
                "è»¸2åˆ¶ç´„ã¨ã®æ•´åˆæ€§å®šæœŸãƒã‚§ãƒƒã‚¯",
                "æº€è¶³åº¦å‘ä¸Šæ–½ç­–ã®åŠ¹æœæ¸¬å®šä½“åˆ¶æ§‹ç¯‰"
            ]
        }
        
        return metadata
    
    def _calculate_data_completeness(self, long_df: pd.DataFrame, wt_df: pd.DataFrame) -> float:
        """ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            required_columns = ['staff', 'ds', 'worktype']
            present_columns = sum(1 for col in required_columns if col in long_df.columns)
            completeness = present_columns / len(required_columns)
            
            # è¿½åŠ è¦ç´ ã®è€ƒæ…®
            if wt_df is not None and not wt_df.empty:
                completeness += 0.1
            
            return min(completeness, 1.0)
        except Exception:
            return 0.0
    
    def _calculate_work_life_balance_score(self, long_df: pd.DataFrame) -> float:
        """ãƒ¯ãƒ¼ã‚¯ãƒ©ã‚¤ãƒ•ãƒãƒ©ãƒ³ã‚¹ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            # é€±ã‚ãŸã‚Šå‹¤å‹™æ—¥æ•°ã®é©åˆ‡æ€§
            long_df_copy = long_df.copy()
            long_df_copy['week'] = pd.to_datetime(long_df_copy['ds']).dt.isocalendar().week
            weekly_work_days = long_df_copy.groupby(['staff', 'week']).size()
            
            ideal_range = [3, 5]  # é€±3-5æ—¥ãŒç†æƒ³
            balance_scores = []
            
            for days in weekly_work_days:
                if ideal_range[0] <= days <= ideal_range[1]:
                    balance_scores.append(1.0)
                else:
                    deviation = min(abs(days - ideal_range[0]), abs(days - ideal_range[1]))
                    score = max(0.0, 1.0 - deviation * 0.2)
                    balance_scores.append(score)
            
            return np.mean(balance_scores) if balance_scores else 0.5
        except Exception:
            return 0.5
    
    def _calculate_fairness_index(self, long_df: pd.DataFrame) -> float:
        """å…¬å¹³æ€§æŒ‡æ¨™ã®è¨ˆç®—"""
        try:
            staff_work_counts = long_df['staff'].value_counts()
            
            if len(staff_work_counts) <= 1:
                return 1.0
            
            # ã‚¸ãƒ‹ä¿‚æ•°çš„ãªå…¬å¹³æ€§æŒ‡æ¨™
            mean_work = staff_work_counts.mean()
            fairness = 1 - (staff_work_counts.std() / mean_work) if mean_work > 0 else 0
            
            return max(0.0, fairness)
        except Exception:
            return 0.0
    
    def _calculate_growth_opportunity_ratio(self, long_df: pd.DataFrame) -> float:
        """æˆé•·æ©Ÿä¼šæ¯”ç‡ã®è¨ˆç®—"""
        try:
            if 'worktype' not in long_df.columns:
                return 0.5
            
            staff_variety = long_df.groupby('staff')['worktype'].nunique()
            total_worktypes = long_df['worktype'].nunique()
            
            # å¤šæ§˜æ€§ã«åŸºã¥ãæˆé•·æ©Ÿä¼š
            growth_ratio = staff_variety.mean() / total_worktypes if total_worktypes > 0 else 0
            
            return min(growth_ratio, 1.0)
        except Exception:
            return 0.5
    
    def _calculate_team_cohesion_score(self, long_df: pd.DataFrame) -> float:
        """ãƒãƒ¼ãƒ çµæŸã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        try:
            # åŒæ—¥å‹¤å‹™ã®ç¶™ç¶šæ€§ã«ã‚ˆã‚‹çµæŸåº¦æ¸¬å®š
            daily_teams = long_df.groupby('ds')['staff'].apply(set)
            
            cohesion_scores = []
            for i in range(1, len(daily_teams)):
                prev_team = daily_teams.iloc[i-1]
                curr_team = daily_teams.iloc[i]
                
                if len(prev_team) > 0 and len(curr_team) > 0:
                    overlap = len(prev_team.intersection(curr_team))
                    total = len(prev_team.union(curr_team))
                    cohesion = overlap / total if total > 0 else 0
                    cohesion_scores.append(cohesion)
            
            return np.mean(cohesion_scores) if cohesion_scores else 0.5
        except Exception:
            return 0.5


# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    import pandas as pd
    from datetime import datetime, timedelta
    
    # ã‚µãƒ³ãƒ—ãƒ«é•·æœŸãƒ‡ãƒ¼ã‚¿
    dates = [datetime(2024, 1, 1) + timedelta(days=i) for i in range(30)]
    staff_list = ['ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨', 'é«˜æ©‹', 'æ¸¡è¾º']
    worktype_list = ['æ—¥å‹¤', 'å¤œå‹¤', 'æ—©ç•ª', 'é…ç•ª']
    
    sample_data = []
    for date in dates:
        for staff in staff_list[:3]:  # æ¯æ—¥3åå‹¤å‹™
            worktype = np.random.choice(worktype_list)
            sample_data.append({
                'ds': date.strftime('%Y-%m-%d'),
                'staff': staff,
                'worktype': worktype
            })
    
    long_df = pd.DataFrame(sample_data)
    
    # ã‚µãƒ³ãƒ—ãƒ«å‹¤å‹™åŒºåˆ†ãƒã‚¹ã‚¿
    wt_df = pd.DataFrame([
        {'worktype': 'æ—¥å‹¤', 'worktype_name': 'æ—¥å‹¤8æ™‚é–“'},
        {'worktype': 'å¤œå‹¤', 'worktype_name': 'å¤œå‹¤12æ™‚é–“'},
        {'worktype': 'æ—©ç•ª', 'worktype_name': 'æ—©ç•ª8æ™‚é–“'},
        {'worktype': 'é…ç•ª', 'worktype_name': 'é…ç•ª8æ™‚é–“'}
    ])
    
    # æŠ½å‡ºå®Ÿè¡Œ
    extractor = StaffSatisfactionMECEFactExtractor()
    results = extractor.extract_axis8_staff_satisfaction_rules(long_df, wt_df)
    
    print("=== è»¸8: ã‚¹ã‚¿ãƒƒãƒ•æº€è¶³åº¦ãƒ»ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶ç´„æŠ½å‡ºçµæœ ===")
    print(results['human_readable'])
    print("\n=== æ©Ÿæ¢°å¯èª­åˆ¶ç´„ ===")
    print(json.dumps(results['machine_readable'], indent=2, ensure_ascii=False))