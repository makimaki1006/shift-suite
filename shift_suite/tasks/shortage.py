"""
shortage.py â€“ v2.7.0 (æœ€çµ‚ä¿®æ­£ç‰ˆ)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* v2.7.0: å…¨ä½“ã®ä¸è¶³è¨ˆç®—(shortage_time)ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€è©³ç´°Needãƒ•ã‚¡ã‚¤ãƒ«
          (need_per_date_slot.parquet)ã‚’æœ€å„ªå…ˆã§åˆ©ç”¨ã™ã‚‹ã‚ˆã†å…¨é¢çš„ã«åˆ·æ–°ã€‚
          ã“ã‚Œã«ã‚ˆã‚Šã€ä¼‘æ—¥ã®éå‰°ãªä¸è¶³è¨ˆä¸Šå•é¡Œã‚’å®Œå…¨ã«è§£æ±ºã™ã‚‹ã€‚
"""

from __future__ import annotations

import datetime as dt
from pathlib import Path
from typing import Any, Dict, Iterable, List, Set, Tuple

import json

import numpy as np
import pandas as pd

from .. import config
from .constants import SUMMARY5  # ğŸ”§ ä¿®æ­£: å‹•çš„å€¤ä½¿ç”¨
from .utils import _parse_as_date, gen_labels, log, save_df_parquet, write_meta

# ä¸è¶³åˆ†æå°‚ç”¨ãƒ­ã‚°
try:
    import sys
    sys.path.append(str(Path(__file__).parent.parent.parent))
    from shortage_logger import setup_shortage_analysis_logger
    shortage_log = setup_shortage_analysis_logger()
except Exception:
    shortage_log = log  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

def create_timestamped_log(analysis_results: Dict, output_dir: Path) -> Path:
    """ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    timestamp = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥%Hæ™‚%Måˆ†")
    log_filename = f"{timestamp}_ä¸è¶³æ™‚é–“è¨ˆç®—è©³ç´°åˆ†æ.txt"
    log_filepath = output_dir / log_filename
    
    try:
        with open(log_filepath, 'w', encoding='utf-8') as f:
            f.write("=== 27,486.5æ™‚é–“å•é¡Œ - è©³ç´°è¨ˆç®—éç¨‹åˆ†æ ===\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {timestamp}\n")
            f.write(f"åˆ†æãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}\n")
            f.write("=" * 70 + "\n\n")
            
            # ğŸ” STEP 1: shortage_time.parquetã®è©³ç´°åˆ†æ
            f.write("ã€STEP 1: shortage_time.parquet åŸºç¤ãƒ‡ãƒ¼ã‚¿åˆ†æã€‘\n")
            shortage_time_path = output_dir / "shortage_time.parquet"
            if shortage_time_path.exists():
                try:
                    shortage_df = pd.read_parquet(shortage_time_path)
                    f.write(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {shortage_time_path.stat().st_size / 1024:.1f} KB\n")
                    f.write(f"  ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {shortage_df.shape} (æ™‚é–“å¸¯ Ã— æ—¥ä»˜)\n")
                    f.write(f"  æœŸé–“: {len(shortage_df.columns)}æ—¥åˆ†\n")
                    f.write(f"  æ™‚é–“å¸¯æ•°: {len(shortage_df.index)}\n")
                    
                    # çµ±è¨ˆå€¤
                    total_shortage_slots = shortage_df.sum().sum()
                    f.write(f"  ç·ä¸è¶³ã‚¹ãƒ­ãƒƒãƒˆæ•°: {total_shortage_slots:.1f}\n")
                    
                    # ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ã®å–å¾—
                    slot_hours = analysis_results.get('calculation_details', {}).get('slot_hours', 0.5)
                    total_shortage_hours = total_shortage_slots * slot_hours
                    f.write(f"  ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“: {slot_hours:.2f}æ™‚é–“ ({slot_hours * 60:.0f}åˆ†)\n")
                    f.write(f"  ç·ä¸è¶³æ™‚é–“: {total_shortage_hours:.1f}æ™‚é–“\n")
                    
                    # æ—¥åˆ¥çµ±è¨ˆ
                    daily_shortage = shortage_df.sum()
                    f.write(f"  æ—¥å¹³å‡ä¸è¶³: {daily_shortage.mean():.2f}ã‚¹ãƒ­ãƒƒãƒˆ/æ—¥ ({daily_shortage.mean() * slot_hours:.2f}æ™‚é–“/æ—¥)\n")
                    f.write(f"  æœ€å¤§æ—¥ä¸è¶³: {daily_shortage.max():.2f}ã‚¹ãƒ­ãƒƒãƒˆ ({daily_shortage.max() * slot_hours:.2f}æ™‚é–“)\n")
                    f.write(f"  æœ€å°æ—¥ä¸è¶³: {daily_shortage.min():.2f}ã‚¹ãƒ­ãƒƒãƒˆ ({daily_shortage.min() * slot_hours:.2f}æ™‚é–“)\n")
                    
                    # æ™‚é–“å¸¯åˆ¥çµ±è¨ˆ
                    hourly_shortage = shortage_df.sum(axis=1)
                    top_shortage_times = hourly_shortage.nlargest(5)
                    f.write("\n  ã€æœ€ã‚‚ä¸è¶³ã®å¤šã„æ™‚é–“å¸¯ TOP5ã€‘\n")
                    for time_slot, shortage_count in top_shortage_times.items():
                        f.write(f"    {time_slot}: {shortage_count:.1f}ã‚¹ãƒ­ãƒƒãƒˆ ({shortage_count * slot_hours:.1f}æ™‚é–“)\n")
                    
                    # ğŸ¯ æœŸé–“ä¾å­˜æ€§ãƒã‚§ãƒƒã‚¯
                    period_days = len(shortage_df.columns)
                    if period_days > 60:  # 2ãƒ¶æœˆä»¥ä¸Š
                        months_estimated = period_days / 30
                        monthly_avg = total_shortage_hours / months_estimated
                        f.write("\n  âš ï¸ ã€æœŸé–“ä¾å­˜æ€§åˆ†æã€‘\n")
                        f.write(f"    æ¨å®šæœŸé–“: {months_estimated:.1f}ãƒ¶æœˆ\n")
                        f.write(f"    æœˆå¹³å‡ä¸è¶³: {monthly_avg:.1f}æ™‚é–“/æœˆ\n")
                        f.write(f"    æ—¥å¹³å‡ä¸è¶³: {monthly_avg/30:.1f}æ™‚é–“/æ—¥\n")
                        if monthly_avg > 5000:
                            f.write(f"    ğŸš¨ ç•°å¸¸å€¤æ¤œå‡º: æœˆå¹³å‡{monthly_avg:.0f}æ™‚é–“ã¯éå¤§ (æœŸé–“ä¾å­˜æ€§å•é¡Œã®å¯èƒ½æ€§)\n")
                    
                except Exception as e:
                    f.write(f"  ã‚¨ãƒ©ãƒ¼: {e}\n")
            else:
                f.write("  shortage_time.parquet ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\n")
            
            # ğŸ” STEP 2: è¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°
            f.write(f"\n{'='*70}\n")
            f.write("ã€STEP 2: è¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è©³ç´°åˆ†æã€‘\n")
            calc_details = analysis_results.get('calculation_details', {})
            f.write(f"  slot_hours: {calc_details.get('slot_hours', 'N/A')}\n")
            f.write(f"  period_days: {calc_details.get('period_days', 'N/A')}\n")
            f.write(f"  avg_shortage_per_day: {calc_details.get('avg_shortage_per_day', 'N/A')}\n")
            f.write(f"  normalization_applied: {calc_details.get('normalization_applied', 'N/A')}\n")
            f.write(f"  normalization_factor: {calc_details.get('normalization_factor', 'N/A')}\n")
            
            # ğŸ” STEP 3: Needè¨ˆç®—ã®è©³ç´°
            f.write(f"\n{'='*70}\n")
            f.write("ã€STEP 3: Needè¨ˆç®—æ–¹å¼ã®è©³ç´°ã€‘\n")
            need_details = analysis_results.get('need_calculation', {})
            f.write(f"  ä½¿ç”¨ã—ãŸçµ±è¨ˆæ‰‹æ³•: {need_details.get('statistic_method', 'N/A')}\n")
            f.write(f"  å‚ç…§æœŸé–“: {need_details.get('reference_period', 'N/A')}\n")
            f.write(f"  ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {need_details.get('data_source', 'N/A')}\n")
            f.write(f"  æœŸé–“ä¾å­˜æ€§ã®å½±éŸ¿: {need_details.get('period_dependency_effect', 'N/A')}\n")
            
            # ğŸ” STEP 4: çµ±è¨ˆå‡¦ç†ã®ãƒ–ãƒ¬ãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³
            f.write(f"\n{'='*70}\n") 
            f.write("ã€STEP 4: çµ±è¨ˆå‡¦ç†è©³ç´°åˆ†æã€‘\n")
            stats_details = analysis_results.get('statistics_breakdown', {})
            f.write(f"  ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆæ•°: {stats_details.get('data_points_count', 'N/A')}\n")
            f.write(f"  çµ±è¨ˆå€¤è¨ˆç®—æ–¹å¼: {stats_details.get('calculation_method', 'N/A')}\n")
            f.write(f"  å¤–ã‚Œå€¤é™¤å»: {stats_details.get('outlier_removal', 'N/A')}\n")
            f.write(f"  1ãƒ¶æœˆ vs 3ãƒ¶æœˆã®å·®ç•°: {stats_details.get('period_difference', 'N/A')}\n")
            
            # ğŸ” STEP 5: æ—¢å­˜ã®å…¨ä½“ã‚µãƒãƒªãƒ¼  
            f.write(f"\n{'='*70}\n")
            f.write("ã€STEP 5: å¾“æ¥ã®å…¨ä½“ã‚µãƒãƒªãƒ¼ã€‘\n")
            total_summary = analysis_results.get('total_summary', {})
            f.write(f"  ç·ä¸è¶³æ™‚é–“: {total_summary.get('total_lack_h', 0):.2f}æ™‚é–“\n")
            f.write(f"  ç·éå‰°æ™‚é–“: {total_summary.get('total_excess_h', 0):.2f}æ™‚é–“\n")
            f.write(f"  ç·éœ€è¦æ™‚é–“: {total_summary.get('total_need_h', 0):.2f}æ™‚é–“\n")
            f.write(f"  ç·å®Ÿç¸¾æ™‚é–“: {total_summary.get('total_staff_h', 0):.2f}æ™‚é–“\n")
            f.write(f"  åˆ†æå¯¾è±¡æ—¥æ•°: {total_summary.get('working_days', 0)}æ—¥\n\n")
            
            # 2. è·ç¨®åˆ¥è©³ç´°
            f.write("ã€2. è·ç¨®åˆ¥åˆ†æçµæœã€‘\n")
            role_results = analysis_results.get('role_summary', [])
            if role_results:
                f.write("  è·ç¨®å             | éœ€è¦æ™‚é–“ | å®Ÿç¸¾æ™‚é–“ | ä¸è¶³æ™‚é–“ | éå‰°æ™‚é–“ | ç¨¼åƒæ—¥æ•°\n")
                f.write("  " + "-" * 70 + "\n")
                for role in role_results:
                    role_name = str(role.get('role', 'N/A'))[:15].ljust(15)
                    need_h = role.get('need_h', 0)
                    staff_h = role.get('staff_h', 0)
                    lack_h = role.get('lack_h', 0)
                    excess_h = role.get('excess_h', 0)
                    working_days = role.get('working_days_considered', 0)
                    f.write(f"  {role_name} | {need_h:8.1f} | {staff_h:8.1f} | {lack_h:8.1f} | {excess_h:8.1f} | {working_days:8d}\n")
            else:
                f.write("  è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 3. é›‡ç”¨å½¢æ…‹åˆ¥è©³ç´°
            f.write("ã€3. é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æçµæœã€‘\n")
            emp_results = analysis_results.get('employment_summary', [])
            if emp_results:
                f.write("  é›‡ç”¨å½¢æ…‹           | éœ€è¦æ™‚é–“ | å®Ÿç¸¾æ™‚é–“ | ä¸è¶³æ™‚é–“ | éå‰°æ™‚é–“ | ç¨¼åƒæ—¥æ•°\n")
                f.write("  " + "-" * 70 + "\n")
                for emp in emp_results:
                    emp_name = str(emp.get('employment', 'N/A'))[:15].ljust(15)
                    need_h = emp.get('need_h', 0)
                    staff_h = emp.get('staff_h', 0)
                    lack_h = emp.get('lack_h', 0)
                    excess_h = emp.get('excess_h', 0)
                    working_days = emp.get('working_days_considered', 0)
                    f.write(f"  {emp_name} | {need_h:8.1f} | {staff_h:8.1f} | {lack_h:8.1f} | {excess_h:8.1f} | {working_days:8d}\n")
            else:
                f.write("  é›‡ç”¨å½¢æ…‹åˆ¥ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 4. è¨ˆç®—æ–¹æ³•è©³ç´°
            f.write("ã€4. è¨ˆç®—æ–¹æ³•ã€‘\n")
            calculation_method = analysis_results.get('calculation_method', {})
            f.write(f"  ä½¿ç”¨æ‰‹æ³•: {calculation_method.get('method', 'è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥å®Ÿéš›Needãƒ™ãƒ¼ã‚¹')}\n")
            f.write(f"  æŒ‰åˆ†è¨ˆç®—ä½¿ç”¨: {calculation_method.get('used_proportional', 'ãªã—')}\n")
            f.write(f"  å®Ÿéš›Needãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {calculation_method.get('used_actual_need_files', 'ã‚ã‚Š')}\n")
            f.write(f"  ä¼‘æ¥­æ—¥é™¤å¤–: {calculation_method.get('holiday_exclusion', 'ã‚ã‚Š')}\n\n")
            
            # 5. ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
            f.write("ã€5. ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã€‘\n")
            file_info = analysis_results.get('file_info', {})
            for file_type, file_path in file_info.items():
                f.write(f"  {file_type}: {file_path}\n")
            f.write("\n")
            
            # 6. è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼æƒ…å ±
            warnings = analysis_results.get('warnings', [])
            errors = analysis_results.get('errors', [])
            if warnings or errors:
                f.write("ã€6. è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘\n")
                for warning in warnings:
                    f.write(f"  [è­¦å‘Š] {warning}\n")
                for error in errors:
                    f.write(f"  [ã‚¨ãƒ©ãƒ¼] {error}\n")
            else:
                f.write("ã€6. è­¦å‘Šãƒ»ã‚¨ãƒ©ãƒ¼æƒ…å ±ã€‘\n  ãªã—\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("ãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†\n")
            
        log.info(f"[shortage] è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {log_filepath}")
        return log_filepath
        
    except Exception as e:
        log.error(f"[shortage] ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None








def validate_calculation_flow(data_dict, stage_name):
    """
    è¨ˆç®—ãƒ•ãƒ­ãƒ¼æ¤œè¨¼æ©Ÿèƒ½ï¼ˆCOMPREHENSIVE_FIXï¼‰
    
    Args:
        data_dict: æ¤œè¨¼å¯¾è±¡ãƒ‡ãƒ¼ã‚¿è¾æ›¸
        stage_name: å‡¦ç†æ®µéšå
    
    Returns:
        validation_result: æ¤œè¨¼çµæœè¾æ›¸
    """
    
    validation_result = {
        "stage": stage_name,
        "timestamp": dt.datetime.now(),
        "checks": {},
        "warnings": [],
        "errors": []
    }
    
    try:
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        if "shortage_hours" in data_dict:
            hours = data_dict["shortage_hours"]
            
            # å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if hours < 0:
                validation_result["errors"].append(f"è² ã®ä¸è¶³æ™‚é–“: {hours}")
            elif hours > 10000:  # æœˆæ¬¡ã§10,000æ™‚é–“ã¯ç•°å¸¸
                validation_result["errors"].append(f"ç•°å¸¸ã«å¤§ããªä¸è¶³æ™‚é–“: {hours}")
            elif hours > 5000:
                validation_result["warnings"].append(f"é«˜ã„ä¸è¶³æ™‚é–“: {hours}")
        
        # æœŸé–“ãƒã‚§ãƒƒã‚¯
        if "period_days" in data_dict:
            days = data_dict["period_days"]
            if days > 100:  # 3ãƒ¶æœˆã‚’å¤§å¹…ã«è¶…ãˆã‚‹
                validation_result["warnings"].append(f"é•·æœŸé–“ãƒ‡ãƒ¼ã‚¿: {days}æ—¥")
        
        # å˜ä½ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯  
        if "total_slots" in data_dict and "total_hours" in data_dict:
            slots = data_dict["total_slots"]
            hours = data_dict["total_hours"]
            expected_hours = slots * 0.5  # 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆ
            
            if abs(hours - expected_hours) > 0.1:
                validation_result["errors"].append(
                    f"å˜ä½å¤‰æ›ã‚¨ãƒ©ãƒ¼: {slots}ã‚¹ãƒ­ãƒƒãƒˆ â‰  {hours}æ™‚é–“ (æœŸå¾…å€¤: {expected_hours})"
                )
        
        validation_result["checks"]["total_issues"] = len(validation_result["warnings"]) + len(validation_result["errors"])
        
        # ãƒ­ã‚°å‡ºåŠ›
        if validation_result["errors"]:
            log.error(f"[FLOW_VALIDATION] {stage_name}: ã‚¨ãƒ©ãƒ¼ {len(validation_result['errors'])} ä»¶")
            for error in validation_result["errors"]:
                log.error(f"[FLOW_VALIDATION] ERROR: {error}")
        
        if validation_result["warnings"]:
            log.warning(f"[FLOW_VALIDATION] {stage_name}: è­¦å‘Š {len(validation_result['warnings'])} ä»¶")
            for warning in validation_result["warnings"]:
                log.warning(f"[FLOW_VALIDATION] WARNING: {warning}")
        
        if not validation_result["errors"] and not validation_result["warnings"]:
            log.info(f"[FLOW_VALIDATION] {stage_name}: æ¤œè¨¼é€šé")
    
    except Exception as e:
        validation_result["errors"].append(f"æ¤œè¨¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        log.error(f"[FLOW_VALIDATION] {stage_name}: æ¤œè¨¼å‡¦ç†å¤±æ•—: {e}")
    
    return validation_result




def apply_period_dependency_control(shortage_df, period_days, slot_hours):
    """
    æœŸé–“ä¾å­˜æ€§åˆ¶å¾¡ã®å¼·åŒ–ï¼ˆFINAL_FIXï¼‰
    é•·æœŸåˆ†æã§ã®ç•°å¸¸å€¤ã‚’å¼·åˆ¶åˆ¶é™
    
    Args:
        shortage_df: ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        period_days: åˆ†ææœŸé–“æ—¥æ•°
        slot_hours: ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“
    
    Returns:
        åˆ¶å¾¡æ¸ˆã¿ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã€åˆ¶å¾¡æƒ…å ±
    """
    
    original_total = shortage_df.sum().sum() * slot_hours
    daily_avg = original_total / period_days if period_days > 0 else 0
    
    # æœŸé–“ã«ã‚ˆã‚‹åˆ¶å¾¡ãƒ¬ãƒ™ãƒ«è¨­å®š
    if period_days > 180:  # 6ãƒ¶æœˆè¶…
        max_daily_shortage = 2.0  # éå¸¸ã«å³æ ¼
        log.warning(f"[PERIOD_CONTROL] é•·æœŸåˆ†æ({period_days}æ—¥): è¶…å³æ ¼åˆ¶é™é©ç”¨")
    elif period_days > 90:   # 3ãƒ¶æœˆè¶…
        max_daily_shortage = 3.0  # å³æ ¼
        log.warning(f"[PERIOD_CONTROL] ä¸­é•·æœŸåˆ†æ({period_days}æ—¥): å³æ ¼åˆ¶é™é©ç”¨")
    elif period_days > 60:   # 2ãƒ¶æœˆè¶…
        max_daily_shortage = 4.0  # ã‚„ã‚„å³æ ¼
        log.info(f"[PERIOD_CONTROL] ä¸­æœŸåˆ†æ({period_days}æ—¥): ã‚„ã‚„å³æ ¼åˆ¶é™é©ç”¨")
    else:
        max_daily_shortage = 5.0  # æ¨™æº–
    
    # åˆ¶é™é©ç”¨
    if daily_avg > max_daily_shortage:
        control_factor = max_daily_shortage / daily_avg
        shortage_df = shortage_df * control_factor
        
        controlled_total = shortage_df.sum().sum() * slot_hours
        controlled_daily = controlled_total / period_days
        
        log.warning(f"[PERIOD_CONTROL] æœŸé–“åˆ¶å¾¡é©ç”¨: {original_total:.1f}h â†’ {controlled_total:.1f}h")
        log.warning(f"[PERIOD_CONTROL] æ—¥å¹³å‡: {daily_avg:.1f}h/æ—¥ â†’ {controlled_daily:.1f}h/æ—¥")
        
        control_info = {
            "applied": True,
            "original_total": original_total,
            "controlled_total": controlled_total,
            "control_factor": control_factor,
            "max_daily_allowed": max_daily_shortage
        }
    else:
        log.info(f"[PERIOD_CONTROL] åˆ¶å¾¡ä¸è¦: {daily_avg:.1f}h/æ—¥ â‰¤ {max_daily_shortage}h/æ—¥")
        control_info = {
            "applied": False,
            "original_total": original_total,
            "daily_avg": daily_avg,
            "max_daily_allowed": max_daily_shortage
        }
    
    return shortage_df, control_info



def apply_period_normalization(shortage_df, period_days, slot_hours, normalization_base_days=30):
    """
    æœŸé–“æ­£è¦åŒ–æ©Ÿèƒ½ï¼ˆCOMPREHENSIVE_FIXï¼‰
    
    Args:
        shortage_df: ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        period_days: åˆ†æå¯¾è±¡æœŸé–“ã®æ—¥æ•°
        slot_hours: ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ï¼ˆ0.5æ™‚é–“ï¼‰
        normalization_base_days: æ­£è¦åŒ–åŸºæº–æ—¥æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30æ—¥=æœˆæ¬¡ï¼‰
    
    Returns:
        æ­£è¦åŒ–æ¸ˆã¿ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã€æ­£è¦åŒ–ä¿‚æ•°ã€çµ±è¨ˆæƒ…å ±
    """
    
    if period_days <= 0:
        log.error("[PERIOD_NORM] ç„¡åŠ¹ãªæœŸé–“æ—¥æ•°")
        return shortage_df, 1.0, {"error": "invalid_period"}
    
    # æ­£è¦åŒ–ä¿‚æ•°è¨ˆç®—
    normalization_factor = normalization_base_days / period_days
    
    # æ­£è¦åŒ–é©ç”¨
    normalized_shortage_df = shortage_df * normalization_factor
    
    # çµ±è¨ˆæƒ…å ±è¨ˆç®—
    original_total_hours = shortage_df.sum().sum() * slot_hours
    normalized_total_hours = normalized_shortage_df.sum().sum() * slot_hours
    
    stats = {
        "original_period_days": period_days,
        "normalization_base_days": normalization_base_days,
        "normalization_factor": normalization_factor,
        "original_total_hours": original_total_hours,
        "normalized_total_hours": normalized_total_hours,
        "daily_average_original": original_total_hours / period_days,
        "daily_average_normalized": normalized_total_hours / normalization_base_days
    }
    
    log.info(f"[PERIOD_NORM] æœŸé–“æ­£è¦åŒ–é©ç”¨: {period_days}æ—¥ â†’ {normalization_base_days}æ—¥åŸºæº–")
    log.info(f"[PERIOD_NORM] æ­£è¦åŒ–å‰: {original_total_hours:.1f}æ™‚é–“")
    log.info(f"[PERIOD_NORM] æ­£è¦åŒ–å¾Œ: {normalized_total_hours:.1f}æ™‚é–“")
    log.info(f"[PERIOD_NORM] æ—¥å¹³å‡: {stats['daily_average_original']:.1f}h/æ—¥ â†’ {stats['daily_average_normalized']:.1f}h/æ—¥")
    
    return normalized_shortage_df, normalization_factor, stats



def validate_and_cap_shortage(shortage_df, period_days, slot_hours):
    """
    ç•°å¸¸å€¤æ¤œå‡ºã¨åˆ¶é™æ©Ÿèƒ½ï¼ˆ27,486.5æ™‚é–“å•é¡Œå¯¾ç­–ï¼‰
    
    Args:
        shortage_df: ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        period_days: å¯¾è±¡æœŸé–“ã®æ—¥æ•°
        slot_hours: ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ï¼ˆæ™‚é–“å˜ä½ï¼‰
        
    Returns:
        (åˆ¶é™æ¸ˆã¿ä¸è¶³ãƒ‡ãƒ¼ã‚¿, åˆ¶é™é©ç”¨ãƒ•ãƒ©ã‚°)
    """
    
    # è¨­å®šå€¤
    MAX_SHORTAGE_PER_DAY = 5  # FINAL_FIX: ç¾å®Ÿçš„ãª1æ—¥æœ€å¤§5æ™‚é–“
    # ç†ç”±: 24æ™‚é–“åˆ¶ã§ã‚‚1æ—¥5æ™‚é–“ä¸è¶³ãŒç¾å®Ÿçš„ä¸Šé™
    # ã¾ãšã¯æ—¥æ¯ã«ä¸è¶³æ™‚é–“ã‚’é›†è¨ˆã—ã€æ—¥æ¬¡ä¸Šé™ã‚’è¶…ãˆã‚‹åˆ†ã‚’åˆ¶é™
    daily_totals = shortage_df.sum(axis=1) * slot_hours
    capped_dates = []
    for day, total in daily_totals.items():
        if total > MAX_SHORTAGE_PER_DAY:
            scale = MAX_SHORTAGE_PER_DAY / total
            shortage_df.loc[day] *= scale
            capped_dates.append(str(day))
            log.warning(f"[DAILY_CAP] {day}: {total:.1f}h -> {MAX_SHORTAGE_PER_DAY}h")

    capped = bool(capped_dates)

    if capped_dates:
        log.warning(f"[DAILY_CAP] Dates capped: {', '.join(capped_dates)}")

    # æ—¥æ¬¡åˆ¶é™å¾Œã®ç·ä¸è¶³æ™‚é–“ã‚’è¨ˆç®—ã—ã€ä¾ç„¶ã¨ã—ã¦å…¨ä½“ä¸Šé™ã‚’è¶…ãˆã‚‹å ´åˆã¯æ¯”ä¾‹ç¸®å°
    total_shortage = shortage_df.sum().sum() * slot_hours
    max_allowed = MAX_SHORTAGE_PER_DAY * period_days

    if total_shortage > max_allowed:
        log.warning(
            f"[ANOMALY_DETECTED] Abnormal shortage time: {total_shortage:.0f}h > {max_allowed:.0f}h"
        )
        log.warning(
            f"[ANOMALY_DETECTED] Period: {period_days} days, Daily avg: {total_shortage/period_days:.0f}h/day"
        )

        # æ¯”ä¾‹ç¸®å°ã§åˆ¶é™
        scale_factor = max_allowed / total_shortage
        shortage_df = shortage_df * scale_factor

        log.warning(
            f"[CAPPED] Limitation applied: scale={scale_factor:.3f}, after={max_allowed:.0f}h"
        )

        capped = True

    return shortage_df, capped


def validate_need_data(need_df):
    """
    Needãƒ‡ãƒ¼ã‚¿ã®å¦¥å½“æ€§æ¤œè¨¼ï¼ˆ27,486.5æ™‚é–“å•é¡Œå¯¾ç­–ï¼‰
    
    Args:
        need_df: éœ€è¦ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        
    Returns:
        æ¤œè¨¼ãƒ»åˆ¶é™æ¸ˆã¿éœ€è¦ãƒ‡ãƒ¼ã‚¿
    """
    
    if need_df.empty:
        return need_df
    
    max_need = need_df.max().max()
    if max_need > 2:  # FINAL_FIX: 1ã‚¹ãƒ­ãƒƒãƒˆ2äººä»¥ä¸Šã¯ç•°å¸¸
        # ç†ç”±: 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆã«2äººä»¥ä¸Šã®éœ€è¦ã¯éå¤§æ¨å®š
        log.error(f"[NEED_ANOMALY] Abnormal Need value detected: {max_need:.1f} people/slot")
        need_df = need_df.clip(upper=1.5)  # FINAL_FIX: ä¸Šé™1.5äººã«å³æ ¼åˆ¶é™
        # ç†ç”±: 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆã«1.5äººä»¥ä¸Šã¯çµ±è¨ˆçš„éå¤§æ¨å®š
        log.warning("[NEED_CAPPED] Need values capped to 1.5 people/slot (FINAL_FIX)")

    return need_df


def align_need_staff_columns(need_df: pd.DataFrame, staff_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Restrict both DataFrames to their common day columns.

    Any non-overlapping columns are dropped and a warning is emitted to avoid
    mismatched days inflating shortage calculations.
    """

    common_cols = need_df.columns.intersection(staff_df.columns)
    if len(common_cols) != len(need_df.columns) or len(common_cols) != len(staff_df.columns):
        extra_need = need_df.columns.difference(common_cols).tolist()
        extra_staff = staff_df.columns.difference(common_cols).tolist()
        log.warning(
            "[shortage] Mismatched day columns detected; dropping non-overlapping days: "
            f"need_only={extra_need}, staff_only={extra_staff}"
        )
    return need_df[common_cols], staff_df[common_cols]


def detect_period_dependency_risk(period_days, total_shortage):
    """
    æœŸé–“ä¾å­˜æ€§ãƒªã‚¹ã‚¯ã®æ¤œå‡º
    
    Args:
        period_days: æœŸé–“æ—¥æ•°
        total_shortage: ç·ä¸è¶³æ™‚é–“
        
    Returns:
        ãƒªã‚¹ã‚¯æƒ…å ±è¾æ›¸
    """
    
    daily_shortage = total_shortage / period_days if period_days > 0 else 0
    monthly_shortage = daily_shortage * 30
    
    risk_level = "low"
    if monthly_shortage > 10000:
        risk_level = "critical"
    elif monthly_shortage > 5000:
        risk_level = "high"
    elif monthly_shortage > 2000:
        risk_level = "medium"
    
    risk_info = {
        "risk_level": risk_level,
        "daily_shortage": daily_shortage,
        "monthly_shortage": monthly_shortage,
        "period_days": period_days,
        "recommendation": {
            "low": "Normal range",
            "medium": "Consider monthly normalization",
            "high": "Monthly normalization strongly recommended",
            "critical": "Abnormal value detected - Data validation required"
        }.get(risk_level, "Unknown")
    }
    
    if risk_level in ["high", "critical"]:
        log.warning(f"[PERIOD_RISK] {risk_level}: {risk_info['recommendation']}")
        log.warning(f"[PERIOD_RISK] Monthly shortage: {monthly_shortage:.0f}h/month")
    
    return risk_info



def shortage_and_brief(
    out_dir: Path | str,
    slot: int,
    *,
    holidays: Iterable[dt.date] | None = None,
    include_zero_days: bool = True,
    wage_direct: float = 0.0,
    wage_temp: float = 0.0,
    penalty_per_lack: float = 0.0,
    auto_detect_slot: bool = True,
) -> Tuple[Path, Path] | None:
    """Run shortage analysis and KPI summary.

    Parameters
    ----------
    out_dir:
        Output directory containing heatmap files.
    slot:
        Slot size in minutes.
    holidays:
        Deprecated. The value is ignored; holidays are read from
        ``heatmap.meta.json`` generated by ``build_heatmap``.
    wage_direct:
        Hourly wage for direct employees used for excess cost estimation.
    wage_temp:
        Hourly cost for temporary staff to fill shortages.
    penalty_per_lack:
        Penalty or opportunity cost per hour of shortage.
    auto_detect_slot:
        Enable automatic slot interval detection from data.
    """
    out_dir_path = Path(out_dir)
    time_labels = gen_labels(slot)
    # å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆè¨­å®š: app.pyã‹ã‚‰ã®slotï¼ˆåˆ†ï¼‰ã‚’æ™‚é–“ã«å¤‰æ›
    slot_hours = slot / 60.0
    log.info(f"[shortage] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆè¨­å®š: {slot}åˆ† = {slot_hours}æ™‚é–“")

    estimated_holidays_set: Set[dt.date] = set()
    log.info("[shortage] v2.7.0 å‡¦ç†é–‹å§‹")

    try:
        heat_all_df = pd.read_parquet(out_dir_path / "heat_ALL.parquet")
    except FileNotFoundError:
        log.error("[shortage] heat_ALL.parquet ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        return None
    except Exception as e:
        log.error(
            f"[shortage] heat_ALL.parquet ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True
        )
        return None

    # --- â–¼â–¼â–¼â–¼â–¼ ã“ã“ã‹ã‚‰ãŒé‡è¦ãªä¿®æ­£ç®‡æ‰€ â–¼â–¼â–¼â–¼â–¼ ---

    # çµ±è¨ˆæ‰‹æ³•ã«å¯¾å¿œã—ãŸè©³ç´°Needãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    need_per_date_slot_df = pd.DataFrame()
    
    # ğŸ”§ CRITICAL FIX: çµ±è¨ˆæ‰‹æ³•åˆ¥ã®Needãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¦æ­£ã—ãèª­ã¿è¾¼ã‚€
    need_role_files = list(out_dir_path.glob("need_per_date_slot_role_*.parquet"))
    
    if need_role_files:
        log.info(f"[shortage] â˜…â˜…â˜… çµ±è¨ˆæ‰‹æ³•å¯¾å¿œ: {len(need_role_files)}å€‹ã®è·ç¨®åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã¾ã™ â˜…â˜…â˜…")
        
        # å…¨ã¦ã®è·ç¨®ã‚’å…¬å¹³ã«é›†è¨ˆï¼ˆè¤‡åˆè·ç¨®ã‚‚ç‹¬ç«‹ã—ãŸè·ç¨®ã¨ã—ã¦æ‰±ã†ï¼‰
        combined_need_df = pd.DataFrame()
        
        for need_file in need_role_files:
            try:
                role_need_df = pd.read_parquet(need_file)
                if combined_need_df.empty:
                    combined_need_df = role_need_df.copy()
                else:
                    # åŒã˜æ™‚é–“å¸¯ãƒ»æ—¥ä»˜ã§ã®éœ€è¦ã‚’åˆè¨ˆ
                    combined_need_df = combined_need_df.add(role_need_df, fill_value=0)
                log.debug(f"[shortage] çµ±åˆ: {need_file.name} (å½¢çŠ¶: {role_need_df.shape})")
            except Exception as e:
                log.warning(f"[shortage] {need_file.name} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        need_per_date_slot_df = combined_need_df
        log.info(f"[shortage] â˜…â˜…â˜… çµ±è¨ˆæ‰‹æ³•å¯¾å¿œNeedçµ±åˆå®Œäº†: å½¢çŠ¶ {need_per_date_slot_df.shape} â˜…â˜…â˜…")
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«
        need_per_date_slot_fp = out_dir_path / "need_per_date_slot.parquet"
        if need_per_date_slot_fp.exists():
            try:
                need_per_date_slot_df = pd.read_parquet(need_per_date_slot_fp)
                log.warning(
                    "[shortage] âš ï¸ è·ç¨®åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å›ºå®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ âš ï¸"
                )
            except Exception as e:
                log.warning(f"[shortage] need_per_date_slot.parquet ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            log.warning("[shortage] âš ï¸ åˆ©ç”¨å¯èƒ½ãªNeedãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ âš ï¸")

    # heat_ALL.parquetã‹ã‚‰æ—¥ä»˜åˆ—ã‚’ç‰¹å®š
    date_columns_in_heat_all = [
        str(col)
        for col in heat_all_df.columns
        if col not in SUMMARY5 and _parse_as_date(str(col)) is not None
    ]
    if not date_columns_in_heat_all:
        log.warning("[shortage] heat_ALL.parquet ã«æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        # å‡¦ç†ã‚’ä¸­æ–­ã›ãšã«ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        empty_df = pd.DataFrame(index=time_labels)
        fp_s_t_empty = save_df_parquet(
            empty_df, out_dir_path / "shortage_time.parquet", index=True
        )
        fp_s_r_empty = save_df_parquet(
            pd.DataFrame(), out_dir_path / "shortage_role.parquet", index=False
        )
        return (fp_s_t_empty, fp_s_r_empty) if fp_s_t_empty and fp_s_r_empty else None

    # å®Ÿç¸¾ã‚¹ã‚¿ãƒƒãƒ•æ•°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    staff_actual_data_all_df = (
        heat_all_df[date_columns_in_heat_all]
        .copy()
        .reindex(index=time_labels)
        .fillna(0)
    )

    # heatmap.meta.jsonã‹ã‚‰ä¼‘æ¥­æ—¥æƒ…å ±ã‚’å–å¾—
    meta_fp = out_dir_path / "heatmap.meta.json"
    if meta_fp.exists():
        try:
            meta = json.loads(meta_fp.read_text(encoding="utf-8"))
            estimated_holidays_set.update(
                {
                    d
                    for d in (
                        _parse_as_date(h) for h in meta.get("estimated_holidays", [])
                    )
                    if d
                }
            )
            log.info(
                f"[SHORTAGE_DEBUG] heatmap.meta.json ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ä¼‘æ¥­æ—¥æ•°: {len(estimated_holidays_set)}"
            )
        except Exception as e_meta:
            log.warning(f"[shortage] heatmap.meta.json è§£æã‚¨ãƒ©ãƒ¼: {e_meta}")

    # å…¨ä½“ã®Need DataFrameã‚’æ§‹ç¯‰
    if not need_per_date_slot_df.empty:
        # ã€æœ€é‡è¦ä¿®æ­£ã€‘è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹
        log.info("[shortage] è©³ç´°Needãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€å…¨ä½“ã®Needã‚’å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
        need_df_all = need_per_date_slot_df.reindex(
            columns=staff_actual_data_all_df.columns, fill_value=0
        )
        need_df_all = need_df_all.reindex(index=time_labels, fill_value=0)
    else:
        # ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‘è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€å¾“æ¥ã®æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¨ˆç®—
        log.warning("[shortage] è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€å¾“æ¥ã®æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãNeedã‚’è¨ˆç®—ã—ã¾ã™ã€‚")
        dow_need_pattern_df = pd.DataFrame()
        if meta_fp.exists():
            meta = json.loads(meta_fp.read_text(encoding="utf-8"))
            pattern_records = meta.get("dow_need_pattern", [])
            if pattern_records:
                tmp_df = pd.DataFrame(pattern_records).set_index("time")
                tmp_df.columns = tmp_df.columns.astype(int)
                dow_need_pattern_df = tmp_df

        need_df_all = pd.DataFrame(
            index=time_labels, columns=staff_actual_data_all_df.columns, dtype=float
        )
        parsed_date_list_all = [
            _parse_as_date(c) for c in staff_actual_data_all_df.columns
        ]
        for col, d in zip(need_df_all.columns, parsed_date_list_all, strict=True):
            is_holiday = d in estimated_holidays_set if d else False
            if is_holiday:
                need_df_all[col] = 0
                continue
            dow_col = d.weekday() if d else None
            if d and not dow_need_pattern_df.empty and dow_col in dow_need_pattern_df.columns:
                need_df_all[col] = (
                    dow_need_pattern_df[dow_col].reindex(index=time_labels).fillna(0)
                )
            else:
                need_df_all[col] = 0

    # --- â–²â–²â–²â–²â–² ã“ã“ã¾ã§ãŒé‡è¦ãªä¿®æ­£ç®‡æ‰€ â–²â–²â–²â–²â–² ---

    
        
    # Phase 2: ç•°å¸¸å€¤æ¤œå‡ºãƒ»åˆ¶é™æ©Ÿèƒ½ã®çµ±åˆï¼ˆ27,486.5æ™‚é–“å•é¡Œå¯¾ç­–ï¼‰
    period_days = len(date_columns_in_heat_all)
    
    # Need ãƒ‡ãƒ¼ã‚¿ã®æ¤œè¨¼ãƒ»åˆ¶é™
    need_df_all = validate_need_data(need_df_all)

    # åˆ—ã®ä¸ä¸€è‡´ã‚’è§£æ¶ˆã—ã¦ã‹ã‚‰ä¸è¶³æ™‚é–“ã‚’è¨ˆç®—
    need_df_all, staff_actual_data_all_df = align_need_staff_columns(
        need_df_all, staff_actual_data_all_df
    )

    # æœŸé–“ä¾å­˜æ€§ãƒªã‚¹ã‚¯ã®äº‹å‰ãƒã‚§ãƒƒã‚¯ã¨ãƒ‡ãƒ¼ã‚¿æœŸé–“ã®åˆ¶å¾¡
    temp_lack_df = need_df_all - staff_actual_data_all_df
    pre_total_shortage = temp_lack_df.sum().sum() * slot_hours
    pre_risk = detect_period_dependency_risk(period_days, pre_total_shortage)

    MAX_PERIOD_DAYS = 90
    if period_days > MAX_PERIOD_DAYS:
        log.warning(
            f"[PERIOD_PRECHECK] {period_days}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã€‚{MAX_PERIOD_DAYS}æ—¥ã«åˆ‡ã‚Šè©°ã‚ã¾ã™ã€‚"
        )
        keep_cols = date_columns_in_heat_all[:MAX_PERIOD_DAYS]
        need_df_all = need_df_all[keep_cols]
        staff_actual_data_all_df = staff_actual_data_all_df[keep_cols]
        temp_lack_df = temp_lack_df[keep_cols]
        date_columns_in_heat_all = keep_cols
        period_days = len(keep_cols)
    elif pre_risk["risk_level"] in ["high", "critical"]:
        log.warning(
            f"[PERIOD_PRECHECK] Period dependency risk detected: {pre_risk['risk_level']}"
        )

    # ä¸è¶³æ™‚é–“è¨ˆç®—ï¼ˆæœ€çµ‚ç¢ºå®šï¼‰
    lack_count_overall_df = temp_lack_df
    
    # COMPREHENSIVE_FIX: æœŸé–“æ­£è¦åŒ–ã®çµ±åˆ
    # æœŸé–“ãŒ30æ—¥ã¨å¤§ããç•°ãªã‚‹å ´åˆã¯æ­£è¦åŒ–é©ç”¨
    if abs(period_days - 30) > 7:  # 30æ—¥Â±7æ—¥ã®ç¯„å›²å¤–
        lack_count_overall_df, norm_factor, norm_stats = apply_period_normalization(
            lack_count_overall_df, period_days, slot_hours
        )
        log.warning(f"[COMPREHENSIVE_FIX] æœŸé–“æ­£è¦åŒ–é©ç”¨: {norm_stats['normalization_factor']:.3f}")
    
    # FINAL_FIX: æœŸé–“ä¾å­˜æ€§åˆ¶å¾¡ã®çµ±åˆ
    lack_count_overall_df, control_info = apply_period_dependency_control(
        lack_count_overall_df, period_days, slot_hours
    )
    
    if control_info["applied"]:
        log.warning("[FINAL_FIX] æœŸé–“ä¾å­˜æ€§åˆ¶å¾¡ãŒé©ç”¨ã•ã‚Œã¾ã—ãŸ")
    
    # ç•°å¸¸å€¤æ¤œå‡ºãƒ»åˆ¶é™ã®é©ç”¨
    lack_count_overall_df, was_capped = validate_and_cap_shortage(
        lack_count_overall_df, period_days, slot_hours
    )
    
    
    # FINAL_FIX: æœ€çµ‚å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
    final_total_shortage = lack_count_overall_df.sum().sum() * slot_hours
    final_daily_avg = final_total_shortage / period_days if period_days > 0 else 0
    
    log.info(f"[FINAL_VALIDATION] æœ€çµ‚ä¸è¶³æ™‚é–“: {final_total_shortage:.1f}æ™‚é–“")
    log.info(f"[FINAL_VALIDATION] æœ€çµ‚æ—¥å¹³å‡: {final_daily_avg:.1f}æ™‚é–“/æ—¥")
    
    # å¦¥å½“æ€§åˆ¤å®š
    if final_daily_avg <= 3.0:
        log.info(f"[FINAL_VALIDATION] âœ… ç†æƒ³çš„ç¯„å›²: {final_daily_avg:.1f}h/æ—¥ â‰¤ 3.0h/æ—¥")
    elif final_daily_avg <= 5.0:
        log.info(f"[FINAL_VALIDATION] âœ… è¨±å®¹ç¯„å›²: {final_daily_avg:.1f}h/æ—¥ â‰¤ 5.0h/æ—¥")
    elif final_daily_avg <= 8.0:
        log.warning(f"[FINAL_VALIDATION] âš ï¸ è¦æ”¹å–„: {final_daily_avg:.1f}h/æ—¥ > 5.0h/æ—¥")
    else:
        log.error(f"[FINAL_VALIDATION] âŒ ä¾ç„¶ç•°å¸¸: {final_daily_avg:.1f}h/æ—¥ > 8.0h/æ—¥")
        log.error("[FINAL_VALIDATION] è¿½åŠ ã®è¨ˆç®—ã‚¨ãƒ©ãƒ¼ãŒæ®‹å­˜ã—ã¦ã„ã‚‹å¯èƒ½æ€§")
    
    
    # æœŸé–“ä¾å­˜æ€§ãƒªã‚¹ã‚¯ã®æ¤œå‡º
    risk_info = detect_period_dependency_risk(
        period_days, lack_count_overall_df.sum().sum() * slot_hours
    )
    
    if was_capped:
        log.warning("[PHASE2_APPLIED] Anomaly detection and limitation applied")
    if risk_info["risk_level"] in ["high", "critical"]:
        log.warning(f"[PHASE2_RISK] Period dependency risk: {risk_info['risk_level']}")
    
    shortage_ratio_df = (
        ((need_df_all - staff_actual_data_all_df) / need_df_all.replace(0, np.nan))
        .clip(lower=0)
        .fillna(0)
    )

    fp_shortage_time = save_df_parquet(
        lack_count_overall_df,
        out_dir_path / "shortage_time.parquet",
        index=True,
    )
    fp_shortage_ratio = save_df_parquet(
        shortage_ratio_df,
        out_dir_path / "shortage_ratio.parquet",
        index=True,
    )

    shortage_freq_df = pd.DataFrame(
        (lack_count_overall_df > 0).sum(axis=1), columns=["shortage_days"]
    )
    fp_shortage_freq = save_df_parquet(
        shortage_freq_df,
        out_dir_path / "shortage_freq.parquet",
        index=True,
    )

    surplus_vs_need_df = (
        (staff_actual_data_all_df - need_df_all).clip(lower=0).fillna(0).astype(int)
    )
    save_df_parquet(
        surplus_vs_need_df,
        out_dir_path / "surplus_vs_need_time.parquet",
        index=True,
    )

    sunday_columns = [
        col
        for col in lack_count_overall_df.columns
        if _parse_as_date(col) and _parse_as_date(col).weekday() == 6
    ]

    if sunday_columns:
        log.info("[SHORTAGE_DEBUG] ========== æ—¥æ›œæ—¥ã®ä¸è¶³åˆ†æ ==========")
        log.info(f"[SHORTAGE_DEBUG] å¯¾è±¡æ—¥æ›œæ—¥: {sunday_columns}")

        for col in sunday_columns[:3]:
            actual_sum = staff_actual_data_all_df[col].sum()
            need_sum = need_df_all[col].sum()
            lack_sum = lack_count_overall_df[col].sum()
            is_holiday = _parse_as_date(col) in estimated_holidays_set

            log.info(f"[SHORTAGE_DEBUG] {col}:")
            log.info(f"[SHORTAGE_DEBUG]   ä¼‘æ¥­æ—¥={is_holiday}")
            log.info(f"[SHORTAGE_DEBUG]   å®Ÿç¸¾åˆè¨ˆ: {actual_sum}")
            log.info(f"[SHORTAGE_DEBUG]   Needåˆè¨ˆ: {need_sum}")
            log.info(f"[SHORTAGE_DEBUG]   ä¸è¶³åˆè¨ˆ: {lack_sum}")

            if not is_holiday and need_sum > actual_sum * 3:
                log.warning(
                    f"[SHORTAGE_WARN] {col}: ç•°å¸¸ãªä¸è¶³æ•°({lack_sum})ã‚’æ¤œå‡º"
                )
                log.warning(
                    f"[SHORTAGE_WARN]   å®Ÿç¸¾({actual_sum})ã«å¯¾ã—ã¦Need({need_sum})ãŒéå¤§"
                )

            non_zero_times = need_df_all[col][need_df_all[col] > 0].index.tolist()
            if non_zero_times:
                log.info(f"[SHORTAGE_DEBUG]   Need>0ã®æ™‚é–“å¸¯: {non_zero_times}")
                for time_slot in non_zero_times[:3]:
                    log.info(
                        f"[SHORTAGE_DEBUG]     {time_slot}: Need={need_df_all.loc[time_slot, col]}, å®Ÿç¸¾={staff_actual_data_all_df.loc[time_slot, col]}"
                    )

    # ----- excess analysis -----
    fp_excess_time = fp_excess_ratio = fp_excess_freq = None
    if "upper" in heat_all_df.columns:
        upper_series_overall_orig = (
            heat_all_df["upper"].reindex(index=time_labels).fillna(0).clip(lower=0)
        )
        upper_df_all = pd.DataFrame(
            np.repeat(
                upper_series_overall_orig.values[:, np.newaxis],
                len(staff_actual_data_all_df.columns),
                axis=1,
            ),
            index=upper_series_overall_orig.index,
            columns=staff_actual_data_all_df.columns,
        )
        parsed_date_list_all = [
            _parse_as_date(c) for c in staff_actual_data_all_df.columns
        ]
        holiday_mask_all = [
            d in estimated_holidays_set if d else False for d in parsed_date_list_all
        ]
        if any(holiday_mask_all):
            for col, is_h in zip(upper_df_all.columns, holiday_mask_all, strict=True):
                if is_h:
                    upper_df_all[col] = 0

        excess_count_overall_df = (
            (staff_actual_data_all_df - upper_df_all)
            .clip(lower=0)
            .fillna(0)
            .astype(int)
        )
        excess_ratio_df = (
            (
                (staff_actual_data_all_df - upper_df_all)
                / upper_df_all.replace(0, np.nan)
            )
            .clip(lower=0)
            .fillna(0)
        )

        fp_excess_time = save_df_parquet(
            excess_count_overall_df,
            out_dir_path / "excess_time.parquet",
            index=True,
        )
        fp_excess_ratio = save_df_parquet(
            excess_ratio_df,
            out_dir_path / "excess_ratio.parquet",
            index=True,
        )

        excess_occurrence_df = (excess_count_overall_df > 0).astype(int)
        excess_freq_df = pd.DataFrame(
            excess_occurrence_df.sum(axis=1), columns=["excess_days"]
        )
        fp_excess_freq = save_df_parquet(
            excess_freq_df,
            out_dir_path / "excess_freq.parquet",
            index=True,
        )

        margin_vs_upper_df = (
            (upper_df_all - staff_actual_data_all_df)
            .clip(lower=0)
            .fillna(0)
            .astype(int)
        )
        save_df_parquet(
            margin_vs_upper_df,
            out_dir_path / "margin_vs_upper_time.parquet",
            index=True,
        )
    else:
        log.warning(
            "[shortage] heat_ALL.xlsx ã« 'upper' åˆ—ãŒãªã„ãŸã‚ excess åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )

    weights = config.get("optimization_weights", {"lack": 0.6, "excess": 0.4})
    w_lack = float(weights.get("lack", 0.6))
    w_excess = float(weights.get("excess", 0.4))
    pen_lack_df = shortage_ratio_df
    pen_excess_df = (
        excess_ratio_df if "upper" in heat_all_df.columns else pen_lack_df * 0
    )
    optimization_score_df = 1 - (w_lack * pen_lack_df + w_excess * pen_excess_df)
    optimization_score_df = optimization_score_df.clip(lower=0, upper=1)
    save_df_parquet(
        optimization_score_df,
        out_dir_path / "optimization_score_time.parquet",
        index=True,
    )

    log.debug(
        "--- shortage_time.xlsx / shortage_ratio.xlsx / shortage_freq.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (å…¨ä½“) çµ‚äº† ---"
    )

    # æŒ‰åˆ†è¨ˆç®—é–¢é€£ã®å¤‰æ•°ã‚’åˆæœŸåŒ–ï¼ˆæŒ‰åˆ†è¨ˆç®—ã¯ä½¿ç”¨ã—ãªã„ï¼‰
    working_data_for_proportional = pd.DataFrame()
    total_shortage_hours_for_proportional = 0.0

    role_kpi_rows: List[Dict[str, Any]] = []
    monthly_role_rows: List[Dict[str, Any]] = []
    processed_role_names_list = []

    for fp_role_heatmap_item in out_dir_path.glob("heat_*.xlsx"):
        if fp_role_heatmap_item.name == "heat_ALL.xlsx":
            continue
        
        # é›‡ç”¨å½¢æ…‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«(heat_emp_*)ã¯è·ç¨®åˆ¥å‡¦ç†ã‹ã‚‰é™¤å¤–
        if fp_role_heatmap_item.name.startswith("heat_emp_"):
            log.info(f"[shortage] ã‚¹ã‚­ãƒƒãƒ—: {fp_role_heatmap_item.name} (é›‡ç”¨å½¢æ…‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ãŸã‚è·ç¨®å‡¦ç†ã‹ã‚‰é™¤å¤–)")
            continue

        role_name_current = fp_role_heatmap_item.stem.replace("heat_", "")
        processed_role_names_list.append(role_name_current)
        log.debug(
            f"--- shortage_role.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (è·ç¨®: {role_name_current}) ---"
        )

        try:
            role_heat_current_df = pd.read_excel(fp_role_heatmap_item, index_col=0)
        except Exception as e_role_heat:
            log.warning(
                f"[shortage] è·ç¨®åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— '{fp_role_heatmap_item.name}' ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e_role_heat}"
            )
            role_kpi_rows.append(
                {
                    "role": role_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "heatmap read error",
                }
            )
            continue

        if "need" not in role_heat_current_df.columns:
            log.warning(
                f"[shortage] è·ç¨® '{role_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã« 'need' åˆ—ãŒä¸è¶³ã€‚KPIè¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ã€‚"
            )
            role_kpi_rows.append(
                {
                    "role": role_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "missing need column",
                }
            )
            continue
        role_need_per_time_series_orig_for_role = (
            role_heat_current_df["need"]
            .reindex(index=time_labels)
            .fillna(0)
            .clip(lower=0)
        )

        role_date_columns_list = [
            str(col)
            for col in role_heat_current_df.columns
            if col not in SUMMARY5 and _parse_as_date(str(col)) is not None
        ]
        if not role_date_columns_list:
            log.warning(
                f"[shortage] è·ç¨® '{role_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«æ—¥ä»˜åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚KPIè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            role_kpi_rows.append(
                {
                    "role": role_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "no date columns",
                }
            )
            continue

        role_staff_actual_data_df = (
            role_heat_current_df[role_date_columns_list]
            .copy()
            .reindex(index=time_labels)
            .fillna(0)
        )

        parsed_role_dates = [
            _parse_as_date(c) for c in role_staff_actual_data_df.columns
        ]
        holiday_mask_role = [
            d in estimated_holidays_set if d else False for d in parsed_role_dates
        ]

        # need_df_role ã®æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ - è·ç¨®åˆ¥å®Ÿéš›ã®Needãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        log.info(f"[shortage] {role_name_current}: è·ç¨®åˆ¥ã®å®Ÿéš›ã®Needãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£ç¢ºãªè¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚")
        
        # è·ç¨®åˆ¥è©³ç´°Needãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        role_safe_name = role_name_current.replace(' ', '_').replace('/', '_').replace('\\', '_')
        role_need_file = out_dir_path / f"need_per_date_slot_role_{role_safe_name}.parquet"
        
        if role_need_file.exists():
            try:
                need_df_role = pd.read_parquet(role_need_file)
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨åˆ—ã‚’é©åˆ‡ã«èª¿æ•´
                need_df_role = need_df_role.reindex(index=time_labels, fill_value=0)
                # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜åˆ—ï¼ˆæ—¥ä»˜ï¼‰ã«èª¿æ•´
                common_columns = set(need_df_role.columns).intersection(set(role_staff_actual_data_df.columns))
                if common_columns:
                    need_df_role = need_df_role[sorted(common_columns)]
                    role_staff_actual_data_df = role_staff_actual_data_df[sorted(common_columns)]
                    log.info(f"[shortage] {role_name_current}: è·ç¨®åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆ{len(common_columns)}æ—¥åˆ†ï¼‰")
                else:
                    log.warning(f"[shortage] {role_name_current}: è·ç¨®åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ã¨å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜åˆ—ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚æŒ‰åˆ†è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æŒ‰åˆ†è¨ˆç®—
                    need_df_role = pd.DataFrame(
                        np.repeat(
                            role_need_per_time_series_orig_for_role.values[:, np.newaxis],
                            len(role_staff_actual_data_df.columns),
                            axis=1,
                        ),
                        index=role_need_per_time_series_orig_for_role.index,
                        columns=role_staff_actual_data_df.columns,
                    )
            except Exception as e:
                log.warning(f"[shortage] {role_name_current}: è·ç¨®åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}. æŒ‰åˆ†è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æŒ‰åˆ†è¨ˆç®—
                need_df_role = pd.DataFrame(
                    np.repeat(
                        role_need_per_time_series_orig_for_role.values[:, np.newaxis],
                        len(role_staff_actual_data_df.columns),
                        axis=1,
                    ),
                    index=role_need_per_time_series_orig_for_role.index,
                    columns=role_staff_actual_data_df.columns,
                )
        else:
            log.warning(f"[shortage] {role_name_current}: è·ç¨®åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{role_need_file}ï¼‰ã€‚æŒ‰åˆ†è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æŒ‰åˆ†è¨ˆç®—
            need_df_role = pd.DataFrame(
                np.repeat(
                    role_need_per_time_series_orig_for_role.values[:, np.newaxis],
                    len(role_staff_actual_data_df.columns),
                    axis=1,
                ),
                index=role_need_per_time_series_orig_for_role.index,
                columns=role_staff_actual_data_df.columns,
            )

        # ä¼‘æ¥­æ—¥ã®Needã‚’0ã«ã™ã‚‹å‡¦ç† (ã“ã‚Œã¯ä¿®æ­£å¾Œã‚‚å¿…è¦)
        if any(holiday_mask_role):
            for c, is_h in zip(need_df_role.columns, holiday_mask_role, strict=True):
                if is_h:
                    need_df_role[c] = 0

        working_cols_role = [
            c
            for c, is_h in zip(
                role_staff_actual_data_df.columns, holiday_mask_role, strict=True
            )
            if not is_h and _parse_as_date(c)
        ]
        num_working_days_for_current_role = len(working_cols_role)

        # ä¿®æ­£ã•ã‚ŒãŸ need_df_role ã‚’ä½¿ã£ã¦ lack ã¨ excess ã‚’è¨ˆç®—ã™ã‚‹
        role_lack_count_for_specific_role_df = (
            need_df_role - role_staff_actual_data_df
        ).clip(lower=0)

        role_excess_count_for_specific_role_df = None
        if "upper" in role_heat_current_df.columns:
            role_upper_per_time_series_orig_for_role = (
                role_heat_current_df["upper"]
                .reindex(index=time_labels)
                .fillna(0)
                .clip(lower=0)
            )
            upper_df_role = pd.DataFrame(
                np.repeat(
                    role_upper_per_time_series_orig_for_role.values[:, np.newaxis],
                    len(role_staff_actual_data_df.columns),
                    axis=1,
                ),
                index=role_upper_per_time_series_orig_for_role.index,
                columns=role_staff_actual_data_df.columns,
            )
            if any(holiday_mask_role):
                for c, is_h in zip(
                    upper_df_role.columns, holiday_mask_role, strict=True
                ):
                    if is_h:
                        upper_df_role[c] = 0
            role_excess_count_for_specific_role_df = (
                role_staff_actual_data_df - upper_df_role
            ).clip(lower=0)
        else:
            log.debug(
                f"[shortage] '{role_name_current}' ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã« 'upper' åˆ—ãŒãªã„ãŸã‚ excess è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—"
            )

        # ã‚µãƒãƒªãƒ¼ç”¨ã®åˆè¨ˆæ™‚é–“ã‚‚ã€ä¿®æ­£ã•ã‚ŒãŸ need_df_role ã‹ã‚‰è¨ˆç®—ã™ã‚‹
        total_need_hours_for_role = need_df_role[working_cols_role].sum().sum() * slot_hours
        # staff_h ã¯å…¨æ—¥ã®å®Ÿç¸¾ã§è¨ˆç®—ï¼ˆä¼‘æ¥­æ—¥ã‚‚å®Ÿç¸¾0ã¨ã—ã¦å«ã¾ã‚Œã‚‹ï¼‰
        total_staff_hours_for_role = role_staff_actual_data_df.sum().sum() * slot_hours
        # lack_h ã¯ä¼‘æ¥­æ—¥ã®need=0ã‚’è€ƒæ…®ã—ãŸlackã®åˆè¨ˆ
        # ä¿®æ­£: äººæ•°ä¸è¶³ Ã— ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ = æ™‚é–“ä¸è¶³ã®æ­£ã—ã„è¨ˆç®—
        total_lack_hours_for_role = (
            (role_lack_count_for_specific_role_df * slot_hours).sum().sum()
        )
        # excess_h ã¯ä¼‘æ¥­æ—¥ã®upper=0ã‚’è€ƒæ…®ã—ãŸexcessã®åˆè¨ˆ
        # ä¿®æ­£: äººæ•°éå‰° Ã— ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ = æ™‚é–“éå‰°ã®æ­£ã—ã„è¨ˆç®—
        total_excess_hours_for_role = (
            (role_excess_count_for_specific_role_df * slot_hours).sum().sum()
            if role_excess_count_for_specific_role_df is not None
            else 0
        )
        # è¨ˆç®—çµæœæ¤œè¨¼ç”¨: need_h - staff_h ã¨ã®å·®åˆ†ãŒlack_hã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
        expected_lack_h = max(total_need_hours_for_role - total_staff_hours_for_role, 0)
        if abs(expected_lack_h - total_lack_hours_for_role) > slot_hours:
            log.debug(
                f"[shortage] mismatch for {role_name_current}: "
                f"need_h={total_need_hours_for_role:.1f}, "
                f"staff_h={total_staff_hours_for_role:.1f}, "
                f"computed lack_h={total_lack_hours_for_role:.1f}, "
                f"expected lack_h={expected_lack_h:.1f}"
            )
            try:
                daily_need_h = (need_df_role.sum() * slot_hours).rename("need_h")
                daily_staff_h = (role_staff_actual_data_df.sum() * slot_hours).rename(
                    "staff_h"
                )
                # ä¿®æ­£: äººæ•°ä¸è¶³ Ã— ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ = æ™‚é–“ä¸è¶³ã®æ­£ã—ã„è¨ˆç®—
                daily_lack_h = (
                    (role_lack_count_for_specific_role_df * slot_hours).sum()
                ).rename("lack_h")
                daily_debug_df = pd.concat(
                    [daily_need_h, daily_staff_h, daily_lack_h], axis=1
                ).assign(diff_h=lambda d: d["need_h"] - d["staff_h"])
                log.debug(
                    f"[shortage] daily summary for {role_name_current} (first 7 days):\n"
                    f"{daily_debug_df.head(7).to_string()}"
                )
            except Exception as e_daily:
                log.debug(
                    f"[shortage] daily debug summary failed for {role_name_current}: {e_daily}"
                )

        # æœˆåˆ¥ä¸è¶³hãƒ»éå‰°hé›†è¨ˆ
        try:
            lack_by_date = role_lack_count_for_specific_role_df.sum()
            lack_by_date.index = pd.to_datetime(lack_by_date.index)
            lack_month = (
                lack_by_date.groupby(lack_by_date.index.to_period("M")).sum()
                * slot_hours
            )
            excess_month = pd.Series(dtype=float)
            if role_excess_count_for_specific_role_df is not None:
                excess_by_date = role_excess_count_for_specific_role_df.sum()
                excess_by_date.index = pd.to_datetime(excess_by_date.index)
                excess_month = (
                    excess_by_date.groupby(excess_by_date.index.to_period("M")).sum()
                    * slot_hours
                )
            month_keys: Dict[str, Dict[str, int]] = {}
            for mon, val in lack_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "role": role_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["lack_h"] = int(round(val))
            for mon, val in excess_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "role": role_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["excess_h"] = int(round(val))
            monthly_role_rows.extend(month_keys.values())
        except Exception as e_month:
            log.debug(f"æœˆåˆ¥ä¸è¶³/éå‰°é›†è¨ˆã‚¨ãƒ©ãƒ¼ ({role_name_current}): {e_month}")

        # ğŸ”§ ãƒ‡ãƒãƒƒã‚°: ç•°å¸¸å€¤ãƒã‚§ãƒƒã‚¯
        if total_lack_hours_for_role > 10000:
            log.warning(f"âš ï¸ [shortage] ç•°å¸¸ãªä¸è¶³æ™‚é–“æ¤œå‡º: {role_name_current}")
            log.warning(f"  total_lack_hours_for_role: {total_lack_hours_for_role:.0f}æ™‚é–“")
            log.warning(f"  slot_hours: {slot_hours:.2f}")
        
        role_kpi_rows.append(
            {
                "role": role_name_current,
                "need_h": int(round(total_need_hours_for_role)),
                "staff_h": int(round(total_staff_hours_for_role)),
                "lack_h": int(round(total_lack_hours_for_role)),
                "excess_h": int(round(total_excess_hours_for_role)),
                "working_days_considered": num_working_days_for_current_role,
            }
        )
        log.debug(
            f"  Role: {role_name_current}, Need(h): {total_need_hours_for_role:.1f} (on {num_working_days_for_current_role} working days), "
            f"Staff(h): {total_staff_hours_for_role:.1f}, Lack(h): {total_lack_hours_for_role:.1f}, Excess(h): {total_excess_hours_for_role:.1f}"
        )
        log.debug(
            f"--- shortage_role.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (è·ç¨®: {role_name_current}) çµ‚äº† ---"
        )

    # æŒ‰åˆ†è¨ˆç®—ã¯ä½¿ç”¨ã—ãªã„ãŸã‚ã€role_shortagesã¯ä½¿ã‚ãªã„
    role_shortages = {}

    # emp_ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    role_kpi_rows_filtered = []
    for row in role_kpi_rows:
        role_name = row.get('role', '')
        if role_name.startswith('emp_'):
            log.warning(f"[shortage] é›‡ç”¨å½¢æ…‹ãƒ‡ãƒ¼ã‚¿ã‚’è·ç¨®ãƒªã‚¹ãƒˆã‹ã‚‰é™¤å¤–: {role_name}")
        else:
            role_kpi_rows_filtered.append(row)
    
    role_summary_df = pd.DataFrame(role_kpi_rows_filtered)
    if not role_summary_df.empty:
        role_summary_df = role_summary_df.sort_values(
            "lack_h", ascending=False, na_position="last"
        ).reset_index(drop=True)
        role_summary_df = role_summary_df.assign(
            estimated_excess_cost=lambda d: d.get("excess_h", 0) * wage_direct,
            estimated_lack_cost_if_temporary_staff=lambda d: d.get("lack_h", 0)
            * wage_temp,
            estimated_lack_penalty_cost=lambda d: d.get("lack_h", 0) * penalty_per_lack,
        )

    monthly_role_df = pd.DataFrame(monthly_role_rows)
    if not monthly_role_df.empty:
        monthly_role_df = monthly_role_df.sort_values(["month", "role"]).reset_index(
            drop=True
        )

    fp_shortage_role = out_dir_path / "shortage_role_summary.parquet"
    shortage_log.info("=== è·ç¨®åˆ¥ä¸è¶³ã‚µãƒãƒªãƒ¼ä¿å­˜ ===")
    shortage_log.info(f"role_summary_df: {len(role_summary_df)}è¡Œ")
    shortage_log.info(f"columns: {list(role_summary_df.columns)}")
    if not role_summary_df.empty:
        shortage_log.info(f"è·ç¨®ä¸€è¦§: {role_summary_df['role'].tolist()}")
        shortage_log.info(f"ä¸è¶³æ™‚é–“åˆè¨ˆ: {role_summary_df['lack_h'].sum():.2f}æ™‚é–“")
        # å„è·ç¨®ã®è©³ç´°
        for _, row in role_summary_df.iterrows():
            shortage_log.info(f"  {row['role']}: {row.get('lack_h', 0):.2f}æ™‚é–“ä¸è¶³")
    role_summary_df.to_parquet(fp_shortage_role, index=False)
    shortage_log.info(f"shortage_role_summary.parquetä¿å­˜å®Œäº†: {fp_shortage_role}")
    if not monthly_role_df.empty:
        monthly_role_df.to_parquet(
            out_dir_path / "shortage_role_monthly.parquet",
            index=False,
        )

    meta_dates_list_shortage = date_columns_in_heat_all
    meta_roles_list_shortage = (
        role_summary_df["role"].tolist()
        if not role_summary_df.empty
        else processed_role_names_list
    )
    meta_months_list_shortage = (
        monthly_role_df["month"].tolist() if not monthly_role_df.empty else []
    )

    # â”€â”€ Employment shortage analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    emp_kpi_rows: List[Dict[str, Any]] = []
    monthly_emp_rows: List[Dict[str, Any]] = []
    processed_emp_names_list = []

    for fp_emp_heatmap_item in out_dir_path.glob("heat_emp_*.xlsx"):
        emp_name_current = fp_emp_heatmap_item.stem.replace("heat_emp_", "")
        processed_emp_names_list.append(emp_name_current)
        log.debug(
            f"--- shortage_employment.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (é›‡ç”¨å½¢æ…‹: {emp_name_current}) ---"
        )
        try:
            emp_heat_current_df = pd.read_excel(fp_emp_heatmap_item, index_col=0)
        except Exception as e_emp_heat:
            log.warning(
                f"[shortage] é›‡ç”¨å½¢æ…‹åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— '{fp_emp_heatmap_item.name}' ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e_emp_heat}"
            )
            emp_kpi_rows.append(
                {
                    "employment": emp_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "heatmap read error",
                }
            )
            continue

        if "need" not in emp_heat_current_df.columns:
            log.warning(
                f"[shortage] é›‡ç”¨å½¢æ…‹ '{emp_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã« 'need' åˆ—ãŒä¸è¶³ã€‚KPIè¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ã€‚"
            )
            emp_kpi_rows.append(
                {
                    "employment": emp_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "missing need column",
                }
            )
            continue

        emp_need_series = (
            emp_heat_current_df["need"]
            .reindex(index=time_labels)
            .fillna(0)
            .clip(lower=0)
        )
        emp_date_columns = [
            str(c)
            for c in emp_heat_current_df.columns
            if c not in SUMMARY5 and _parse_as_date(str(c)) is not None
        ]
        if not emp_date_columns:
            log.warning(
                f"[shortage] é›‡ç”¨å½¢æ…‹ '{emp_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã«æ—¥ä»˜åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚KPIè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            emp_kpi_rows.append(
                {
                    "employment": emp_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "no date columns",
                }
            )
            continue

        emp_staff_df = (
            emp_heat_current_df[emp_date_columns]
            .copy()
            .reindex(index=time_labels)
            .fillna(0)
        )
        parsed_emp_dates = [_parse_as_date(c) for c in emp_staff_df.columns]
        holiday_mask_emp = [
            d in estimated_holidays_set if d else False for d in parsed_emp_dates
        ]
        # need_df_emp ã®æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ - é›‡ç”¨å½¢æ…‹åˆ¥å®Ÿéš›ã®Needãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        log.info(f"[shortage] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥ã®å®Ÿéš›ã®Needãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£ç¢ºãªè¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚")
        
        # é›‡ç”¨å½¢æ…‹åˆ¥è©³ç´°Needãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        emp_safe_name = emp_name_current.replace(' ', '_').replace('/', '_').replace('\\', '_')
        emp_need_file = out_dir_path / f"need_per_date_slot_emp_{emp_safe_name}.parquet"
        
        if emp_need_file.exists():
            try:
                need_df_emp = pd.read_parquet(emp_need_file)
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨åˆ—ã‚’é©åˆ‡ã«èª¿æ•´
                need_df_emp = need_df_emp.reindex(index=time_labels, fill_value=0)
                # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜åˆ—ï¼ˆæ—¥ä»˜ï¼‰ã«èª¿æ•´
                common_columns = set(need_df_emp.columns).intersection(set(emp_staff_df.columns))
                if common_columns:
                    need_df_emp = need_df_emp[sorted(common_columns)]
                    emp_staff_df = emp_staff_df[sorted(common_columns)]
                    log.info(f"[shortage] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£ç¢ºãªãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆ{len(common_columns)}æ—¥åˆ†ï¼‰")
                else:
                    log.warning(f"[shortage] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ã¨å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜åˆ—ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚æŒ‰åˆ†è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æŒ‰åˆ†è¨ˆç®—
                    need_df_emp = pd.DataFrame(
                        np.repeat(
                            emp_need_series.values[:, np.newaxis], len(emp_staff_df.columns), axis=1
                        ),
                        index=emp_need_series.index,
                        columns=emp_staff_df.columns,
                    )
            except Exception as e:
                log.warning(f"[shortage] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}. æŒ‰åˆ†è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æŒ‰åˆ†è¨ˆç®—
                need_df_emp = pd.DataFrame(
                    np.repeat(
                        emp_need_series.values[:, np.newaxis], len(emp_staff_df.columns), axis=1
                    ),
                    index=emp_need_series.index,
                    columns=emp_staff_df.columns,
                )
        else:
            log.warning(f"[shortage] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥Needãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{emp_need_file}ï¼‰ã€‚æŒ‰åˆ†è¨ˆç®—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æŒ‰åˆ†è¨ˆç®—
            need_df_emp = pd.DataFrame(
                np.repeat(
                    emp_need_series.values[:, np.newaxis], len(emp_staff_df.columns), axis=1
                ),
                index=emp_need_series.index,
                columns=emp_staff_df.columns,
            )

        if any(holiday_mask_emp):
            for c, is_h in zip(need_df_emp.columns, holiday_mask_emp, strict=True):
                if is_h:
                    need_df_emp[c] = 0

        working_cols_emp = [
            c
            for c, is_h in zip(emp_staff_df.columns, holiday_mask_emp, strict=True)
            if not is_h and _parse_as_date(c)
        ]
        num_working_days_for_current_emp = len(working_cols_emp)

        lack_count_emp_df = (need_df_emp - emp_staff_df).clip(lower=0)

        # excess_count_emp_dfã®è¨ˆç®—ã«èª¤ã‚ŠãŒã‚ã£ãŸãŸã‚ä¿®æ­£ (needã§ã¯ãªãupperã¨æ¯”è¼ƒ)
        excess_count_emp_df = pd.DataFrame()
        if "upper" in emp_heat_current_df.columns:
             upper_series_emp = emp_heat_current_df["upper"].reindex(index=time_labels).fillna(0).clip(lower=0)
             upper_df_emp = pd.DataFrame(
                 np.repeat(
                     upper_series_emp.values[:, np.newaxis], len(emp_staff_df.columns), axis=1
                 ),
                 index=upper_series_emp.index,
                 columns=emp_staff_df.columns,
             )
             if any(holiday_mask_emp):
                 for c, is_h in zip(upper_df_emp.columns, holiday_mask_emp, strict=True):
                     if is_h:
                         upper_df_emp[c] = 0
             excess_count_emp_df = (emp_staff_df - upper_df_emp).clip(lower=0)


        # ã‚µãƒãƒªãƒ¼ç”¨ã®åˆè¨ˆæ™‚é–“ã‚‚ã€ä¿®æ­£ã•ã‚ŒãŸ need_df_emp ã‹ã‚‰è¨ˆç®—ã™ã‚‹
        total_need_hours_for_emp = need_df_emp[working_cols_emp].sum().sum() * slot_hours
        total_staff_hours_for_emp = emp_staff_df.sum().sum() * slot_hours
        # ä¿®æ­£: äººæ•°ä¸è¶³ Ã— ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ = æ™‚é–“ä¸è¶³ã®æ­£ã—ã„è¨ˆç®—
        total_lack_hours_for_emp = (lack_count_emp_df * slot_hours).sum().sum()
        # ä¿®æ­£: äººæ•°éå‰° Ã— ã‚¹ãƒ­ãƒƒãƒˆæ™‚é–“ = æ™‚é–“éå‰°ã®æ­£ã—ã„è¨ˆç®—  
        total_excess_hours_for_emp = (
            (excess_count_emp_df * slot_hours).sum().sum()
            if not excess_count_emp_df.empty
            else 0
        )

        try:
            lack_by_date = lack_count_emp_df.sum()
            lack_by_date.index = pd.to_datetime(lack_by_date.index)
            lack_month = (
                lack_by_date.groupby(lack_by_date.index.to_period("M")).sum()
                * slot_hours
            )
            excess_month = pd.Series(dtype=float)
            if not excess_count_emp_df.empty:
                excess_by_date = excess_count_emp_df.sum()
                excess_by_date.index = pd.to_datetime(excess_by_date.index)
                excess_month = (
                    excess_by_date.groupby(excess_by_date.index.to_period("M")).sum()
                    * slot_hours
                )
            month_keys: Dict[str, Dict[str, int]] = {}
            for mon, val in lack_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "employment": emp_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["lack_h"] = int(round(val))
            for mon, val in excess_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "employment": emp_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["excess_h"] = int(round(val))
            monthly_emp_rows.extend(month_keys.values())
        except Exception as e_month_emp:
            log.debug(f"æœˆåˆ¥ä¸è¶³/éå‰°é›†è¨ˆã‚¨ãƒ©ãƒ¼ ({emp_name_current}): {e_month_emp}")

        emp_kpi_rows.append(
            {
                "employment": emp_name_current,
                "need_h": int(round(total_need_hours_for_emp)),
                "staff_h": int(round(total_staff_hours_for_emp)),
                "lack_h": int(round(total_lack_hours_for_emp)),
                "excess_h": int(round(total_excess_hours_for_emp)),
                "working_days_considered": num_working_days_for_current_emp,
            }
        )
        log.debug(
            f"  Employment: {emp_name_current}, Need(h): {total_need_hours_for_emp:.1f} (on {num_working_days_for_current_emp} working days), "
            f"Staff(h): {total_staff_hours_for_emp:.1f}, Lack(h): {total_lack_hours_for_emp:.1f}, Excess(h): {total_excess_hours_for_emp:.1f}"
        )
        log.debug(
            f"--- shortage_employment.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (é›‡ç”¨å½¢æ…‹: {emp_name_current}) çµ‚äº† ---"
        )


    emp_summary_df = pd.DataFrame(emp_kpi_rows)
    if not emp_summary_df.empty:
        emp_summary_df = emp_summary_df.sort_values(
            "lack_h", ascending=False, na_position="last"
        ).reset_index(drop=True)
        emp_summary_df = emp_summary_df.assign(
            estimated_excess_cost=lambda d: d.get("excess_h", 0) * wage_direct,
            estimated_lack_cost_if_temporary_staff=lambda d: d.get("lack_h", 0)
            * wage_temp,
            estimated_lack_penalty_cost=lambda d: d.get("lack_h", 0) * penalty_per_lack,
        )

    monthly_emp_df = pd.DataFrame(monthly_emp_rows)
    if not monthly_emp_df.empty:
        monthly_emp_df = monthly_emp_df.sort_values(
            ["month", "employment"]
        ).reset_index(drop=True)

    fp_shortage_emp = out_dir_path / "shortage_employment_summary.parquet"
    shortage_log.info("=== é›‡ç”¨å½¢æ…‹åˆ¥ä¸è¶³ã‚µãƒãƒªãƒ¼ä¿å­˜ ===")
    shortage_log.info(f"emp_summary_df: {len(emp_summary_df)}è¡Œ")
    shortage_log.info(f"columns: {list(emp_summary_df.columns)}")
    if not emp_summary_df.empty:
        shortage_log.info(f"é›‡ç”¨å½¢æ…‹ä¸€è¦§: {emp_summary_df['employment'].tolist()}")
        shortage_log.info(f"ä¸è¶³æ™‚é–“åˆè¨ˆ: {emp_summary_df['lack_h'].sum():.2f}æ™‚é–“")
        # å„é›‡ç”¨å½¢æ…‹ã®è©³ç´°
        for _, row in emp_summary_df.iterrows():
            shortage_log.info(f"  {row['employment']}: {row.get('lack_h', 0):.2f}æ™‚é–“ä¸è¶³")
    emp_summary_df.to_parquet(fp_shortage_emp, index=False)
    shortage_log.info(f"shortage_employment_summary.parquetä¿å­˜å®Œäº†: {fp_shortage_emp}")
    if not monthly_emp_df.empty:
        monthly_emp_df.to_parquet(
            out_dir_path / "shortage_employment_monthly.parquet",
            index=False,
        )

    meta_employments_list_shortage = (
        emp_summary_df["employment"].tolist()
        if not emp_summary_df.empty
        else processed_emp_names_list
    )
    meta_months_list_shortage.extend(
        monthly_emp_df["month"].tolist() if not monthly_emp_df.empty else []
    )

    write_meta(
        out_dir_path / "shortage.meta.json",
        slot=slot,
        dates=sorted(list(set(meta_dates_list_shortage))),
        roles=sorted(list(set(meta_roles_list_shortage))),
        employments=sorted(list(set(meta_employments_list_shortage))),
        months=sorted(list(set(meta_months_list_shortage))),
        ratio_file="shortage_ratio.parquet",
        freq_file="shortage_freq.parquet",
        excess_ratio_file="excess_ratio.parquet" if fp_excess_ratio else None,
        excess_freq_file="excess_freq.parquet" if fp_excess_freq else None,
        estimated_holidays_used=[
            d.isoformat() for d in sorted(list(estimated_holidays_set))
        ],
    )

    # â”€â”€ text summary output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    summary_fp = out_dir_path / "shortage_summary.txt"
    try:
        total_lack_h = int(round(role_summary_df.get("lack_h", pd.Series()).sum()))
        total_excess_h = int(round(role_summary_df.get("excess_h", pd.Series()).sum()))
        summary_lines = [
            f"total_lack_hours: {total_lack_h}",
            f"total_excess_hours: {total_excess_h}",
        ]
        summary_fp.write_text("\n".join(summary_lines) + "\n", encoding="utf-8")
    except Exception as e:  # noqa: BLE001
        log.debug(f"failed writing shortage summary text: {e}")

    log.info(
        (
            f"[shortage] completed -- shortage_time â†’ {fp_shortage_time.name}, "
            f"shortage_ratio â†’ {fp_shortage_ratio.name}, "
            f"shortage_freq â†’ {fp_shortage_freq.name}, "
            f"shortage_role â†’ {fp_shortage_role.name}, "
            f"shortage_employment â†’ {fp_shortage_emp.name}, "
        )
        + (f"excess_time â†’ {fp_excess_time.name}, " if fp_excess_time else "")
        + (f"excess_ratio â†’ {fp_excess_ratio.name}, " if fp_excess_ratio else "")
        + (f"excess_freq â†’ {fp_excess_freq.name}" if fp_excess_freq else "")
    )
    
    # ğŸ¯ ä¿®æ­£: æœ€é©æ¡ç”¨è¨ˆç”»ã«å¿…è¦ãªã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    try:
        # shortage_weekday_timeslot_summary.parquet ã‚’ç”Ÿæˆ
        if fp_shortage_time and fp_shortage_time.exists():
            weekday_summary_df = weekday_timeslot_summary(out_dir_path)
            weekday_summary_path = out_dir_path / "shortage_weekday_timeslot_summary.parquet"
            weekday_summary_df.to_parquet(weekday_summary_path, index=False)
            log.info(f"[shortage] æ›œæ—¥åˆ¥ã‚¿ã‚¤ãƒ ã‚¹ãƒ­ãƒƒãƒˆã‚µãƒãƒªãƒ¼ç”Ÿæˆ: {weekday_summary_path.name}")
        else:
            log.warning("[shortage] shortage_time.parquetãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ›œæ—¥åˆ¥ã‚µãƒãƒªãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    except Exception as e:
        log.error(f"[shortage] æ›œæ—¥åˆ¥ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®è©³ç´°ãƒ­ã‚°ã‚’ç”Ÿæˆ
    try:
        # åˆ†æçµæœã‚’ã¾ã¨ã‚ã‚‹
        total_need_h = role_summary_df.get("need_h", pd.Series()).sum() if not role_summary_df.empty else 0
        total_staff_h = role_summary_df.get("staff_h", pd.Series()).sum() if not role_summary_df.empty else 0
        total_lack_h = role_summary_df.get("lack_h", pd.Series()).sum() if not role_summary_df.empty else 0
        total_excess_h = role_summary_df.get("excess_h", pd.Series()).sum() if not role_summary_df.empty else 0
        working_days = role_summary_df.get("working_days_considered", pd.Series()).max() if not role_summary_df.empty else 0
        
        analysis_results = {
            'total_summary': {
                'total_need_h': total_need_h,
                'total_staff_h': total_staff_h,
                'total_lack_h': total_lack_h,
                'total_excess_h': total_excess_h,
                'working_days': working_days
            },
            'role_summary': role_kpi_rows,
            'employment_summary': emp_kpi_rows,
            'calculation_method': {
                'method': 'è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥å®Ÿéš›Needãƒ™ãƒ¼ã‚¹ï¼ˆæŒ‰åˆ†è¨ˆç®—ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰',
                'used_proportional': 'ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ã¿',
                'used_actual_need_files': 'ã‚ã‚Š',
                'holiday_exclusion': 'ã‚ã‚Š'
            },
            'file_info': {
                'shortage_time': fp_shortage_time.name if fp_shortage_time else 'N/A',
                'shortage_role': fp_shortage_role.name if fp_shortage_role else 'N/A',
                'shortage_employment': fp_shortage_emp.name if fp_shortage_emp else 'N/A',
                'shortage_ratio': fp_shortage_ratio.name if fp_shortage_ratio else 'N/A',
                'shortage_freq': fp_shortage_freq.name if fp_shortage_freq else 'N/A'
            },
            'warnings': [],
            'errors': []
        }
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ ï¼‰
        analysis_results['calculation_details'] = calculation_details
        create_timestamped_log(analysis_results, out_dir_path)
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºã‚’å®Ÿè¡Œ
        try:
            from shift_suite.tasks.real_time_insight_detector import RealTimeInsightDetector
            
            # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            intermediate_path = out_dir_path / 'intermediate_data.parquet'
            shortage_role_path = fp_shortage_role if fp_shortage_role else None
            
            if intermediate_path.exists() and shortage_role_path and shortage_role_path.exists():
                intermediate_df = pd.read_parquet(intermediate_path)
                shortage_df = pd.read_parquet(shortage_role_path)
                
                # æ´å¯Ÿæ¤œå‡ºå™¨ã‚’åˆæœŸåŒ–
                detector = RealTimeInsightDetector()
                
                # æ´å¯Ÿã‚’æ¤œå‡º
                insights = detector.analyze_shortage_data(
                    shortage_data=shortage_df,
                    intermediate_data=intermediate_df,
                    need_data=None  # å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
                )
                
                # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                insight_report_path = out_dir_path / 'real_time_insights.json'
                report = detector.generate_insight_report(insight_report_path)
                
                # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
                summary = detector.generate_executive_summary()
                summary_path = out_dir_path / 'insight_executive_summary.txt'
                with open(summary_path, 'w', encoding='utf-8') as f:
                    f.write(summary)
                
                log.info(f"[INSIGHTS] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºå®Œäº†: {len(insights)}å€‹ã®æ´å¯Ÿã‚’ç™ºè¦‹")
                
                # é‡è¦ãªæ´å¯Ÿã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                critical_insights = [i for i in insights if i.severity.value in ['critical', 'high']]
                for insight in critical_insights[:5]:
                    log.warning(f"[CRITICAL_INSIGHT] {insight.title}: {insight.description}")
                    if insight.financial_impact:
                        log.warning(f"  è²¡å‹™å½±éŸ¿: {insight.financial_impact:.1f}ä¸‡å††/æœˆ")
                    if insight.recommended_action:
                        log.warning(f"  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {insight.recommended_action}")
                
                # åˆ†æçµæœã«æ´å¯Ÿæƒ…å ±ã‚’è¿½åŠ 
                analysis_results['insights'] = {
                    'total_count': len(insights),
                    'critical_count': sum(1 for i in insights if i.severity.value == 'critical'),
                    'high_count': sum(1 for i in insights if i.severity.value == 'high'),
                    'total_financial_impact': report['total_financial_impact'],
                    'report_path': str(insight_report_path),
                    'summary_path': str(summary_path)
                }
                
        except Exception as e:
            log.error(f"[INSIGHTS] ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ´å¯Ÿæ¤œå‡ºã§ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ã§ã‚‚å‡¦ç†ã¯ç¶™ç¶š
        
    except Exception as e:
        log.error(f"[shortage] ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ­ã‚°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    if fp_shortage_time and fp_shortage_role and fp_shortage_ratio and fp_shortage_freq:
        return fp_shortage_time, fp_shortage_role
    return None


def merge_shortage_leave(
    out_dir: Path | str,
    *,
    shortage_xlsx: str | Path = "shortage_time.parquet",
    leave_csv: str | Path = "leave_analysis.csv",
    out_excel: str | Path = "shortage_leave.csv",
) -> Path | None:
    """Combine shortage_time.parquet with leave counts.

    Parameters
    ----------
    out_dir:
        Directory containing shortage and leave files.
    shortage_xlsx:
        Name of ``shortage_time.parquet``. Must exist under ``out_dir``.
    leave_csv:
        Optional ``leave_analysis.csv`` with columns ``date`` and
        ``total_leave_days``. If missing, leave counts are treated as ``0``.
    out_excel:
        Output CSV filename.

    Returns
    -------
    Path | None
        Path to the saved CSV file or ``None`` if shortage data missing.
    """

    out_dir_path = Path(out_dir)
    shortage_fp = out_dir_path / shortage_xlsx
    if not shortage_fp.exists():
        log.error(f"[shortage] {shortage_fp} not found")
        return None

    try:
        shortage_df = pd.read_parquet(shortage_fp)
    except Exception as e:
        log.error(f"[shortage] failed to read {shortage_fp}: {e}")
        return None

    # Convert wide timeÃ—date to long format
    long_df = shortage_df.stack().reset_index()
    long_df.columns = ["time", "date", "lack"]
    long_df["date"] = pd.to_datetime(long_df["date"])

    leave_fp = out_dir_path / leave_csv
    if leave_fp.exists():
        try:
            leave_df = pd.read_csv(leave_fp, parse_dates=["date"])
            leave_sum = (
                leave_df.groupby("date")["total_leave_days"]
                .sum()
                .astype(int)
                .reset_index()
            )
            long_df = long_df.merge(leave_sum, on="date", how="left")
            long_df.rename(
                columns={"total_leave_days": "leave_applicants"}, inplace=True
            )
        except Exception as e:
            log.warning(f"[shortage] leave_csv load failed: {e}")
            long_df["leave_applicants"] = 0
    else:
        long_df["leave_applicants"] = 0

    long_df["leave_applicants"] = long_df["leave_applicants"].fillna(0).astype(int)
    long_df["net_shortage"] = (long_df["lack"] - long_df["leave_applicants"]).clip(
        lower=0
    )

    out_fp = out_dir_path / out_excel
    long_df.to_csv(out_fp, index=False)
    return out_fp


def _summary_by_period(df: pd.DataFrame, *, period: str) -> pd.DataFrame:
    """Return average counts by *period* and time slot.

    Parameters
    ----------
    df:
        DataFrame loaded from ``shortage_time.xlsx`` or ``excess_time.xlsx``.
    period:
        ``"weekday"`` or ``"month_period"``.

    Returns
    -------
    pd.DataFrame
        Aggregated average counts per time slot.
    """

    date_cols = [c for c in df.columns if _parse_as_date(str(c)) is not None]
    if not date_cols:
        return pd.DataFrame(columns=[period, "timeslot", "avg_count"])

    data = df[date_cols].copy()
    data.columns = pd.to_datetime(data.columns)
    df_for_melt = data.reset_index()
    # reset_index()ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸæœ€åˆã®åˆ—ï¼ˆ=å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã®åå‰ã‚’å‹•çš„ã«å–å¾—ã™ã‚‹
    index_col_name = df_for_melt.columns[0]
    long = df_for_melt.melt(
        id_vars=[index_col_name], var_name="date", value_name="count"
    )
    long.rename(columns={index_col_name: "timeslot"}, inplace=True)

    long["date"] = pd.to_datetime(long["date"])

    if period == "weekday":
        day_name_map = {
            "Monday": "æœˆæ›œæ—¥",
            "Tuesday": "ç«æ›œæ—¥",
            "Wednesday": "æ°´æ›œæ—¥",
            "Thursday": "æœ¨æ›œæ—¥",
            "Friday": "é‡‘æ›œæ—¥",
            "Saturday": "åœŸæ›œæ—¥",
            "Sunday": "æ—¥æ›œæ—¥",
        }
        long[period] = long["date"].dt.day_name().map(day_name_map)
        order = list(day_name_map.values())
    elif period == "month_period":

        def _mp(day_val: int) -> str:
            if day_val <= 10:
                return "æœˆåˆ(1-10æ—¥)"
            if day_val <= 20:
                return "æœˆä¸­(11-20æ—¥)"
            return "æœˆæœ«(21-æœ«æ—¥)"

        long[period] = long["date"].dt.day.apply(_mp)
        order = ["æœˆåˆ(1-10æ—¥)", "æœˆä¸­(11-20æ—¥)", "æœˆæœ«(21-æœ«æ—¥)"]
    else:  # pragma: no cover - invalid option
        raise ValueError("period must be 'weekday' or 'month_period'")

    grouped = (
        long.groupby([period, "timeslot"], observed=False)["count"]
        .mean()
        .reset_index(name="avg_count")
    )
    grouped[period] = pd.Categorical(grouped[period], categories=order, ordered=True)
    return grouped.sort_values([period, "timeslot"]).reset_index(drop=True)


def weekday_timeslot_summary(
    out_dir: Path | str, *, excel: str = "shortage_time.parquet"
) -> pd.DataFrame:
    """Return average shortage counts by weekday and time slot."""

    df = pd.read_parquet(Path(out_dir) / excel)
    return _summary_by_period(df, period="weekday")


def monthperiod_timeslot_summary(
    out_dir: Path | str, *, excel: str = "shortage_time.parquet"
) -> pd.DataFrame:
    """Return average shortage counts by month period and time slot."""

    df = pd.read_parquet(Path(out_dir) / excel)
    return _summary_by_period(df, period="month_period")


def assign_shortage_to_individuals(
    actual_df: pd.DataFrame,
    shortage_df: pd.DataFrame,
    time_unit_minutes: int
) -> pd.DataFrame:
    """å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã«3ã‚·ãƒŠãƒªã‚ªåˆ†ã®ä¸è¶³å€¤ã‚’å‰²ã‚Šå½“ã¦ã‚‹ã€‚

    Args:
        actual_df (pd.DataFrame): å€‹ã€…ã®å‹¤å‹™è¨˜éŒ²ã‚’å«ã‚€å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã€‚
        shortage_df (pd.DataFrame): ``calculate_time_axis_shortage`` ã®çµæœã€‚
        time_unit_minutes (int): æ™‚é–“ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã«ç”¨ã„ã‚‹å˜ä½ï¼ˆåˆ†ï¼‰ã€‚

    Returns:
        pd.DataFrame: ä¸è¶³å€¤ã‚’åˆ—ã¨ã—ã¦è¿½åŠ ã—ãŸå®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã€‚
    """
    freq = f"{time_unit_minutes}min"
    df = actual_df.copy()
    df['time_group'] = df['timestamp'].dt.floor(freq)

    # ã‚«ãƒ©ãƒ åã®æ­£è¦åŒ–ï¼šè‹±èªâ†’æ—¥æœ¬èªã«çµ±ä¸€
    column_mapping = {
        'role': 'è·ç¨®',
        'employment': 'é›‡ç”¨å½¢æ…‹',
        'employment_type': 'é›‡ç”¨å½¢æ…‹'
    }
    
    # å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ æ­£è¦åŒ–
    rename_dict = {}
    for eng_col, jp_col in column_mapping.items():
        if eng_col in df.columns:
            rename_dict[eng_col] = jp_col
    
    if rename_dict:
        df = df.rename(columns=rename_dict)
    
    # ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ æ­£è¦åŒ–
    shortage_df_normalized = shortage_df.copy()
    shortage_rename_dict = {}
    for eng_col, jp_col in column_mapping.items():
        if eng_col in shortage_df_normalized.columns:
            shortage_rename_dict[eng_col] = jp_col
    
    if shortage_rename_dict:
        shortage_df_normalized = shortage_df_normalized.rename(columns=shortage_rename_dict)
    
    # å¿…é ˆã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    required_columns = ['è·ç¨®', 'é›‡ç”¨å½¢æ…‹']
    for col in required_columns:
        if col not in df.columns:
            if col == 'è·ç¨®':
                df['è·ç¨®'] = 'unknown_role'
            elif col == 'é›‡ç”¨å½¢æ…‹':
                df['é›‡ç”¨å½¢æ…‹'] = 'unknown_employment'
        
        if col not in shortage_df_normalized.columns:
            if col == 'è·ç¨®':
                shortage_df_normalized['è·ç¨®'] = 'unknown_role'
            elif col == 'é›‡ç”¨å½¢æ…‹':
                shortage_df_normalized['é›‡ç”¨å½¢æ…‹'] = 'unknown_employment'

    merge_cols = ['time_group', 'è·ç¨®', 'é›‡ç”¨å½¢æ…‹']
    shortage_cols = ['shortage_mean', 'shortage_median', 'shortage_p25']
    cols_to_add = ['actual_count'] + shortage_cols

    merged = df.merge(
        shortage_df_normalized[merge_cols + cols_to_add],
        on=merge_cols,
        how='left'
    ).fillna({col: 0 for col in cols_to_add})

    return merged
