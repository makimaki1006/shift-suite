"""
shortage.py â€“ v2.7.0 (æœ€çµ‚ä¿®æ­£ç‰ˆ)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* v2.7.0: å…¨ä½“ã®ä¸è¶³è¨ˆç®—(shortage_time)ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€è©³ç´°Needãƒ•ã‚¡ã‚¤ãƒ«
          (need_per_date_slot.parquet)ã‚’æœ€å„ªå…ˆã§åˆ©ç”¨ã™ã‚‹ã‚ˆã†å…¨é¢çš„ã«åˆ·æ–°ã€‚
          ã“ã‚Œã«ã‚ˆã‚Šã€ä¼‘æ—¥ã®éŽå‰°ãªä¸è¶³è¨ˆä¸Šå•é¡Œã‚’å®Œå…¨ã«è§£æ±ºã™ã‚‹ã€‚
"""

from __future__ import annotations

import datetime as dt
from pathlib import Path
from typing import Any, Dict, Iterable, List, Set, Tuple

import json

import numpy as np
import pandas as pd

from .. import config
from .constants import SUMMARY5
from .utils import _parse_as_date, gen_labels, log, save_df_parquet, write_meta


def shortage_and_brief(
    out_dir: Path | str,
    slot: int,
    *,
    holidays: Iterable[dt.date] | None = None,
    include_zero_days: bool = True,
    wage_direct: float = 0.0,
    wage_temp: float = 0.0,
    penalty_per_lack: float = 0.0,
) -> Tuple[Path, Path] | None:
    """Run shortage analysis and KPI summary.

    Parameters
    ----------
    out_dir:
        Output directory containing heatmap files.
    slot:
        Slot size in minutes.
    holidays:
        Deprecated. The value is ignored; holidays are read from
        ``heatmap.meta.json`` generated by ``build_heatmap``.
    wage_direct:
        Hourly wage for direct employees used for excess cost estimation.
    wage_temp:
        Hourly cost for temporary staff to fill shortages.
    penalty_per_lack:
        Penalty or opportunity cost per hour of shortage.
    """
    out_dir_path = Path(out_dir)
    time_labels = gen_labels(slot)
    slot_hours = slot / 60.0

    estimated_holidays_set: Set[dt.date] = set()
    log.info("[shortage] v2.7.0 å‡¦ç†é–‹å§‹")

    try:
        heat_all_df = pd.read_parquet(out_dir_path / "heat_ALL.parquet")
    except FileNotFoundError:
        log.error("[shortage] heat_ALL.parquet ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™ã€‚")
        return None
    except Exception as e:
        log.error(
            f"[shortage] heat_ALL.parquet ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True
        )
        return None

    # --- â–¼â–¼â–¼â–¼â–¼ ã“ã“ã‹ã‚‰ãŒé‡è¦ãªä¿®æ­£ç®‡æ‰€ â–¼â–¼â–¼â–¼â–¼ ---

    # æ—¥ä»˜ã”ã¨ã®è©³ç´°Needãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    need_per_date_slot_df = pd.DataFrame()
    need_per_date_slot_fp = out_dir_path / "need_per_date_slot.parquet"
    if need_per_date_slot_fp.exists():
        try:
            need_per_date_slot_df = pd.read_parquet(need_per_date_slot_fp)
            log.info(
                "[shortage] â˜†â˜†â˜† need_per_date_slot.parquet ã‚’èª­ã¿è¾¼ã¿ã€ã“ã‚Œã‚’ãƒžã‚¹ã‚¿ãƒ¼Needã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ â˜†â˜†â˜†"
            )
        except Exception as e:
            log.warning(f"[shortage] need_per_date_slot.parquet ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # heat_ALL.parquetã‹ã‚‰æ—¥ä»˜åˆ—ã‚’ç‰¹å®š
    date_columns_in_heat_all = [
        str(col)
        for col in heat_all_df.columns
        if col not in SUMMARY5 and _parse_as_date(str(col)) is not None
    ]
    if not date_columns_in_heat_all:
        log.warning("[shortage] heat_ALL.parquet ã«æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        # å‡¦ç†ã‚’ä¸­æ–­ã›ãšã«ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        empty_df = pd.DataFrame(index=time_labels)
        fp_s_t_empty = save_df_parquet(
            empty_df, out_dir_path / "shortage_time.parquet", index=True
        )
        fp_s_r_empty = save_df_parquet(
            pd.DataFrame(), out_dir_path / "shortage_role.parquet", index=False
        )
        return (fp_s_t_empty, fp_s_r_empty) if fp_s_t_empty and fp_s_r_empty else None

    # å®Ÿç¸¾ã‚¹ã‚¿ãƒƒãƒ•æ•°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
    staff_actual_data_all_df = (
        heat_all_df[date_columns_in_heat_all]
        .copy()
        .reindex(index=time_labels)
        .fillna(0)
    )

    # heatmap.meta.jsonã‹ã‚‰ä¼‘æ¥­æ—¥æƒ…å ±ã‚’å–å¾—
    meta_fp = out_dir_path / "heatmap.meta.json"
    if meta_fp.exists():
        try:
            meta = json.loads(meta_fp.read_text(encoding="utf-8"))
            estimated_holidays_set.update(
                {
                    d
                    for d in (
                        _parse_as_date(h) for h in meta.get("estimated_holidays", [])
                    )
                    if d
                }
            )
            log.info(
                f"[SHORTAGE_DEBUG] heatmap.meta.json ã‹ã‚‰èª­ã¿è¾¼ã‚“ã ä¼‘æ¥­æ—¥æ•°: {len(estimated_holidays_set)}"
            )
        except Exception as e_meta:
            log.warning(f"[shortage] heatmap.meta.json è§£æžã‚¨ãƒ©ãƒ¼: {e_meta}")

    # å…¨ä½“ã®Need DataFrameã‚’æ§‹ç¯‰
    if not need_per_date_slot_df.empty:
        # ã€æœ€é‡è¦ä¿®æ­£ã€‘è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹
        log.info("[shortage] è©³ç´°Needãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€å…¨ä½“ã®Needã‚’å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
        need_df_all = need_per_date_slot_df.reindex(
            columns=staff_actual_data_all_df.columns, fill_value=0
        )
        need_df_all = need_df_all.reindex(index=time_labels, fill_value=0)
    else:
        # ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‘è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã€å¾“æ¥ã®æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã§è¨ˆç®—
        log.warning("[shortage] è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€å¾“æ¥ã®æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãNeedã‚’è¨ˆç®—ã—ã¾ã™ã€‚")
        dow_need_pattern_df = pd.DataFrame()
        if meta_fp.exists():
            meta = json.loads(meta_fp.read_text(encoding="utf-8"))
            pattern_records = meta.get("dow_need_pattern", [])
            if pattern_records:
                tmp_df = pd.DataFrame(pattern_records).set_index("time")
                tmp_df.columns = tmp_df.columns.astype(int)
                dow_need_pattern_df = tmp_df

        need_df_all = pd.DataFrame(
            index=time_labels, columns=staff_actual_data_all_df.columns, dtype=float
        )
        parsed_date_list_all = [
            _parse_as_date(c) for c in staff_actual_data_all_df.columns
        ]
        for col, d in zip(need_df_all.columns, parsed_date_list_all, strict=True):
            is_holiday = d in estimated_holidays_set if d else False
            if is_holiday:
                need_df_all[col] = 0
                continue
            dow_col = d.weekday() if d else None
            if d and not dow_need_pattern_df.empty and dow_col in dow_need_pattern_df.columns:
                need_df_all[col] = (
                    dow_need_pattern_df[dow_col].reindex(index=time_labels).fillna(0)
                )
            else:
                need_df_all[col] = 0

    # --- â–²â–²â–²â–²â–² ã“ã“ã¾ã§ãŒé‡è¦ãªä¿®æ­£ç®‡æ‰€ â–²â–²â–²â–²â–² ---

    lack_count_overall_df = (
        (need_df_all - staff_actual_data_all_df).clip(lower=0).fillna(0)
    )
    shortage_ratio_df = (
        ((need_df_all - staff_actual_data_all_df) / need_df_all.replace(0, np.nan))
        .clip(lower=0)
        .fillna(0)
    )

    fp_shortage_time = save_df_parquet(
        lack_count_overall_df,
        out_dir_path / "shortage_time.parquet",
        index=True,
    )
    fp_shortage_ratio = save_df_parquet(
        shortage_ratio_df,
        out_dir_path / "shortage_ratio.parquet",
        index=True,
    )

    shortage_freq_df = pd.DataFrame(
        (lack_count_overall_df > 0).sum(axis=1), columns=["shortage_days"]
    )
    fp_shortage_freq = save_df_parquet(
        shortage_freq_df,
        out_dir_path / "shortage_freq.parquet",
        index=True,
    )

    surplus_vs_need_df = (
        (staff_actual_data_all_df - need_df_all).clip(lower=0).fillna(0).astype(int)
    )
    save_df_parquet(
        surplus_vs_need_df,
        out_dir_path / "surplus_vs_need_time.parquet",
        index=True,
    )

    sunday_columns = [
        col
        for col in lack_count_overall_df.columns
        if _parse_as_date(col) and _parse_as_date(col).weekday() == 6
    ]

    if sunday_columns:
        log.info("[SHORTAGE_DEBUG] ========== æ—¥æ›œæ—¥ã®ä¸è¶³åˆ†æž ==========")
        log.info(f"[SHORTAGE_DEBUG] å¯¾è±¡æ—¥æ›œæ—¥: {sunday_columns}")

        for col in sunday_columns[:3]:
            actual_sum = staff_actual_data_all_df[col].sum()
            need_sum = need_df_all[col].sum()
            lack_sum = lack_count_overall_df[col].sum()
            is_holiday = _parse_as_date(col) in estimated_holidays_set

            log.info(f"[SHORTAGE_DEBUG] {col}:")
            log.info(f"[SHORTAGE_DEBUG]   ä¼‘æ¥­æ—¥={is_holiday}")
            log.info(f"[SHORTAGE_DEBUG]   å®Ÿç¸¾åˆè¨ˆ: {actual_sum}")
            log.info(f"[SHORTAGE_DEBUG]   Needåˆè¨ˆ: {need_sum}")
            log.info(f"[SHORTAGE_DEBUG]   ä¸è¶³åˆè¨ˆ: {lack_sum}")

            if not is_holiday and need_sum > actual_sum * 3:
                log.warning(
                    f"[SHORTAGE_WARN] {col}: ç•°å¸¸ãªä¸è¶³æ•°({lack_sum})ã‚’æ¤œå‡º"
                )
                log.warning(
                    f"[SHORTAGE_WARN]   å®Ÿç¸¾({actual_sum})ã«å¯¾ã—ã¦Need({need_sum})ãŒéŽå¤§"
                )

            non_zero_times = need_df_all[col][need_df_all[col] > 0].index.tolist()
            if non_zero_times:
                log.info(f"[SHORTAGE_DEBUG]   Need>0ã®æ™‚é–“å¸¯: {non_zero_times}")
                for time_slot in non_zero_times[:3]:
                    log.info(
                        f"[SHORTAGE_DEBUG]     {time_slot}: Need={need_df_all.loc[time_slot, col]}, å®Ÿç¸¾={staff_actual_data_all_df.loc[time_slot, col]}"
                    )

    # ----- excess analysis -----
    fp_excess_time = fp_excess_ratio = fp_excess_freq = None
    if "upper" in heat_all_df.columns:
        upper_series_overall_orig = (
            heat_all_df["upper"].reindex(index=time_labels).fillna(0).clip(lower=0)
        )
        upper_df_all = pd.DataFrame(
            np.repeat(
                upper_series_overall_orig.values[:, np.newaxis],
                len(staff_actual_data_all_df.columns),
                axis=1,
            ),
            index=upper_series_overall_orig.index,
            columns=staff_actual_data_all_df.columns,
        )
        parsed_date_list_all = [
            _parse_as_date(c) for c in staff_actual_data_all_df.columns
        ]
        holiday_mask_all = [
            d in estimated_holidays_set if d else False for d in parsed_date_list_all
        ]
        if any(holiday_mask_all):
            for col, is_h in zip(upper_df_all.columns, holiday_mask_all, strict=True):
                if is_h:
                    upper_df_all[col] = 0

        excess_count_overall_df = (
            (staff_actual_data_all_df - upper_df_all)
            .clip(lower=0)
            .fillna(0)
            .astype(int)
        )
        excess_ratio_df = (
            (
                (staff_actual_data_all_df - upper_df_all)
                / upper_df_all.replace(0, np.nan)
            )
            .clip(lower=0)
            .fillna(0)
        )

        fp_excess_time = save_df_parquet(
            excess_count_overall_df,
            out_dir_path / "excess_time.parquet",
            index=True,
        )
        fp_excess_ratio = save_df_parquet(
            excess_ratio_df,
            out_dir_path / "excess_ratio.parquet",
            index=True,
        )

        excess_occurrence_df = (excess_count_overall_df > 0).astype(int)
        excess_freq_df = pd.DataFrame(
            excess_occurrence_df.sum(axis=1), columns=["excess_days"]
        )
        fp_excess_freq = save_df_parquet(
            excess_freq_df,
            out_dir_path / "excess_freq.parquet",
            index=True,
        )

        margin_vs_upper_df = (
            (upper_df_all - staff_actual_data_all_df)
            .clip(lower=0)
            .fillna(0)
            .astype(int)
        )
        save_df_parquet(
            margin_vs_upper_df,
            out_dir_path / "margin_vs_upper_time.parquet",
            index=True,
        )
    else:
        log.warning(
            "[shortage] heat_ALL.xlsx ã« 'upper' åˆ—ãŒãªã„ãŸã‚ excess åˆ†æžã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
        )

    weights = config.get("optimization_weights", {"lack": 0.6, "excess": 0.4})
    w_lack = float(weights.get("lack", 0.6))
    w_excess = float(weights.get("excess", 0.4))
    pen_lack_df = shortage_ratio_df
    pen_excess_df = (
        excess_ratio_df if "upper" in heat_all_df.columns else pen_lack_df * 0
    )
    optimization_score_df = 1 - (w_lack * pen_lack_df + w_excess * pen_excess_df)
    optimization_score_df = optimization_score_df.clip(lower=0, upper=1)
    save_df_parquet(
        optimization_score_df,
        out_dir_path / "optimization_score_time.parquet",
        index=True,
    )

    log.debug(
        "--- shortage_time.xlsx / shortage_ratio.xlsx / shortage_freq.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (å…¨ä½“) çµ‚äº† ---"
    )

    # ===== å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³æ–¹å¼: å®Ÿãƒ‡ãƒ¼ã‚¿ç›´æŽ¥ä½¿ç”¨ =====
    log.info(f"[DYNAMIC_NEED] å®Œå…¨ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªå‹•çš„Needè¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™ã€‚")
    log.info(f"[DYNAMIC_NEED] å„è·ç¨®ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã‹ã‚‰å®Ÿéš›ã®Needå€¤ã‚’ç›´æŽ¥ä½¿ç”¨ã—ã¾ã™ã€‚")

    role_kpi_rows: List[Dict[str, Any]] = []
    monthly_role_rows: List[Dict[str, Any]] = []
    processed_role_names_list = []

    for fp_role_heatmap_item in out_dir_path.glob("heat_*.xlsx"):
        if fp_role_heatmap_item.name == "heat_ALL.xlsx":
            continue
        
        # ðŸ”§ ä¿®æ­£: é›‡ç”¨å½¢æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆheat_emp_*ï¼‰ã‚’é™¤å¤–
        if fp_role_heatmap_item.name.startswith("heat_emp_"):
            continue

        role_name_current = fp_role_heatmap_item.stem.replace("heat_", "")
        processed_role_names_list.append(role_name_current)
        log.debug(
            f"--- shortage_role.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (è·ç¨®: {role_name_current}) ---"
        )

        try:
            role_heat_current_df = pd.read_excel(fp_role_heatmap_item, index_col=0)
        except Exception as e_role_heat:
            log.warning(
                f"[shortage] è·ç¨®åˆ¥ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ— '{fp_role_heatmap_item.name}' ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e_role_heat}"
            )
            role_kpi_rows.append(
                {
                    "role": role_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "heatmap read error",
                }
            )
            continue

        if "need" not in role_heat_current_df.columns:
            log.warning(
                f"[shortage] è·ç¨® '{role_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã« 'need' åˆ—ãŒä¸è¶³ã€‚KPIè¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ã€‚"
            )
            role_kpi_rows.append(
                {
                    "role": role_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "missing need column",
                }
            )
            continue
        role_need_per_time_series_orig_for_role = (
            role_heat_current_df["need"]
            .reindex(index=time_labels)
            .fillna(0)
            .clip(lower=0)
        )

        role_date_columns_list = [
            str(col)
            for col in role_heat_current_df.columns
            if col not in SUMMARY5 and _parse_as_date(str(col)) is not None
        ]
        if not role_date_columns_list:
            log.warning(
                f"[shortage] è·ç¨® '{role_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã«æ—¥ä»˜åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚KPIè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            role_kpi_rows.append(
                {
                    "role": role_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "no date columns",
                }
            )
            continue

        role_staff_actual_data_df = (
            role_heat_current_df[role_date_columns_list]
            .copy()
            .reindex(index=time_labels)
            .fillna(0)
        )

        parsed_role_dates = [
            _parse_as_date(c) for c in role_staff_actual_data_df.columns
        ]
        holiday_mask_role = [
            d in estimated_holidays_set if d else False for d in parsed_role_dates
        ]

        # ===== è·ç¨®åˆ¥è©³ç´°Needå€¤ã‚’ä½¿ç”¨ =====
        try:
            # â˜…â˜…â˜… ä¿®æ­£ï¼šè·ç¨®åˆ¥è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ â˜…â˜…â˜…
            role_safe_name = role_name_current.replace("/", "_").replace("\\", "_").replace(":", "_")
            role_need_per_date_slot_file = out_dir_path / f"need_per_date_slot_role_{role_safe_name}.parquet"
            
            if role_need_per_date_slot_file.exists():
                log.info(f"[ROLE_NEED] {role_name_current}: è·ç¨®åˆ¥è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                
                # è·ç¨®åˆ¥ã®è©³ç´°Needå€¤ã‚’èª­ã¿è¾¼ã¿
                need_df_role = pd.read_parquet(role_need_per_date_slot_file)
                
                # æ™‚é–“è»¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹
                need_df_role = need_df_role.reindex(index=time_labels, fill_value=0)
                
                # Needå€¤æ¤œè¨¼ãƒ­ã‚°
                total_role_need = need_df_role.sum().sum()
                log.info(f"[ROLE_NEED] {role_name_current}: è·ç¨®åˆ¥è©³ç´°Needç·è¨ˆ={total_role_need:.1f}")
                
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
                log.warning(f"[ROLE_NEED] {role_name_current}: è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                
                # æ—¥ä»˜åˆ—ã‚’ç‰¹å®š
                date_columns_in_role_heat = [
                    str(col) for col in role_heat_current_df.columns 
                    if col not in SUMMARY5 and _parse_as_date(str(col)) is not None
                ]
                
                if date_columns_in_role_heat:
                    # ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã®å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‚’Needå€¤ã¨ã—ã¦ä½¿ç”¨ï¼ˆå¾“æ¥æ–¹å¼ï¼‰
                    log.info(f"[FALLBACK] {role_name_current}: {len(date_columns_in_role_heat)}å€‹ã®æ—¥ä»˜åˆ—ã‚’ç™ºè¦‹")
                    need_df_role = role_heat_current_df[date_columns_in_role_heat].copy()
                    
                    # æ™‚é–“è»¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹
                    need_df_role = need_df_role.reindex(index=time_labels, fill_value=0)
                    
                    # å®Ÿãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ­ã‚°
                    total_actual_need = need_df_role.sum().sum()
                    log.info(f"[FALLBACK] {role_name_current}: å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ã—ãŸç·Need={total_actual_need:.1f}")
                    
                else:
                    log.warning(f"[FALLBACK] {role_name_current}: æ—¥ä»˜åˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€ã‚¨ãƒ©ãƒ¼")
                    raise ValueError("æ—¥ä»˜åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            log.warning(f"[DYNAMIC_NEED] {role_name_current}: å®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã«å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å¹³å‡Needå€¤ã‚’ä½¿ç”¨
            need_df_role = pd.DataFrame(
                np.repeat(
                    role_need_per_time_series_orig_for_role.values[:, np.newaxis],
                    len(role_staff_actual_data_df.columns),
                    axis=1,
                ),
                index=role_need_per_time_series_orig_for_role.index,
                columns=role_staff_actual_data_df.columns,
            )
            log.info(f"[DYNAMIC_NEED] {role_name_current}: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ãŒå®Œäº†")
        
        if 'need_df_role' not in locals():
            # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            log.warning(f"[DYNAMIC_NEED] {role_name_current}: need_df_roleãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ - ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ")
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®å¹³å‡Needã«åŸºã¥ãè¨ˆç®—æ–¹æ³•
            log.info(f"[shortage] {role_name_current}: å¾“æ¥ã®å¹³å‡Needãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            need_df_role = pd.DataFrame(
                np.repeat(
                    role_need_per_time_series_orig_for_role.values[:, np.newaxis],
                    len(role_staff_actual_data_df.columns),
                    axis=1,
                ),
                index=role_need_per_time_series_orig_for_role.index,
                columns=role_staff_actual_data_df.columns,
            )

        # ä¼‘æ¥­æ—¥ã®Needã‚’0ã«ã™ã‚‹å‡¦ç† (ã“ã‚Œã¯ä¿®æ­£å¾Œã‚‚å¿…è¦)
        if any(holiday_mask_role):
            for c, is_h in zip(need_df_role.columns, holiday_mask_role, strict=True):
                if is_h:
                    need_df_role[c] = 0

        working_cols_role = [
            c
            for c, is_h in zip(
                role_staff_actual_data_df.columns, holiday_mask_role, strict=True
            )
            if not is_h and _parse_as_date(c)
        ]
        num_working_days_for_current_role = len(working_cols_role)

        # ä¿®æ­£ã•ã‚ŒãŸ need_df_role ã‚’ä½¿ã£ã¦ lack ã¨ excess ã‚’è¨ˆç®—ã™ã‚‹
        role_lack_count_for_specific_role_df = (
            need_df_role - role_staff_actual_data_df
        ).clip(lower=0)

        role_excess_count_for_specific_role_df = None
        if "upper" in role_heat_current_df.columns:
            role_upper_per_time_series_orig_for_role = (
                role_heat_current_df["upper"]
                .reindex(index=time_labels)
                .fillna(0)
                .clip(lower=0)
            )
            upper_df_role = pd.DataFrame(
                np.repeat(
                    role_upper_per_time_series_orig_for_role.values[:, np.newaxis],
                    len(role_staff_actual_data_df.columns),
                    axis=1,
                ),
                index=role_upper_per_time_series_orig_for_role.index,
                columns=role_staff_actual_data_df.columns,
            )
            if any(holiday_mask_role):
                for c, is_h in zip(
                    upper_df_role.columns, holiday_mask_role, strict=True
                ):
                    if is_h:
                        upper_df_role[c] = 0
            role_excess_count_for_specific_role_df = (
                role_staff_actual_data_df - upper_df_role
            ).clip(lower=0)
        else:
            log.debug(
                f"[shortage] '{role_name_current}' ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã« 'upper' åˆ—ãŒãªã„ãŸã‚ excess è¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—"
            )

        # ã‚µãƒžãƒªãƒ¼ç”¨ã®åˆè¨ˆæ™‚é–“ã‚‚ã€ä¿®æ­£ã•ã‚ŒãŸ need_df_role ã‹ã‚‰è¨ˆç®—ã™ã‚‹
        total_need_hours_for_role = need_df_role[working_cols_role].sum().sum() * slot_hours
        # staff_h ã¯å…¨æ—¥ã®å®Ÿç¸¾ã§è¨ˆç®—ï¼ˆä¼‘æ¥­æ—¥ã‚‚å®Ÿç¸¾0ã¨ã—ã¦å«ã¾ã‚Œã‚‹ï¼‰
        total_staff_hours_for_role = role_staff_actual_data_df.sum().sum() * slot_hours
        # lack_h ã¯ä¼‘æ¥­æ—¥ã®need=0ã‚’è€ƒæ…®ã—ãŸlackã®åˆè¨ˆ
        total_lack_hours_for_role = (
            role_lack_count_for_specific_role_df.sum().sum() * slot_hours
        )
        # excess_h ã¯ä¼‘æ¥­æ—¥ã®upper=0ã‚’è€ƒæ…®ã—ãŸexcessã®åˆè¨ˆ
        total_excess_hours_for_role = (
            role_excess_count_for_specific_role_df.sum().sum() * slot_hours
            if role_excess_count_for_specific_role_df is not None
            else 0
        )
        # è¨ˆç®—çµæžœæ¤œè¨¼ç”¨: need_h - staff_h ã¨ã®å·®åˆ†ãŒlack_hã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
        expected_lack_h = max(total_need_hours_for_role - total_staff_hours_for_role, 0)
        if abs(expected_lack_h - total_lack_hours_for_role) > slot_hours:
            log.debug(
                f"[shortage] mismatch for {role_name_current}: "
                f"need_h={total_need_hours_for_role:.1f}, "
                f"staff_h={total_staff_hours_for_role:.1f}, "
                f"computed lack_h={total_lack_hours_for_role:.1f}, "
                f"expected lack_h={expected_lack_h:.1f}"
            )
            try:
                daily_need_h = (need_df_role.sum() * slot_hours).rename("need_h")
                daily_staff_h = (role_staff_actual_data_df.sum() * slot_hours).rename(
                    "staff_h"
                )
                daily_lack_h = (
                    role_lack_count_for_specific_role_df.sum() * slot_hours
                ).rename("lack_h")
                daily_debug_df = pd.concat(
                    [daily_need_h, daily_staff_h, daily_lack_h], axis=1
                ).assign(diff_h=lambda d: d["need_h"] - d["staff_h"])
                log.debug(
                    f"[shortage] daily summary for {role_name_current} (first 7 days):\n"
                    f"{daily_debug_df.head(7).to_string()}"
                )
            except Exception as e_daily:
                log.debug(
                    f"[shortage] daily debug summary failed for {role_name_current}: {e_daily}"
                )

        # æœˆåˆ¥ä¸è¶³hãƒ»éŽå‰°hé›†è¨ˆ
        try:
            lack_by_date = role_lack_count_for_specific_role_df.sum()
            lack_by_date.index = pd.to_datetime(lack_by_date.index)
            lack_month = (
                lack_by_date.groupby(lack_by_date.index.to_period("M")).sum()
                * slot_hours
            )
            excess_month = pd.Series(dtype=float)
            if role_excess_count_for_specific_role_df is not None:
                excess_by_date = role_excess_count_for_specific_role_df.sum()
                excess_by_date.index = pd.to_datetime(excess_by_date.index)
                excess_month = (
                    excess_by_date.groupby(excess_by_date.index.to_period("M")).sum()
                    * slot_hours
                )
            month_keys: Dict[str, Dict[str, int]] = {}
            for mon, val in lack_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "role": role_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["lack_h"] = int(round(val))
            for mon, val in excess_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "role": role_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["excess_h"] = int(round(val))
            monthly_role_rows.extend(month_keys.values())
        except Exception as e_month:
            log.debug(f"æœˆåˆ¥ä¸è¶³/éŽå‰°é›†è¨ˆã‚¨ãƒ©ãƒ¼ ({role_name_current}): {e_month}")

        role_kpi_rows.append(
            {
                "role": role_name_current,
                "need_h": int(round(total_need_hours_for_role)),
                "staff_h": int(round(total_staff_hours_for_role)),
                "lack_h": int(round(total_lack_hours_for_role)),
                "excess_h": int(round(total_excess_hours_for_role)),
                "working_days_considered": num_working_days_for_current_role,
            }
        )
        log.debug(
            f"  Role: {role_name_current}, Need(h): {total_need_hours_for_role:.1f} (on {num_working_days_for_current_role} working days), "
            f"Staff(h): {total_staff_hours_for_role:.1f}, Lack(h): {total_lack_hours_for_role:.1f}, Excess(h): {total_excess_hours_for_role:.1f}"
        )
        log.debug(
            f"--- shortage_role.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (è·ç¨®: {role_name_current}) çµ‚äº† ---"
        )

    role_summary_df = pd.DataFrame(role_kpi_rows)
    if not role_summary_df.empty:
        role_summary_df = role_summary_df.sort_values(
            "lack_h", ascending=False, na_position="last"
        ).reset_index(drop=True)
        role_summary_df = role_summary_df.assign(
            estimated_excess_cost=lambda d: d.get("excess_h", 0) * wage_direct,
            estimated_lack_cost_if_temporary_staff=lambda d: d.get("lack_h", 0)
            * wage_temp,
            estimated_lack_penalty_cost=lambda d: d.get("lack_h", 0) * penalty_per_lack,
        )

    monthly_role_df = pd.DataFrame(monthly_role_rows)
    if not monthly_role_df.empty:
        monthly_role_df = monthly_role_df.sort_values(["month", "role"]).reset_index(
            drop=True
        )

    # ðŸ”§ ä¿®æ­£: è·ç¨®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä¿å­˜ï¼ˆé›‡ç”¨å½¢æ…‹ãƒ‡ãƒ¼ã‚¿ã®æ··å…¥ã‚’é˜²ãï¼‰
    fp_shortage_role = out_dir_path / "shortage_role_summary.parquet"
    if not role_summary_df.empty:
        # è·ç¨®ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆemp_ã§å§‹ã¾ã‚‹é›‡ç”¨å½¢æ…‹ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ï¼‰
        clean_role_df = role_summary_df[~role_summary_df['role'].str.startswith('emp_', na=False)].copy()
        clean_role_df.to_parquet(fp_shortage_role, index=False)
        log.info(f"[SHORTAGE_FIX] è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ã¿ä¿å­˜: {len(clean_role_df)}ä»¶ (é™¤å¤–: {len(role_summary_df) - len(clean_role_df)}ä»¶)")
        
        # ðŸ”§ é‡è¦: ä»¥é™ã®å‡¦ç†ã§ã‚‚clean_role_dfã‚’ä½¿ç”¨ï¼ˆmetaãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå«ã‚€ï¼‰
        role_summary_df = clean_role_df
    else:
        role_summary_df.to_parquet(fp_shortage_role, index=False)
    if not monthly_role_df.empty:
        monthly_role_df.to_parquet(
            out_dir_path / "shortage_role_monthly.parquet",
            index=False,
        )

    meta_dates_list_shortage = date_columns_in_heat_all
    meta_roles_list_shortage = (
        role_summary_df["role"].tolist()
        if not role_summary_df.empty
        else processed_role_names_list
    )
    meta_months_list_shortage = (
        monthly_role_df["month"].tolist() if not monthly_role_df.empty else []
    )

    # â”€â”€ Employment shortage analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    emp_kpi_rows: List[Dict[str, Any]] = []
    monthly_emp_rows: List[Dict[str, Any]] = []
    processed_emp_names_list = []

    for fp_emp_heatmap_item in out_dir_path.glob("heat_emp_*.xlsx"):
        emp_name_current = fp_emp_heatmap_item.stem.replace("heat_emp_", "")
        processed_emp_names_list.append(emp_name_current)
        log.debug(
            f"--- shortage_employment.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (é›‡ç”¨å½¢æ…‹: {emp_name_current}) ---"
        )
        try:
            emp_heat_current_df = pd.read_excel(fp_emp_heatmap_item, index_col=0)
        except Exception as e_emp_heat:
            log.warning(
                f"[shortage] é›‡ç”¨å½¢æ…‹åˆ¥ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ— '{fp_emp_heatmap_item.name}' ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e_emp_heat}"
            )
            emp_kpi_rows.append(
                {
                    "employment": emp_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "heatmap read error",
                }
            )
            continue

        if "need" not in emp_heat_current_df.columns:
            log.warning(
                f"[shortage] é›‡ç”¨å½¢æ…‹ '{emp_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã« 'need' åˆ—ãŒä¸è¶³ã€‚KPIè¨ˆç®—ã‚¹ã‚­ãƒƒãƒ—ã€‚"
            )
            emp_kpi_rows.append(
                {
                    "employment": emp_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "missing need column",
                }
            )
            continue

        # â˜…â˜…â˜… ä¿®æ­£ï¼šé›‡ç”¨å½¢æ…‹åˆ¥è©³ç´°Needå€¤ã‚’ä½¿ç”¨ â˜…â˜…â˜…
        emp_safe_name = emp_name_current.replace("/", "_").replace("\\", "_").replace(":", "_")
        emp_need_per_date_slot_file = out_dir_path / f"need_per_date_slot_emp_{emp_safe_name}.parquet"
        
        if emp_need_per_date_slot_file.exists():
            log.info(f"[EMP_NEED] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            
            # é›‡ç”¨å½¢æ…‹åˆ¥ã®è©³ç´°Needå€¤ã‚’èª­ã¿è¾¼ã¿
            emp_need_df = pd.read_parquet(emp_need_per_date_slot_file)
            
            # æ™‚é–“è»¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹
            emp_need_df = emp_need_df.reindex(index=time_labels, fill_value=0)
            
            # Needå€¤æ¤œè¨¼ãƒ­ã‚°
            total_emp_need = emp_need_df.sum().sum()
            log.info(f"[EMP_NEED] {emp_name_current}: é›‡ç”¨å½¢æ…‹åˆ¥è©³ç´°Needç·è¨ˆ={total_emp_need:.1f}")
            
            # æ™‚é–“å¸¯åˆ¥å¹³å‡Needå€¤ã‚’ç®—å‡ºï¼ˆæ—¢å­˜ã®å‡¦ç†ã¨ã®äº’æ›æ€§ã®ãŸã‚ï¼‰
            emp_need_series = emp_need_df.mean(axis=1).fillna(0).clip(lower=0)
            
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨
            log.warning(f"[EMP_NEED] {emp_name_current}: è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            emp_need_series = (
                emp_heat_current_df["need"]
                .reindex(index=time_labels)
                .fillna(0)
                .clip(lower=0)
            )
        emp_date_columns = [
            str(c)
            for c in emp_heat_current_df.columns
            if c not in SUMMARY5 and _parse_as_date(str(c)) is not None
        ]
        if not emp_date_columns:
            log.warning(
                f"[shortage] é›‡ç”¨å½¢æ…‹ '{emp_name_current}' ã®ãƒ’ãƒ¼ãƒˆãƒžãƒƒãƒ—ã«æ—¥ä»˜åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚KPIè¨ˆç®—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚"
            )
            emp_kpi_rows.append(
                {
                    "employment": emp_name_current,
                    "need_h": 0,
                    "staff_h": 0,
                    "lack_h": 0,
                    "working_days_considered": 0,
                    "note": "no date columns",
                }
            )
            continue

        emp_staff_df = (
            emp_heat_current_df[emp_date_columns]
            .copy()
            .reindex(index=time_labels)
            .fillna(0)
        )
        parsed_emp_dates = [_parse_as_date(c) for c in emp_staff_df.columns]
        holiday_mask_emp = [
            d in estimated_holidays_set if d else False for d in parsed_emp_dates
        ]
        # need_df_emp ã®æ§‹ç¯‰ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£
        if not need_per_date_slot_df.empty:
            log.info(f"[shortage] {emp_name_current}: need_per_date_slot.parquet ã‚’ä½¿ç”¨ã—ã¦Needã‚’å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
            need_df_emp = pd.DataFrame(index=time_labels, columns=emp_staff_df.columns, dtype=float).fillna(0)

            emp_time_indices = emp_need_series[emp_need_series > 0].index

            if not emp_time_indices.empty:
                for date_col in need_df_emp.columns:
                    if date_col in need_per_date_slot_df.columns:
                        need_df_emp.loc[emp_time_indices, date_col] = need_per_date_slot_df.loc[emp_time_indices, date_col]
        else:
            log.info(f"[shortage] {emp_name_current}: å¾“æ¥ã®å¹³å‡Needãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            need_df_emp = pd.DataFrame(
                np.repeat(
                    emp_need_series.values[:, np.newaxis], len(emp_staff_df.columns), axis=1
                ),
                index=emp_need_series.index,
                columns=emp_staff_df.columns,
            )

        if any(holiday_mask_emp):
            for c, is_h in zip(need_df_emp.columns, holiday_mask_emp, strict=True):
                if is_h:
                    need_df_emp[c] = 0

        working_cols_emp = [
            c
            for c, is_h in zip(emp_staff_df.columns, holiday_mask_emp, strict=True)
            if not is_h and _parse_as_date(c)
        ]
        num_working_days_for_current_emp = len(working_cols_emp)

        lack_count_emp_df = (need_df_emp - emp_staff_df).clip(lower=0)

        # excess_count_emp_dfã®è¨ˆç®—ã«èª¤ã‚ŠãŒã‚ã£ãŸãŸã‚ä¿®æ­£ (needã§ã¯ãªãupperã¨æ¯”è¼ƒ)
        excess_count_emp_df = pd.DataFrame()
        if "upper" in emp_heat_current_df.columns:
             upper_series_emp = emp_heat_current_df["upper"].reindex(index=time_labels).fillna(0).clip(lower=0)
             upper_df_emp = pd.DataFrame(
                 np.repeat(
                     upper_series_emp.values[:, np.newaxis], len(emp_staff_df.columns), axis=1
                 ),
                 index=upper_series_emp.index,
                 columns=emp_staff_df.columns,
             )
             if any(holiday_mask_emp):
                 for c, is_h in zip(upper_df_emp.columns, holiday_mask_emp, strict=True):
                     if is_h:
                         upper_df_emp[c] = 0
             excess_count_emp_df = (emp_staff_df - upper_df_emp).clip(lower=0)


        # ã‚µãƒžãƒªãƒ¼ç”¨ã®åˆè¨ˆæ™‚é–“ã‚‚ã€ä¿®æ­£ã•ã‚ŒãŸ need_df_emp ã‹ã‚‰è¨ˆç®—ã™ã‚‹
        total_need_hours_for_emp = need_df_emp[working_cols_emp].sum().sum() * slot_hours
        total_staff_hours_for_emp = emp_staff_df.sum().sum() * slot_hours
        total_lack_hours_for_emp = lack_count_emp_df.sum().sum() * slot_hours
        total_excess_hours_for_emp = (
            excess_count_emp_df.sum().sum() * slot_hours
            if not excess_count_emp_df.empty
            else 0
        )

        try:
            lack_by_date = lack_count_emp_df.sum()
            lack_by_date.index = pd.to_datetime(lack_by_date.index)
            lack_month = (
                lack_by_date.groupby(lack_by_date.index.to_period("M")).sum()
                * slot_hours
            )
            excess_month = pd.Series(dtype=float)
            if not excess_count_emp_df.empty:
                excess_by_date = excess_count_emp_df.sum()
                excess_by_date.index = pd.to_datetime(excess_by_date.index)
                excess_month = (
                    excess_by_date.groupby(excess_by_date.index.to_period("M")).sum()
                    * slot_hours
                )
            month_keys: Dict[str, Dict[str, int]] = {}
            for mon, val in lack_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "employment": emp_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["lack_h"] = int(round(val))
            for mon, val in excess_month.items():
                month_keys.setdefault(
                    str(mon),
                    {
                        "employment": emp_name_current,
                        "month": str(mon),
                        "lack_h": 0,
                        "excess_h": 0,
                    },
                )
                month_keys[str(mon)]["excess_h"] = int(round(val))
            monthly_emp_rows.extend(month_keys.values())
        except Exception as e_month_emp:
            log.debug(f"æœˆåˆ¥ä¸è¶³/éŽå‰°é›†è¨ˆã‚¨ãƒ©ãƒ¼ ({emp_name_current}): {e_month_emp}")

        emp_kpi_rows.append(
            {
                "employment": emp_name_current,
                "need_h": int(round(total_need_hours_for_emp)),
                "staff_h": int(round(total_staff_hours_for_emp)),
                "lack_h": int(round(total_lack_hours_for_emp)),
                "excess_h": int(round(total_excess_hours_for_emp)),
                "working_days_considered": num_working_days_for_current_emp,
            }
        )
        log.debug(
            f"  Employment: {emp_name_current}, Need(h): {total_need_hours_for_emp:.1f} (on {num_working_days_for_current_emp} working days), "
            f"Staff(h): {total_staff_hours_for_emp:.1f}, Lack(h): {total_lack_hours_for_emp:.1f}, Excess(h): {total_excess_hours_for_emp:.1f}"
        )
        log.debug(
            f"--- shortage_employment.xlsx è¨ˆç®—ãƒ‡ãƒãƒƒã‚° (é›‡ç”¨å½¢æ…‹: {emp_name_current}) çµ‚äº† ---"
        )

    emp_summary_df = pd.DataFrame(emp_kpi_rows)
    if not emp_summary_df.empty:
        emp_summary_df = emp_summary_df.sort_values(
            "lack_h", ascending=False, na_position="last"
        ).reset_index(drop=True)
        emp_summary_df = emp_summary_df.assign(
            estimated_excess_cost=lambda d: d.get("excess_h", 0) * wage_direct,
            estimated_lack_cost_if_temporary_staff=lambda d: d.get("lack_h", 0)
            * wage_temp,
            estimated_lack_penalty_cost=lambda d: d.get("lack_h", 0) * penalty_per_lack,
        )

    monthly_emp_df = pd.DataFrame(monthly_emp_rows)
    if not monthly_emp_df.empty:
        monthly_emp_df = monthly_emp_df.sort_values(
            ["month", "employment"]
        ).reset_index(drop=True)

    fp_shortage_emp = out_dir_path / "shortage_employment_summary.parquet"
    emp_summary_df.to_parquet(fp_shortage_emp, index=False)
    if not monthly_emp_df.empty:
        monthly_emp_df.to_parquet(
            out_dir_path / "shortage_employment_monthly.parquet",
            index=False,
        )

    meta_employments_list_shortage = (
        emp_summary_df["employment"].tolist()
        if not emp_summary_df.empty
        else processed_emp_names_list
    )
    meta_months_list_shortage.extend(
        monthly_emp_df["month"].tolist() if not monthly_emp_df.empty else []
    )

    write_meta(
        out_dir_path / "shortage.meta.json",
        slot=slot,
        dates=sorted(list(set(meta_dates_list_shortage))),
        roles=sorted(list(set(meta_roles_list_shortage))),
        employments=sorted(list(set(meta_employments_list_shortage))),
        months=sorted(list(set(meta_months_list_shortage))),
        ratio_file="shortage_ratio.parquet",
        freq_file="shortage_freq.parquet",
        excess_ratio_file="excess_ratio.parquet" if fp_excess_ratio else None,
        excess_freq_file="excess_freq.parquet" if fp_excess_freq else None,
        estimated_holidays_used=[
            d.isoformat() for d in sorted(list(estimated_holidays_set))
        ],
    )

    # â”€â”€ text summary output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    summary_fp = out_dir_path / "shortage_summary.txt"
    try:
        # ðŸ”§ ä¿®æ­£: è·ç¨®ã®ã¿ã‚’å¯¾è±¡ã¨ã—ãŸåˆè¨ˆè¨ˆç®—ï¼ˆé›‡ç”¨å½¢æ…‹ã®é‡è¤‡ã‚’é™¤å¤–ï¼‰
        role_only_df = role_summary_df[~role_summary_df['role'].str.startswith('emp_', na=False)] if not role_summary_df.empty else pd.DataFrame()
        
        if not role_only_df.empty:
            total_lack_h = int(round(role_only_df.get("lack_h", pd.Series()).sum()))
            total_excess_h = int(round(role_only_df.get("excess_h", pd.Series()).sum()))
        else:
            total_lack_h = 0
            total_excess_h = 0
            
        log.info(f"[SHORTAGE_FIX] è·ç¨®ã®ã¿ã§ã®åˆè¨ˆè¨ˆç®—: ä¸è¶³={total_lack_h}h, éŽå‰°={total_excess_h}h")
        summary_lines = [
            f"total_lack_hours: {total_lack_h}",
            f"total_excess_hours: {total_excess_h}",
        ]
        summary_fp.write_text("\n".join(summary_lines) + "\n", encoding="utf-8")
    except Exception as e:  # noqa: BLE001
        log.debug(f"failed writing shortage summary text: {e}")

    log.info(
        (
            f"[shortage] completed â€” shortage_time â†’ {fp_shortage_time.name}, "
            f"shortage_ratio â†’ {fp_shortage_ratio.name}, "
            f"shortage_freq â†’ {fp_shortage_freq.name}, "
            f"shortage_role â†’ {fp_shortage_role.name}, "
            f"shortage_employment â†’ {fp_shortage_emp.name}, "
        )
        + (f"excess_time â†’ {fp_excess_time.name}, " if fp_excess_time else "")
        + (f"excess_ratio â†’ {fp_excess_ratio.name}, " if fp_excess_ratio else "")
        + (f"excess_freq â†’ {fp_excess_freq.name}" if fp_excess_freq else "")
    )
    if fp_shortage_time and fp_shortage_role and fp_shortage_ratio and fp_shortage_freq:
        return fp_shortage_time, fp_shortage_role
    return None


def merge_shortage_leave(
    out_dir: Path | str,
    *,
    shortage_xlsx: str | Path = "shortage_time.parquet",
    leave_csv: str | Path = "leave_analysis.csv",
    out_excel: str | Path = "shortage_leave.csv",
) -> Path | None:
    """Combine shortage_time.parquet with leave counts.

    Parameters
    ----------
    out_dir:
        Directory containing shortage and leave files.
    shortage_xlsx:
        Name of ``shortage_time.parquet``. Must exist under ``out_dir``.
    leave_csv:
        Optional ``leave_analysis.csv`` with columns ``date`` and
        ``total_leave_days``. If missing, leave counts are treated as ``0``.
    out_excel:
        Output CSV filename.

    Returns
    -------
    Path | None
        Path to the saved CSV file or ``None`` if shortage data missing.
    """

    out_dir_path = Path(out_dir)
    shortage_fp = out_dir_path / shortage_xlsx
    if not shortage_fp.exists():
        log.error(f"[shortage] {shortage_fp} not found")
        return None

    try:
        shortage_df = pd.read_parquet(shortage_fp)
    except Exception as e:
        log.error(f"[shortage] failed to read {shortage_fp}: {e}")
        return None

    # Convert wide timeÃ—date to long format
    long_df = shortage_df.stack().reset_index()
    long_df.columns = ["time", "date", "lack"]
    long_df["date"] = pd.to_datetime(long_df["date"])

    leave_fp = out_dir_path / leave_csv
    if leave_fp.exists():
        try:
            leave_df = pd.read_csv(leave_fp, parse_dates=["date"])
            leave_sum = (
                leave_df.groupby("date")["total_leave_days"]
                .sum()
                .astype(int)
                .reset_index()
            )
            long_df = long_df.merge(leave_sum, on="date", how="left")
            long_df.rename(
                columns={"total_leave_days": "leave_applicants"}, inplace=True
            )
        except Exception as e:
            log.warning(f"[shortage] leave_csv load failed: {e}")
            long_df["leave_applicants"] = 0
    else:
        long_df["leave_applicants"] = 0

    long_df["leave_applicants"] = long_df["leave_applicants"].fillna(0).astype(int)
    long_df["net_shortage"] = (long_df["lack"] - long_df["leave_applicants"]).clip(
        lower=0
    )

    out_fp = out_dir_path / out_excel
    long_df.to_csv(out_fp, index=False)
    return out_fp


def _summary_by_period(df: pd.DataFrame, *, period: str) -> pd.DataFrame:
    """Return average counts by *period* and time slot.

    Parameters
    ----------
    df:
        DataFrame loaded from ``shortage_time.xlsx`` or ``excess_time.xlsx``.
    period:
        ``"weekday"`` or ``"month_period"``.

    Returns
    -------
    pd.DataFrame
        Aggregated average counts per time slot.
    """

    date_cols = [c for c in df.columns if _parse_as_date(str(c)) is not None]
    if not date_cols:
        return pd.DataFrame(columns=[period, "timeslot", "avg_count"])

    data = df[date_cols].copy()
    data.columns = pd.to_datetime(data.columns)
    df_for_melt = data.reset_index()
    # reset_index()ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸæœ€åˆã®åˆ—ï¼ˆ=å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰ã®åå‰ã‚’å‹•çš„ã«å–å¾—ã™ã‚‹
    index_col_name = df_for_melt.columns[0]
    long = df_for_melt.melt(
        id_vars=[index_col_name], var_name="date", value_name="count"
    )
    long.rename(columns={index_col_name: "timeslot"}, inplace=True)

    long["date"] = pd.to_datetime(long["date"])

    if period == "weekday":
        day_name_map = {
            "Monday": "æœˆæ›œæ—¥",
            "Tuesday": "ç«æ›œæ—¥",
            "Wednesday": "æ°´æ›œæ—¥",
            "Thursday": "æœ¨æ›œæ—¥",
            "Friday": "é‡‘æ›œæ—¥",
            "Saturday": "åœŸæ›œæ—¥",
            "Sunday": "æ—¥æ›œæ—¥",
        }
        long[period] = long["date"].dt.day_name().map(day_name_map)
        order = list(day_name_map.values())
    elif period == "month_period":

        def _mp(day_val: int) -> str:
            if day_val <= 10:
                return "æœˆåˆ(1-10æ—¥)"
            if day_val <= 20:
                return "æœˆä¸­(11-20æ—¥)"
            return "æœˆæœ«(21-æœ«æ—¥)"

        long[period] = long["date"].dt.day.apply(_mp)
        order = ["æœˆåˆ(1-10æ—¥)", "æœˆä¸­(11-20æ—¥)", "æœˆæœ«(21-æœ«æ—¥)"]
    else:  # pragma: no cover - invalid option
        raise ValueError("period must be 'weekday' or 'month_period'")

    grouped = (
        long.groupby([period, "timeslot"], observed=False)["count"]
        .mean()
        .reset_index(name="avg_count")
    )
    grouped[period] = pd.Categorical(grouped[period], categories=order, ordered=True)
    return grouped.sort_values([period, "timeslot"]).reset_index(drop=True)


def weekday_timeslot_summary(
    out_dir: Path | str, *, excel: str = "shortage_time.parquet"
) -> pd.DataFrame:
    """Return average shortage counts by weekday and time slot."""

    df = pd.read_parquet(Path(out_dir) / excel)
    return _summary_by_period(df, period="weekday")


def monthperiod_timeslot_summary(
    out_dir: Path | str, *, excel: str = "shortage_time.parquet"
) -> pd.DataFrame:
    """Return average shortage counts by month period and time slot."""

    df = pd.read_parquet(Path(out_dir) / excel)
    return _summary_by_period(df, period="month_period")
