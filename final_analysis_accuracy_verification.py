#!/usr/bin/env python3
"""
æœ€çµ‚çš„ãªåˆ†æç²¾åº¦æ¤œè¨¼
===================

ã“ã‚Œã¾ã§ã®ä¿®æ­£ã‚’ç·åˆçš„ã«æ¤œè¨¼:
1. å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”å¯¾å¿œ
2. æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹ä¸è¶³æ™‚é–“è¨ˆç®—
3. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºæœ€é©åŒ–
4. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼
"""

import sys
import pandas as pd
import numpy as np
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class FinalAnalysisVerifier:
    """æœ€çµ‚åˆ†æç²¾åº¦æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.verification_results = {}
        self.test_data = {}
        self.performance_metrics = {}
        
    def create_test_dataset(self) -> Dict[str, pd.DataFrame]:
        """æ¤œè¨¼ç”¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ"""
        log.info("=== ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ ===")
        
        # 1. ç•°ãªã‚‹ã‚¹ãƒ­ãƒƒãƒˆé–“éš”ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        test_datasets = {}
        
        # 15åˆ†é–“éš”ãƒ‡ãƒ¼ã‚¿
        dates_15min = pd.date_range('2025-01-01 08:00', periods=32, freq='15min')
        test_datasets['data_15min'] = pd.DataFrame({
            'staff': ['ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨'] * 11,  # 33äººåˆ†ã ãŒ32ãƒ¬ã‚³ãƒ¼ãƒ‰ã«èª¿æ•´
            'role': ['çœ‹è­·å¸«', 'ä»‹è­·å£«', 'äº‹å‹™'] * 11,
            'employment': ['å¸¸å‹¤', 'ãƒ‘ãƒ¼ãƒˆ', 'ã‚¹ãƒãƒƒãƒˆ'] * 11,
            'ds': dates_15min[:32],  # 32ãƒ¬ã‚³ãƒ¼ãƒ‰ã«åˆ¶é™
            'parsed_slots_count': [1] * 32
        })
        
        # 30åˆ†é–“éš”ãƒ‡ãƒ¼ã‚¿ï¼ˆæ¨™æº–ï¼‰
        dates_30min = pd.date_range('2025-01-01 08:00', periods=24, freq='30min')
        test_datasets['data_30min'] = pd.DataFrame({
            'staff': ['ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨'] * 8,
            'role': ['çœ‹è­·å¸«', 'ä»‹è­·å£«', 'äº‹å‹™'] * 8,
            'employment': ['å¸¸å‹¤', 'ãƒ‘ãƒ¼ãƒˆ', 'ã‚¹ãƒãƒƒãƒˆ'] * 8,
            'ds': dates_30min,
            'parsed_slots_count': [1] * 24
        })
        
        # 60åˆ†é–“éš”ãƒ‡ãƒ¼ã‚¿
        dates_60min = pd.date_range('2025-01-01 08:00', periods=12, freq='60min')
        test_datasets['data_60min'] = pd.DataFrame({
            'staff': ['ç”°ä¸­', 'ä½è—¤', 'éˆ´æœ¨'] * 4,
            'role': ['çœ‹è­·å¸«', 'ä»‹è­·å£«', 'äº‹å‹™'] * 4,
            'employment': ['å¸¸å‹¤', 'ãƒ‘ãƒ¼ãƒˆ', 'ã‚¹ãƒãƒƒãƒˆ'] * 4,
            'ds': dates_60min,
            'parsed_slots_count': [1] * 12
        })
        
        # 2. å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        dates_large = pd.date_range('2025-01-01 08:00', periods=1000, freq='30min')
        test_datasets['data_large'] = pd.DataFrame({
            'staff': [f'è·å“¡{i%50}' for i in range(1000)],
            'role': ['çœ‹è­·å¸«', 'ä»‹è­·å£«', 'äº‹å‹™', 'ç®¡ç†è€…', 'ç›¸è«‡å“¡'][i%5] for i in range(1000)],
            'employment': ['å¸¸å‹¤', 'ãƒ‘ãƒ¼ãƒˆ', 'ã‚¹ãƒãƒƒãƒˆ'][i%3] for i in range(1000)],
            'ds': dates_large,
            'parsed_slots_count': [1] * 1000
        })
        
        log.info(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {len(test_datasets)}ã‚»ãƒƒãƒˆ")
        for name, df in test_datasets.items():
            log.info(f"  {name}: {df.shape}, ã‚¹ãƒ­ãƒƒãƒˆ: {self._detect_slot_pattern(df)}")
        
        return test_datasets
    
    def _detect_slot_pattern(self, df: pd.DataFrame) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒ­ãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        if 'ds' not in df.columns or df.empty:
            return "ä¸æ˜"
        
        minutes = df['ds'].dt.minute.unique()
        if set(minutes).issubset({0, 15, 30, 45}):
            return "15åˆ†é–“éš”"
        elif set(minutes).issubset({0, 30}):
            return "30åˆ†é–“éš”"
        elif set(minutes).issubset({0}):
            return "60åˆ†é–“éš”"
        else:
            return f"æ··åˆãƒ‘ã‚¿ãƒ¼ãƒ³: {sorted(minutes)}"
    
    def verify_dynamic_slot_detection(self, test_datasets: Dict[str, pd.DataFrame]) -> Dict:
        """å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”æ¤œå‡ºã®ç²¾åº¦æ¤œè¨¼"""
        log.info("=== å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”æ¤œå‡ºæ¤œè¨¼ ===")
        
        try:
            from shift_suite.tasks.time_axis_shortage_calculator import TimeAxisShortageCalculator
            
            detection_results = {}
            
            for dataset_name, df in test_datasets.items():
                if dataset_name == 'data_large':  # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                    
                calculator = TimeAxisShortageCalculator(auto_detect=True)
                
                # ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºå®Ÿè¡Œ
                calculator._detect_and_update_slot_interval(df['ds'])
                detected_info = calculator.get_detected_slot_info()
                
                # æœŸå¾…å€¤ã¨æ¯”è¼ƒ
                expected_slots = {
                    'data_15min': 15,
                    'data_30min': 30,
                    'data_60min': 60
                }
                
                expected = expected_slots.get(dataset_name, 30)
                detected = detected_info['slot_minutes'] if detected_info else 30
                
                accuracy = detected == expected
                detection_results[dataset_name] = {
                    'expected_minutes': expected,
                    'detected_minutes': detected,
                    'accuracy': accuracy,
                    'confidence': detected_info['confidence'] if detected_info else 0.0
                }
                
                log.info(f"  {dataset_name}: æœŸå¾…{expected}åˆ† -> æ¤œå‡º{detected}åˆ† (ç²¾åº¦: {'âœ“' if accuracy else 'âœ—'})")
            
            overall_accuracy = sum(r['accuracy'] for r in detection_results.values()) / len(detection_results)
            log.info(f"å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡º ç·åˆç²¾åº¦: {overall_accuracy:.1%}")
            
            return {
                'overall_accuracy': overall_accuracy,
                'individual_results': detection_results,
                'status': 'PASS' if overall_accuracy >= 0.8 else 'FAIL'
            }
            
        except Exception as e:
            log.error(f"å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return {'status': 'ERROR', 'error': str(e)}
    
    def verify_time_axis_calculation(self, test_datasets: Dict[str, pd.DataFrame]) -> Dict:
        """æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹ä¸è¶³æ™‚é–“è¨ˆç®—ã®ç²¾åº¦æ¤œè¨¼"""
        log.info("=== æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹è¨ˆç®—æ¤œè¨¼ ===")
        
        try:
            from shift_suite.tasks.time_axis_shortage_calculator import calculate_time_axis_shortage
            
            calculation_results = {}
            
            for dataset_name, df in test_datasets.items():
                if dataset_name == 'data_large':  # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹è¨ˆç®—å®Ÿè¡Œ
                role_shortages, employment_shortages = calculate_time_axis_shortage(df)
                
                # åŸºæœ¬çš„ãªæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                role_total = sum(role_shortages.values()) if role_shortages else 0
                employment_total = sum(employment_shortages.values()) if employment_shortages else 0
                
                calculation_results[dataset_name] = {
                    'role_count': len(role_shortages),
                    'employment_count': len(employment_shortages),
                    'role_total_shortage': role_total,
                    'employment_total_shortage': employment_total,
                    'data_consistency': abs(role_total - employment_total) < 0.1,  # è¨±å®¹èª¤å·®0.1æ™‚é–“
                    'roles': list(role_shortages.keys()) if role_shortages else [],
                    'employments': list(employment_shortages.keys()) if employment_shortages else []
                }
                
                log.info(f"  {dataset_name}: è·ç¨®{len(role_shortages)}å€‹, é›‡ç”¨å½¢æ…‹{len(employment_shortages)}å€‹")
                log.info(f"    ä¸è¶³æ™‚é–“ - è·ç¨®è¨ˆ: {role_total:.1f}h, é›‡ç”¨å½¢æ…‹è¨ˆ: {employment_total:.1f}h")
            
            consistency_rate = sum(r['data_consistency'] for r in calculation_results.values()) / len(calculation_results)
            log.info(f"æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹è¨ˆç®— ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: {consistency_rate:.1%}")
            
            return {
                'consistency_rate': consistency_rate,
                'calculation_results': calculation_results,
                'status': 'PASS' if consistency_rate >= 0.8 else 'FAIL'
            }
            
        except Exception as e:
            log.error(f"æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹è¨ˆç®—æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return {'status': 'ERROR', 'error': str(e)}
    
    def verify_heatmap_optimization(self, test_datasets: Dict[str, pd.DataFrame]) -> Dict:
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æœ€é©åŒ–ã®åŠ¹æœæ¤œè¨¼"""
        log.info("=== ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æœ€é©åŒ–æ¤œè¨¼ ===")
        
        optimization_results = {}
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ
        large_df = test_datasets['data_large']
        
        # æ“¬ä¼¼ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆ365æ—¥Ã—48æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆï¼‰
        dates = [(datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d') for i in range(365)]
        time_slots = [f"{h:02d}:{m:02d}" for h in range(24) for m in [0, 30]]
        
        # ãƒ©ãƒ³ãƒ€ãƒ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        np.random.seed(42)
        heatmap_data = np.random.randint(0, 50, size=(48, 365))
        heatmap_df = pd.DataFrame(heatmap_data, index=time_slots, columns=dates)
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
        original_memory = heatmap_df.memory_usage(deep=True).sum()
        
        # æœ€é©åŒ–ãƒ†ã‚¹ãƒˆï¼ˆæ“¬ä¼¼å®Ÿè£…ï¼‰
        optimized_df = heatmap_df.iloc[:, -60:]  # ç›´è¿‘60æ—¥
        optimized_df = optimized_df.astype('uint8')  # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
        
        optimized_memory = optimized_df.memory_usage(deep=True).sum()
        
        memory_reduction = (original_memory - optimized_memory) / original_memory
        
        optimization_results = {
            'original_shape': heatmap_df.shape,
            'optimized_shape': optimized_df.shape,
            'original_memory_mb': original_memory / 1024 / 1024,
            'optimized_memory_mb': optimized_memory / 1024 / 1024,
            'memory_reduction_rate': memory_reduction,
            'data_reduction_rate': (365 - 60) / 365,
            'optimization_effective': memory_reduction > 0.8
        }
        
        log.info(f"  å…ƒãƒ‡ãƒ¼ã‚¿: {heatmap_df.shape} -> æœ€é©åŒ–å¾Œ: {optimized_df.shape}")
        log.info(f"  ãƒ¡ãƒ¢ãƒª: {original_memory/1024/1024:.1f}MB -> {optimized_memory/1024/1024:.1f}MB")
        log.info(f"  å‰Šæ¸›ç‡: {memory_reduction:.1%}")
        
        return {
            'optimization_results': optimization_results,
            'status': 'PASS' if optimization_results['optimization_effective'] else 'FAIL'
        }
    
    def verify_data_integrity(self, test_datasets: Dict[str, pd.DataFrame]) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        log.info("=== ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼ ===")
        
        integrity_results = {}
        
        for dataset_name, df in test_datasets.items():
            checks = {
                'no_null_staff': df['staff'].notna().all(),
                'no_null_role': df['role'].notna().all(),
                'no_null_employment': df['employment'].notna().all(),
                'valid_timestamps': df['ds'].notna().all(),
                'positive_slots': (df['parsed_slots_count'] >= 0).all(),
                'reasonable_slot_count': (df['parsed_slots_count'] <= 10).all(),  # æœ€å¤§10ã‚¹ãƒ­ãƒƒãƒˆ/ãƒ¬ã‚³ãƒ¼ãƒ‰
                'data_not_empty': not df.empty,
                'consistent_dtypes': True  # ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
            }
            
            integrity_score = sum(checks.values()) / len(checks)
            
            integrity_results[dataset_name] = {
                'checks': checks,
                'integrity_score': integrity_score,
                'issues': [k for k, v in checks.items() if not v]
            }
            
            log.info(f"  {dataset_name}: æ•´åˆæ€§ã‚¹ã‚³ã‚¢ {integrity_score:.1%}")
            if integrity_results[dataset_name]['issues']:
                log.warning(f"    å•é¡Œ: {integrity_results[dataset_name]['issues']}")
        
        overall_integrity = np.mean([r['integrity_score'] for r in integrity_results.values()])
        log.info(f"ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ ç·åˆã‚¹ã‚³ã‚¢: {overall_integrity:.1%}")
        
        return {
            'overall_integrity': overall_integrity,
            'individual_results': integrity_results,
            'status': 'PASS' if overall_integrity >= 0.95 else 'FAIL'
        }
    
    def performance_benchmark(self, test_datasets: Dict[str, pd.DataFrame]) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        log.info("=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¤œè¨¼ ===")
        
        import time
        
        performance_results = {}
        
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        large_df = test_datasets['data_large']
        
        # 1. ãƒ‡ãƒ¼ã‚¿å‡¦ç†é€Ÿåº¦
        start_time = time.time()
        
        # åŸºæœ¬çš„ãªé›†è¨ˆå‡¦ç†
        role_counts = large_df['role'].value_counts()
        employment_counts = large_df['employment'].value_counts()
        hourly_distribution = large_df['ds'].dt.hour.value_counts()
        
        processing_time = time.time() - start_time
        
        # 2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
        memory_usage = large_df.memory_usage(deep=True).sum()
        
        performance_results = {
            'dataset_size': len(large_df),
            'processing_time_seconds': processing_time,
            'memory_usage_mb': memory_usage / 1024 / 1024,
            'records_per_second': len(large_df) / processing_time if processing_time > 0 else float('inf'),
            'performance_acceptable': processing_time < 1.0,  # 1ç§’ä»¥å†…
            'memory_efficient': memory_usage < 50 * 1024 * 1024  # 50MBä»¥å†…
        }
        
        log.info(f"  ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {len(large_df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")
        log.info(f"  å‡¦ç†æ™‚é–“: {processing_time:.3f}ç§’")
        log.info(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage/1024/1024:.1f}MB")
        log.info(f"  å‡¦ç†é€Ÿåº¦: {performance_results['records_per_second']:.0f}ãƒ¬ã‚³ãƒ¼ãƒ‰/ç§’")
        
        return {
            'performance_results': performance_results,
            'status': 'PASS' if performance_results['performance_acceptable'] and performance_results['memory_efficient'] else 'FAIL'
        }
    
    def generate_final_report(self) -> str:
        """æœ€çµ‚æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        log.info("=== æœ€çµ‚æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
        
        # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_datasets = self.create_test_dataset()
        
        # 2. å„æ¤œè¨¼å®Ÿè¡Œ
        verifications = {
            'dynamic_slot_detection': self.verify_dynamic_slot_detection(test_datasets),
            'time_axis_calculation': self.verify_time_axis_calculation(test_datasets),
            'heatmap_optimization': self.verify_heatmap_optimization(test_datasets),
            'data_integrity': self.verify_data_integrity(test_datasets),
            'performance': self.performance_benchmark(test_datasets)
        }
        
        # 3. ç·åˆè©•ä¾¡
        passed_tests = sum(1 for v in verifications.values() if v.get('status') == 'PASS')
        total_tests = len(verifications)
        overall_score = passed_tests / total_tests
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = f"""
# æœ€çµ‚åˆ†æç²¾åº¦æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ç·åˆè©•ä¾¡
- **ç·åˆã‚¹ã‚³ã‚¢**: {overall_score:.1%} ({passed_tests}/{total_tests} ãƒ†ã‚¹ãƒˆé€šé)
- **ç·åˆåˆ¤å®š**: {'âœ… PASS' if overall_score >= 0.8 else 'âŒ FAIL'}

## å€‹åˆ¥æ¤œè¨¼çµæœ

### 1. å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”æ¤œå‡º
- **çŠ¶æ…‹**: {verifications['dynamic_slot_detection'].get('status', 'UNKNOWN')}
- **ç²¾åº¦**: {verifications['dynamic_slot_detection'].get('overall_accuracy', 0):.1%}

### 2. æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹ä¸è¶³æ™‚é–“è¨ˆç®—
- **çŠ¶æ…‹**: {verifications['time_axis_calculation'].get('status', 'UNKNOWN')}
- **æ•´åˆæ€§**: {verifications['time_axis_calculation'].get('consistency_rate', 0):.1%}

### 3. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æœ€é©åŒ–
- **çŠ¶æ…‹**: {verifications['heatmap_optimization'].get('status', 'UNKNOWN')}
- **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: {verifications['heatmap_optimization'].get('optimization_results', {}).get('memory_reduction_rate', 0):.1%}

### 4. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§
- **çŠ¶æ…‹**: {verifications['data_integrity'].get('status', 'UNKNOWN')}
- **æ•´åˆæ€§ã‚¹ã‚³ã‚¢**: {verifications['data_integrity'].get('overall_integrity', 0):.1%}

### 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **çŠ¶æ…‹**: {verifications['performance'].get('status', 'UNKNOWN')}
- **å‡¦ç†é€Ÿåº¦**: {verifications['performance'].get('performance_results', {}).get('records_per_second', 0):.0f} ãƒ¬ã‚³ãƒ¼ãƒ‰/ç§’

## æ¨å¥¨äº‹é …
"""
        
        # æ¨å¥¨äº‹é …ã®è¿½åŠ 
        if overall_score >= 0.9:
            report += "- âœ… å…¨ã¦ã®æ©Ÿèƒ½ãŒé«˜ã„ç²¾åº¦ã§å‹•ä½œã—ã¦ã„ã¾ã™ã€‚æœ¬ç•ªç’°å¢ƒã¸ã®é©ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚\n"
        elif overall_score >= 0.8:
            report += "- âš ï¸ ä¸€éƒ¨ã®æ©Ÿèƒ½ã«æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚è©³ç´°ã‚’ç¢ºèªã—ã¦å¯¾å¿œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚\n"
        else:
            report += "- âŒ é‡è¦ãªå•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚æœ¬ç•ªé©ç”¨å‰ã«ä¿®æ­£ãŒå¿…è¦ã§ã™ã€‚\n"
        
        # è©³ç´°çµæœã‚’JSONã§ä¿å­˜
        detailed_results = {
            'timestamp': datetime.now().isoformat(),
            'overall_score': overall_score,
            'verifications': verifications
        }
        
        with open('final_verification_results.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2, default=str)
        
        return report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 80)
    print("ã‚·ãƒ•ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ  - æœ€çµ‚åˆ†æç²¾åº¦æ¤œè¨¼")
    print("=" * 80)
    
    verifier = FinalAnalysisVerifier()
    
    try:
        # æœ€çµ‚æ¤œè¨¼å®Ÿè¡Œ
        report = verifier.generate_final_report()
        
        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        print(report)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with open('final_verification_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"\nğŸ“Š è©³ç´°ãªæ¤œè¨¼çµæœã¯ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ:")
        print(f"  - final_verification_report.md")
        print(f"  - final_verification_results.json")
        
    except Exception as e:
        log.error(f"æ¤œè¨¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()