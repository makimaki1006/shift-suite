#!/usr/bin/env python3
"""
çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  å…¨ä½“ãƒ•ãƒ­ãƒ¼åˆ†æ
å…¨ä½“æœ€é©ã®è¦³ç‚¹ã‹ã‚‰å•é¡Œã‚’ç¶²ç¾…çš„ã«æ¤œå‡º
"""

import re
from pathlib import Path
from datetime import datetime

class ComprehensiveFlowAnalyzer:
    """å…¨ä½“ãƒ•ãƒ­ãƒ¼ã®åŒ…æ‹¬çš„åˆ†æ"""
    
    def __init__(self):
        self.issues = []
        self.flow_map = {}
        self.critical_paths = []
        
    def analyze_complete_flow(self):
        """å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã‚’åˆ†æ"""
        print("ğŸ” çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  å…¨ä½“ãƒ•ãƒ­ãƒ¼åˆ†æ")
        print("=" * 80)
        
        # 1. ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ•ãƒ­ãƒ¼
        self.analyze_data_input_flow()
        
        # 2. åˆ†æå®Ÿè¡Œãƒ•ãƒ­ãƒ¼
        self.analyze_analysis_execution_flow()
        
        # 3. çµæœä¿å­˜ãƒ•ãƒ­ãƒ¼
        self.analyze_result_storage_flow()
        
        # 4. ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ•ãƒ­ãƒ¼
        self.analyze_data_retrieval_flow()
        
        # 5. å‡ºåŠ›ç”Ÿæˆãƒ•ãƒ­ãƒ¼
        self.analyze_output_generation_flow()
        
        # 6. ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ•ãƒ­ãƒ¼
        self.analyze_error_handling_flow()
        
        # 7. è¨­å®šä¼æ’­ãƒ•ãƒ­ãƒ¼
        self.analyze_configuration_propagation_flow()
        
        return self.generate_comprehensive_report()
    
    def analyze_data_input_flow(self):
        """ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n1ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        # app.pyã§ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’ç¢ºèª
        app_path = Path("app.py")
        if app_path.exists():
            with open(app_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰é–¢é€£ã®å‡¦ç†ã‚’æ¤œç´¢
            upload_patterns = [
                r'st\.file_uploader',
                r'uploaded_file',
                r'file_name\s*=',
                r'Path\(.*\)\.stem'
            ]
            
            for pattern in upload_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    print(f"  âœ“ {pattern}: {len(matches)}ç®‡æ‰€")
            
            # å•é¡Œ: ãƒ•ã‚¡ã‚¤ãƒ«åã®æ‰±ã„ãŒä¸€è²«ã—ã¦ã„ãªã„å¯èƒ½æ€§
            if 'Path(' in content and '.stem' in content:
                stem_count = content.count('.stem')
                path_count = content.count('Path(')
                if stem_count < path_count / 2:
                    self.issues.append({
                        'severity': 'HIGH',
                        'category': 'ãƒ‡ãƒ¼ã‚¿å…¥åŠ›',
                        'issue': 'ãƒ•ã‚¡ã‚¤ãƒ«åã®æ‰±ã„ãŒä¸€è²«ã—ã¦ã„ãªã„',
                        'detail': f'Pathä½¿ç”¨: {path_count}å›, stemä½¿ç”¨: {stem_count}å›',
                        'fix': 'ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«åå‡¦ç†ã§ä¸€è²«ã—ã¦Path().stemã‚’ä½¿ç”¨'
                    })
    
    def analyze_analysis_execution_flow(self):
        """åˆ†æå®Ÿè¡Œãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n2ï¸âƒ£ åˆ†æå®Ÿè¡Œãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        # å„åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ã‚’ç¢ºèª
        modules = ['shortage', 'fatigue', 'fairness']
        app_path = Path("app.py")
        
        if app_path.exists():
            with open(app_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for module in modules:
                print(f"\n  {module}åˆ†æ:")
                
                # run_taskãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™
                run_pattern = f'run_task.*{module}'
                run_matches = re.findall(run_pattern, content, re.IGNORECASE)
                print(f"    run_taskå‘¼ã³å‡ºã—: {len(run_matches)}å›")
                
                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ç™»éŒ²ã‚’æ¢ã™
                unified_pattern = f'unified_analysis_manager\\.create_{module}_analysis'
                unified_matches = re.findall(unified_pattern, content)
                print(f"    çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ç™»éŒ²: {len(unified_matches)}å›")
                
                # å•é¡Œ: åˆ†æå®Ÿè¡Œã¨çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ç™»éŒ²ã®ä¸ä¸€è‡´
                if len(run_matches) != len(unified_matches):
                    self.issues.append({
                        'severity': 'CRITICAL',
                        'category': 'åˆ†æå®Ÿè¡Œ',
                        'issue': f'{module}åˆ†æã®å®Ÿè¡Œã¨çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ç™»éŒ²ã®ä¸ä¸€è‡´',
                        'detail': f'å®Ÿè¡Œ: {len(run_matches)}å›, ç™»éŒ²: {len(unified_matches)}å›',
                        'fix': 'å„åˆ†æå®Ÿè¡Œå¾Œã«å¿…ãšçµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ç™»éŒ²ã‚’è¡Œã†'
                    })
    
    def analyze_result_storage_flow(self):
        """çµæœä¿å­˜ãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n3ï¸âƒ£ çµæœä¿å­˜ãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        # unified_analysis_manager.pyã®çµæœä¿å­˜å‡¦ç†ã‚’ç¢ºèª
        uam_path = Path("shift_suite/tasks/unified_analysis_manager.py")
        if uam_path.exists():
            with open(uam_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¸ã®ä¿å­˜å‡¦ç†ã‚’ç¢ºèª
            registry_saves = content.count('self.results_registry[')
            print(f"  ãƒ¬ã‚¸ã‚¹ãƒˆãƒªä¿å­˜: {registry_saves}ç®‡æ‰€")
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®ç¢ºèª
            cleanup_exists = 'cleanup_old_results' in content
            print(f"  ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½: {'ã‚ã‚Š' if cleanup_exists else 'ãªã—'}")
            
            # å•é¡Œ: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒè‡ªå‹•å®Ÿè¡Œã•ã‚Œã¦ã„ãªã„
            if cleanup_exists:
                app_content = open("app.py", 'r', encoding='utf-8').read()
                if 'cleanup_old_results' not in app_content:
                    self.issues.append({
                        'severity': 'MEDIUM',
                        'category': 'çµæœä¿å­˜',
                        'issue': 'ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒè‡ªå‹•å®Ÿè¡Œã•ã‚Œã¦ã„ãªã„',
                        'detail': 'cleanup_old_resultsãƒ¡ã‚½ãƒƒãƒ‰ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚‹ãŒå‘¼ã°ã‚Œã¦ã„ãªã„',
                        'fix': 'åˆ†æå®Œäº†å¾Œã«è‡ªå‹•çš„ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ'
                    })
    
    def analyze_data_retrieval_flow(self):
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n4ï¸âƒ£ ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        # get_ai_compatible_resultsã®ä½¿ç”¨çŠ¶æ³ã‚’ç¢ºèª
        app_path = Path("app.py")
        if app_path.exists():
            with open(app_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # get_ai_compatible_resultså‘¼ã³å‡ºã—ã‚’æ¢ã™
            get_pattern = r'get_ai_compatible_results\((.*?)\)'
            get_matches = re.findall(get_pattern, content)
            
            print(f"  get_ai_compatible_resultså‘¼ã³å‡ºã—: {len(get_matches)}å›")
            for i, match in enumerate(get_matches):
                print(f"    {i+1}: get_ai_compatible_results({match})")
                
                # å•é¡Œ: ãƒ•ã‚¡ã‚¤ãƒ«åã®å½¢å¼ãŒä¸€è²«ã—ã¦ã„ãªã„
                if 'file_name' in match and '.stem' not in content[max(0, content.find(match)-200):content.find(match)]:
                    self.issues.append({
                        'severity': 'HIGH',
                        'category': 'ãƒ‡ãƒ¼ã‚¿å–å¾—',
                        'issue': 'ãƒ•ã‚¡ã‚¤ãƒ«åå½¢å¼ã®ä¸ä¸€è‡´',
                        'detail': f'get_ai_compatible_results({match})ã§æ‹¡å¼µå­ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨',
                        'fix': 'Path(file_name).stemã‚’ä½¿ç”¨ã—ã¦æ‹¡å¼µå­ã‚’é™¤å»'
                    })
    
    def analyze_output_generation_flow(self):
        """å‡ºåŠ›ç”Ÿæˆãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n5ï¸âƒ£ å‡ºåŠ›ç”Ÿæˆãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        # AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ç¢ºèª
        ai_gen_path = Path("shift_suite/tasks/ai_comprehensive_report_generator.py")
        if ai_gen_path.exists():
            with open(ai_gen_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã®ç¢ºèª
            integrity_checks = content.count('data_integrity')
            print(f"  ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯: {integrity_checks}ç®‡æ‰€")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ä½¿ç”¨ç¢ºèª
            default_patterns = [
                r'get\([\'"].*?[\'"],\s*0\)',
                r'get\([\'"].*?[\'"],\s*0\.0\)',
                r'get\([\'"].*?[\'"],\s*[\'"]N/A[\'"]'
            ]
            
            default_count = 0
            for pattern in default_patterns:
                default_count += len(re.findall(pattern, content))
            
            print(f"  ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨: {default_count}ç®‡æ‰€")
            
            # å•é¡Œ: éåº¦ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ä½¿ç”¨
            if default_count > 50:
                self.issues.append({
                    'severity': 'MEDIUM',
                    'category': 'å‡ºåŠ›ç”Ÿæˆ',
                    'issue': 'éåº¦ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®ä½¿ç”¨',
                    'detail': f'{default_count}ç®‡æ‰€ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨',
                    'fix': 'å®Ÿãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’ç¢ºèªã—ã¦ã‹ã‚‰é©åˆ‡ãªå‡¦ç†ã‚’è¡Œã†'
                })
    
    def analyze_error_handling_flow(self):
        """ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n6ï¸âƒ£ ã‚¨ãƒ©ãƒ¼å‡¦ç†ãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        files_to_check = [
            "app.py",
            "shift_suite/tasks/unified_analysis_manager.py",
            "shift_suite/tasks/shortage.py"
        ]
        
        total_try = 0
        total_except = 0
        bare_except = 0
        
        for file_path in files_to_check:
            if Path(file_path).exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                file_try = content.count('try:')
                file_except = content.count('except')
                file_bare = len(re.findall(r'except:\s*\n', content))
                
                total_try += file_try
                total_except += file_except
                bare_except += file_bare
                
                print(f"  {Path(file_path).name}: try={file_try}, except={file_except}, bare={file_bare}")
        
        # å•é¡Œ: æ±ç”¨çš„ã™ãã‚‹exceptç¯€
        if bare_except > 0:
            self.issues.append({
                'severity': 'LOW',
                'category': 'ã‚¨ãƒ©ãƒ¼å‡¦ç†',
                'issue': 'æ±ç”¨çš„ãªexceptç¯€ã®ä½¿ç”¨',
                'detail': f'{bare_except}ç®‡æ‰€ã§å…·ä½“çš„ãªä¾‹å¤–ã‚’æŒ‡å®šã—ã¦ã„ãªã„',
                'fix': 'å…·ä½“çš„ãªä¾‹å¤–ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šï¼ˆException as eãªã©ï¼‰'
            })
    
    def analyze_configuration_propagation_flow(self):
        """è¨­å®šä¼æ’­ãƒ•ãƒ­ãƒ¼ã®åˆ†æ"""
        print("\n7ï¸âƒ£ è¨­å®šä¼æ’­ãƒ•ãƒ­ãƒ¼åˆ†æ")
        
        # ã‚¹ãƒ­ãƒƒãƒˆè¨­å®šã®ä¼æ’­ã‚’ç¢ºèª
        slot_usage = {}
        files_to_check = [
            "shift_suite/tasks/shortage.py",
            "shift_suite/tasks/fatigue.py",
            "shift_suite/tasks/heatmap.py"
        ]
        
        for file_path in files_to_check:
            if Path(file_path).exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ã‚¹ãƒ­ãƒƒãƒˆé–¢é€£ã®ä½¿ç”¨ã‚’ç¢ºèª
                slot_minutes = content.count('slot_minutes')
                slot_hours = content.count('slot_hours')
                slot_fixed = content.count('SLOT_HOURS')
                
                slot_usage[Path(file_path).name] = {
                    'slot_minutes': slot_minutes,
                    'slot_hours': slot_hours,
                    'SLOT_HOURS': slot_fixed
                }
                
                print(f"  {Path(file_path).name}: minutes={slot_minutes}, hours={slot_hours}, FIXED={slot_fixed}")
                
                # å•é¡Œ: å›ºå®šå€¤ã®ä½¿ç”¨
                if slot_fixed > 0:
                    self.issues.append({
                        'severity': 'HIGH',
                        'category': 'è¨­å®šä¼æ’­',
                        'issue': f'{Path(file_path).name}ã§å›ºå®šSLOT_HOURSä½¿ç”¨',
                        'detail': f'{slot_fixed}ç®‡æ‰€ã§å›ºå®šå€¤ã‚’ä½¿ç”¨',
                        'fix': 'å‹•çš„ãªslot_minutesãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨'
                    })
    
    def generate_comprehensive_report(self):
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        print("\n" + "=" * 80)
        print("ğŸ“Š å…¨ä½“ãƒ•ãƒ­ãƒ¼åˆ†æçµæœ")
        print("=" * 80)
        
        # é‡è¦åº¦åˆ¥ã«å•é¡Œã‚’åˆ†é¡
        critical_issues = [i for i in self.issues if i['severity'] == 'CRITICAL']
        high_issues = [i for i in self.issues if i['severity'] == 'HIGH']
        medium_issues = [i for i in self.issues if i['severity'] == 'MEDIUM']
        low_issues = [i for i in self.issues if i['severity'] == 'LOW']
        
        print(f"\nç™ºè¦‹ã•ã‚ŒãŸå•é¡Œ: åˆè¨ˆ{len(self.issues)}ä»¶")
        print(f"  ğŸ”´ CRITICAL: {len(critical_issues)}ä»¶")
        print(f"  ğŸŸ  HIGH: {len(high_issues)}ä»¶")
        print(f"  ğŸŸ¡ MEDIUM: {len(medium_issues)}ä»¶")
        print(f"  ğŸŸ¢ LOW: {len(low_issues)}ä»¶")
        
        # è©³ç´°è¡¨ç¤º
        for severity, issues in [
            ('CRITICAL', critical_issues),
            ('HIGH', high_issues),
            ('MEDIUM', medium_issues),
            ('LOW', low_issues)
        ]:
            if issues:
                print(f"\n{severity}ãƒ¬ãƒ™ãƒ«ã®å•é¡Œ:")
                for issue in issues:
                    print(f"\n  [{issue['category']}] {issue['issue']}")
                    print(f"    è©³ç´°: {issue['detail']}")
                    print(f"    ä¿®æ­£: {issue['fix']}")
        
        return self.issues

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    analyzer = ComprehensiveFlowAnalyzer()
    issues = analyzer.analyze_complete_flow()
    
    # ä¿®æ­£è¨ˆç”»ã®ç”Ÿæˆ
    print("\n" + "=" * 80)
    print("ğŸ”§ å…¨ä½“æœ€é©åŒ–ä¿®æ­£è¨ˆç”»")
    print("=" * 80)
    
    if not issues:
        print("âœ… é‡å¤§ãªå•é¡Œã¯ç™ºè¦‹ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    else:
        print("\nå„ªå…ˆé †ä½ã«å¾“ã£ã¦ä»¥ä¸‹ã®ä¿®æ­£ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ï¼š")
        
        critical_count = len([i for i in issues if i['severity'] == 'CRITICAL'])
        if critical_count > 0:
            print(f"\n1. å³åº§ã«ä¿®æ­£ã™ã¹ãCRITICALå•é¡Œ: {critical_count}ä»¶")
            print("   ã“ã‚Œã‚‰ã¯çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬å‹•ä½œã«å½±éŸ¿ã—ã¾ã™")
        
        high_count = len([i for i in issues if i['severity'] == 'HIGH'])
        if high_count > 0:
            print(f"\n2. æœ¬æ—¥ä¸­ã«ä¿®æ­£ã™ã¹ãHIGHå•é¡Œ: {high_count}ä»¶")
            print("   ã“ã‚Œã‚‰ã¯ãƒ‡ãƒ¼ã‚¿ã®æ­£ç¢ºæ€§ã«å½±éŸ¿ã—ã¾ã™")

if __name__ == "__main__":
    main()