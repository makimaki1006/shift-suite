#!/usr/bin/env python3
"""
è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ  - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±åˆçš„åˆ¶ç´„ç™ºè¦‹ã‚’å®Ÿè¡Œ
é«˜åº¦æ„å›³ç™ºè¦‹ã¨åˆ¶ç´„æ˜‡è¯ã‚’è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã§çµ±åˆå®Ÿè¡Œ
"""

import sys
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime
from collections import Counter
from dataclasses import dataclass, asdict
from enum import Enum

# æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from advanced_constraint_discovery_engine import (
        AdvancedIntentionDiscovery,
        ConstraintElevationEngine,
        ConstraintRule, ConstraintType, ConstraintAxis
    )
    from direct_excel_reader import DirectExcelReader, ShiftPatternAnalyzer
except ImportError as e:
    print(f"å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“: {e}")
    sys.exit(1)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class CompoundConstraintDiscoverySystem:
    """è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ  - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆåˆ†æ"""
    
    def __init__(self):
        self.system_name = "è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ "
        self.version = "3.0.0"
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.reader = DirectExcelReader()
        self.analyzer = ShiftPatternAnalyzer()
        self.intention_engine = AdvancedIntentionDiscovery()
        self.constraint_engine = ConstraintElevationEngine()
        
        # çµæœæ ¼ç´
        self.all_constraint_rules = []
        self.all_patterns = {}
        self.processed_files = []
        self.file_analysis_stats = {}
    
    def discover_compound_constraints(self, excel_directory: str = ".") -> Dict[str, Any]:
        """è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
        print("=" * 80)
        print(f"{self.system_name} v{self.version} - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«çµ±åˆåˆ¶ç´„ç™ºè¦‹")
        print("=" * 80)
        
        # Phase 1: ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã¨å‰å‡¦ç†
        excel_files = self._discover_excel_files(excel_directory)
        if not excel_files:
            print("[ERROR] åˆ†æå¯¾è±¡ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}
        
        print(f"ç™ºè¦‹ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«: {len(excel_files)}å€‹")
        for i, f in enumerate(excel_files, 1):
            print(f"  {i}. {f.name} ({f.stat().st_size:,}ãƒã‚¤ãƒˆ)")
        
        # Phase 2: å„ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ¶ç´„ç™ºè¦‹
        print(f"\n{'='*60}")
        print("Phase 2: å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶ç´„ç™ºè¦‹")
        print(f"{'='*60}")
        
        for excel_file in excel_files:
            self._process_single_file(excel_file)
        
        # Phase 3: è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        print(f"\n{'='*60}")
        print(f"Phase 3: è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        print(f"{'='*60}")
        
        compound_patterns = self._analyze_compound_patterns()
        
        # Phase 4: çµ±åˆåˆ¶ç´„ç”Ÿæˆ
        print(f"\n{'='*60}")
        print("Phase 4: çµ±åˆåˆ¶ç´„ç”Ÿæˆ")
        print(f"{'='*60}")
        
        integrated_constraints = self._generate_integrated_constraints(compound_patterns)
        
        # Phase 5: çµæœçµ±åˆã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        final_report = self._generate_comprehensive_report(integrated_constraints)
        
        return final_report
    
    def _discover_excel_files(self, directory: str) -> List[Path]:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹"""
        path = Path(directory)
        excel_files = list(path.glob("*.xlsx"))
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–
        filtered_files = []
        for f in excel_files:
            if not f.name.startswith('~') and not f.name.endswith('.tmp'):
                filtered_files.append(f)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã§ã‚½ãƒ¼ãƒˆï¼ˆå¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ï¼‰
        return sorted(filtered_files, key=lambda x: x.stat().st_size, reverse=True)
    
    def _process_single_file(self, excel_file: Path) -> bool:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ¶ç´„ç™ºè¦‹å‡¦ç†"""
        print(f"\n--- åˆ†æä¸­: {excel_file.name} ---")
        
        try:
            # Excelèª­ã¿è¾¼ã¿
            data = self.reader.read_xlsx_as_zip(str(excel_file))
            if not data:
                print(f"[SKIP] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—: {excel_file.name}")
                return False
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            patterns = self.analyzer.analyze_raw_data(data)
            if not patterns.get("staff_shifts"):
                print(f"[SKIP] ã‚¹ã‚¿ãƒƒãƒ•ãƒ‡ãƒ¼ã‚¿ãªã—: {excel_file.name}")
                return False
            
            # æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹
            deep_patterns = self.intention_engine.discover_deep_patterns(data)
            
            # åˆ¶ç´„æ˜‡è¯
            constraint_rules = self.constraint_engine.elevate_to_constraints(deep_patterns)
            
            # çµæœè“„ç©
            self.all_constraint_rules.extend(constraint_rules)
            self.all_patterns[str(excel_file)] = patterns
            self.processed_files.append(excel_file.name)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆè¨˜éŒ²
            self.file_analysis_stats[excel_file.name] = {
                "staff_count": len(patterns.get("staff_shifts", {})),
                "shift_codes": len(patterns.get("shift_codes", set())),
                "constraint_rules": len(constraint_rules),
                "implicit_rules": len(patterns.get("implicit_rules", [])),
                "file_size": excel_file.stat().st_size
            }
            
            print(f"[OK] {excel_file.name}: {len(constraint_rules)}å€‹ã®åˆ¶ç´„ç™ºè¦‹")
            print(f"     ã‚¹ã‚¿ãƒƒãƒ•: {len(patterns.get('staff_shifts', {}))}å")
            print(f"     ã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰: {len(patterns.get('shift_codes', set()))}ç¨®é¡")
            
            return True
            
        except Exception as e:
            print(f"[ERROR] {excel_file.name}: {e}")
            return False
    
    def _analyze_compound_patterns(self) -> Dict[str, Any]:
        """è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ - ãƒ•ã‚¡ã‚¤ãƒ«é–“å…±é€šæ€§ã®ç™ºè¦‹"""
        if not self.processed_files:
            return {}
        
        print(f"è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æé–‹å§‹: {len(self.processed_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        
        compound_patterns = {
            "cross_file_staff_patterns": {},
            "universal_shift_codes": {},
            "constraint_consistency": {},
            "organizational_constants": {}
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«é–“ã‚¹ã‚¿ãƒƒãƒ•åˆ†æ
        staff_appearances = {}
        for file_path, patterns in self.all_patterns.items():
            for staff_name in patterns.get("staff_shifts", {}):
                if staff_name not in staff_appearances:
                    staff_appearances[staff_name] = []
                staff_appearances[staff_name].append(file_path)
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç™»å ´ã‚¹ã‚¿ãƒƒãƒ•
        cross_file_staff = {
            name: files for name, files in staff_appearances.items() 
            if len(files) > 1
        }
        compound_patterns["cross_file_staff_patterns"] = cross_file_staff
        
        # å…±é€šã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰åˆ†æ
        shift_code_frequency = {}
        for patterns in self.all_patterns.values():
            for code in patterns.get("shift_codes", set()):
                shift_code_frequency[code] = shift_code_frequency.get(code, 0) + 1
        
        universal_codes = {
            code: freq for code, freq in shift_code_frequency.items() 
            if freq >= len(self.processed_files) * 0.5  # 50%ä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ç™»å ´
        }
        compound_patterns["universal_shift_codes"] = universal_codes
        
        # åˆ¶ç´„ä¸€è²«æ€§åˆ†æ
        constraint_rule_patterns = {}
        for rule in self.all_constraint_rules:
            pattern_key = f"{rule.constraint_type.value}_{rule.axis.value}"
            if pattern_key not in constraint_rule_patterns:
                constraint_rule_patterns[pattern_key] = []
            constraint_rule_patterns[pattern_key].append(rule)
        
        compound_patterns["constraint_consistency"] = {
            pattern: len(rules) for pattern, rules in constraint_rule_patterns.items()
        }
        
        print(f"è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹:")
        print(f"  - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç™»å ´ã‚¹ã‚¿ãƒƒãƒ•: {len(cross_file_staff)}å")
        print(f"  - å…±é€šã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰: {len(universal_codes)}ç¨®é¡")
        print(f"  - åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(constraint_rule_patterns)}ç¨®é¡")
        
        return compound_patterns
    
    def _generate_integrated_constraints(self, compound_patterns: Dict[str, Any]) -> List[ConstraintRule]:
        """çµ±åˆåˆ¶ç´„ç”Ÿæˆ - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‹ã‚‰å¼·åŒ–åˆ¶ç´„ã‚’ç”Ÿæˆ"""
        print("çµ±åˆåˆ¶ç´„ç”Ÿæˆé–‹å§‹")
        
        integrated_constraints = []
        
        # æ—¢å­˜åˆ¶ç´„ã®å¼·åŒ–ï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã§åŒæ§˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆï¼‰
        staff_constraint_strengthening = {}
        
        # ã‚¹ã‚¿ãƒƒãƒ•ã”ã¨ã®åˆ¶ç´„å¼·åŒ–åˆ†æ
        for rule in self.all_constraint_rules:
            if rule.axis == ConstraintAxis.STAFF_AXIS:
                # ãƒ«ãƒ¼ãƒ«ã‹ã‚‰ã‚¹ã‚¿ãƒƒãƒ•åã‚’æŠ½å‡º
                staff_match = rule.condition.split("==")[1].strip(" '")
                
                if staff_match not in staff_constraint_strengthening:
                    staff_constraint_strengthening[staff_match] = []
                staff_constraint_strengthening[staff_match].append(rule)
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã§ä¸€è²«æ€§ã®ã‚ã‚‹ã‚¹ã‚¿ãƒƒãƒ•åˆ¶ç´„ã‚’å¼·åŒ–
        for staff_name, rules in staff_constraint_strengthening.items():
            if len(rules) > 1:  # è¤‡æ•°åˆ¶ç´„ãŒå­˜åœ¨
                # åˆ¶ç´„ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
                consistent_rules = self._find_consistent_rules(rules)
                
                if consistent_rules:
                    # å¼·åŒ–åˆ¶ç´„ç”Ÿæˆ
                    enhanced_rule = self._create_enhanced_constraint(
                        staff_name, consistent_rules, compound_patterns
                    )
                    if enhanced_rule:
                        integrated_constraints.append(enhanced_rule)
        
        # çµ„ç¹”ãƒ¬ãƒ™ãƒ«åˆ¶ç´„ç”Ÿæˆ
        organizational_constraints = self._generate_organizational_constraints(compound_patterns)
        integrated_constraints.extend(organizational_constraints)
        
        print(f"çµ±åˆåˆ¶ç´„ç”Ÿæˆå®Œäº†: {len(integrated_constraints)}å€‹ã®å¼·åŒ–åˆ¶ç´„")
        
        return integrated_constraints
    
    def _find_consistent_rules(self, rules: List[ConstraintRule]) -> List[ConstraintRule]:
        """ä¸€è²«æ€§ã®ã‚ã‚‹åˆ¶ç´„ãƒ«ãƒ¼ãƒ«ã‚’ç™ºè¦‹"""
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        action_groups = {}
        for rule in rules:
            action_key = rule.action.split("(")[0].strip()  # æ‹¬å¼§å‰ã®éƒ¨åˆ†ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            if action_key not in action_groups:
                action_groups[action_key] = []
            action_groups[action_key].append(rule)
        
        # æœ€ã‚‚å¤šã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¡ç”¨
        if action_groups:
            most_common_action = max(action_groups, key=lambda k: len(action_groups[k]))
            return action_groups[most_common_action]
        
        return []
    
    def _create_enhanced_constraint(self, staff_name: str, rules: List[ConstraintRule], 
                                  compound_patterns: Dict[str, Any]) -> Optional[ConstraintRule]:
        """å¼·åŒ–åˆ¶ç´„ä½œæˆ"""
        if not rules:
            return None
        
        # ç¢ºä¿¡åº¦å¹³å‡å€¤è¨ˆç®—
        avg_confidence = sum(r.confidence for r in rules) / len(rules)
        avg_measurement = sum(r.measurement for r in rules) / len(rules)
        
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ä¸€è²«æ€§ã«ã‚ˆã‚Šåˆ¶ç´„ã‚¿ã‚¤ãƒ—ã‚’å¼·åŒ–
        if len(rules) >= 3:  # 3ãƒ•ã‚¡ã‚¤ãƒ«ä»¥ä¸Šã§åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³
            constraint_type = ConstraintType.STATIC_HARD
            violation_penalty = "ERROR: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è²«æ€§é•å"
        elif len(rules) >= 2:
            constraint_type = ConstraintType.STATIC_SOFT
            violation_penalty = "WARNING: ãƒ•ã‚¡ã‚¤ãƒ«é–“ä¸æ•´åˆ"
        else:
            constraint_type = ConstraintType.DYNAMIC_SOFT
            violation_penalty = "INFO: ä¸€èˆ¬çš„åˆ¶ç´„"
        
        # å…±é€šã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
        base_rule = rules[0]
        enhanced_action = f"{base_rule.action} [è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªæ¸ˆã¿: {len(rules)}ãƒ•ã‚¡ã‚¤ãƒ«]"
        
        enhanced_rule = ConstraintRule(
            rule_id=f"ENHANCED_{staff_name}_{len(rules)}FILES",
            constraint_type=constraint_type,
            axis=ConstraintAxis.STAFF_AXIS,
            condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}'",
            action=enhanced_action,
            confidence=avg_confidence,
            evidence={
                "file_consistency_count": len(rules),
                "evidence_rules": [r.rule_id for r in rules],
                "avg_measurement": avg_measurement,
                "consistency_strength": "HIGH" if len(rules) >= 3 else "MEDIUM"
            },
            measurement=avg_measurement,
            violation_penalty=violation_penalty
        )
        
        return enhanced_rule
    
    def _generate_organizational_constraints(self, compound_patterns: Dict[str, Any]) -> List[ConstraintRule]:
        """çµ„ç¹”ãƒ¬ãƒ™ãƒ«åˆ¶ç´„ç”Ÿæˆ"""
        org_constraints = []
        
        # å…±é€šã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰åˆ¶ç´„
        for shift_code, frequency in compound_patterns.get("universal_shift_codes", {}).items():
            if frequency >= len(self.processed_files):  # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«ç™»å ´
                rule = ConstraintRule(
                    rule_id=f"ORG_UNIVERSAL_{shift_code}",
                    constraint_type=ConstraintType.STATIC_SOFT,
                    axis=ConstraintAxis.TASK_AXIS,
                    condition=f"çµ„ç¹”é‹ç”¨ == 'å…¨ä½“'",
                    action=f"'{shift_code}'ã‚·ãƒ•ãƒˆã‚’æ¨™æº–é‹ç”¨ã¨ã—ã¦ç¶­æŒ",
                    confidence=1.0,
                    evidence={
                        "file_coverage": frequency,
                        "universality": "COMPLETE"
                    },
                    measurement=100.0,
                    violation_penalty="WARNING: æ¨™æº–é‹ç”¨é€¸è„±"
                )
                org_constraints.append(rule)
        
        return org_constraints
    
    def _generate_comprehensive_report(self, integrated_constraints: List[ConstraintRule]) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"\n{'='*80}")
        print("ã€è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ æœ€çµ‚çµæœã€‘")
        print(f"{'='*80}")
        
        total_constraints = len(self.all_constraint_rules) + len(integrated_constraints)
        
        print(f"\nâ—† å‡¦ç†ã‚µãƒãƒªãƒ¼")
        print(f"  - åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(self.processed_files)}")
        print(f"  - åŸºæœ¬åˆ¶ç´„ãƒ«ãƒ¼ãƒ«: {len(self.all_constraint_rules)}å€‹")
        print(f"  - å¼·åŒ–åˆ¶ç´„ãƒ«ãƒ¼ãƒ«: {len(integrated_constraints)}å€‹")
        print(f"  - ç·åˆ¶ç´„ãƒ«ãƒ¼ãƒ«æ•°: {total_constraints}å€‹")
        
        # åˆ¶ç´„ã‚¿ã‚¤ãƒ—çµ±è¨ˆ
        all_constraints = self.all_constraint_rules + integrated_constraints
        type_stats = Counter(r.constraint_type.value for r in all_constraints)
        axis_stats = Counter(r.axis.value for r in all_constraints)
        
        print(f"\nâ—† åˆ¶ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ:")
        for ctype, count in sorted(type_stats.items()):
            print(f"  - {ctype}: {count}å€‹")
        
        print(f"\nâ—† åˆ¶ç´„è»¸åˆ†å¸ƒ:")
        for axis, count in sorted(axis_stats.items()):
            print(f"  - {axis}: {count}å€‹")
        
        # é«˜ç¢ºä¿¡åº¦åˆ¶ç´„è¡¨ç¤º
        high_confidence = [r for r in all_constraints if r.confidence > 0.8]
        print(f"\nâ—† é«˜ç¢ºä¿¡åº¦åˆ¶ç´„ (>80%): {len(high_confidence)}å€‹")
        
        for rule in sorted(high_confidence, key=lambda x: x.confidence, reverse=True)[:10]:
            print(f"\n[{rule.rule_id}] ç¢ºä¿¡åº¦: {rule.confidence:.1%}")
            print(f"  æ¡ä»¶: {rule.condition}")
            print(f"  ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {rule.action}")
            print(f"  æ¸¬å®šå€¤: {rule.measurement:.1f}")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        final_report = {
            "system_metadata": {
                "system_name": self.system_name,
                "version": self.version,
                "timestamp": datetime.now().isoformat(),
                "analyzed_files": self.processed_files,
                "total_files": len(self.processed_files)
            },
            "constraint_discovery_summary": {
                "basic_constraints": len(self.all_constraint_rules),
                "enhanced_constraints": len(integrated_constraints),
                "total_constraints": total_constraints,
                "high_confidence_count": len(high_confidence),
                "constraint_type_distribution": dict(type_stats),
                "constraint_axis_distribution": dict(axis_stats)
            },
            "file_analysis_stats": self.file_analysis_stats,
            "all_constraint_rules": [
                {
                    "rule_id": rule.rule_id,
                    "constraint_type": rule.constraint_type.value,
                    "axis": rule.axis.value,
                    "condition": rule.condition,
                    "action": rule.action,
                    "confidence": rule.confidence,
                    "measurement": rule.measurement,
                    "violation_penalty": rule.violation_penalty,
                    "evidence": rule.evidence,
                    "rule_category": "enhanced" if rule in integrated_constraints else "basic"
                }
                for rule in sorted(all_constraints, key=lambda x: x.confidence, reverse=True)
            ],
            "compound_analysis_insights": {
                "cross_file_consistency": len([r for r in integrated_constraints if "è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«" in r.action]),
                "organizational_constraints": len([r for r in integrated_constraints if r.rule_id.startswith("ORG_")]),
                "constraint_strengthening_success": len(integrated_constraints) > 0,
                "analysis_depth_improvement": "significant" if total_constraints > 50 else "moderate"
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_filename = f"compound_constraint_discovery_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, "w", encoding="utf-8") as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
        
        print(f"\n[å®Œäº†] è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†")
        print(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_filename}")
        print(f"ğŸ¯ åˆ¶ç´„ç™ºè¦‹æˆæœ: {total_constraints}å€‹ã®åˆ¶ç´„ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ")
        
        return final_report

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    system = CompoundConstraintDiscoverySystem()
    
    try:
        report = system.discover_compound_constraints(".")
        
        if report and report.get("constraint_discovery_summary", {}).get("total_constraints", 0) > 0:
            print(f"\nâœ… æˆåŠŸ: è¤‡åˆåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸å®Œäº†")
            return 0
        else:
            print(f"\nâŒ åˆ¶ç´„ç™ºè¦‹ã«å¤±æ•—")
            return 1
            
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())