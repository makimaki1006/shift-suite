#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A3.1.2 ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
Phase 2/3.1é–¢é€£ã‚¨ãƒ©ãƒ¼ã®å°‚é–€çš„æ¤œçŸ¥ãƒ»åˆ†æ
"""

import os
import sys
import time
import json
import re
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class ErrorPattern:
    """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©"""
    pattern: str
    severity: str
    category: str
    description: str

class ErrorLogMonitor:
    """Phase 2/3.1å°‚é–€ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–"""
    
    def __init__(self):
        self.monitoring_dir = Path("logs/monitoring")
        self.monitoring_dir.mkdir(parents=True, exist_ok=True)
        
        # Phase 2/3.1å°‚é–€ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.error_patterns = [
            # SLOT_HOURSé–¢é€£
            ErrorPattern(
                r"SLOT_HOURS.*(?:not.*defined|undefined|NameError)",
                "critical",
                "calculation",
                "SLOT_HOURSå®šæ•°æœªå®šç¾©ã‚¨ãƒ©ãƒ¼"
            ),
            ErrorPattern(
                r"parsed_slots_count.*(?:already.*hours|double.*conversion)",
                "warning", 
                "calculation",
                "ä¸æ­£ãªé‡è¤‡å¤‰æ›ã‚³ãƒ¡ãƒ³ãƒˆæ¤œå‡º"
            ),
            
            # Phase 2ç‰¹æœ‰ã‚¨ãƒ©ãƒ¼
            ErrorPattern(
                r"FactExtractorPrototype.*(?:Error|Exception|Failed)",
                "error",
                "phase2",
                "Phase 2ãƒ•ã‚¡ã‚¯ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼"
            ),
            ErrorPattern(
                r"fact_extractor.*(?:import.*error|module.*not.*found)",
                "critical",
                "phase2",
                "Phase 2ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼"
            ),
            
            # Phase 3.1ç‰¹æœ‰ã‚¨ãƒ©ãƒ¼
            ErrorPattern(
                r"LightweightAnomalyDetector.*(?:Error|Exception|Failed)",
                "error",
                "phase31",
                "Phase 3.1ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼"
            ),
            ErrorPattern(
                r"anomaly_detector.*(?:import.*error|module.*not.*found)",
                "critical",
                "phase31",
                "Phase 3.1ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼"
            ),
            
            # çµ±åˆãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼é–¢é€£
            ErrorPattern(
                r"FactBookVisualizer.*(?:Error|Exception|Failed)",
                "error",
                "integration",
                "çµ±åˆå¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼"
            ),
            ErrorPattern(
                r"dash_fact_book.*(?:Error|Exception|Failed)",
                "error",
                "integration",
                "Dashçµ±åˆã‚¨ãƒ©ãƒ¼"
            ),
            
            # æ•°å€¤è¨ˆç®—é–¢é€£
            ErrorPattern(
                r"(?:ValueError|TypeError).*(?:hours|slots|calculation)",
                "error",
                "calculation",
                "æ•°å€¤è¨ˆç®—å‹ã‚¨ãƒ©ãƒ¼"
            ),
            ErrorPattern(
                r"(?:670|shortage).*(?:mismatch|inconsistent|error)",
                "warning",
                "calculation",
                "åŸºæº–å€¤ä¸æ•´åˆè­¦å‘Š"
            ),
            
            # ã‚·ã‚¹ãƒ†ãƒ å…¨èˆ¬
            ErrorPattern(
                r"(?:CRITICAL|FATAL).*(?:Phase|SLOT|hours)",
                "critical",
                "system",
                "ã‚·ã‚¹ãƒ†ãƒ é‡å¤§ã‚¨ãƒ©ãƒ¼"
            ),
            ErrorPattern(
                r"(?:ImportError|ModuleNotFoundError).*(?:shift_suite|tasks)",
                "critical",
                "system",
                "é‡è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸åœ¨ã‚¨ãƒ©ãƒ¼"
            )
        ]
    
    def find_log_files(self) -> List[Path]:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        
        log_locations = [
            "logs",
            ".",
            "shift_suite",
            "temp_analysis_check"
        ]
        
        log_files = []
        
        for location in log_locations:
            location_path = Path(location)
            if location_path.exists():
                # .logãƒ•ã‚¡ã‚¤ãƒ«
                log_files.extend(location_path.glob("*.log"))
                # .txtãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼å‡ºåŠ›ï¼‰
                log_files.extend(location_path.glob("*error*.txt"))
                # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®.logãƒ•ã‚¡ã‚¤ãƒ«
                log_files.extend(location_path.glob("**/*.log"))
        
        # é‡è¤‡é™¤å»ãƒ»å­˜åœ¨ç¢ºèª
        unique_files = []
        for file_path in log_files:
            if file_path.exists() and file_path not in unique_files:
                unique_files.append(file_path)
        
        return unique_files
    
    def scan_log_file(self, file_path: Path, hours_back: int = 24) -> Dict[str, Any]:
        """å˜ä¸€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³"""
        
        results = {
            "file": str(file_path),
            "size_bytes": 0,
            "lines_scanned": 0,
            "errors_found": [],
            "status": "ok"
        }
        
        try:
            stat = file_path.stat()
            results["size_bytes"] = stat.st_size
            results["modified"] = datetime.fromtimestamp(stat.st_mtime).isoformat()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã‚‹å ´åˆã¯æœ«å°¾ã‹ã‚‰èª­ã‚€
            if stat.st_size > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                with open(file_path, 'rb') as f:
                    f.seek(-1024*1024, 2)  # æœ€å¾Œã®1MBã®ã¿
                    content = f.read().decode('utf-8', errors='ignore')
            else:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            
            lines = content.splitlines()
            results["lines_scanned"] = len(lines)
            
            # æ™‚åˆ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            cutoff_time = datetime.now() - timedelta(hours=hours_back)
            
            for line_num, line in enumerate(lines, 1):
                # ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                for pattern in self.error_patterns:
                    if re.search(pattern.pattern, line, re.IGNORECASE):
                        error_entry = {
                            "line_number": line_num,
                            "content": line.strip()[:200],  # æœ€åˆã®200æ–‡å­—
                            "pattern": pattern.pattern,
                            "severity": pattern.severity,
                            "category": pattern.category,
                            "description": pattern.description,
                            "timestamp": self.extract_timestamp(line)
                        }
                        results["errors_found"].append(error_entry)
                        
                        if pattern.severity in ["critical", "error"]:
                            results["status"] = pattern.severity
            
        except Exception as e:
            results["error"] = str(e)
            results["status"] = "error"
        
        return results
    
    def extract_timestamp(self, line: str) -> Optional[str]:
        """ãƒ­ã‚°è¡Œã‹ã‚‰ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æŠ½å‡º"""
        
        timestamp_patterns = [
            r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}',
            r'\d{2}/\d{2}/\d{4} \d{2}:\d{2}:\d{2}',
            r'\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}'
        ]
        
        for pattern in timestamp_patterns:
            match = re.search(pattern, line)
            if match:
                return match.group()
        
        return None
    
    def analyze_error_trends(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼å‚¾å‘åˆ†æ"""
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "total_files_scanned": len(all_results),
            "total_errors": 0,
            "severity_breakdown": {"critical": 0, "error": 0, "warning": 0},
            "category_breakdown": {},
            "recent_errors": [],
            "trending_patterns": [],
            "risk_assessment": "low"
        }
        
        all_errors = []
        
        for file_result in all_results:
            if file_result["status"] != "error":
                analysis["total_errors"] += len(file_result["errors_found"])
                all_errors.extend(file_result["errors_found"])
        
        # é‡è¦åº¦åˆ¥é›†è¨ˆ
        for error in all_errors:
            severity = error["severity"]
            analysis["severity_breakdown"][severity] += 1
            
            category = error["category"]
            if category not in analysis["category_breakdown"]:
                analysis["category_breakdown"][category] = 0
            analysis["category_breakdown"][category] += 1
        
        # æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ï¼ˆæœ€æ–°10ä»¶ï¼‰
        recent_errors = sorted(all_errors, 
                              key=lambda x: x.get("timestamp", ""), 
                              reverse=True)[:10]
        analysis["recent_errors"] = recent_errors
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        pattern_counts = {}
        for error in all_errors:
            desc = error["description"]
            if desc not in pattern_counts:
                pattern_counts[desc] = 0
            pattern_counts[desc] += 1
        
        analysis["trending_patterns"] = [
            {"pattern": pattern, "count": count}
            for pattern, count in sorted(pattern_counts.items(), 
                                       key=lambda x: x[1], reverse=True)[:5]
        ]
        
        # ãƒªã‚¹ã‚¯è©•ä¾¡
        critical_count = analysis["severity_breakdown"]["critical"]
        error_count = analysis["severity_breakdown"]["error"]
        
        if critical_count > 0:
            analysis["risk_assessment"] = "critical"
        elif error_count > 5:
            analysis["risk_assessment"] = "high"
        elif error_count > 0 or analysis["severity_breakdown"]["warning"] > 10:
            analysis["risk_assessment"] = "medium"
        else:
            analysis["risk_assessment"] = "low"
        
        return analysis
    
    def generate_monitoring_report(self, analysis: Dict[str, Any]) -> str:
        """ã‚¨ãƒ©ãƒ¼ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        risk_icons = {
            "critical": "ğŸ”´",
            "high": "ğŸŸ ", 
            "medium": "ğŸŸ¡",
            "low": "ğŸŸ¢"
        }
        
        risk_icon = risk_icons.get(analysis["risk_assessment"], "â“")
        
        report = f"""
ğŸš¨ **A3.1.2 ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ**
å®Ÿè¡Œæ—¥æ™‚: {analysis['timestamp']}
ãƒªã‚¹ã‚¯è©•ä¾¡: {risk_icon} {analysis['risk_assessment'].upper()}

ğŸ“Š **ç›£è¦–çµæœã‚µãƒãƒªãƒ¼**
- ã‚¹ã‚­ãƒ£ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«: {analysis['total_files_scanned']}ãƒ•ã‚¡ã‚¤ãƒ«
- æ¤œå‡ºã‚¨ãƒ©ãƒ¼ç·æ•°: {analysis['total_errors']}ä»¶
- é‡å¤§ã‚¨ãƒ©ãƒ¼: {analysis['severity_breakdown']['critical']}ä»¶
- ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼: {analysis['severity_breakdown']['error']}ä»¶  
- è­¦å‘Š: {analysis['severity_breakdown']['warning']}ä»¶

ğŸ¯ **ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ**"""

        for category, count in analysis["category_breakdown"].items():
            if count > 0:
                report += f"\n- {category}: {count}ä»¶"
        
        if analysis["recent_errors"]:
            report += f"""

ğŸ” **æœ€æ–°ã‚¨ãƒ©ãƒ¼ï¼ˆä¸Šä½3ä»¶ï¼‰**"""
            for i, error in enumerate(analysis["recent_errors"][:3], 1):
                severity_icon = {"critical": "ğŸ”´", "error": "ğŸŸ ", "warning": "ğŸŸ¡"}.get(error["severity"], "â“")
                report += f"""
{i}. {severity_icon} {error['description']}
   å†…å®¹: {error['content'][:100]}..."""

        if analysis["trending_patterns"]:
            report += f"""

ğŸ“ˆ **é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³**"""
            for pattern in analysis["trending_patterns"][:3]:
                report += f"\n- {pattern['pattern']}: {pattern['count']}å›"

        report += f"""

ğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**"""
        
        if analysis["risk_assessment"] == "critical":
            report += """
ğŸš¨ å³åº§å¯¾å¿œãŒå¿…è¦ã§ã™:
  1. é‡å¤§ã‚¨ãƒ©ãƒ¼ã®è©³ç´°èª¿æŸ»
  2. Phase 2/3.1æ©Ÿèƒ½ã®åœæ­¢æ¤œè¨
  3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ã®å¾©æ—§æº–å‚™"""
        elif analysis["risk_assessment"] == "high":
            report += """
âš ï¸ æ—©æ€¥ãªå¯¾å¿œãŒå¿…è¦ã§ã™:
  1. ã‚¨ãƒ©ãƒ¼åŸå› ã®ç‰¹å®š
  2. é–¢é€£æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
  3. äºˆé˜²çš„å¯¾ç­–ã®å®Ÿæ–½"""
        elif analysis["risk_assessment"] == "medium":
            report += """
ğŸ“‹ ç›£è¦–å¼·åŒ–ãŒå¿…è¦ã§ã™:
  1. ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¶™ç¶šç›£è¦–
  2. è­¦å‘Šãƒ¬ãƒ™ãƒ«ã®è©³ç´°ç¢ºèª
  3. å®šæœŸãƒã‚§ãƒƒã‚¯ã®å¼·åŒ–"""
        else:
            report += """
âœ… æ­£å¸¸ç¨¼åƒä¸­ã§ã™:
  1. ç¶™ç¶šçš„ç›£è¦–ã®ç¶­æŒ
  2. A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã¸é€²è¡Œ
  3. å®šæœŸãƒ¬ãƒãƒ¼ãƒˆã®ç¢ºèª"""
        
        return report
    
    def save_monitoring_results(self, analysis: Dict[str, Any], all_results: List[Dict[str, Any]]) -> str:
        """ç›£è¦–çµæœä¿å­˜"""
        
        result_file = self.monitoring_dir / f"error_log_monitoring_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        monitoring_data = {
            "monitoring_version": "error_log_1.0",
            "timestamp": datetime.now().isoformat(),
            "analysis": analysis,
            "detailed_results": all_results,
            "metadata": {
                "monitoring_tool": "A3_ERROR_LOG_MONITOR",
                "scan_duration": "24_hours",
                "patterns_checked": len(self.error_patterns)
            }
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(monitoring_data, f, indent=2, ensure_ascii=False)
        
        return str(result_file)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    print("ğŸš¨ A3.1.2 ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("ğŸ¯ Phase 2/3.1é–¢é€£ã‚¨ãƒ©ãƒ¼ã®å°‚é–€æ¤œçŸ¥")
    print("=" * 80)
    
    try:
        monitor = ErrorLogMonitor()
        
        # 1. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
        print("ğŸ“ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢...")
        log_files = monitor.find_log_files()
        print(f"  æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {len(log_files)}ä»¶")
        
        if not log_files:
            print("  âš ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return True
        
        # 2. å„ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³
        print("\nğŸ” ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³...")
        all_results = []
        
        for i, log_file in enumerate(log_files, 1):
            print(f"  {i}/{len(log_files)} {log_file.name}: ", end="")
            result = monitor.scan_log_file(log_file)
            
            error_count = len(result["errors_found"])
            if error_count > 0:
                print(f"âš ï¸ {error_count}ä»¶ã‚¨ãƒ©ãƒ¼æ¤œå‡º")
            else:
                print("âœ… æ­£å¸¸")
                
            all_results.append(result)
        
        # 3. ã‚¨ãƒ©ãƒ¼å‚¾å‘åˆ†æ
        print("\nğŸ“Š ã‚¨ãƒ©ãƒ¼å‚¾å‘åˆ†æ...")
        analysis = monitor.analyze_error_trends(all_results)
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ“‹ ã‚¨ãƒ©ãƒ¼ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        
        report = monitor.generate_monitoring_report(analysis)
        print(report)
        
        # 5. çµæœä¿å­˜
        result_file = monitor.save_monitoring_results(analysis, all_results)
        print(f"\nğŸ“ ç›£è¦–çµæœä¿å­˜: {result_file}")
        
        # 6. æˆåŠŸåˆ¤å®š
        success = analysis["risk_assessment"] in ["low", "medium"]
        status_text = "âœ… å®Œäº†" if success else "âŒ è¦å¯¾å¿œ"
        print(f"\nğŸ¯ A3.1.2 ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–: {status_text}")
        
        return success
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)