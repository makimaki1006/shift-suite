#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - å…¨æ©Ÿèƒ½ã®çŸ›ç›¾ãƒ»æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ©Ÿèƒ½é–“çŸ›ç›¾ã‚’æ¤œå‡ºã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´åˆæ€§ã‚’ç¢ºèªã™ã‚‹
"""

import logging
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple
import importlib
import inspect
import ast
import re

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class SystemVerificationEngine:
    """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çŸ›ç›¾ãƒ»æ•´åˆæ€§æ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.shift_suite_path = project_root / "shift_suite"
        self.verification_results = {
            "import_consistency": [],
            "function_compatibility": [],
            "data_flow_integrity": [], 
            "ui_consistency": [],
            "configuration_conflicts": [],
            "error_handling_coverage": [],
            "memory_management_consistency": [],
            "18_section_integration_status": []
        }
        
    def run_comprehensive_verification(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        log.info("ğŸ” ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®åŒ…æ‹¬çš„æ¤œè¨¼ã‚’é–‹å§‹...")
        
        try:
            # 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            self._verify_import_consistency()
            
            # 2. æ©Ÿèƒ½äº’æ›æ€§ãƒã‚§ãƒƒã‚¯  
            self._verify_function_compatibility()
            
            # 3. ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            self._verify_data_flow_integrity()
            
            # 4. UIä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
            self._verify_ui_consistency()
            
            # 5. è¨­å®šç«¶åˆãƒã‚§ãƒƒã‚¯
            self._verify_configuration_conflicts()
            
            # 6. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯
            self._verify_error_handling_coverage()
            
            # 7. ãƒ¡ãƒ¢ãƒªç®¡ç†æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            self._verify_memory_management_consistency()
            
            # 8. 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆçŠ¶æ³ãƒã‚§ãƒƒã‚¯
            self._verify_18_section_integration()
            
            # çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            summary = self._generate_verification_summary()
            
            log.info("âœ… åŒ…æ‹¬çš„æ¤œè¨¼å®Œäº†")
            return {
                "status": "completed",
                "summary": summary,
                "detailed_results": self.verification_results,
                "recommendation": self._generate_recommendations()
            }
            
        except Exception as e:
            log.error(f"âŒ æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return {
                "status": "error", 
                "error": str(e),
                "partial_results": self.verification_results
            }
    
    def _verify_import_consistency(self):
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´åˆæ€§ã®æ¤œè¨¼"""
        log.info("ğŸ“¦ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
        main_files = ["app.py", "dash_app.py"]
        init_file = self.shift_suite_path / "tasks" / "__init__.py"
        
        try:
            # __init__.pyã®å†…å®¹ç¢ºèª
            if init_file.exists():
                with open(init_file, 'r', encoding='utf-8') as f:
                    init_content = f.read()
                
                # 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã™ã¹ã¦ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                required_modules = [
                    "AIComprehensiveReportGenerator",
                    "CognitivePsychologyAnalyzer", 
                    "OrganizationalPatternAnalyzer",
                    "SystemThinkingAnalyzer",
                    "BlueprintDeepAnalysisEngine",
                    "IntegratedMECEAnalysisEngine", 
                    "PredictiveOptimizationIntegrationEngine"
                ]
                
                for module in required_modules:
                    if module not in init_content:
                        issues.append({
                            "type": "missing_module_export",
                            "file": "__init__.py",
                            "module": module,
                            "severity": "high"
                        })
            
            # app.py ã¨ dash_app.py ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´åˆæ€§ç¢ºèª
            for main_file in main_files:
                file_path = self.project_root / main_file
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®å¯ç”¨æ€§ãƒ•ãƒ©ã‚°ç¢ºèª
                    if "AI_REPORT_GENERATOR_AVAILABLE" in content:
                        if "from shift_suite.tasks.ai_comprehensive_report_generator import" not in content:
                            issues.append({
                                "type": "import_mismatch",
                                "file": main_file,
                                "issue": "AI_REPORT_GENERATOR_AVAILABLEãƒ•ãƒ©ã‚°ã¯ã‚ã‚‹ãŒimportãŒè¦‹ã¤ã‹ã‚‰ãªã„",
                                "severity": "medium"
                            })
                    
                    # çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ç¢ºèª
                    if "UNIFIED_ANALYSIS_AVAILABLE" in content:
                        if "from shift_suite.tasks.unified_analysis_manager import" not in content:
                            issues.append({
                                "type": "import_mismatch", 
                                "file": main_file,
                                "issue": "UNIFIED_ANALYSIS_AVAILABLEãƒ•ãƒ©ã‚°ã¯ã‚ã‚‹ãŒimportãŒè¦‹ã¤ã‹ã‚‰ãªã„",
                                "severity": "medium"
                            })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["import_consistency"] = issues
        log.info(f"ğŸ“¦ ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_function_compatibility(self):
        """æ©Ÿèƒ½äº’æ›æ€§ã®æ¤œè¨¼"""
        log.info("âš™ï¸ æ©Ÿèƒ½äº’æ›æ€§ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # é‡è¤‡ã™ã‚‹æ©Ÿèƒ½å‘¼ã³å‡ºã—ã®æ¤œå‡º
            app_file = self.project_root / "app.py"
            if app_file.exists():
                with open(app_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
                ai_report_calls = re.findall(r'ai_generator\.generate_comprehensive_report', content)
                if len(ai_report_calls) > 2:
                    issues.append({
                        "type": "duplicate_function_call",
                        "function": "AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ",
                        "count": len(ai_report_calls),
                        "severity": "medium",
                        "recommendation": "é‡è¤‡ã™ã‚‹å‘¼ã³å‡ºã—ã‚’çµ±åˆã™ã‚‹"
                    })
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                cleanup_calls = re.findall(r'cleanup_old_results\(max_age_hours=(\d+)\)', content)
                if cleanup_calls:
                    unique_hours = set(cleanup_calls)
                    if len(unique_hours) > 1:
                        issues.append({
                            "type": "inconsistent_parameter",
                            "function": "cleanup_old_results",
                            "values": list(unique_hours),
                            "severity": "low",
                            "recommendation": "ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–“éš”ã‚’çµ±ä¸€ã™ã‚‹"
                        })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["function_compatibility"] = issues
        log.info(f"âš™ï¸ æ©Ÿèƒ½äº’æ›æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_data_flow_integrity(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´åˆæ€§ã®æ¤œè¨¼"""
        log.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # shortage.pyã®å¤‰æ•°é †åºç¢ºèª
            shortage_file = self.shift_suite_path / "tasks" / "shortage.py"
            if shortage_file.exists():
                with open(shortage_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # lack_count_overall_dfã®å®šç¾©ã¨ä½¿ç”¨é †åºç¢ºèª
                lines = content.split('\n')
                lack_count_def_line = None
                lack_count_use_lines = []
                
                for i, line in enumerate(lines):
                    if 'lack_count_overall_df =' in line and '(' in line:
                        lack_count_def_line = i
                    elif 'lack_count_overall_df' in line and 'lack_count_overall_df =' not in line:
                        lack_count_use_lines.append(i)
                
                if lack_count_def_line is not None:
                    early_uses = [line for line in lack_count_use_lines if line < lack_count_def_line]
                    if early_uses:
                        issues.append({
                            "type": "variable_order_issue",
                            "file": "shortage.py",
                            "variable": "lack_count_overall_df",
                            "definition_line": lack_count_def_line + 1,
                            "early_use_lines": [line + 1 for line in early_uses],
                            "severity": "high",
                            "status": "already_fixed"
                        })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["data_flow_integrity"] = issues
        log.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_ui_consistency(self):
        """UIä¸€è²«æ€§ã®æ¤œè¨¼"""
        log.info("ğŸ¨ UIä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # app.pyã¨dash_app.pyã®UIçµ±åˆç¢ºèª
            app_file = self.project_root / "app.py"
            dash_file = self.project_root / "dash_app.py"
            
            app_content = ""
            dash_content = ""
            
            if app_file.exists():
                with open(app_file, 'r', encoding='utf-8') as f:
                    app_content = f.read()
            
            if dash_file.exists():
                with open(dash_file, 'r', encoding='utf-8') as f:
                    dash_content = f.read()
            
            # 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤ºã®ä¸€è²«æ€§ç¢ºèª
            if "18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚·ã‚¹ãƒ†ãƒ " in app_content:
                if "18ã‚»ã‚¯ã‚·ãƒ§ãƒ³" not in dash_content:
                    issues.append({
                        "type": "ui_inconsistency",
                        "issue": "app.pyã«ã¯18ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤ºãŒã‚ã‚‹ãŒdash_app.pyã«ã¯ãªã„",
                        "severity": "medium",
                        "recommendation": "dash_app.pyã«ã‚‚18ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤ºã‚’è¿½åŠ "
                    })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["ui_consistency"] = issues
        log.info(f"ğŸ¨ UIä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_configuration_conflicts(self):
        """è¨­å®šç«¶åˆã®æ¤œè¨¼"""
        log.info("âš™ï¸ è¨­å®šç«¶åˆãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # constants.pyã®è¨­å®šå€¤ç¢ºèª
            constants_file = self.shift_suite_path / "tasks" / "constants.py"
            if constants_file.exists():
                with open(constants_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # é‡è¤‡ã™ã‚‹å®šæ•°å®šç¾©ã®ç¢ºèª
                slot_minutes_matches = re.findall(r'DEFAULT_SLOT_MINUTES\s*=\s*(\d+)', content)
                if len(set(slot_minutes_matches)) > 1:
                    issues.append({
                        "type": "duplicate_constant",
                        "constant": "DEFAULT_SLOT_MINUTES",
                        "values": list(set(slot_minutes_matches)),
                        "severity": "medium"
                    })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["configuration_conflicts"] = issues
        log.info(f"âš™ï¸ è¨­å®šç«¶åˆãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_error_handling_coverage(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¶²ç¾…æ€§ã®æ¤œè¨¼"""
        log.info("ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®try-exceptç¶²ç¾…æ€§ç¢ºèª
            critical_files = [
                "app.py",
                "dash_app.py", 
                "shift_suite/tasks/shortage.py",
                "shift_suite/tasks/ai_comprehensive_report_generator.py"
            ]
            
            for file_path in critical_files:
                full_path = self.project_root / file_path
                if full_path.exists():
                    with open(full_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # try-exceptãƒ–ãƒ­ãƒƒã‚¯ã®æ•°ã¨importæ–‡ã®æ•°ã‚’æ¯”è¼ƒ
                    try_count = len(re.findall(r'\btry:', content))
                    import_count = len(re.findall(r'^from .+ import|^import .+', content, re.MULTILINE))
                    
                    if import_count > 0 and try_count == 0:
                        issues.append({
                            "type": "missing_error_handling",
                            "file": file_path,
                            "imports": import_count,
                            "try_blocks": try_count,
                            "severity": "medium",
                            "recommendation": "ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’è¿½åŠ "
                        })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["error_handling_coverage"] = issues
        log.info(f"ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ç¶²ç¾…æ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_memory_management_consistency(self):
        """ãƒ¡ãƒ¢ãƒªç®¡ç†æ•´åˆæ€§ã®æ¤œè¨¼"""
        log.info("ğŸ§  ãƒ¡ãƒ¢ãƒªç®¡ç†æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # cleanup_old_resultsã®å‘¼ã³å‡ºã—ä¸€è²«æ€§ç¢ºèª
            app_file = self.project_root / "app.py"
            if app_file.exists():
                with open(app_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # max_age_hoursãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ±ä¸€ç¢ºèª
                cleanup_patterns = re.findall(r'cleanup_old_results\(max_age_hours=(\d+)\)', content)
                if cleanup_patterns:
                    unique_values = set(cleanup_patterns)
                    if len(unique_values) == 1 and unique_values == {'24'}:
                        issues.append({
                            "type": "memory_management_consistent",
                            "parameter": "max_age_hours=24",
                            "status": "consistent",
                            "severity": "info"
                        })
                    elif len(unique_values) > 1:
                        issues.append({
                            "type": "memory_management_inconsistent",
                            "values": list(unique_values),
                            "severity": "medium",
                            "recommendation": "ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–“éš”ã‚’çµ±ä¸€"
                        })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["memory_management_consistency"] = issues
        log.info(f"ğŸ§  ãƒ¡ãƒ¢ãƒªç®¡ç†æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
    
    def _verify_18_section_integration(self):
        """18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆçŠ¶æ³ã®æ¤œè¨¼"""
        log.info("ğŸš€ 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆçŠ¶æ³ãƒã‚§ãƒƒã‚¯...")
        
        issues = []
        
        try:
            # 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            required_files = [
                "shift_suite/tasks/ai_comprehensive_report_generator.py",
                "shift_suite/tasks/cognitive_psychology_analyzer.py",
                "shift_suite/tasks/organizational_pattern_analyzer.py",
                "shift_suite/tasks/system_thinking_analyzer.py",
                "shift_suite/tasks/blueprint_deep_analysis_engine.py",
                "shift_suite/tasks/integrated_mece_analysis_engine.py",
                "shift_suite/tasks/predictive_optimization_integration_engine.py"
            ]
            
            missing_files = []
            existing_files = []
            
            for file_path in required_files:
                full_path = self.project_root / file_path
                if full_path.exists():
                    existing_files.append(file_path)
                else:
                    missing_files.append(file_path)
            
            if missing_files:
                issues.append({
                    "type": "missing_18_section_files",
                    "missing_files": missing_files,
                    "severity": "high"
                })
            
            if existing_files:
                issues.append({
                    "type": "18_section_files_present",
                    "existing_files": existing_files,
                    "count": len(existing_files),
                    "status": "integration_complete",
                    "severity": "info"
                })
            
            # __init__.pyã§ã®18ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç¢ºèª
            init_file = self.shift_suite_path / "tasks" / "__init__.py"
            if init_file.exists():
                with open(init_file, 'r', encoding='utf-8') as f:
                    init_content = f.read()
                
                section_comment = "18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚·ã‚¹ãƒ†ãƒ " in init_content
                if section_comment:
                    issues.append({
                        "type": "18_section_exports_configured",
                        "status": "configured",
                        "severity": "info"
                    })
        
        except Exception as e:
            issues.append({
                "type": "verification_error",
                "error": str(e),
                "severity": "high"
            })
        
        self.verification_results["18_section_integration_status"] = issues
        log.info(f"ğŸš€ 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆçŠ¶æ³ãƒã‚§ãƒƒã‚¯å®Œäº†: {len(issues)}ä»¶ã®é …ç›®ã‚’ç¢ºèª")
    
    def _generate_verification_summary(self) -> Dict[str, Any]:
        """æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        total_issues = 0
        high_severity_issues = 0
        medium_severity_issues = 0
        info_items = 0
        
        for category, issues in self.verification_results.items():
            total_issues += len(issues)
            for issue in issues:
                severity = issue.get('severity', 'unknown')
                if severity == 'high':
                    high_severity_issues += 1
                elif severity == 'medium':
                    medium_severity_issues += 1
                elif severity == 'info':
                    info_items += 1
        
        return {
            "total_checks": len(self.verification_results),
            "total_issues": total_issues,
            "high_severity": high_severity_issues,
            "medium_severity": medium_severity_issues,
            "info_items": info_items,
            "overall_status": "healthy" if high_severity_issues == 0 else "needs_attention"
        }
    
    def _generate_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # å„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æ¨å¥¨äº‹é …ã‚’æŠ½å‡º
        for category, issues in self.verification_results.items():
            for issue in issues:
                if issue.get('recommendation'):
                    recommendations.append(f"[{category}] {issue['recommendation']}")
                elif issue.get('severity') == 'high':
                    recommendations.append(f"[{category}] é«˜å„ªå…ˆåº¦å•é¡Œã®ä¿®æ­£ãŒå¿…è¦: {issue.get('type', 'ä¸æ˜')}")
        
        if not recommendations:
            recommendations.append("âœ… ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ•´åˆæ€§ãŒä¿ãŸã‚Œã¦ã„ã¾ã™")
        
        return recommendations

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    project_root = Path(__file__).parent
    verifier = SystemVerificationEngine(project_root)
    
    log.info("ğŸ” ã‚·ãƒ•ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬çš„æ¤œè¨¼ã‚’é–‹å§‹...")
    results = verifier.run_comprehensive_verification()
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = project_root / "comprehensive_system_verification_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    # çµæœã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ” ã‚·ã‚¹ãƒ†ãƒ åŒ…æ‹¬çš„æ¤œè¨¼çµæœ")
    print("="*80)
    
    summary = results['summary']
    print(f"ğŸ“Š ç·åˆçŠ¶æ³: {summary['overall_status']}")
    print(f"ğŸ” æ¤œè¨¼é …ç›®æ•°: {summary['total_checks']}")
    print(f"ğŸ“‹ æ¤œå‡ºäº‹é …æ•°: {summary['total_issues']}")
    print(f"ğŸš¨ é«˜å„ªå…ˆåº¦: {summary['high_severity']}ä»¶")
    print(f"âš ï¸ ä¸­å„ªå…ˆåº¦: {summary['medium_severity']}ä»¶")
    print(f"â„¹ï¸ æƒ…å ±: {summary['info_items']}ä»¶")
    
    print("\nğŸ“ æ¨å¥¨äº‹é …:")
    for rec in results['recommendation']:
        print(f"  â€¢ {rec}")
    
    print(f"\nğŸ“„ è©³ç´°çµæœ: {output_file}")
    print("="*80)
    
    return results

if __name__ == "__main__":
    main()