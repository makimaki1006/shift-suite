# APIãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ä»•æ§˜æ›¸

**å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: Shift-Suite Phase 2/3.1  
**API ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**ä½œæˆæ—¥**: 2025å¹´08æœˆ03æ—¥

## ğŸ“‹ æ¦‚è¦

æœ¬æ–‡æ›¸ã¯ã€Shift-Suiteã‚·ã‚¹ãƒ†ãƒ å†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã¨APIä»•æ§˜ã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

## ğŸŒŠ ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å…¨ä½“å›³

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Excel     â”‚â”€â”€â”€â–¶â”‚  io_excel   â”‚â”€â”€â”€â–¶â”‚ long_df     â”‚
â”‚ å‹¤å‹™ãƒ‡ãƒ¼ã‚¿   â”‚    â”‚ èª­ã¿è¾¼ã¿     â”‚    â”‚ æ­£è¦åŒ–      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚ Phase 3.1   â”‚â—€â”€â”€â”€â”‚   Phase 2   â”‚â—€â”€â”€â”€â”€â”€â”˜
â”‚LightWeight  â”‚    â”‚FactExtractorâ”‚
â”‚AnomalyDet   â”‚    â”‚Prototype    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚FactBook     â”‚â”€â”€â”€â–¶â”‚DashFactBook â”‚â”€â”€â”€â–¶â”‚  dash_app   â”‚
â”‚Visualizer   â”‚    â”‚Integration  â”‚    â”‚ WebUI       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä»•æ§˜

### 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆExcelï¼‰

#### å¿…é ˆåˆ—
```json
{
  "date": "å‹¤å‹™æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰",
  "staff_id": "è·å“¡IDï¼ˆæ–‡å­—åˆ—ï¼‰",
  "start_time": "é–‹å§‹æ™‚é–“ï¼ˆHH:MMå½¢å¼ï¼‰",
  "end_time": "çµ‚äº†æ™‚é–“ï¼ˆHH:MMå½¢å¼ï¼‰",
  "work_type": "å‹¤å‹™ã‚¿ã‚¤ãƒ—ï¼ˆæ—¥å‹¤/å¤œå‹¤ç­‰ï¼‰",
  "department": "éƒ¨ç½²ãƒ»ç—…æ£Ÿï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
}
```

#### ãƒ‡ãƒ¼ã‚¿ä¾‹
```csv
date,staff_id,start_time,end_time,work_type,department
2024-08-01,STAFF001,09:00,17:00,æ—¥å‹¤,å†…ç§‘ç—…æ£Ÿ
2024-08-01,STAFF002,17:00,09:00,å¤œå‹¤,å†…ç§‘ç—…æ£Ÿ
```

### 2. ä¸­é–“ãƒ‡ãƒ¼ã‚¿ï¼ˆlong_dfï¼‰

#### parsed_slots_countï¼ˆé‡è¦ï¼‰
```python
# ãƒ‡ãƒ¼ã‚¿å‹: int
# æ„å‘³: 30åˆ†å˜ä½ã®ã‚¹ãƒ­ãƒƒãƒˆæ•°
# ä¾‹: 8æ™‚é–“å‹¤å‹™ = 16ã‚¹ãƒ­ãƒƒãƒˆ
parsed_slots_count = 16

# æ™‚é–“å¤‰æ›ï¼ˆPhase 2/3.1ã§å®Ÿæ–½ï¼‰
SLOT_HOURS = 0.5
actual_hours = parsed_slots_count * SLOT_HOURS  # 16 * 0.5 = 8.0æ™‚é–“
```

### 3. å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿

#### shortage_summary.txt
```
total_lack_hours: 670
total_excess_hours: 505
```

## ğŸ”§ Phase 2 APIä»•æ§˜

### FactExtractorPrototype

#### ã‚¯ãƒ©ã‚¹å®šç¾©
```python
class FactExtractorPrototype:
    def __init__(self, config: Dict[str, Any])
    def extract_facts(self, data: pd.DataFrame) -> Dict[str, Any]
    def calculate_hours(self, slots_data: pd.Series) -> pd.Series
```

#### ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

##### calculate_hours()
```python
def calculate_hours(self, slots_data: pd.Series) -> pd.Series:
    """ã‚¹ãƒ­ãƒƒãƒˆæ•°ã‚’æ™‚é–“ã«å¤‰æ›"""
    SLOT_HOURS = 0.5
    return slots_data * SLOT_HOURS  # ä¿®æ­£æ¸ˆã¿: æ­£ç¢ºãªå¤‰æ›
```

#### é‡è¦ãªä¿®æ­£ç‚¹
```python
# ä¿®æ­£å‰ï¼ˆèª¤ã‚Šï¼‰
total_hours = group['parsed_slots_count'].sum()  # ã‚¹ãƒ­ãƒƒãƒˆæ•°ã‚’ãã®ã¾ã¾æ™‚é–“æ‰±ã„

# ä¿®æ­£å¾Œï¼ˆæ­£ç¢ºï¼‰
total_hours = group['parsed_slots_count'].sum() * SLOT_HOURS  # æ­£ã—ã„æ™‚é–“å¤‰æ›
```

## ğŸ” Phase 3.1 APIä»•æ§˜

### LightweightAnomalyDetector

#### ã‚¯ãƒ©ã‚¹å®šç¾©
```python
class LightweightAnomalyDetector:
    def __init__(self, config: Dict[str, Any])
    def detect_anomalies(self, data: pd.DataFrame) -> Dict[str, Any]
    def calculate_monthly_hours(self, work_data: pd.DataFrame) -> pd.DataFrame
```

#### ä¸»è¦ãƒ¡ã‚½ãƒƒãƒ‰

##### calculate_monthly_hours()
```python
def calculate_monthly_hours(self, work_data: pd.DataFrame) -> pd.DataFrame:
    """æœˆæ¬¡åŠ´åƒæ™‚é–“è¨ˆç®—"""
    SLOT_HOURS = 0.5
    monthly_hours = work_data.groupby(['staff', 'year_month'])['parsed_slots_count'].sum() * SLOT_HOURS
    return monthly_hours
```

## ğŸ¨ çµ±åˆãƒ¬ã‚¤ãƒ¤ãƒ¼ API

### FactBookVisualizer

#### æ©Ÿèƒ½
- Phase 2/3.1ã®çµæœã‚’å¯è¦–åŒ–ç”¨ã«å¤‰æ›
- ã‚°ãƒ©ãƒ•ãƒ»ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
- ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ç”¨ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢

#### ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
```python
class FactBookVisualizer:
    def create_heatmap_data(self, facts: Dict) -> Dict
    def create_timeline_data(self, facts: Dict) -> Dict
    def create_summary_stats(self, facts: Dict) -> Dict
```

### DashFactBookIntegration

#### æ©Ÿèƒ½
- Dash Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®çµ±åˆ
- ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–UIç”¨ãƒ‡ãƒ¼ã‚¿æä¾›
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°å¯¾å¿œ

#### ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
```python
@app.callback(...)
def update_heatmap(selected_date):
    # FactBookVisualizerã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—
    # Dash ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”¨ã«å¤‰æ›
    return updated_figure

@app.callback(...)
def update_summary(filters):
    # æ¡ä»¶ã«åŸºã¥ã„ãŸé›†è¨ˆ
    # ã‚µãƒãƒªãƒ¼æƒ…å ±ã®æ›´æ–°
    return summary_data
```

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿å¤‰æ›ä»•æ§˜

### 1. Excel â†’ long_df

#### å¤‰æ›å‡¦ç†
```python
def excel_to_longdf(excel_path: str) -> pd.DataFrame:
    """Excelå‹¤å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–å½¢å¼ã«å¤‰æ›"""
    
    # 1. Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    raw_data = pd.read_excel(excel_path)
    
    # 2. æ™‚é–“è¨ˆç®—ï¼ˆ30åˆ†ã‚¹ãƒ­ãƒƒãƒˆï¼‰
    def calculate_slots(start_time: str, end_time: str) -> int:
        # é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»ã‹ã‚‰ã‚¹ãƒ­ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
        duration_minutes = get_duration_minutes(start_time, end_time)
        return duration_minutes // 30  # 30åˆ†å˜ä½ã®ã‚¹ãƒ­ãƒƒãƒˆ
    
    # 3. parsed_slots_countåˆ—ç”Ÿæˆ
    raw_data['parsed_slots_count'] = raw_data.apply(
        lambda row: calculate_slots(row['start_time'], row['end_time']), 
        axis=1
    )
    
    return raw_data
```

### 2. long_df â†’ Phase 2/3.1

#### Phase 2å¤‰æ›
```python
def process_phase2(data: pd.DataFrame) -> Dict[str, Any]:
    """Phase 2ãƒ•ã‚¡ã‚¯ãƒˆæŠ½å‡º"""
    SLOT_HOURS = 0.5
    
    # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥é›†è¨ˆï¼ˆæ™‚é–“å˜ä½ã«å¤‰æ›ï¼‰
    group_stats = data.groupby(['department', 'work_type']).agg({
        'parsed_slots_count': ['count', 'sum', 'mean']
    })
    
    # ã‚¹ãƒ­ãƒƒãƒˆæ•°ã‚’æ™‚é–“ã«å¤‰æ›
    group_stats['total_hours'] = group_stats['parsed_slots_count']['sum'] * SLOT_HOURS
    
    return group_stats.to_dict()
```

#### Phase 3.1å¤‰æ›
```python
def process_phase31(data: pd.DataFrame) -> Dict[str, Any]:
    """Phase 3.1ç•°å¸¸æ¤œçŸ¥"""
    SLOT_HOURS = 0.5
    
    # æœˆæ¬¡åŠ´åƒæ™‚é–“è¨ˆç®—
    monthly_hours = data.groupby(['staff', 'year_month'])['parsed_slots_count'].sum() * SLOT_HOURS
    
    # ç•°å¸¸æ¤œçŸ¥ï¼ˆéåŠ´åƒç­‰ï¼‰
    anomalies = monthly_hours[monthly_hours > 160]  # æœˆ160æ™‚é–“è¶…é
    
    return {
        'monthly_hours': monthly_hours.to_dict(),
        'anomalies': anomalies.to_dict()
    }
```

## ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼

#### ãƒ‡ãƒ¼ã‚¿å½¢å¼ã‚¨ãƒ©ãƒ¼
```python
class DataFormatError(Exception):
    """ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒä¸æ­£ãªå ´åˆ"""
    pass

# ä½¿ç”¨ä¾‹
try:
    parsed_slots = calculate_slots(start_time, end_time)
except ValueError as e:
    raise DataFormatError(f"æ™‚åˆ»å½¢å¼ãŒä¸æ­£: {e}")
```

#### è¨ˆç®—ã‚¨ãƒ©ãƒ¼
```python
class CalculationError(Exception):
    """è¨ˆç®—å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ"""
    pass

# SLOT_HOURSæœªå®šç¾©ã‚¨ãƒ©ãƒ¼
try:
    hours = slots * SLOT_HOURS
except NameError:
    raise CalculationError("SLOT_HOURSå®šæ•°ãŒæœªå®šç¾©")
```

### API ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼

#### æˆåŠŸæ™‚
```json
{
  "status": "success",
  "data": {
    "total_hours": 670.0,
    "processed_records": 1234,
    "calculation_method": "slots * SLOT_HOURS"
  },
  "metadata": {
    "timestamp": "2024-08-03T18:30:00Z",
    "version": "1.0"
  }
}
```

#### ã‚¨ãƒ©ãƒ¼æ™‚
```json
{
  "status": "error",
  "error": {
    "code": "CALCULATION_ERROR",
    "message": "SLOT_HOURSå®šæ•°ãŒæœªå®šç¾©",
    "details": {
      "file": "fact_extractor_prototype.py",
      "line": 123
    }
  },
  "metadata": {
    "timestamp": "2024-08-03T18:30:00Z",
    "version": "1.0"
  }
}
```

## ğŸ” ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

### ãƒ­ã‚°å‡ºåŠ›ä»•æ§˜

#### Phase 2ãƒ­ã‚°
```python
import logging

logger = logging.getLogger('phase2.fact_extractor')

def extract_facts(self, data):
    logger.info(f"å‡¦ç†é–‹å§‹: {len(data)}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰")
    
    # SLOT_HOURSä½¿ç”¨ãƒ­ã‚°
    SLOT_HOURS = 0.5
    logger.debug(f"SLOT_HOURSå®šæ•°: {SLOT_HOURS}")
    
    total_hours = group['parsed_slots_count'].sum() * SLOT_HOURS
    logger.debug(f"æ™‚é–“å¤‰æ›: {group['parsed_slots_count'].sum()}ã‚¹ãƒ­ãƒƒãƒˆ â†’ {total_hours}æ™‚é–“")
```

#### ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è¿½è·¡
```python
# å„æ®µéšã§ã®ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
def validate_data_flow():
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼ã®æ•´åˆæ€§æ¤œè¨¼"""
    
    # 1. å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    assert 'parsed_slots_count' in data.columns
    assert data['parsed_slots_count'].dtype == 'int64'
    
    # 2. å¤‰æ›çµæœæ¤œè¨¼
    hours = data['parsed_slots_count'] * SLOT_HOURS
    assert hours.dtype == 'float64'
    assert (hours >= 0).all()
    
    # 3. å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
    assert 'total_hours' in result
    assert isinstance(result['total_hours'], (int, float))
```

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä»•æ§˜

### å‡¦ç†æ€§èƒ½è¦ä»¶

#### ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“
- **Phase 2å‡¦ç†**: 1000ä»¶ã‚ãŸã‚Š < 1ç§’
- **Phase 3.1å‡¦ç†**: 1000ä»¶ã‚ãŸã‚Š < 0.5ç§’
- **çµ±åˆå‡¦ç†**: ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ < 5ç§’

#### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
- **åŸºæœ¬å‡¦ç†**: < 100MB
- **å¤§é‡ãƒ‡ãƒ¼ã‚¿**: < 500MB
- **ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯**: ãªã—ï¼ˆã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

#### ãƒ‡ãƒ¼ã‚¿é‡å¯¾å¿œ
```python
# å¤§é‡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼ˆãƒãƒ£ãƒ³ã‚¯å‡¦ç†ï¼‰
def process_large_dataset(data: pd.DataFrame, chunk_size: int = 1000):
    """å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†"""
    
    results = []
    for chunk in data.groupby(data.index // chunk_size):
        chunk_result = process_chunk(chunk[1])
        results.append(chunk_result)
    
    return combine_results(results)
```

---
*æœ¬APIä»•æ§˜æ›¸ã¯å®Ÿè£…å¤‰æ›´ã«ä¼´ã„ç¶™ç¶šçš„ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚*

**æœ€çµ‚æ›´æ–°**: 2025å¹´08æœˆ03æ—¥
