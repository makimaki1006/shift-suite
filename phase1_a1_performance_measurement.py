#!/usr/bin/env python3
"""
Phase1 A1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
ç¾åœ¨ã®çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¸¬å®š
"""

import time
import psutil
import threading
import gc
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import json
import sys
import traceback

class PerformanceMeasurement:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šå™¨"""
    
    def __init__(self):
        self.measurements = {}
        self.baseline_established = False
        
    def measure_current_unified_system(self):
        """ç¾åœ¨ã®çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        print("=== A1: ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š ===")
        
        measurement_results = {
            'system_info': self._get_system_info(),
            'unified_system_performance': {},
            'traditional_system_performance': {},
            'comparison_analysis': {}
        }
        
        print("ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
        for key, value in measurement_results['system_info'].items():
            print(f"  {key}: {value}")
        
        # 1. çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        print("\nã€çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ æ¸¬å®šé–‹å§‹ã€‘")
        unified_perf = self._measure_unified_system_initialization()
        measurement_results['unified_system_performance'] = unified_perf
        
        # 2. å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šï¼ˆæ¯”è¼ƒç”¨ï¼‰
        print("\nã€å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ æ¸¬å®šé–‹å§‹ã€‘")
        traditional_perf = self._measure_traditional_system()
        measurement_results['traditional_system_performance'] = traditional_perf
        
        # 3. æ¯”è¼ƒåˆ†æ
        print("\nã€æ¯”è¼ƒåˆ†æã€‘")
        comparison = self._analyze_performance_comparison(unified_perf, traditional_perf)
        measurement_results['comparison_analysis'] = comparison
        
        self.measurements = measurement_results
        return measurement_results
    
    def _get_system_info(self):
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        try:
            system_info = {
                'platform': sys.platform,
                'python_version': sys.version.split()[0],
                'cpu_count': psutil.cpu_count(),
                'cpu_count_logical': psutil.cpu_count(logical=True),
                'memory_total_gb': round(psutil.virtual_memory().total / (1024**3), 2),
                'disk_type': self._detect_disk_type(),
                'current_directory': str(Path.cwd()),
                'measurement_time': datetime.now().isoformat()
            }
            return system_info
        except Exception as e:
            return {'error': str(e)}
    
    def _detect_disk_type(self):
        """ãƒ‡ã‚£ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            # Windowsç’°å¢ƒã§ã®ç°¡æ˜“SSDæ¤œå‡º
            if sys.platform == 'win32':
                import subprocess
                result = subprocess.run(['powershell', '-Command', 
                                       'Get-PhysicalDisk | Select-Object MediaType'], 
                                       capture_output=True, text=True, timeout=5)
                if 'SSD' in result.stdout:
                    return 'SSD'
                else:
                    return 'HDD_or_Unknown'
            else:
                return 'Unknown'
        except:
            return 'Detection_Failed'
    
    def _measure_unified_system_initialization(self):
        """çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã®æ¸¬å®š"""
        print("çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–æ¸¬å®šä¸­...")
        
        performance_data = {
            'initialization_attempts': [],
            'average_metrics': {},
            'resource_usage': {},
            'error_count': 0
        }
        
        # è¤‡æ•°å›æ¸¬å®šã—ã¦å¹³å‡ã‚’å–ã‚‹
        num_attempts = 3
        successful_attempts = []
        
        for attempt in range(num_attempts):
            print(f"  è©¦è¡Œ {attempt + 1}/{num_attempts}")
            
            try:
                # ãƒ¡ãƒ¢ãƒªã¨CPUä½¿ç”¨é‡ã®æ¸¬å®šé–‹å§‹
                initial_memory = psutil.virtual_memory().used
                initial_cpu_percent = psutil.cpu_percent(interval=None)
                
                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–æ™‚é–“æ¸¬å®š
                start_time = time.time()
                start_perf_time = time.perf_counter()
                
                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åˆæœŸåŒ–
                sys.path.insert(0, '.')
                
                # æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¯ãƒªã‚¢ï¼ˆæ–°é®®ãªæ¸¬å®šã®ãŸã‚ï¼‰
                modules_to_clear = [m for m in sys.modules.keys() 
                                  if 'unified_data_pipeline' in m]
                for module in modules_to_clear:
                    if module in sys.modules:
                        del sys.modules[module]
                
                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                try:
                    from unified_data_pipeline_architecture import (
                        get_unified_registry, UnifiedDataRegistry, DataType
                    )
                    import_success = True
                except ImportError as e:
                    print(f"    âš ï¸ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                    import_success = False
                    performance_data['error_count'] += 1
                    continue
                
                if import_success:
                    # çµ±ä¸€ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®åˆæœŸåŒ–
                    try:
                        registry = get_unified_registry()
                        initialization_success = True
                    except Exception as e:
                        print(f"    âš ï¸ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                        initialization_success = False
                        performance_data['error_count'] += 1
                        continue
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚­ãƒ£ãƒ³ã®å®Ÿè¡Œ
                    if initialization_success:
                        try:
                            # å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’è©¦è¡Œ
                            test_data = registry.get_data(DataType.PROPORTIONAL_ABOLITION_ROLE)
                            data_retrieval_success = test_data is not None
                        except Exception as e:
                            print(f"    âš ï¸ ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                            data_retrieval_success = False
                            performance_data['error_count'] += 1
                
                # æ¸¬å®šçµ‚äº†
                end_time = time.time()
                end_perf_time = time.perf_counter()
                
                # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡æ¸¬å®š
                final_memory = psutil.virtual_memory().used
                final_cpu_percent = psutil.cpu_percent(interval=0.1)
                
                # çµæœè¨˜éŒ²
                attempt_result = {
                    'attempt_number': attempt + 1,
                    'total_time_seconds': end_time - start_time,
                    'precise_time_seconds': end_perf_time - start_perf_time,
                    'memory_used_mb': (final_memory - initial_memory) / (1024 * 1024),
                    'cpu_usage_percent': final_cpu_percent - initial_cpu_percent,
                    'import_success': import_success,
                    'initialization_success': initialization_success if import_success else False,
                    'data_retrieval_success': data_retrieval_success if import_success and initialization_success else False,
                    'timestamp': datetime.now().isoformat()
                }
                
                performance_data['initialization_attempts'].append(attempt_result)
                
                if import_success and initialization_success:
                    successful_attempts.append(attempt_result)
                    print(f"    OK æˆåŠŸ: {attempt_result['precise_time_seconds']:.3f}ç§’")
                else:
                    print(f"    âŒ å¤±æ•—")
                
                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()
                time.sleep(0.5)  # ã‚·ã‚¹ãƒ†ãƒ å®‰å®šåŒ–å¾…æ©Ÿ
                
            except Exception as e:
                print(f"    ERROR äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                performance_data['error_count'] += 1
                performance_data['initialization_attempts'].append({
                    'attempt_number': attempt + 1,
                    'error': str(e),
                    'traceback': traceback.format_exc(),
                    'timestamp': datetime.now().isoformat()
                })
        
        # å¹³å‡å€¤è¨ˆç®—
        if successful_attempts:
            avg_time = sum(a['precise_time_seconds'] for a in successful_attempts) / len(successful_attempts)
            avg_memory = sum(a['memory_used_mb'] for a in successful_attempts) / len(successful_attempts)
            avg_cpu = sum(a['cpu_usage_percent'] for a in successful_attempts) / len(successful_attempts)
            
            performance_data['average_metrics'] = {
                'avg_initialization_time_seconds': avg_time,
                'avg_memory_usage_mb': avg_memory,
                'avg_cpu_usage_percent': avg_cpu,
                'success_rate': len(successful_attempts) / num_attempts,
                'successful_attempts': len(successful_attempts),
                'total_attempts': num_attempts
            }
            
            print(f"\nçµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
            print(f"  åˆæœŸåŒ–æ™‚é–“: {avg_time:.3f}ç§’")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {avg_memory:.1f}MB")
            print(f"  CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
            print(f"  æˆåŠŸç‡: {len(successful_attempts)}/{num_attempts} ({len(successful_attempts)/num_attempts*100:.1f}%)")
        else:
            print("âŒ å…¨ã¦ã®åˆæœŸåŒ–è©¦è¡ŒãŒå¤±æ•—ã—ã¾ã—ãŸ")
            performance_data['average_metrics'] = {
                'avg_initialization_time_seconds': 0,
                'avg_memory_usage_mb': 0,
                'avg_cpu_usage_percent': 0,
                'success_rate': 0,
                'successful_attempts': 0,
                'total_attempts': num_attempts
            }
        
        return performance_data
    
    def _measure_traditional_system(self):
        """å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ï¼‰ã®æ¸¬å®š"""
        print("å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ æ¸¬å®šä¸­...")
        
        performance_data = {
            'direct_access_attempts': [],
            'average_metrics': {},
            'file_operations': {},
            'error_count': 0
        }
        
        # æŒ‰åˆ†å»ƒæ­¢ãƒ•ã‚¡ã‚¤ãƒ«ã®ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹æ¸¬å®š
        target_files = [
            'proportional_abolition_role_summary.parquet',
            'proportional_abolition_organization_summary.parquet'
        ]
        
        num_attempts = 3
        successful_attempts = []
        
        for attempt in range(num_attempts):
            print(f"  è©¦è¡Œ {attempt + 1}/{num_attempts}")
            
            try:
                initial_memory = psutil.virtual_memory().used
                start_time = time.perf_counter()
                
                # ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹
                accessed_files = []
                total_file_size = 0
                
                for file_name in target_files:
                    file_path = Path('.') / file_name
                    
                    if file_path.exists():
                        try:
                            # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆæƒ…å ±å–å¾—
                            file_stat = file_path.stat()
                            total_file_size += file_stat.st_size
                            
                            # Parquetãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
                            import pandas as pd
                            df = pd.read_parquet(file_path)
                            
                            accessed_files.append({
                                'file_name': file_name,
                                'file_size_bytes': file_stat.st_size,
                                'rows': len(df),
                                'columns': len(df.columns),
                                'success': True
                            })
                            
                        except Exception as e:
                            accessed_files.append({
                                'file_name': file_name,
                                'error': str(e),
                                'success': False
                            })
                            performance_data['error_count'] += 1
                    else:
                        accessed_files.append({
                            'file_name': file_name,
                            'error': 'File not found',
                            'success': False
                        })
                        performance_data['error_count'] += 1
                
                end_time = time.perf_counter()
                final_memory = psutil.virtual_memory().used
                
                # çµæœè¨˜éŒ²
                successful_files = [f for f in accessed_files if f['success']]
                attempt_result = {
                    'attempt_number': attempt + 1,
                    'total_time_seconds': end_time - start_time,
                    'memory_used_mb': (final_memory - initial_memory) / (1024 * 1024),
                    'files_accessed': len(successful_files),
                    'total_files': len(target_files),
                    'total_file_size_mb': total_file_size / (1024 * 1024),
                    'files_details': accessed_files,
                    'success_rate': len(successful_files) / len(target_files),
                    'timestamp': datetime.now().isoformat()
                }
                
                performance_data['direct_access_attempts'].append(attempt_result)
                
                if len(successful_files) > 0:
                    successful_attempts.append(attempt_result)
                    print(f"    OK æˆåŠŸ: {attempt_result['total_time_seconds']:.3f}ç§’, {len(successful_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
                else:
                    print(f"    âŒ å¤±æ•—: ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯")
                
                gc.collect()
                time.sleep(0.5)
                
            except Exception as e:
                print(f"    ERROR äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                performance_data['error_count'] += 1
        
        # å¹³å‡å€¤è¨ˆç®—
        if successful_attempts:
            avg_time = sum(a['total_time_seconds'] for a in successful_attempts) / len(successful_attempts)
            avg_memory = sum(a['memory_used_mb'] for a in successful_attempts) / len(successful_attempts)
            avg_files = sum(a['files_accessed'] for a in successful_attempts) / len(successful_attempts)
            
            performance_data['average_metrics'] = {
                'avg_access_time_seconds': avg_time,
                'avg_memory_usage_mb': avg_memory,
                'avg_files_accessed': avg_files,
                'success_rate': len(successful_attempts) / num_attempts,
                'successful_attempts': len(successful_attempts),
                'total_attempts': num_attempts
            }
            
            print(f"\nå¾“æ¥ã‚·ã‚¹ãƒ†ãƒ å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹:")
            print(f"  ã‚¢ã‚¯ã‚»ã‚¹æ™‚é–“: {avg_time:.3f}ç§’")
            print(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {avg_memory:.1f}MB")
            print(f"  ã‚¢ã‚¯ã‚»ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {avg_files:.1f}")
            print(f"  æˆåŠŸç‡: {len(successful_attempts)}/{num_attempts} ({len(successful_attempts)/num_attempts*100:.1f}%)")
        else:
            print("âŒ å…¨ã¦ã®å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã‚¢ã‚¯ã‚»ã‚¹ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            performance_data['average_metrics'] = {
                'avg_access_time_seconds': 0,
                'avg_memory_usage_mb': 0,
                'avg_files_accessed': 0,
                'success_rate': 0,
                'successful_attempts': 0,
                'total_attempts': num_attempts
            }
        
        return performance_data
    
    def _analyze_performance_comparison(self, unified_perf, traditional_perf):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒåˆ†æ"""
        print("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒåˆ†æä¸­...")
        
        comparison = {
            'baseline_established': False,
            'performance_ratio': {},
            'bottleneck_analysis': {},
            'recommendations': []
        }
        
        # æˆåŠŸã—ãŸæ¸¬å®šãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        unified_success = unified_perf['average_metrics']['success_rate'] > 0
        traditional_success = traditional_perf['average_metrics']['success_rate'] > 0
        
        if unified_success and traditional_success:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
            unified_time = unified_perf['average_metrics']['avg_initialization_time_seconds']
            traditional_time = traditional_perf['average_metrics']['avg_access_time_seconds']
            
            unified_memory = unified_perf['average_metrics']['avg_memory_usage_mb']
            traditional_memory = traditional_perf['average_metrics']['avg_memory_usage_mb']
            
            comparison['performance_ratio'] = {
                'time_ratio': unified_time / traditional_time if traditional_time > 0 else 0,
                'memory_ratio': unified_memory / traditional_memory if traditional_memory > 0 else 0,
                'unified_slower_by_factor': unified_time / traditional_time if traditional_time > 0 else 0,
                'unified_uses_more_memory_by_factor': unified_memory / traditional_memory if traditional_memory > 0 else 0
            }
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹
            comparison['baseline_established'] = True
            self.baseline_established = True
            
            print(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒçµæœ:")
            print(f"  çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ : {unified_time:.3f}ç§’, {unified_memory:.1f}MB")
            print(f"  å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ : {traditional_time:.3f}ç§’, {traditional_memory:.1f}MB")
            print(f"  æ™‚é–“æ¯”: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯{unified_time/traditional_time:.1f}å€é…ã„")
            print(f"  ãƒ¡ãƒ¢ãƒªæ¯”: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯{unified_memory/traditional_memory:.1f}å€å¤šã„")
            
            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
            if unified_time > traditional_time * 2:  # 2å€ä»¥ä¸Šé…ã„å ´åˆ
                comparison['bottleneck_analysis']['time_bottleneck'] = {
                    'severity': 'high',
                    'factor': unified_time / traditional_time,
                    'description': 'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–æ™‚é–“ãŒå¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®2å€ä»¥ä¸Š'
                }
                comparison['recommendations'].append('åˆæœŸåŒ–ãƒ—ãƒ­ã‚»ã‚¹ã®æœ€é©åŒ–ãŒå¿…è¦')
            
            if unified_memory > traditional_memory * 5:  # 5å€ä»¥ä¸Šãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨
                comparison['bottleneck_analysis']['memory_bottleneck'] = {
                    'severity': 'high', 
                    'factor': unified_memory / traditional_memory,
                    'description': 'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®5å€ä»¥ä¸Š'
                }
                comparison['recommendations'].append('ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æœ€é©åŒ–ãŒå¿…è¦')
            
        else:
            comparison['baseline_established'] = False
            self.baseline_established = False
            print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹å¤±æ•—: ä¸€æ–¹ã¾ãŸã¯ä¸¡æ–¹ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã‚¨ãƒ©ãƒ¼")
            
            if not unified_success:
                comparison['recommendations'].append('çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬å‹•ä½œä¿®å¾©ãŒå¿…è¦')
            if not traditional_success:
                comparison['recommendations'].append('æŒ‰åˆ†å»ƒæ­¢ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªãŒå¿…è¦')
        
        return comparison
    
    def generate_baseline_report(self):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\n=== A1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
        
        report = {
            'metadata': {
                'measurement_type': 'A1_performance_baseline',
                'timestamp': datetime.now().isoformat(),
                'baseline_established': self.baseline_established
            },
            'measurements': self.measurements,
            'conclusions': self._generate_conclusions(),
            'next_phase_readiness': self._assess_next_phase_readiness()
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        report_path = Path(f'phase1_a1_performance_baseline_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_path}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print(f"\nA1ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šçµæœ:")
        print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹: {'âœ… æˆåŠŸ' if self.baseline_established else 'âŒ å¤±æ•—'}")
        
        if self.baseline_established:
            unified_time = self.measurements['unified_system_performance']['average_metrics']['avg_initialization_time_seconds']
            traditional_time = self.measurements['traditional_system_performance']['average_metrics']['avg_access_time_seconds']
            
            print(f"  çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–: {unified_time:.3f}ç§’")
            print(f"  å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã‚¢ã‚¯ã‚»ã‚¹: {traditional_time:.3f}ç§’")
            print(f"  æ€§èƒ½å·®: {unified_time/traditional_time:.1f}å€")
            
            # 990ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³å•é¡Œã®å®Ÿè¨¼
            if unified_time > traditional_time * 10:  # 10å€ä»¥ä¸Šé…ã„å ´åˆ
                print(f"  ğŸš¨ é‡è¦ç™ºè¦‹: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯{unified_time/traditional_time:.1f}å€é…ãã€æœ€é©åŒ–ãŒå¿…è¦")
            elif unified_time > traditional_time * 2:  # 2å€ä»¥ä¸Šé…ã„å ´åˆ
                print(f"  âš ï¸ æ”¹å–„ä½™åœ°: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯{unified_time/traditional_time:.1f}å€é…ãã€æœ€é©åŒ–æ¨å¥¨")
            else:
                print(f"  âœ… è‰¯å¥½: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯è¨±å®¹ç¯„å›²")
        
        return report
    
    def _generate_conclusions(self):
        """çµè«–ç”Ÿæˆ"""
        conclusions = []
        
        if self.baseline_established:
            comparison = self.measurements['comparison_analysis']
            
            # 990ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³å•é¡Œã®å®Ÿè¨¼çµæœ
            time_ratio = comparison['performance_ratio']['time_ratio']
            
            if time_ratio > 10:
                conclusions.append({
                    'type': 'critical_finding',
                    'conclusion': '990ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³å•é¡ŒãŒå®Ÿè¨¼ã•ã‚ŒãŸ',
                    'evidence': f'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®{time_ratio:.1f}å€ã®æ™‚é–“ã‚’è¦ã™ã‚‹',
                    'implication': 'æœ€é©åŒ–ã«ã‚ˆã‚‹å¤§å¹…ãªæ€§èƒ½æ”¹å–„ãŒæœŸå¾…ã§ãã‚‹'
                })
            elif time_ratio > 2:
                conclusions.append({
                    'type': 'moderate_finding',
                    'conclusion': 'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã«æ€§èƒ½ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå­˜åœ¨',
                    'evidence': f'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®{time_ratio:.1f}å€ã®æ™‚é–“ã‚’è¦ã™ã‚‹',
                    'implication': 'æœ€é©åŒ–ã«ã‚ˆã‚‹æ€§èƒ½æ”¹å–„ãŒå¯èƒ½'
                })
            else:
                conclusions.append({
                    'type': 'minimal_impact',
                    'conclusion': 'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯è»½å¾®',
                    'evidence': f'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¯å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®{time_ratio:.1f}å€ã®æ™‚é–“',
                    'implication': 'æœ€é©åŒ–ã®å„ªå…ˆåº¦ã¯ä½ã„'
                })
            
            conclusions.append({
                'type': 'baseline_success',
                'conclusion': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹æˆåŠŸ',
                'evidence': 'çµ±ä¸€ãƒ»å¾“æ¥ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã®æ¸¬å®šå®Œäº†',
                'implication': 'Phase2ä»¥é™ã®è©³ç´°æ¤œè¨¼ãŒå¯èƒ½'
            })
        else:
            conclusions.append({
                'type': 'baseline_failure',
                'conclusion': 'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹å¤±æ•—',
                'evidence': 'çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¾ãŸã¯å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã®æ¸¬å®šå¤±æ•—',
                'implication': 'ã‚·ã‚¹ãƒ†ãƒ ä¿®å¾©ã¾ãŸã¯Phase1ã®å†å®Ÿè¡ŒãŒå¿…è¦'
            })
        
        return conclusions
    
    def _assess_next_phase_readiness(self):
        """æ¬¡ãƒ•ã‚§ãƒ¼ã‚ºæº–å‚™çŠ¶æ³è©•ä¾¡"""
        if self.baseline_established:
            return {
                'ready_for_phase2': True,
                'confidence_level': 'high',
                'prerequisites_met': [
                    'ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†',
                    'çµ±ä¸€ãƒ»å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½',
                    'ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®šæ¸ˆã¿'
                ],
                'recommended_next_steps': [
                    'A2: æ©Ÿèƒ½å‹•ä½œç¢ºèªã®å®Ÿè¡Œ',
                    'B1: å®Ÿè£…å¯èƒ½æ€§æ¤œè¨¼ã®é–‹å§‹'
                ]
            }
        else:
            return {
                'ready_for_phase2': False,
                'confidence_level': 'low',
                'blocking_issues': [
                    'ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹å¤±æ•—',
                    'ã‚·ã‚¹ãƒ†ãƒ åŸºæœ¬å‹•ä½œã«å•é¡Œ'
                ],
                'required_actions': [
                    'ã‚·ã‚¹ãƒ†ãƒ ä¿®å¾©',
                    'A1ã®å†å®Ÿè¡Œ',
                    'åŸºæœ¬å‹•ä½œç¢ºèª'
                ]
            }

def main():
    print("=" * 70)
    print("*** Phase1 A1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šé–‹å§‹ ***")
    print("ç›®çš„: ç¾åœ¨ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹")
    print("=" * 70)
    
    measurer = PerformanceMeasurement()
    
    try:
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šå®Ÿè¡Œ
        results = measurer.measure_current_unified_system()
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = measurer.generate_baseline_report()
        
        print("\n" + "=" * 70)
        print("*** A1: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šå®Œäº† ***")
        if measurer.baseline_established:
            print("OK ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹æˆåŠŸ - Phase2æ¤œè¨¼å¯èƒ½")
        else:
            print("âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹å¤±æ•— - ã‚·ã‚¹ãƒ†ãƒ ä¿®å¾©å¿…è¦")
        print("=" * 70)
        
        return report
        
    except Exception as e:
        print(f"\nERROR A1æ¸¬å®šä¸­ã«äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        print("ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:")
        print(traceback.format_exc())
        return None

if __name__ == "__main__":
    main()