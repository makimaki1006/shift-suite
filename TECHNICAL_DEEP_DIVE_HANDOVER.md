# ğŸ”¬ ã‚·ãƒ•ãƒˆåˆ†æã‚·ã‚¹ãƒ†ãƒ æŠ€è¡“è©³ç´°å¼•ç¶™ãæ–‡æ›¸

## å‰ç½®ã
ã“ã®æ–‡æ›¸ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã®æ·±å±¤éƒ¨ã¾ã§å®Œå…¨ã«ç†è§£ã™ã‚‹ãŸã‚ã®æŠ€è¡“è©³ç´°æ›¸ã§ã™ã€‚è¡¨é¢çš„ãªç†è§£ã§ã¯ä¿å®ˆãƒ»æ”¹ä¿®ãŒä¸å¯èƒ½ãªãŸã‚ã€å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰å‹•ä½œã¨ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’è©³ç´°ã«è§£èª¬ã—ã¾ã™ã€‚

---

## 1. ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼è©³ç´°

### 1.1 ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°å®Ÿè£…

#### Step 1: Excelãƒ•ã‚¡ã‚¤ãƒ«è§£æï¼ˆapp.py:1000-1200è¡Œï¼‰
```python
# ã‚¦ã‚£ã‚¶ãƒ¼ãƒ‰å½¢å¼ã§ã®ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—è§£æ
def wizard_mode():
    if step == 1:  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚·ãƒ¼ãƒˆé¸æŠ
        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨ã‚·ãƒ¼ãƒˆåã‚’å–å¾—
        sheet_names = pd.ExcelFile(uploaded_file).sheet_names
        
    if step == 2:  # å„ã‚·ãƒ¼ãƒˆã®æ§‹é€ è§£æ
        for sheet in selected_sheets:
            # å¹´æœˆæƒ…å ±ã‚»ãƒ«ä½ç½®ã®æŒ‡å®šï¼ˆä¾‹ï¼šD1ï¼‰
            ym = st.text_input("å¹´æœˆæƒ…å ±ã‚»ãƒ«ä½ç½®", value="D1")
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œç•ªå·ã®æŒ‡å®š
            hdr = st.number_input("åˆ—åãƒ˜ãƒƒãƒ€ãƒ¼è¡Œç•ªå·", 1, 20, value=1)
            # ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œã®æŒ‡å®š
            data_start = st.number_input("ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œç•ªå·", 1, 20, value=3)
            
            # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼ˆæœ€åˆã®10è¡Œï¼‰
            df_preview = pd.read_excel(file, sheet_name=sheet, 
                                     header=int(hdr)-1, nrows=10)
            
            # ğŸ“… é‡è¦: å‚ç…§æœŸé–“ã®è‡ªå‹•æ¨å®š
            auto_date_range = estimate_date_range_from_excel(
                file_path, sheet, int(hdr)-1
            )
```

#### Step 2: åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°ã®è‡ªå‹•æ¨å®šï¼ˆapp.py:1080-1110è¡Œï¼‰
```python
# SHEET_COL_ALIASè¾æ›¸ã«ã‚ˆã‚‹åˆ—åæ­£è¦åŒ–
SHEET_COL_ALIAS = {
    "æ°å": "staff", "åå‰": "staff", "ã‚¹ã‚¿ãƒƒãƒ•": "staff",
    "è·ç¨®": "role", "å½¹è·": "role", "å‹¤å‹™": "role",
    "é›‡ç”¨å½¢æ…‹": "employment", "é›‡ç”¨": "employment"
}

def auto_column_mapping():
    guessed = {}
    for column in excel_columns:
        canonical = SHEET_COL_ALIAS.get(_normalize(str(column)))
        if canonical and canonical not in guessed:
            guessed[canonical] = column
```

#### Step 3: ãƒ‡ãƒ¼ã‚¿å–ã‚Šè¾¼ã¿ï¼ˆingest_excelé–¢æ•°ï¼‰
```python
def ingest_excel(excel_path, shift_sheets, header_row, slot_minutes, 
                year_month_cell_location):
    # 1. å„ã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    # 2. åˆ—åã®æ­£è¦åŒ–
    # 3. æ—¥ä»˜åˆ—ã®è­˜åˆ¥ã¨è§£æ
    # 4. ã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰ã®è§£æã¨æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆå¤‰æ›
    # 5. long_dfï¼ˆé•·å½¢å¼DataFrameï¼‰ã¸ã®å¤‰æ›
    
    return long_df, metadata, unknown_codes
```

### 1.2 ãƒ‡ãƒ¼ã‚¿åˆ†è§£ãƒ»æ­£è¦åŒ–ãƒ—ãƒ­ã‚»ã‚¹

#### build_stats.pyã®è©³ç´°å‡¦ç†ãƒ•ãƒ­ãƒ¼
```python
def build_stats_main():
    # Phase 1: åŸºæœ¬çµ±è¨ˆã®æ§‹ç¯‰
    time_labels = gen_labels(slot_minutes)  # ["00:00", "00:30", ...]
    
    # Phase 2: ä¼‘æ¥­æ—¥ã®æ¨å®š
    estimated_holidays_set = estimate_holidays_automatically(date_columns)
    
    # Phase 3: å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®é›†è¨ˆ
    staff_actual_df = aggregate_actual_staff_data(long_df, time_labels)
    
    # Phase 4: æ—¥æ¬¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
    for date_col in date_columns:
        parsed_date = _parse_as_date(date_col)
        is_working_day = 1 if parsed_date not in estimated_holidays_set else 0
        
        # å®Ÿç¸¾ã‚¹ãƒ­ãƒƒãƒˆæ•°ã®è¨ˆç®—
        actual_slots_today = staff_actual_df[date_col].sum()
        
        # ä¸è¶³ãƒ»éå‰°ã®è¨ˆç®—ï¼ˆä¼‘æ¥­æ—¥ã¯need=0ã§å†è¨ˆç®—ï¼‰
        if is_working_day:
            need_today = need_per_timeslot_series
            upper_today = upper_per_timeslot_series
        else:
            need_today = pd.Series(0, index=time_labels)
            upper_today = staff_actual_df[date_col]
            
        lack_slots = (need_today - staff_actual_df[date_col]).clip(lower=0)
        excess_slots = (staff_actual_df[date_col] - upper_today).clip(lower=0)
        
        # æ™‚é–“æ›ç®—
        daily_metrics.append({
            "date": parsed_date,
            "actual_hours": actual_slots_today * slot_hours,
            "lack_hours": lack_slots.sum() * slot_hours,
            "excess_hours": excess_slots.sum() * slot_hours,
            "is_working_day": is_working_day
        })
```

## 2. æ ¸å¿ƒåˆ†æã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®è©³ç´°

### 2.1 ä¸è¶³åˆ†æã®å¤šå±¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ï¼ˆshortage.pyï¼‰

#### Layer 1: Needãƒ‡ãƒ¼ã‚¿ã®å†æ§‹ç¯‰
```python
def rebuild_need_data():
    if not need_per_date_slot_df.empty:
        # ã€æœ€é‡è¦ã€‘è©³ç´°Needãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã€ãã‚Œã‚’ãã®ã¾ã¾ä½¿ç”¨
        need_df_all = need_per_date_slot_df.reindex(
            columns=staff_actual_data_all_df.columns, fill_value=0
        )
    else:
        # ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‘æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã§Needè¨ˆç®—
        for col, date in zip(need_df_all.columns, parsed_dates):
            is_holiday = date in estimated_holidays_set
            if is_holiday:
                need_df_all[col] = 0  # ä¼‘æ¥­æ—¥ã¯Need=0
            else:
                dow = date.weekday()  # 0=æœˆæ›œ, 6=æ—¥æ›œ
                if dow in dow_need_pattern_df.columns:
                    need_df_all[col] = dow_need_pattern_df[dow]
```

#### Layer 2: ç•°å¸¸å€¤æ¤œå‡ºãƒ»åˆ¶é™ï¼ˆ27,486.5æ™‚é–“å•é¡Œå¯¾ç­–ï¼‰
```python
def validate_and_cap_shortage(lack_df, period_days, slot_hours):
    total_shortage_hours = lack_df.sum().sum() * slot_hours
    daily_avg = total_shortage_hours / period_days
    
    # ç•°å¸¸å€¤åˆ¤å®šé–¾å€¤
    if daily_avg > 8.0:  # 1æ—¥8æ™‚é–“è¶…ã®ä¸è¶³ã¯ç•°å¸¸
        log.error(f"ç•°å¸¸ãªä¸è¶³æ¤œå‡º: {daily_avg:.1f}æ™‚é–“/æ—¥")
        
        # åˆ¶é™ã®é©ç”¨
        reasonable_daily_shortage = 5.0  # åˆç†çš„ãªä¸Šé™
        cap_factor = reasonable_daily_shortage / daily_avg
        lack_df_capped = lack_df * cap_factor
        
        return lack_df_capped, True
    
    return lack_df, False
```

#### Layer 3: æœŸé–“æ­£è¦åŒ–
```python
def apply_period_normalization(lack_df, period_days, slot_hours):
    standard_period = 30  # æ¨™æº–æœˆé–“æ—¥æ•°
    
    if abs(period_days - standard_period) > 7:  # Â±7æ—¥ã®ç¯„å›²å¤–
        normalization_factor = standard_period / period_days
        normalized_df = lack_df * normalization_factor
        
        norm_stats = {
            "normalization_factor": normalization_factor,
            "original_days": period_days,
            "normalized_to": standard_period
        }
        
        return normalized_df, normalization_factor, norm_stats
```

#### Layer 4: æœŸé–“ä¾å­˜æ€§åˆ¶å¾¡
```python
def apply_period_dependency_control(lack_df, period_days, slot_hours):
    # çŸ­æœŸé–“ï¼ˆ<20æ—¥ï¼‰ã§ã®åˆ†æã§ã¯ä¸è¶³ãŒéå¤§ã«ãªã‚‹å‚¾å‘ã‚’è£œæ­£
    if period_days < 20:
        adjustment_factor = 0.8  # 20%æ¸›ç®—
        controlled_df = lack_df * adjustment_factor
        
        return controlled_df, {"applied": True, "factor": adjustment_factor}
    
    return lack_df, {"applied": False}
```

#### Layer 5: æœ€çµ‚å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
```python
def final_validation(lack_df, period_days, slot_hours):
    final_total = lack_df.sum().sum() * slot_hours
    final_daily_avg = final_total / period_days
    
    if final_daily_avg <= 3.0:
        log.info("âœ… ç†æƒ³çš„ç¯„å›²: â‰¤3.0h/æ—¥")
    elif final_daily_avg <= 5.0:
        log.info("âœ… è¨±å®¹ç¯„å›²: â‰¤5.0h/æ—¥")
    elif final_daily_avg <= 8.0:
        log.warning("âš ï¸ è¦æ”¹å–„: >5.0h/æ—¥")
    else:
        log.error("âŒ ä¾ç„¶ç•°å¸¸: >8.0h/æ—¥")
        # è¿½åŠ ã®è¨ˆç®—ã‚¨ãƒ©ãƒ¼ãŒæ®‹å­˜
```

### 2.2 ä¼‘æš‡é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®è©³ç´°å®Ÿè£…ï¼ˆutils.pyï¼‰

```python
def apply_rest_exclusion_filter(df, context, for_display=False):
    # Layer 1: ã‚¹ã‚¿ãƒƒãƒ•åã«ã‚ˆã‚‹é™¤å¤–ï¼ˆæœ€é‡è¦ï¼‰
    rest_patterns = [
        'Ã—', 'X', 'x',           # åŸºæœ¬ä¼‘ã¿è¨˜å·
        'ä¼‘', 'ä¼‘ã¿', 'ä¼‘æš‡',      # æ—¥æœ¬èªä¼‘ã¿
        'æ¬ ', 'æ¬ å‹¤',             # æ¬ å‹¤
        'OFF', 'off', 'Off',     # ã‚ªãƒ•
        '-', 'âˆ’', 'â€•',           # ãƒã‚¤ãƒ•ãƒ³é¡
        'nan', 'NaN', 'null',    # NULLå€¤
        'æœ‰', 'æœ‰ä¼‘',             # æœ‰çµ¦
        'ç‰¹', 'ç‰¹ä¼‘',             # ç‰¹ä¼‘
        'ä»£', 'ä»£ä¼‘',             # ä»£ä¼‘
        'æŒ¯', 'æŒ¯ä¼‘'              # æŒ¯æ›¿ä¼‘æ—¥
    ]
    
    excluded_by_pattern = {}
    for pattern in rest_patterns:
        pattern_mask = (
            (df['staff'].str.strip() == pattern) |
            (df['staff'].str.contains(pattern, na=False, regex=False))
        )
        excluded_count = pattern_mask.sum()
        if excluded_count > 0:
            excluded_by_pattern[pattern] = excluded_count
            df = df[~pattern_mask]
    
    # Layer 2: 0ã‚¹ãƒ­ãƒƒãƒˆé™¤å¤–
    if 'parsed_slots_count' in df.columns:
        zero_slots_mask = df['parsed_slots_count'] <= 0
        df = df[~zero_slots_mask]
    
    # Layer 3: è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†é›¢
    if 'staff_count' in df.columns and not for_display:
        # åˆ†æç”¨: å®Ÿç¸¾0ã‚’é™¤å¤–ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
        zero_staff_mask = df['staff_count'] <= 0
        df = df[~zero_staff_mask]
    elif for_display:
        # è¡¨ç¤ºç”¨: å®Ÿç¸¾0ã‚‚ä¿æŒï¼ˆä¿¯ç°è¦³å¯Ÿç”¨ï¼‰
        pass
    
    exclusion_rate = (original_count - len(df)) / original_count
    log.info(f"é™¤å¤–ç‡: {exclusion_rate:.1%}")
    
    return df
```

## 3. 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…è©³ç´°

### 3.1 èªçŸ¥å¿ƒç†å­¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆ752è¡Œï¼‰

```python
class CognitivePsychologyAnalyzer:
    def __init__(self):
        self.theories = {
            "maslach_burnout": MaslachBurnoutAnalyzer(),
            "selye_stress": SelyeStressAnalyzer(),
            "self_determination": SelfDeterminationAnalyzer(),
            "cognitive_load": CognitiveLoadAnalyzer(),
            "jdc_model": JobDemandControlAnalyzer()
        }
    
    def analyze_burnout_maslach(self, fatigue_data, shift_data):
        """Maslachãƒãƒ¼ãƒ³ã‚¢ã‚¦ãƒˆç†è«–ã«åŸºã¥ãåˆ†æ"""
        # 3æ¬¡å…ƒãƒ¢ãƒ‡ãƒ«: æƒ…ç·’çš„æ¶ˆè€—ãƒ»è„±äººæ ¼åŒ–ãƒ»å€‹äººçš„é”æˆæ„Ÿã®ä½ä¸‹
        
        emotional_exhaustion = self._calculate_emotional_exhaustion(
            consecutive_days=shift_data['consecutive_work_days'],
            night_shifts=shift_data['night_shift_count'],
            overtime_hours=shift_data['overtime_hours']
        )
        
        depersonalization = self._calculate_depersonalization(
            stress_indicators=fatigue_data['stress_score'],
            workload_pressure=shift_data['workload_intensity']
        )
        
        personal_accomplishment = self._calculate_personal_accomplishment(
            role_clarity=shift_data['role_clarity_score'],
            feedback_quality=shift_data['feedback_score']
        )
        
        # Maslach Burnout Inventory (MBI) ã‚¹ã‚³ã‚¢è¨ˆç®—
        mbi_score = self._calculate_mbi_composite(
            emotional_exhaustion, depersonalization, personal_accomplishment
        )
        
        return {
            "theory": "Maslach Burnout Theory",
            "dimensions": {
                "emotional_exhaustion": emotional_exhaustion,
                "depersonalization": depersonalization, 
                "personal_accomplishment": personal_accomplishment
            },
            "composite_score": mbi_score,
            "risk_level": self._categorize_burnout_risk(mbi_score),
            "recommendations": self._generate_burnout_interventions(mbi_score)
        }
```

### 3.2 çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆ1,499è¡Œï¼‰

```python
class OrganizationalPatternAnalyzer:
    def analyze_schein_culture(self, shift_data, organizational_data):
        """Scheinçµ„ç¹”æ–‡åŒ–3å±¤ãƒ¢ãƒ‡ãƒ«åˆ†æ"""
        
        # Layer 1: Artifactsï¼ˆäººå·¥ç‰©å±¤ï¼‰
        artifacts = self._analyze_artifacts(
            shift_patterns=shift_data['shift_distribution'],
            communication_patterns=organizational_data['comm_frequency'],
            physical_workspace=organizational_data['workspace_config']
        )
        
        # Layer 2: Espoused Valuesï¼ˆä¾¡å€¤è¦³å±¤ï¼‰
        espoused_values = self._analyze_espoused_values(
            policy_adherence=organizational_data['policy_compliance'],
            stated_priorities=organizational_data['priority_statements'],
            goal_alignment=organizational_data['goal_consistency']
        )
        
        # Layer 3: Basic Assumptionsï¼ˆåŸºæœ¬ä»®å®šå±¤ï¼‰
        basic_assumptions = self._analyze_basic_assumptions(
            decision_patterns=organizational_data['decision_history'],
            conflict_resolution=organizational_data['conflict_styles'],
            learning_patterns=organizational_data['adaptation_history']
        )
        
        # æ–‡åŒ–å‹ã®åˆ¤å®šï¼ˆã‚¯ãƒ©ãƒ³vsã‚¢ãƒ‰ãƒ›ã‚¯ãƒ©ã‚·ãƒ¼vsãƒãƒ¼ã‚±ãƒƒãƒˆvsãƒ’ã‚¨ãƒ©ãƒ«ã‚­ãƒ¼ï¼‰
        culture_type = self._determine_culture_type(
            artifacts, espoused_values, basic_assumptions
        )
        
        return {
            "theory": "Schein Organizational Culture Model",
            "culture_layers": {
                "artifacts": artifacts,
                "espoused_values": espoused_values,
                "basic_assumptions": basic_assumptions
            },
            "culture_type": culture_type,
            "alignment_score": self._calculate_culture_alignment(),
            "change_readiness": self._assess_change_readiness()
        }
```

### 3.3 AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®è©³ç´°ï¼ˆ2,907è¡Œï¼‰

```python
class AIComprehensiveReportGenerator:
    def generate_comprehensive_report(self, analysis_results, input_file, output_dir):
        """18ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        
        # Section 1-12: åŸºæœ¬åˆ†æï¼ˆå¾“æ¥æ©Ÿèƒ½ï¼‰
        basic_sections = self._generate_basic_sections(analysis_results)
        
        # Section 13: èªçŸ¥å¿ƒç†å­¦çš„æ·±åº¦åˆ†æ
        cognitive_section = self._generate_cognitive_analysis(
            fatigue_data=analysis_results.get('fatigue_analysis'),
            shift_patterns=analysis_results.get('shift_patterns'),
            psychological_theories=[
                "maslach_burnout", "selye_stress", "self_determination",
                "cognitive_load", "jdc_model"
            ]
        )
        
        # Section 14: çµ„ç¹”ãƒ‘ã‚¿ãƒ¼ãƒ³æ·±åº¦åˆ†æ
        organizational_section = self._generate_organizational_analysis(
            organizational_data=analysis_results.get('organizational_metrics'),
            theories=["schein_culture", "power_dynamics", "social_network", 
                     "french_raven_power", "institutional_theory"]
        )
        
        # Section 15: ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒå¤šå±¤å› æœåˆ†æ
        systems_section = self._generate_systems_analysis(
            system_data=analysis_results.get('system_metrics'),
            theories=["system_dynamics", "complexity_theory", "toc",
                     "social_ecological", "chaos_theory"]
        )
        
        # Section 16: ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆæ·±åº¦åˆ†æ
        blueprint_section = self._generate_blueprint_analysis(
            blueprint_data=analysis_results.get('blueprint_data'),
            decision_frameworks=9
        )
        
        # Section 17: MECEçµ±åˆåˆ†æ
        mece_section = self._generate_mece_analysis(
            mece_data=analysis_results.get('mece_integration'),
            axes_count=12
        )
        
        # Section 18: äºˆæ¸¬æœ€é©åŒ–çµ±åˆåˆ†æ
        predictive_section = self._generate_predictive_analysis(
            prediction_data=analysis_results.get('predictive_optimization'),
            frameworks=13
        )
        
        # å…¨ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±åˆ
        comprehensive_report = {
            "metadata": self._generate_metadata(input_file),
            "sections": basic_sections + [
                cognitive_section,
                organizational_section, 
                systems_section,
                blueprint_section,
                mece_section,
                predictive_section
            ],
            "integration_summary": self._generate_integration_summary(),
            "quality_metrics": {
                "total_theories_integrated": 18,
                "analysis_depth_score": 1.10,  # 110% of baseline
                "framework_coverage": "comprehensive"
            }
        }
        
        return comprehensive_report
```

## 4. ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã®å…·ä½“ä¾‹

### 4.1 å…¥åŠ›Excelã‹ã‚‰å‡ºåŠ›ã¾ã§ã®å®Ÿãƒ‡ãƒ¼ã‚¿è¿½è·¡

#### å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ä¾‹:
```
| æ°å   | è·ç¨® | 4/1  | 4/2  | 4/3  |
|--------|------|------|------|------|
| ç”°ä¸­   | ä»‹è­· | æ—¥E  | æ—¥D  | ä¼‘   |
| ä½è—¤   | çœ‹è­· | å¤œA  | Ã—   | æ—©B  |
```

#### Step 1: ã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰è§£æ
```python
SHIFT_CODE_MAPPING = {
    "æ—¥E": {"start": "08:30", "end": "17:00", "slots": 17},  # 8.5æ™‚é–“
    "æ—¥D": {"start": "09:00", "end": "18:00", "slots": 18},  # 9æ™‚é–“
    "å¤œA": {"start": "22:00", "end": "07:00", "slots": 18},  # 9æ™‚é–“ï¼ˆç¿Œæ—¥è·¨ãï¼‰
    "æ—©B": {"start": "06:00", "end": "15:00", "slots": 18},  # 9æ™‚é–“
    "ä¼‘": {"slots": 0},
    "Ã—": {"slots": 0}
}
```

#### Step 2: long_dfå¤‰æ›
```
| staff | role | date       | parsed_slots_count | employment |
|-------|------|------------|-------------------|------------|
| ç”°ä¸­  | ä»‹è­· | 2025-04-01 | 17                | æ­£ç¤¾å“¡     |
| ç”°ä¸­  | ä»‹è­· | 2025-04-02 | 18                | æ­£ç¤¾å“¡     |
| ç”°ä¸­  | ä»‹è­· | 2025-04-03 | 0                 | æ­£ç¤¾å“¡     |
| ä½è—¤  | çœ‹è­· | 2025-04-01 | 18                | ãƒ‘ãƒ¼ãƒˆ     |
| ä½è—¤  | çœ‹è­· | 2025-04-02 | 0                 | ãƒ‘ãƒ¼ãƒˆ     |
| ä½è—¤  | çœ‹è­· | 2025-04-03 | 18                | ãƒ‘ãƒ¼ãƒˆ     |
```

#### Step 3: æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆå±•é–‹
```python
# 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆã®å ´åˆï¼ˆslot_minutes=30ï¼‰
time_labels = ["00:00", "00:30", "01:00", ..., "23:30"]  # 48ã‚¹ãƒ­ãƒƒãƒˆ

# ç”°ä¸­ã•ã‚“ã®4/1ï¼ˆæ—¥E: 08:30-17:00ï¼‰
ç”°ä¸­_4_1_slots = {
    "08:30": 1, "09:00": 1, "09:30": 1, ..., "16:30": 1  # 17ã‚¹ãƒ­ãƒƒãƒˆ
}
```

#### Step 4: needè¨ˆç®—ã¨ä¸è¶³åˆ†æ
```python
# æ›œæ—¥åˆ¥Needï¼ˆ4/1=ç«æ›œæ—¥ã®å ´åˆï¼‰
tuesday_need = {
    "08:30": 2,  # 8:30ã«ã¯2äººå¿…è¦
    "09:00": 3,  # 9:00ã«ã¯3äººå¿…è¦
    ...
}

# å®Ÿç¸¾
actual_4_1 = {
    "08:30": 1,  # ç”°ä¸­ã®ã¿ï¼ˆä½è—¤ã¯å¤œå‹¤ã§ä¸åœ¨ï¼‰
    "09:00": 1,
    ...
}

# ä¸è¶³è¨ˆç®—
lack_4_1 = {
    "08:30": max(0, 2 - 1) = 1,  # 1äººä¸è¶³
    "09:00": max(0, 3 - 1) = 2,  # 2äººä¸è¶³
    ...
}
```

#### Step 5: æœ€çµ‚å‡ºåŠ›
```
stats_summary.txt:
lack_hours_total: 373
excess_hours_total: 58

heat_ALL.xlsx:
ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ãƒ‡ãƒ¼ã‚¿ï¼‰
æ™‚é–“/æ—¥ä»˜  | 4/1 | 4/2 | 4/3 |
08:30     | -1  | 0   | -2  |  # è² æ•°ã¯ä¸è¶³
09:00     | -2  | +1  | 0   |  # æ­£æ•°ã¯éå‰°
```

## 5. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ç•°å¸¸å€¤å¯¾ç­–

### 5.1 27,486.5æ™‚é–“å•é¡Œã®è§£æ±ºç­–

ã“ã®å•é¡Œã¯æœŸé–“ã®çŸ­ã•ã«èµ·å› ã™ã‚‹è¨ˆç®—å¢—å¹…ã‚¨ãƒ©ãƒ¼ã§ã—ãŸï¼š

```python
# å•é¡Œ: 7æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿ã§æœˆé–“ä¸è¶³ã‚’è¨ˆç®—ã™ã‚‹ã¨ç•°å¸¸ãªå€¤
original_shortage = 984.5  # æ™‚é–“/7æ—¥
monthly_projection = original_shortage * (30/7) = 4,219.3  # ç•°å¸¸å€¤

# è§£æ±ºç­–1: æœŸé–“æ­£è¦åŒ–
if period_days < 20:
    normalization_factor = 30 / period_days
    normalized_shortage = original_shortage * normalization_factor * 0.7  # 30%å‰²å¼•

# è§£æ±ºç­–2: ä¸Šé™ã‚­ãƒ£ãƒƒãƒ—
daily_avg = total_shortage / period_days
if daily_avg > 8.0:  # 1æ—¥8æ™‚é–“è¶…ã¯ç•°å¸¸
    capped_shortage = total_shortage * (5.0 / daily_avg)  # 5æ™‚é–“/æ—¥ã«åˆ¶é™
```

### 5.2 ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯

```python
def comprehensive_data_validation(df):
    validation_results = []
    
    # Check 1: ç©ºãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡º
    if df.empty:
        validation_results.append("CRITICAL: Empty dataset")
        
    # Check 2: ç•°å¸¸å€¤ã®æ¤œå‡º
    for col in df.select_dtypes(include=[np.number]).columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        outliers = ((df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)).sum()
        if outliers > len(df) * 0.1:  # 10%è¶…ãŒå¤–ã‚Œå€¤
            validation_results.append(f"WARNING: {col} has {outliers} outliers")
    
    # Check 3: æ—¥ä»˜ã®é€£ç¶šæ€§
    date_gaps = find_date_gaps(df.index)
    if date_gaps:
        validation_results.append(f"INFO: Date gaps found: {date_gaps}")
    
    return validation_results
```

## 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®å®Ÿè£…

### 6.1 ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

```python
@st.cache_data(show_spinner=False, ttl=3600)
def load_data_cached(file_path, file_mtime=None, is_parquet=False):
    """ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ›´æ™‚åˆ»ãƒ™ãƒ¼ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    if is_parquet:
        return pd.read_parquet(file_path)
    return safe_read_excel(file_path)

@st.cache_data(show_spinner=False, ttl=1800)  
def compute_heatmap_ratio_cached(heat_df, need_series):
    """é«˜è² è·ãªæ¯”ç‡è¨ˆç®—ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    clean_df = heat_df.drop(columns=SUMMARY5_CONST, errors="ignore")
    need_series_safe = need_series.replace(0, np.nan)
    return clean_df.div(need_series_safe, axis=0).clip(lower=0, upper=2)
```

### 6.2 ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–

```python
def safe_slot_calculation(data, slot_minutes, operation="sum"):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸè¨ˆç®—"""
    slot_hours = slot_minutes / 60.0
    
    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    if hasattr(data, 'memory_usage'):
        data_size_mb = data.memory_usage(deep=True).sum() / 1024 / 1024
    else:
        data_size_mb = 0
    
    # å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ50MBè¶…ï¼‰ã®å ´åˆã¯åŠ¹ç‡çš„è¨ˆç®—
    if data_size_mb > 50:
        log.info(f"å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º({data_size_mb:.1f}MB): åŠ¹ç‡çš„è¨ˆç®—ä½¿ç”¨")
        
        if operation == "sum":
            # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§å‡¦ç†
            chunk_size = 10000
            total_result = 0
            for i in range(0, len(data), chunk_size):
                chunk = data.iloc[i:i+chunk_size]
                total_result += (chunk * slot_hours).sum()
            return total_result
    else:
        # é€šå¸¸ã‚µã‚¤ã‚ºã¯æ¨™æº–å‡¦ç†
        return getattr(data * slot_hours, operation)()
```

## 7. çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°

### 7.1 ã‚·ãƒŠãƒªã‚ªãƒ™ãƒ¼ã‚¹åˆ†æã®å®Ÿè£…

```python
class UnifiedAnalysisManager:
    def __init__(self):
        self.scenario_registries = {
            "mean_based": {},      # å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹
            "median_based": {},    # ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            "p25_based": {}        # 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹
        }
        self.default_scenario = "median_based"  # çµ±è¨ˆçš„ã«æœ€å®‰å®š
    
    def create_shortage_analysis(self, file_name, scenario_key, role_df):
        """ã‚·ãƒŠãƒªã‚ªå¯¾å¿œä¸è¶³åˆ†æçµæœä½œæˆ"""
        analysis_key = self.key_manager.generate_scenario_analysis_key(
            file_name, scenario_key, "shortage"
        )
        
        result = UnifiedAnalysisResult(analysis_key, "shortage_analysis")
        result.metadata["scenario"] = scenario_key
        
        # å‹•çš„ãƒ‡ãƒ¼ã‚¿å‡¦ç† - ã‚«ãƒ©ãƒ å­˜åœ¨ç¢ºèªå¾Œå‡¦ç†
        if "lack_h" in role_df.columns and not role_df.empty:
            total_shortage = self.converter.safe_float(
                role_df["lack_h"].sum(), 0.0, "total_shortage_hours"
            )
            
            # é‡è¦åº¦ã®å‹•çš„è¨ˆç®—
            severity = self._calculate_severity(total_shortage)
            
            result.add_core_metric("total_shortage_hours", total_shortage)
            result.extended_data = {
                "severity_level": severity,
                "top_shortage_roles": self._extract_top_roles(role_df),
                "data_completeness": self._calculate_completeness(role_df)
            }
        
        # çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã¨ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
        self.results_registry[analysis_key] = result
        self.scenario_registries[scenario_key][analysis_key] = result
        
        return result
```

## 8. å®Ÿéš›ã®å•é¡Œè§£æ±ºäº‹ä¾‹

### 8.1 å¤‰æ•°é †åºãƒã‚°ã®ä¿®æ­£ï¼ˆshortage.py:663-668ï¼‰

**å•é¡Œ**: `lack_count_overall_df`ãŒå®šç¾©å‰ã«ä½¿ç”¨ã•ã‚Œã‚¨ãƒ©ãƒ¼

**ä¿®æ­£å‰**:
```python
# line 663: ä½¿ç”¨ï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰
if lack_count_overall_df.sum().sum() > threshold:
    
# line 684: å®šç¾©ï¼ˆé…ã™ãã‚‹ï¼‰
lack_count_overall_df = need_df_all - staff_actual_data_all_df
```

**ä¿®æ­£å¾Œ**:
```python
# line 665: å®šç¾©ã‚’å‰ã«ç§»å‹•
lack_count_overall_df = (
    (need_df_all - staff_actual_data_all_df)
)

# line 668ä»¥é™: å®‰å…¨ã«ä½¿ç”¨
if lack_count_overall_df.sum().sum() > threshold:
```

### 8.2 æ—¥æœ¬èªãƒ‘ã‚¹å•é¡Œã®è§£æ±º

**å•é¡Œ**: `C:\Users\fuji1\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\ã‚·ãƒ•ãƒˆåˆ†æ`ã§ä»®æƒ³ç’°å¢ƒã‚¨ãƒ©ãƒ¼

**è§£æ±ºç­–**:
```bash
# è‹±èªãƒ‘ã‚¹ã¸ã®ç§»å‹•
move "C:\Users\fuji1\OneDrive\ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—\ã‚·ãƒ•ãƒˆåˆ†æ" "C:\ShiftAnalysis"

# ä»®æƒ³ç’°å¢ƒã®å†æ§‹ç¯‰
cd C:\ShiftAnalysis
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

## 9. ãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼æ‰‹é †

### 9.1 æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```python
def comprehensive_system_test():
    """åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    
    # Test 1: ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ†ã‚¹ãƒˆ
    test_excel = "test_shift_data.xlsx"
    long_df, metadata, unknown_codes = ingest_excel(test_excel)
    assert not long_df.empty, "ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿å¤±æ•—"
    assert len(unknown_codes) == 0, f"æœªçŸ¥ã‚³ãƒ¼ãƒ‰: {unknown_codes}"
    
    # Test 2: ä¸è¶³åˆ†æãƒ†ã‚¹ãƒˆ  
    shortage_results = shortage_and_brief(long_df)
    assert "lack_hours_total" in shortage_results, "ä¸è¶³åˆ†æå¤±æ•—"
    
    daily_avg = shortage_results["lack_hours_total"] / 30
    assert daily_avg <= 8.0, f"ç•°å¸¸ãªä¸è¶³: {daily_avg:.1f}h/æ—¥"
    
    # Test 3: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    heatmap_files = generate_heatmaps(shortage_results)
    for file_path in heatmap_files:
        assert file_path.exists(), f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æœªç”Ÿæˆ: {file_path}"
    
    # Test 4: AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
    if AI_REPORT_GENERATOR_AVAILABLE:
        ai_report = generate_comprehensive_report(shortage_results)
        assert len(ai_report["sections"]) == 18, "18ã‚»ã‚¯ã‚·ãƒ§ãƒ³æœªé”æˆ"
    
    return "ALL_TESTS_PASSED"
```

### 9.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ

```python
def performance_benchmark():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
    import time
    import psutil
    
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    result = run_full_analysis("large_test_file.xlsx")
    
    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
    
    metrics = {
        "execution_time": end_time - start_time,
        "memory_usage": end_memory - start_memory,
        "output_size_mb": get_output_size_mb(),
        "records_processed": len(result)
    }
    
    # æ€§èƒ½è¦ä»¶ãƒã‚§ãƒƒã‚¯
    assert metrics["execution_time"] < 60, "å‡¦ç†æ™‚é–“è¶…é"
    assert metrics["memory_usage"] < 500, "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¶…é"
    
    return metrics
```

## çµè«–

ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯è¡¨é¢çš„ã«ã¯ã€Œã‚·ãƒ•ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«ã€ã§ã™ãŒã€å®Ÿéš›ã«ã¯ï¼š

1. **è¤‡é›‘ãªå¤šå±¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ **ï¼ˆç•°å¸¸å€¤æ¤œå‡ºã€æœŸé–“æ­£è¦åŒ–ã€ä¾å­˜æ€§åˆ¶å¾¡ç­‰ï¼‰
2. **18ã®ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆ**ï¼ˆèªçŸ¥å¿ƒç†å­¦ã€çµ„ç¹”ç†è«–ã€ã‚·ã‚¹ãƒ†ãƒ æ€è€ƒç­‰ï¼‰
3. **é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç­‰ï¼‰

ã‚’å«ã‚€å¤§è¦æ¨¡ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚

**ä¿å®ˆã®éš›ã®æ³¨æ„ç‚¹**:
- å˜ç´”ãªä¿®æ­£ã§ã‚‚å¤šå±¤çš„ãªå½±éŸ¿ã‚’è€ƒæ…®ã™ã‚‹
- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å¿…ãšå‹•ä½œç¢ºèªã™ã‚‹  
- ãƒ­ã‚°å‡ºåŠ›ã‚’è©³ç´°ã«ç¢ºèªã™ã‚‹
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ã‚’æ¸¬å®šã™ã‚‹

**æ¨å¥¨ã•ã‚Œã‚‹æ”¹ä¿®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**:
1. æ ¸å¿ƒæ©Ÿèƒ½ï¼ˆä¸è¶³ãƒ»éå‰°åˆ†æï¼‰ã‹ã‚‰ç†è§£ã™ã‚‹
2. 1ã¤ãšã¤ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è©³ç´°ã«èª¿æŸ»ã™ã‚‹
3. ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ååˆ†ã«æ¤œè¨¼ã™ã‚‹
4. æ®µéšçš„ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹

---

**ä½œæˆæ—¥**: 2025å¹´8æœˆ5æ—¥  
**ä½œæˆè€…**: ã‚·ã‚¹ãƒ†ãƒ åˆ†æçµæœã«åŸºã¥ãè©³ç´°æŠ€è¡“æ–‡æ›¸  
**ç”¨é€”**: å®Œå…¨ãªæŠ€è¡“çš„ç†è§£ã¨ã‚·ã‚¹ãƒ†ãƒ ä¿å®ˆã®ãŸã‚ã®åŒ…æ‹¬çš„å¼•ç¶™ãè³‡æ–™