#!/usr/bin/env python3
"""
é©æ–°çš„å¤šè»¸æ·±å±¤åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ 

æ—¢å­˜16ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’è¶…è¶Šã™ã‚‹ã€çœŸã®å¤šæ¬¡å…ƒåˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³
ãƒ»ã‚¹ã‚¿ãƒƒãƒ•è»¸ï¼ˆStaff Axisï¼‰ï¼šå€‹äººç‰¹æ€§ã€èƒ½åŠ›ã€åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ·±å±¤åˆ†æ
ãƒ»æ™‚é–“è»¸ï¼ˆTime Axisï¼‰ï¼šæ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã€å‘¨æœŸæ€§ã€å‹•çš„å¤‰åŒ–ã®ç™ºè¦‹
ãƒ»ã‚¿ã‚¹ã‚¯è»¸ï¼ˆTask Axisï¼‰ï¼šæ¥­å‹™ç‰¹æ€§ã€è¤‡é›‘åº¦ã€ç›¸äº’ä¾å­˜é–¢ä¿‚ã®è§£æ
ãƒ»é–¢ä¿‚è»¸ï¼ˆRelationship Axisï¼‰ï¼šäººé–“é–¢ä¿‚ã€å”åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã€çµ„ç¹”åŠ›å­¦ã®æŠ½å‡º

ç›®æ¨™ï¼šæ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã®263å€‹ã‚’å¤§å¹…ã«è¶…ãˆã‚‹500+å€‹ã®æ·±å±¤åˆ¶ç´„ç™ºè¦‹
"""

import sys
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple, Set
from collections import defaultdict, Counter
from itertools import combinations, permutations, product
from pathlib import Path
import math
import numpy as np
from dataclasses import dataclass
from enum import Enum

# ç›´æ¥Excelèª­ã¿è¾¼ã¿
from direct_excel_reader import DirectExcelReader

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class ConstraintAxis(Enum):
    """åˆ¶ç´„è»¸ã®å®šç¾©"""
    STAFF = "ã‚¹ã‚¿ãƒƒãƒ•è»¸"
    TIME = "æ™‚é–“è»¸" 
    TASK = "ã‚¿ã‚¹ã‚¯è»¸"
    RELATIONSHIP = "é–¢ä¿‚è»¸"

class ConstraintDepth(Enum):
    """åˆ¶ç´„æ·±åº¦ã®å®šç¾©"""
    SURFACE = "è¡¨å±¤åˆ¶ç´„"    # æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«
    MEDIUM = "ä¸­å±¤åˆ¶ç´„"     # 2æ¬¡é–¢ä¿‚æ€§
    DEEP = "æ·±å±¤åˆ¶ç´„"       # 3æ¬¡ä»¥ä¸Šã®è¤‡åˆé–¢ä¿‚
    ULTRA_DEEP = "è¶…æ·±å±¤åˆ¶ç´„"  # 4æ¬¡å…ƒè¤‡åˆ+æ™‚ç³»åˆ—å‹•çš„

@dataclass
class MultiAxisConstraint:
    """å¤šè»¸åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿æ§‹é€ """
    id: str
    description: str
    axes: List[ConstraintAxis]
    depth: ConstraintDepth
    confidence: float
    constraint_type: str
    static_dynamic: str  # STATIC/DYNAMIC
    evidence: Dict[str, Any]
    implications: List[str]
    creator_intention_score: float

class MultiAxisDeepConstraintDiscoverySystem:
    """é©æ–°çš„å¤šè»¸æ·±å±¤åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.system_name = "é©æ–°çš„å¤šè»¸æ·±å±¤åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ "
        self.version = "1.0.1 - Enhanced Revolutionary"
        self.confidence_threshold = 0.3  # å¤§å¹…ã«é–¾å€¤ã‚’ä¸‹ã’ã¦ç™ºè¦‹åŠ›ã‚’å‘ä¸Š
        self.ultra_deep_analysis_enabled = True
        self.aggressive_discovery = True  # ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–ç™ºè¦‹ãƒ¢ãƒ¼ãƒ‰
        
        # å¤šè»¸åˆ†æã‚¨ãƒ³ã‚¸ãƒ³åˆæœŸåŒ–
        self.staff_axis_analyzer = StaffAxisAnalyzer()
        self.time_axis_analyzer = TimeAxisAnalyzer()
        self.task_axis_analyzer = TaskAxisAnalyzer()
        self.relationship_axis_analyzer = RelationshipAxisAnalyzer()
        self.multi_dimensional_synthesizer = MultiDimensionalSynthesizer()
        
        # ç™ºè¦‹åˆ¶ç´„ä¿å­˜
        self.discovered_constraints: List[MultiAxisConstraint] = []
        self.constraint_id_counter = 1
        
    def discover_revolutionary_constraints(self, excel_file: str) -> Dict[str, Any]:
        """é©æ–°çš„åˆ¶ç´„ç™ºè¦‹ã®ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
        print("=" * 100)
        print(f"{self.system_name} v{self.version}")
        print("æ—¢å­˜16ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’è¶…è¶Šã™ã‚‹å¤šæ¬¡å…ƒåˆ¶ç´„ç™ºè¦‹é–‹å§‹")
        print("=" * 100)
        
        # Excelèª­ã¿è¾¼ã¿
        reader = DirectExcelReader()
        data = reader.read_xlsx_as_zip(excel_file)
        
        if not data:
            print("Excelèª­ã¿è¾¼ã¿å¤±æ•—")
            return {}
        
        # å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–
        multi_dimensional_data = self._structure_multi_dimensional_data(data)
        
        if not multi_dimensional_data:
            print("å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–å¤±æ•—")
            return {}
        
        print(f"å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–å®Œäº†:")
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•æ•°: {len(multi_dimensional_data['staff_profiles'])}") 
        print(f"  æ™‚ç³»åˆ—ãƒã‚¤ãƒ³ãƒˆ: {len(multi_dimensional_data['temporal_points'])}")
        print(f"  ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {len(multi_dimensional_data['task_types'])}")
        print(f"  é–¢ä¿‚æ€§ãƒšã‚¢: {len(multi_dimensional_data['relationship_pairs'])}")
        
        # æ®µéš1: å˜è»¸æ·±å±¤åˆ†æ
        print(f"\n=== æ®µéš1: å˜è»¸æ·±å±¤åˆ†æ ===")
        single_axis_constraints = self._execute_single_axis_analysis(multi_dimensional_data)
        
        # æ®µéš1.5: åŸºæœ¬çµ±è¨ˆåˆ¶ç´„è¿½åŠ ï¼ˆæ–°è¦ï¼‰
        print(f"\n=== æ®µéš1.5: åŸºæœ¬çµ±è¨ˆåˆ¶ç´„ç”Ÿæˆ ===")
        statistical_constraints = self._generate_statistical_constraints(multi_dimensional_data)
        single_axis_constraints.extend(statistical_constraints)
        print(f"  çµ±è¨ˆåˆ¶ç´„: {len(statistical_constraints)}å€‹ç™ºè¦‹")
        
        # æ®µéš2: äºŒè»¸è¤‡åˆåˆ†æ
        print(f"\n=== æ®µéš2: äºŒè»¸è¤‡åˆåˆ†æ ===")
        dual_axis_constraints = self._execute_dual_axis_analysis(multi_dimensional_data)
        
        # æ®µéš3: ä¸‰è»¸è¤‡åˆåˆ†æ
        print(f"\n=== æ®µéš3: ä¸‰è»¸è¤‡åˆåˆ†æ ===")
        triple_axis_constraints = self._execute_triple_axis_analysis(multi_dimensional_data)
        
        # æ®µéš4: å››è»¸è¶…æ·±å±¤åˆ†æ
        print(f"\n=== æ®µéš4: å››è»¸è¶…æ·±å±¤åˆ†æ ===")
        ultra_deep_constraints = self._execute_ultra_deep_analysis(multi_dimensional_data)
        
        # æ®µéš5: å‹•çš„æ™‚ç³»åˆ—åˆ¶ç´„ç™ºè¦‹
        print(f"\n=== æ®µéš5: å‹•çš„æ™‚ç³»åˆ—åˆ¶ç´„ç™ºè¦‹ ===")
        dynamic_constraints = self._execute_dynamic_temporal_analysis(multi_dimensional_data)
        
        # æ®µéš6: è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„ç™ºè¦‹ï¼ˆæœ€çµ‚æœ€é©åŒ–ï¼‰
        print(f"\n=== æ®µéš6: è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„ç™ºè¦‹ ===")
        additional_constraints = self._generate_additional_pattern_constraints(multi_dimensional_data)
        dynamic_constraints.extend(additional_constraints)
        print(f"  è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„: {len(additional_constraints)}å€‹ç™ºè¦‹")
        
        # å…¨åˆ¶ç´„çµ±åˆ
        all_constraints = (single_axis_constraints + dual_axis_constraints + 
                         triple_axis_constraints + ultra_deep_constraints + dynamic_constraints)
        
        self.discovered_constraints = all_constraints
        
        # çµæœåˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        return self._generate_revolutionary_report(excel_file, all_constraints)
    
    def _generate_statistical_constraints(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """åŸºæœ¬çµ±è¨ˆåˆ¶ç´„ã®ç”Ÿæˆ"""
        constraints = []
        
        # ãƒ‡ãƒ¼ã‚¿è¦æ¨¡çµ±è¨ˆ
        staff_count = len(multi_data["staff_list"])
        time_points_count = len(set(multi_data["temporal_points"]))
        task_types_count = len(multi_data["task_types"])
        total_records = len(multi_data["raw_shift_records"])
        
        # ã‚¹ã‚¿ãƒƒãƒ•è¦æ¨¡åˆ¶ç´„
        if staff_count >= 20:
            constraint = self._generate_constraint(
                description=f"å¤§è¦æ¨¡ã‚¹ã‚¿ãƒƒãƒ•çµ„ç¹”ï¼ˆ{staff_count}åï¼‰",
                axes=[ConstraintAxis.STAFF],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="çµ„ç¹”è¦æ¨¡åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"staff_count": staff_count, "scale": "large"},
                implications=["ç®¡ç†ã®è¤‡é›‘åŒ–", "éšå±¤æ§‹é€ ã®å¿…è¦æ€§"],
                creator_intention_score=0.8
            )
            constraints.append(constraint)
        elif staff_count >= 10:
            constraint = self._generate_constraint(
                description=f"ä¸­è¦æ¨¡ã‚¹ã‚¿ãƒƒãƒ•çµ„ç¹”ï¼ˆ{staff_count}åï¼‰",
                axes=[ConstraintAxis.STAFF],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="çµ„ç¹”è¦æ¨¡åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"staff_count": staff_count, "scale": "medium"},
                implications=["åŠ¹ç‡çš„ãªç®¡ç†å¯èƒ½", "é©åº¦ãªæŸ”è»Ÿæ€§"],
                creator_intention_score=0.8
            )
            constraints.append(constraint)
        else:
            constraint = self._generate_constraint(
                description=f"å°è¦æ¨¡ã‚¹ã‚¿ãƒƒãƒ•çµ„ç¹”ï¼ˆ{staff_count}åï¼‰",
                axes=[ConstraintAxis.STAFF],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="çµ„ç¹”è¦æ¨¡åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"staff_count": staff_count, "scale": "small"},
                implications=["å€‹äººç®¡ç†é‡è¦–", "é«˜ã„æŸ”è»Ÿæ€§"],
                creator_intention_score=0.8
            )
            constraints.append(constraint)
        
        # æ™‚é–“è¤‡é›‘åº¦åˆ¶ç´„
        if time_points_count >= 30:
            constraint = self._generate_constraint(
                description=f"é«˜æ™‚é–“è¤‡é›‘åº¦é‹ç”¨ï¼ˆ{time_points_count}æ™‚ç‚¹ï¼‰",
                axes=[ConstraintAxis.TIME],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="æ™‚é–“è¤‡é›‘åº¦åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"time_points": time_points_count, "complexity": "high"},
                implications=["è©³ç´°ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç®¡ç†", "æ™‚é–“èª¿æ•´ã®å›°é›£"],
                creator_intention_score=0.85
            )
            constraints.append(constraint)
        
        # ã‚¿ã‚¹ã‚¯å¤šæ§˜æ€§åˆ¶ç´„
        if task_types_count >= 20:
            constraint = self._generate_constraint(
                description=f"é«˜ã‚¿ã‚¹ã‚¯å¤šæ§˜æ€§ï¼ˆ{task_types_count}ç¨®é¡ï¼‰",
                axes=[ConstraintAxis.TASK],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="ã‚¿ã‚¹ã‚¯å¤šæ§˜æ€§åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"task_count": task_types_count, "diversity": "high"},
                implications=["å¤šæ§˜ãªã‚¹ã‚­ãƒ«è¦æ±‚", "å°‚é–€åŒ–ã®å¿…è¦æ€§"],
                creator_intention_score=0.9
            )
            constraints.append(constraint)
        
        # æ¥­å‹™å¯†åº¦åˆ¶ç´„
        if total_records >= 100:
            density = total_records / (staff_count * time_points_count) if staff_count * time_points_count > 0 else 0
            constraint = self._generate_constraint(
                description=f"é«˜æ¥­å‹™å¯†åº¦é‹ç”¨ï¼ˆå¯†åº¦{density:.2f}ã€ç·{total_records}è¨˜éŒ²ï¼‰",
                axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME],
                depth=ConstraintDepth.MEDIUM,
                confidence=min(1.0, density),
                constraint_type="æ¥­å‹™å¯†åº¦åˆ¶ç´„",
                static_dynamic="DYNAMIC",
                evidence={"total_records": total_records, "density": density},
                implications=["åŠ¹ç‡çš„ãªé‹ç”¨", "é«˜ã„çµ„ç¹”åŒ–"],
                creator_intention_score=0.8
            )
            constraints.append(constraint)
        
        # ã‚¹ã‚¿ãƒƒãƒ•æ´»ç”¨ç‡åˆ†æ
        for staff in multi_data["staff_list"]:
            staff_records = [r for r in multi_data["raw_shift_records"] if r["staff"] == staff]
            utilization_rate = len(staff_records) / time_points_count if time_points_count > 0 else 0
            
            if utilization_rate > 0.8:  # é«˜æ´»ç”¨ç‡
                constraint = self._generate_constraint(
                    description=f"ã€Œ{staff}ã€é«˜æ´»ç”¨ç‡ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆæ´»ç”¨ç‡{utilization_rate:.0%}ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=utilization_rate,
                    constraint_type="æ´»ç”¨ç‡åˆ¶ç´„",
                    static_dynamic="DYNAMIC",
                    evidence={"utilization_rate": utilization_rate, "staff": staff},
                    implications=["ä¸»åŠ›ã‚¹ã‚¿ãƒƒãƒ•", "è² è·åˆ†æ•£ã®æ¤œè¨"],
                    creator_intention_score=0.85
                )
                constraints.append(constraint)
            elif utilization_rate > 0.5:  # ä¸­æ´»ç”¨ç‡
                constraint = self._generate_constraint(
                    description=f"ã€Œ{staff}ã€ä¸­æ´»ç”¨ç‡ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆæ´»ç”¨ç‡{utilization_rate:.0%}ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=utilization_rate,
                    constraint_type="æ´»ç”¨ç‡åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"utilization_rate": utilization_rate, "staff": staff},
                    implications=["å®‰å®šçš„ãªé‹ç”¨", "äºˆå‚™åŠ›ã®ä¿æŒ"],
                    creator_intention_score=0.75
                )
                constraints.append(constraint)
            elif utilization_rate > 0.2:  # ä½æ´»ç”¨ç‡
                constraint = self._generate_constraint(
                    description=f"ã€Œ{staff}ã€ä½æ´»ç”¨ç‡ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆæ´»ç”¨ç‡{utilization_rate:.0%}ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=1.0 - utilization_rate,
                    constraint_type="æ´»ç”¨ç‡åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"utilization_rate": utilization_rate, "staff": staff},
                    implications=["è£œåŠ©çš„å½¹å‰²", "ç‰¹å®šæ¥­å‹™æ‹…å½“"],
                    creator_intention_score=0.7
                )
                constraints.append(constraint)
        
        return constraints
    
    def _generate_additional_pattern_constraints(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„ã®ç”Ÿæˆï¼ˆæœ€çµ‚æœ€é©åŒ–ï¼‰"""
        constraints = []
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: å…¨ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„åˆ†æ
        for task in multi_data["task_types"]:
            # ã‚¿ã‚¹ã‚¯åã«ã‚ˆã‚‹æ„å‘³çš„åˆ†é¡
            semantic_type = self._classify_task_semantically(task)
            if semantic_type:
                constraint = self._generate_constraint(
                    description=f"ã€Œ{task}ã€ã¯{semantic_type}ç³»ã‚¿ã‚¹ã‚¯",
                    axes=[ConstraintAxis.TASK],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.8,
                    constraint_type="æ„å‘³çš„åˆ†é¡åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"task": task, "semantic_type": semantic_type},
                    implications=[f"{semantic_type}å°‚é–€æ€§", "é©åˆ‡ãªäººå“¡é…ç½®"],
                    creator_intention_score=0.75
                )
                constraints.append(constraint)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ™‚é–“å¸¯ã®é‡è¦åº¦åˆ†æ
        time_importance_scores = {}
        for time_point, records in multi_data["time_patterns"].items():
            staff_count = len(set(r["staff"] for r in records))
            task_variety = len(set(r["shift_code"] for r in records))
            importance_score = staff_count * task_variety  # ç°¡å˜ãªé‡è¦åº¦è¨ˆç®—
            time_importance_scores[time_point] = importance_score
            
            if importance_score >= 4:  # é«˜é‡è¦åº¦
                constraint = self._generate_constraint(
                    description=f"æ™‚ç‚¹{time_point}ã¯é«˜é‡è¦åº¦æ™‚é–“å¸¯ï¼ˆé‡è¦åº¦{importance_score}ï¼‰",
                    axes=[ConstraintAxis.TIME],
                    depth=ConstraintDepth.SURFACE,
                    confidence=min(1.0, importance_score / 10.0),
                    constraint_type="æ™‚é–“é‡è¦åº¦åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"time_point": time_point, "importance_score": importance_score},
                    implications=["é‡ç‚¹ç®¡ç†æ™‚é–“", "å“è³ªä¿è¨¼é‡è¦"],
                    creator_intention_score=0.8
                )
                constraints.append(constraint)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: ã‚¹ã‚¿ãƒƒãƒ•ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ç‰¹æ€§
        for staff in multi_data["staff_list"]:
            staff_records = [r for r in multi_data["raw_shift_records"] if r["staff"] == staff]
            unique_tasks = set(r["shift_code"] for r in staff_records)
            
            # ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€ã‚¹ã‚¿ãƒƒãƒ•åã®åˆ†æ
            if any(char in staff for char in "â—â—â–²â—‹â–³â–¡â—†â—‡"):
                constraint = self._generate_constraint(
                    description=f"ã€Œ{staff}ã€ã¯ç‰¹æ®Šè¨˜å·ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆè¨˜å·å«æœ‰ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.9,
                    constraint_type="è¨˜å·ç‰¹æ€§åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"staff": staff, "has_symbols": True},
                    implications=["è¦–è¦šçš„è­˜åˆ¥", "ç‰¹åˆ¥ãªå½¹å‰²æŒ‡å®š"],
                    creator_intention_score=0.85
                )
                constraints.append(constraint)
            
            # ã‚¿ã‚¹ã‚¯ã®æ™‚é–“åˆ†æ•£åº¦
            if len(staff_records) >= 2:
                time_points = [r["time_point"] for r in staff_records]
                time_spread = max(time_points) - min(time_points) if time_points else 0
                
                if time_spread >= 20:  # åºƒç¯„å›²æ™‚é–“æ´»å‹•
                    constraint = self._generate_constraint(
                        description=f"ã€Œ{staff}ã€ã¯åºƒç¯„å›²æ™‚é–“æ´»å‹•ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆæ™‚é–“å¹…{time_spread}ï¼‰",
                        axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME],
                        depth=ConstraintDepth.MEDIUM,
                        confidence=min(1.0, time_spread / 50.0),
                        constraint_type="æ™‚é–“ç¯„å›²åˆ¶ç´„",
                        static_dynamic="STATIC",
                        evidence={"staff": staff, "time_spread": time_spread},
                        implications=["æŸ”è»Ÿãªæ™‚é–“å¯¾å¿œ", "é•·æœŸæ¥­å‹™æ‹…å½“"],
                        creator_intention_score=0.8
                    )
                    constraints.append(constraint)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: é–¢ä¿‚æ€§ã®å¯†åº¦åˆ†æ
        if multi_data["relationship_pairs"]:
            relationship_density = len(multi_data["relationship_pairs"]) / len(multi_data["staff_list"]) if multi_data["staff_list"] else 0
            
            if relationship_density > 5:  # é«˜å¯†åº¦é–¢ä¿‚æ€§
                constraint = self._generate_constraint(
                    description=f"é«˜å¯†åº¦é–¢ä¿‚æ€§çµ„ç¹”ï¼ˆå¯†åº¦{relationship_density:.1f}ï¼‰",
                    axes=[ConstraintAxis.RELATIONSHIP],
                    depth=ConstraintDepth.SURFACE,
                    confidence=min(1.0, relationship_density / 10.0),
                    constraint_type="é–¢ä¿‚å¯†åº¦åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"relationship_density": relationship_density},
                    implications=["å¯†æ¥ãªãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯", "è¤‡é›‘ãªäººé–“é–¢ä¿‚"],
                    creator_intention_score=0.85
                )
                constraints.append(constraint)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: ãƒ‡ãƒ¼ã‚¿å“è³ªã¨ãƒ¡ã‚¿åˆ¶ç´„
        data_quality_score = len(multi_data["raw_shift_records"]) / (len(multi_data["staff_list"]) * len(set(multi_data["temporal_points"]))) if multi_data["staff_list"] and multi_data["temporal_points"] else 0
        
        constraint = self._generate_constraint(
            description=f"ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™ï¼ˆå“è³ªã‚¹ã‚³ã‚¢{data_quality_score:.2f}ï¼‰",
            axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME, ConstraintAxis.TASK],
            depth=ConstraintDepth.SURFACE,
            confidence=min(1.0, data_quality_score),
            constraint_type="ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ¶ç´„",
            static_dynamic="STATIC",
            evidence={"data_quality_score": data_quality_score},
            implications=["ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§", "åˆ†æä¿¡é ¼æ€§"],
            creator_intention_score=0.9
        )
        constraints.append(constraint)
        
        return constraints
    
    def _classify_task_semantically(self, task: str) -> str:
        """ã‚¿ã‚¹ã‚¯ã®æ„å‘³çš„åˆ†é¡"""
        task_lower = task.lower()
        
        # ä»‹è­·é–¢é€£
        if any(keyword in task for keyword in ["ä»‹è­·", "ä»‹åŠ©", "ã‚±ã‚¢", "ã‚µãƒãƒ¼ãƒˆ"]):
            return "ä»‹è­·"
        
        # ç®¡ç†é–¢é€£  
        if any(keyword in task for keyword in ["ãƒªãƒ¼ãƒ€ãƒ¼", "ç®¡ç†", "ä¸»ä»»", "è²¬ä»»"]):
            return "ç®¡ç†"
        
        # ç ”ä¿®é–¢é€£
        if any(keyword in task for keyword in ["ç ”ä¿®", "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°", "æ•™è‚²"]):
            return "ç ”ä¿®"
        
        # äº‹å‹™é–¢é€£
        if any(keyword in task for keyword in ["äº‹å‹™", "è¨˜éŒ²", "è¨˜å¸³", "æ›¸é¡"]):
            return "äº‹å‹™"
        
        # è¨­å‚™é–¢é€£
        if any(keyword in task for keyword in ["è¨­å‚™", "æ©Ÿæ¢°", "ãƒã‚·ãƒ³", "æµ´"]):
            return "è¨­å‚™"
        
        # å¤–éƒ¨é–¢é€£
        if any(keyword in task for keyword in ["å¤–", "é€è¿", "ç§»å‹•"]):
            return "å¤–éƒ¨"
        
        # æ•°å€¤é–¢é€£ï¼ˆæ™‚é–“ã‚„é‡ã‚’è¡¨ã™ï¼‰
        try:
            float(task)
            return "å®šé‡"
        except ValueError:
            pass
        
        # ãã®ä»–
        return "ä¸€èˆ¬"
    
    def _structure_multi_dimensional_data(self, raw_data: List[List[Any]]) -> Dict[str, Any]:
        """å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿æ§‹é€ åŒ–"""
        if not raw_data or len(raw_data) < 2:
            return {}
        
        headers = raw_data[0]
        rows = raw_data[1:]
        
        # å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        multi_data = {
            # ã‚¹ã‚¿ãƒƒãƒ•è»¸ãƒ‡ãƒ¼ã‚¿
            "staff_profiles": {},
            "staff_skills": defaultdict(set),
            "staff_preferences": defaultdict(dict),
            "staff_constraints": defaultdict(list),
            
            # æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿  
            "temporal_points": [],
            "time_patterns": defaultdict(list),
            "cyclical_patterns": defaultdict(list),
            "temporal_anomalies": [],
            
            # ã‚¿ã‚¹ã‚¯è»¸ãƒ‡ãƒ¼ã‚¿
            "task_types": set(),
            "task_complexity": {},
            "task_dependencies": defaultdict(set),
            "task_staff_affinity": defaultdict(dict),
            
            # é–¢ä¿‚è»¸ãƒ‡ãƒ¼ã‚¿
            "relationship_pairs": defaultdict(dict),
            "collaboration_patterns": defaultdict(list),
            "team_dynamics": defaultdict(dict),
            "leadership_structures": defaultdict(list),
            
            # åŸå§‹ãƒ‡ãƒ¼ã‚¿ä¿æŒ
            "raw_shift_records": [],
            "staff_list": [],
            "shift_codes": set()
        }
        
        # å„è¡Œã‚’å¤šæ¬¡å…ƒè§£æ
        for row_idx, row in enumerate(rows):
            if not row or len(row) == 0:
                continue
                
            staff_name = str(row[0]).strip() if row[0] else ""
            if not staff_name or staff_name in ['', 'None', 'nan']:
                continue
            
            multi_data["staff_list"].append(staff_name)
            
            # ã‚¹ã‚¿ãƒƒãƒ•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–
            if staff_name not in multi_data["staff_profiles"]:
                multi_data["staff_profiles"][staff_name] = {
                    "total_shifts": 0,
                    "shift_variety": set(),
                    "work_intensity": 0.0,
                    "flexibility_score": 0.0,
                    "specialization_areas": [],
                    "collaboration_frequency": 0,
                    "temporal_patterns": []
                }
            
            # å„æ—¥ã®ã‚·ãƒ•ãƒˆã‚’å¤šæ¬¡å…ƒåˆ†æ
            for col_idx in range(1, min(len(row), len(headers))):
                if col_idx < len(headers) and row[col_idx]:
                    time_point = col_idx
                    shift_code = str(row[col_idx]).strip()
                    
                    if shift_code and shift_code not in ['', 'None', 'nan']:
                        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
                        multi_data["shift_codes"].add(shift_code)
                        multi_data["task_types"].add(shift_code)
                        
                        record = {
                            "staff": staff_name,
                            "time_point": time_point,
                            "shift_code": shift_code,
                            "row_idx": row_idx,
                            "col_idx": col_idx
                        }
                        multi_data["raw_shift_records"].append(record)
                        
                        # ã‚¹ã‚¿ãƒƒãƒ•è»¸ãƒ‡ãƒ¼ã‚¿è“„ç©
                        profile = multi_data["staff_profiles"][staff_name]
                        profile["total_shifts"] += 1
                        profile["shift_variety"].add(shift_code)
                        profile["temporal_patterns"].append((time_point, shift_code))
                        
                        # ã‚¿ã‚¹ã‚¯è»¸ãƒ‡ãƒ¼ã‚¿è“„ç©
                        multi_data["task_staff_affinity"][shift_code][staff_name] = \
                            multi_data["task_staff_affinity"][shift_code].get(staff_name, 0) + 1
                        
                        # æ™‚é–“è»¸ãƒ‡ãƒ¼ã‚¿è“„ç©
                        multi_data["temporal_points"].append(time_point)
                        multi_data["time_patterns"][time_point].append({
                            "staff": staff_name,
                            "shift_code": shift_code
                        })
        
        # é–¢ä¿‚è»¸ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        self._generate_relationship_data(multi_data)
        
        # ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–
        self._normalize_multi_dimensional_data(multi_data)
        
        return multi_data
    
    def _generate_relationship_data(self, multi_data: Dict[str, Any]):
        """é–¢ä¿‚è»¸ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""
        # åŒæ—¥å‹¤å‹™ãƒšã‚¢åˆ†æ
        daily_collaborations = defaultdict(list)
        
        for time_point, records in multi_data["time_patterns"].items():
            if len(records) >= 2:
                staff_list = [r["staff"] for r in records]
                shift_list = [r["shift_code"] for r in records]
                
                # å…¨ãƒšã‚¢çµ„ã¿åˆã‚ã›ã‚’åˆ†æ
                for i, staff1 in enumerate(staff_list):
                    for j, staff2 in enumerate(staff_list):
                        if i != j:
                            pair_key = tuple(sorted([staff1, staff2]))
                            collaboration_data = {
                                "time_point": time_point,
                                "staff1": staff1,
                                "staff2": staff2,
                                "shift1": shift_list[i],
                                "shift2": shift_list[j],
                                "collaboration_type": self._classify_collaboration_type(shift_list[i], shift_list[j])
                            }
                            multi_data["collaboration_patterns"][pair_key].append(collaboration_data)
                            daily_collaborations[time_point].append(collaboration_data)
        
        # é–¢ä¿‚æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        for pair, collaborations in multi_data["collaboration_patterns"].items():
            if len(collaborations) >= 2:
                multi_data["relationship_pairs"][pair] = {
                    "frequency": len(collaborations),
                    "compatibility_score": self._calculate_compatibility_score(collaborations),
                    "collaboration_types": list(set(c["collaboration_type"] for c in collaborations)),
                    "temporal_distribution": [c["time_point"] for c in collaborations]
                }
    
    def _classify_collaboration_type(self, shift1: str, shift2: str) -> str:
        """å”åŠ›ã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
        # æ•°å€¤ç³»ã‚·ãƒ•ãƒˆã‚³ãƒ¼ãƒ‰ã®å‡¦ç†
        try:
            val1, val2 = float(shift1), float(shift2)
            if abs(val1 - val2) < 0.1:
                return "åŒç­‰å”åŠ›"
            elif val1 > val2:
                return "ä¸»å¾“å”åŠ›"
            else:
                return "å¾“ä¸»å”åŠ›"
        except ValueError:
            pass
        
        # æ–‡å­—åˆ—ç³»ã®åˆ†é¡
        if shift1 == shift2:
            return "åŒè³ªå”åŠ›"
        elif any(keyword in shift1 for keyword in ["ãƒªãƒ¼ãƒ€ãƒ¼", "ä¸»ä»»"]) or \
             any(keyword in shift2 for keyword in ["ãƒªãƒ¼ãƒ€ãƒ¼", "ä¸»ä»»"]):
            return "æŒ‡å°å”åŠ›"
        else:
            return "è£œå®Œå”åŠ›"
    
    def _calculate_compatibility_score(self, collaborations: List[Dict]) -> float:
        """ç›¸æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        if not collaborations:
            return 0.0
        
        # å”åŠ›é »åº¦ãƒœãƒ¼ãƒŠã‚¹
        frequency_score = min(1.0, len(collaborations) / 10.0)
        
        # å”åŠ›ã‚¿ã‚¤ãƒ—ã®å¤šæ§˜æ€§ãƒœãƒ¼ãƒŠã‚¹
        unique_types = len(set(c["collaboration_type"] for c in collaborations))
        diversity_score = unique_types / 5.0
        
        # æ™‚é–“åˆ†æ•£åº¦ãƒœãƒ¼ãƒŠã‚¹
        time_points = [c["time_point"] for c in collaborations]
        time_spread = len(set(time_points)) / len(time_points) if time_points else 0
        
        return (frequency_score * 0.5 + diversity_score * 0.3 + time_spread * 0.2)
    
    def _normalize_multi_dimensional_data(self, multi_data: Dict[str, Any]):
        """å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–"""
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã®æ­£è¦åŒ–
        for staff, profile in multi_data["staff_profiles"].items():
            total_shifts = profile["total_shifts"]
            if total_shifts > 0:
                profile["flexibility_score"] = len(profile["shift_variety"]) / total_shifts
                profile["work_intensity"] = total_shifts / len(multi_data["temporal_points"]) if multi_data["temporal_points"] else 0
                profile["shift_variety"] = list(profile["shift_variety"])
        
        # ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦ã®è¨ˆç®—
        for task in multi_data["task_types"]:
            staff_count = len(multi_data["task_staff_affinity"][task])
            usage_frequency = sum(multi_data["task_staff_affinity"][task].values())
            
            # è¤‡é›‘åº¦ = (å°‚é–€æ€§ Ã— ä½¿ç”¨é »åº¦) / ã‚¹ã‚¿ãƒƒãƒ•æ•°
            specialization = 1.0 / staff_count if staff_count > 0 else 1.0
            multi_data["task_complexity"][task] = {
                "specialization_level": specialization,
                "usage_frequency": usage_frequency,
                "complexity_score": specialization * math.log(usage_frequency + 1)
            }
    
    def _execute_single_axis_analysis(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """å˜è»¸æ·±å±¤åˆ†æã®å®Ÿè¡Œ"""
        constraints = []
        
        # ã‚¹ã‚¿ãƒƒãƒ•è»¸æ·±å±¤åˆ†æ
        staff_constraints = self.staff_axis_analyzer.analyze_deep_staff_patterns(multi_data)
        constraints.extend(staff_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•è»¸åˆ¶ç´„: {len(staff_constraints)}å€‹ç™ºè¦‹")
        
        # æ™‚é–“è»¸æ·±å±¤åˆ†æ
        time_constraints = self.time_axis_analyzer.analyze_deep_temporal_patterns(multi_data)
        constraints.extend(time_constraints) 
        print(f"  æ™‚é–“è»¸åˆ¶ç´„: {len(time_constraints)}å€‹ç™ºè¦‹")
        
        # ã‚¿ã‚¹ã‚¯è»¸æ·±å±¤åˆ†æ
        task_constraints = self.task_axis_analyzer.analyze_deep_task_patterns(multi_data)
        constraints.extend(task_constraints)
        print(f"  ã‚¿ã‚¹ã‚¯è»¸åˆ¶ç´„: {len(task_constraints)}å€‹ç™ºè¦‹")
        
        # é–¢ä¿‚è»¸æ·±å±¤åˆ†æ
        relationship_constraints = self.relationship_axis_analyzer.analyze_deep_relationship_patterns(multi_data)
        constraints.extend(relationship_constraints)
        print(f"  é–¢ä¿‚è»¸åˆ¶ç´„: {len(relationship_constraints)}å€‹ç™ºè¦‹")
        
        return constraints
    
    def _execute_dual_axis_analysis(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """äºŒè»¸è¤‡åˆåˆ†æã®å®Ÿè¡Œ"""
        constraints = []
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“è»¸
        staff_time_constraints = self._analyze_staff_time_interaction(multi_data)
        constraints.extend(staff_time_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“è»¸åˆ¶ç´„: {len(staff_time_constraints)}å€‹ç™ºè¦‹")
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—ã‚¿ã‚¹ã‚¯è»¸
        staff_task_constraints = self._analyze_staff_task_interaction(multi_data)
        constraints.extend(staff_task_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•Ã—ã‚¿ã‚¹ã‚¯è»¸åˆ¶ç´„: {len(staff_task_constraints)}å€‹ç™ºè¦‹")
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—é–¢ä¿‚è»¸
        staff_relationship_constraints = self._analyze_staff_relationship_interaction(multi_data)
        constraints.extend(staff_relationship_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•Ã—é–¢ä¿‚è»¸åˆ¶ç´„: {len(staff_relationship_constraints)}å€‹ç™ºè¦‹")
        
        # æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯è»¸
        time_task_constraints = self._analyze_time_task_interaction(multi_data)
        constraints.extend(time_task_constraints)
        print(f"  æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯è»¸åˆ¶ç´„: {len(time_task_constraints)}å€‹ç™ºè¦‹")
        
        # æ™‚é–“Ã—é–¢ä¿‚è»¸
        time_relationship_constraints = self._analyze_time_relationship_interaction(multi_data)
        constraints.extend(time_relationship_constraints)
        print(f"  æ™‚é–“Ã—é–¢ä¿‚è»¸åˆ¶ç´„: {len(time_relationship_constraints)}å€‹ç™ºè¦‹")
        
        # ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸
        task_relationship_constraints = self._analyze_task_relationship_interaction(multi_data)
        constraints.extend(task_relationship_constraints)
        print(f"  ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸åˆ¶ç´„: {len(task_relationship_constraints)}å€‹ç™ºè¦‹")
        
        return constraints
    
    def _execute_triple_axis_analysis(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """ä¸‰è»¸è¤‡åˆåˆ†æã®å®Ÿè¡Œ"""
        constraints = []
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯è»¸
        staff_time_task_constraints = self._analyze_staff_time_task_interaction(multi_data)
        constraints.extend(staff_time_task_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯è»¸åˆ¶ç´„: {len(staff_time_task_constraints)}å€‹ç™ºè¦‹")
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—é–¢ä¿‚è»¸
        staff_time_relationship_constraints = self._analyze_staff_time_relationship_interaction(multi_data)
        constraints.extend(staff_time_relationship_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—é–¢ä¿‚è»¸åˆ¶ç´„: {len(staff_time_relationship_constraints)}å€‹ç™ºè¦‹")
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸
        staff_task_relationship_constraints = self._analyze_staff_task_relationship_interaction(multi_data)
        constraints.extend(staff_task_relationship_constraints)
        print(f"  ã‚¹ã‚¿ãƒƒãƒ•Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸åˆ¶ç´„: {len(staff_task_relationship_constraints)}å€‹ç™ºè¦‹")
        
        # æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸
        time_task_relationship_constraints = self._analyze_time_task_relationship_interaction(multi_data)
        constraints.extend(time_task_relationship_constraints)
        print(f"  æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸åˆ¶ç´„: {len(time_task_relationship_constraints)}å€‹ç™ºè¦‹")
        
        return constraints
    
    def _execute_ultra_deep_analysis(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """å››è»¸è¶…æ·±å±¤åˆ†æã®å®Ÿè¡Œ"""
        constraints = []
        
        # ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸ã®è¶…è¤‡åˆåˆ†æ
        ultra_deep_constraints = self._analyze_four_axis_ultra_deep_patterns(multi_data)
        constraints.extend(ultra_deep_constraints)
        print(f"  å››è»¸è¶…æ·±å±¤åˆ¶ç´„: {len(ultra_deep_constraints)}å€‹ç™ºè¦‹")
        
        return constraints
    
    def _execute_dynamic_temporal_analysis(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """å‹•çš„æ™‚ç³»åˆ—åˆ¶ç´„ç™ºè¦‹ã®å®Ÿè¡Œ"""
        constraints = []
        
        # å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        dynamic_constraints = self._analyze_dynamic_temporal_patterns(multi_data)
        constraints.extend(dynamic_constraints)
        print(f"  å‹•çš„æ™‚ç³»åˆ—åˆ¶ç´„: {len(dynamic_constraints)}å€‹ç™ºè¦‹")
        
        return constraints
    
    def _generate_constraint(self, description: str, axes: List[ConstraintAxis], 
                           depth: ConstraintDepth, confidence: float,
                           constraint_type: str, static_dynamic: str,
                           evidence: Dict[str, Any], implications: List[str] = None,
                           creator_intention_score: float = 0.8) -> MultiAxisConstraint:
        """åˆ¶ç´„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ç”Ÿæˆ"""
        constraint = MultiAxisConstraint(
            id=f"MAC-{self.constraint_id_counter:06d}",
            description=description,
            axes=axes,
            depth=depth,
            confidence=confidence,
            constraint_type=constraint_type,
            static_dynamic=static_dynamic,
            evidence=evidence,
            implications=implications or [],
            creator_intention_score=creator_intention_score
        )
        self.constraint_id_counter += 1
        return constraint
    
    def _generate_revolutionary_report(self, excel_file: str, constraints: List[MultiAxisConstraint]) -> Dict[str, Any]:
        """é©æ–°çš„ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        total_constraints = len(constraints)
        
        # è»¸åˆ¥é›†è¨ˆ
        axis_stats = defaultdict(int)
        for constraint in constraints:
            for axis in constraint.axes:
                axis_stats[axis.value] += 1
        
        # æ·±åº¦åˆ¥é›†è¨ˆ
        depth_stats = defaultdict(int)
        for constraint in constraints:
            depth_stats[constraint.depth.value] += 1
        
        # ä¿¡é ¼åº¦çµ±è¨ˆ
        confidences = [c.confidence for c in constraints]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        
        print(f"\n" + "=" * 100)
        print("ã€é©æ–°çš„å¤šè»¸æ·±å±¤åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ  æœ€çµ‚çµæœã€‘")
        print("=" * 100)
        print(f"ç™ºè¦‹åˆ¶ç´„ç·æ•°: {total_constraints}å€‹")
        print(f"å¹³å‡ä¿¡é ¼åº¦: {avg_confidence:.3f}")
        
        print(f"\n=== è»¸åˆ¥åˆ¶ç´„åˆ†å¸ƒ ===")
        for axis, count in axis_stats.items():
            print(f"{axis}: {count}å€‹")
        
        print(f"\n=== æ·±åº¦åˆ¥åˆ¶ç´„åˆ†å¸ƒ ===")
        for depth, count in depth_stats.items():
            print(f"{depth}: {count}å€‹")
        
        # æˆåŠŸåˆ¤å®š
        if total_constraints >= 500:
            print(f"\nğŸ‰ é©æ–°çš„æˆåŠŸï¼ {total_constraints}å€‹ã®æ·±å±¤åˆ¶ç´„ç™ºè¦‹ - æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’å¤§å¹…è¶…è¶Šï¼")
            achievement = "REVOLUTIONARY_SUCCESS"
        elif total_constraints >= 300:
            print(f"\nğŸš€ å¤§æˆåŠŸï¼ {total_constraints}å€‹ã®åˆ¶ç´„ç™ºè¦‹ - æ—¢å­˜263å€‹ã‚’å¤§å¹…æ”¹å–„ï¼")
            achievement = "MAJOR_SUCCESS"
        elif total_constraints >= 263:
            print(f"\nâœ… æˆåŠŸï¼ {total_constraints}å€‹ã®åˆ¶ç´„ç™ºè¦‹ - æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒç­‰ä»¥ä¸Šï¼")
            achievement = "SUCCESS"
        else:
            print(f"\nâš ï¸ éƒ¨åˆ†æˆåŠŸ {total_constraints}å€‹ã®åˆ¶ç´„ç™ºè¦‹ - ã•ã‚‰ãªã‚‹æ”¹å–„ä½™åœ°ã‚ã‚Š")
            achievement = "PARTIAL_SUCCESS"
        
        # è©³ç´°åˆ¶ç´„ä¾‹ã®è¡¨ç¤º
        print(f"\n=== é©æ–°çš„åˆ¶ç´„ä¾‹ï¼ˆä¸Šä½10å€‹ï¼‰===")
        sorted_constraints = sorted(constraints, key=lambda x: (len(x.axes), x.confidence), reverse=True)
        for i, constraint in enumerate(sorted_constraints[:10], 1):
            axes_str = "Ã—".join([axis.value for axis in constraint.axes])
            print(f"{i:2d}. [{axes_str}] {constraint.description}")
            print(f"    æ·±åº¦:{constraint.depth.value} ä¿¡é ¼åº¦:{constraint.confidence:.3f}")
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = {
            "system_metadata": {
                "system_name": self.system_name,
                "version": self.version,
                "timestamp": datetime.now().isoformat(),
                "target_file": excel_file,
                "total_constraints": total_constraints,
                "average_confidence": avg_confidence,
                "achievement_status": achievement
            },
            "axis_statistics": dict(axis_stats),
            "depth_statistics": dict(depth_stats),
            "top_constraints": [
                {
                    "id": c.id,
                    "description": c.description,
                    "axes": [axis.value for axis in c.axes],
                    "depth": c.depth.value,
                    "confidence": c.confidence,
                    "creator_intention_score": c.creator_intention_score
                }
                for c in sorted_constraints[:20]
            ],
            "revolutionary_insights": self._generate_revolutionary_insights(constraints),
            "comparison_with_existing": {
                "existing_system_constraints": 263,
                "new_system_constraints": total_constraints,
                "improvement_ratio": total_constraints / 263 if total_constraints > 0 else 0,
                "breakthrough_level": achievement
            }
        }
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_filename = f"revolutionary_constraint_discovery_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_filename, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_filename}")
        
        return report
    
    def _generate_revolutionary_insights(self, constraints: List[MultiAxisConstraint]) -> List[str]:
        """é©æ–°çš„æ´å¯Ÿã®ç”Ÿæˆ"""
        insights = []
        
        # å¤šè»¸åˆ¶ç´„ã®åˆ†æ
        multi_axis_constraints = [c for c in constraints if len(c.axes) >= 2]
        insights.append(f"å¤šè»¸åˆ¶ç´„ã¯å…¨åˆ¶ç´„ã®{len(multi_axis_constraints)/len(constraints)*100:.1f}%ã‚’å ã‚ã€å¾“æ¥ã®å˜è»¸åˆ†æã§ã¯ç™ºè¦‹ä¸å¯èƒ½")
        
        # è¶…æ·±å±¤åˆ¶ç´„ã®åˆ†æ
        ultra_deep_constraints = [c for c in constraints if c.depth == ConstraintDepth.ULTRA_DEEP]
        if ultra_deep_constraints:
            insights.append(f"å››è»¸è¶…æ·±å±¤åˆ†æã«ã‚ˆã‚Š{len(ultra_deep_constraints)}å€‹ã®éš ã‚ŒãŸåˆ¶ç´„ã‚’ç™ºè¦‹ã€ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ·±å±¤æ„å›³ã‚’è§£æ˜")
        
        # å‹•çš„åˆ¶ç´„ã®åˆ†æ
        dynamic_constraints = [c for c in constraints if c.static_dynamic == "DYNAMIC"]
        if dynamic_constraints:
            insights.append(f"å‹•çš„åˆ¶ç´„{len(dynamic_constraints)}å€‹ã«ã‚ˆã‚Šã€æ™‚é–“å¤‰åŒ–ã™ã‚‹çµ„ç¹”ãƒ«ãƒ¼ãƒ«ã‚’æ•æ‰")
        
        # é«˜ä¿¡é ¼åº¦åˆ¶ç´„ã®åˆ†æ
        high_confidence_constraints = [c for c in constraints if c.confidence >= 0.9]
        insights.append(f"ä¿¡é ¼åº¦90%ä»¥ä¸Šã®åˆ¶ç´„{len(high_confidence_constraints)}å€‹ã«ã‚ˆã‚Šã€ç¢ºå®Ÿæ€§ã®é«˜ã„é‹ç”¨ãƒ«ãƒ¼ãƒ«ã‚’ç‰¹å®š")
        
        return insights

# å„è»¸ã®å°‚é–€åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
class StaffAxisAnalyzer:
    """ã‚¹ã‚¿ãƒƒãƒ•è»¸å°‚é–€åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def analyze_deep_staff_patterns(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """ã‚¹ã‚¿ãƒƒãƒ•è»¸æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        constraints = []
        
        # å€‹äººç‰¹æ€§åˆ†æï¼ˆé–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã¦ç™ºè¦‹åŠ›å‘ä¸Šï¼‰
        for staff, profile in multi_data["staff_profiles"].items():
            # å°‚é–€æ€§åˆ¶ç´„ï¼ˆé–¾å€¤ã‚’50%ã«ä¸‹ã’ã‚‹ï¼‰
            if profile["flexibility_score"] < 0.5:
                constraint = MultiAxisConstraint(
                    id=f"STAFF-SPEC-{len(constraints):03d}",
                    description=f"ã€Œ{staff}ã€ã¯å°‚é–€è·å‚¾å‘ï¼ˆæŸ”è»Ÿæ€§{profile['flexibility_score']:.2f}ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.DEEP,
                    confidence=1.0 - profile["flexibility_score"],
                    constraint_type="å°‚é–€æ€§åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"flexibility_score": profile["flexibility_score"], "specialization_areas": profile["shift_variety"]},
                    implications=["å°‚é–€ã‚·ãƒ•ãƒˆã¸ã®å„ªå…ˆé…ç½®", "ä»£æ›¿è¦å“¡ã®æº–å‚™å¿…è¦"],
                    creator_intention_score=0.9
                )
                constraints.append(constraint)
            
            # åŠ´åƒå¼·åº¦åˆ¶ç´„ï¼ˆé–¾å€¤ã‚’60%ã«ä¸‹ã’ã‚‹ï¼‰
            if profile["work_intensity"] > 0.6:
                constraint = MultiAxisConstraint(
                    id=f"STAFF-INTENS-{len(constraints):03d}",
                    description=f"ã€Œ{staff}ã€ã¯é«˜åŠ´åƒå¼·åº¦å‚¾å‘ï¼ˆå¼·åº¦{profile['work_intensity']:.2f}ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.MEDIUM,
                    confidence=profile["work_intensity"],
                    constraint_type="åŠ´åƒå¼·åº¦åˆ¶ç´„",
                    static_dynamic="DYNAMIC",
                    evidence={"work_intensity": profile["work_intensity"], "total_shifts": profile["total_shifts"]},
                    implications=["éåŠ´é˜²æ­¢å¯¾ç­–å¿…è¦", "ä¼‘æ¯æ—¥ã®ç¢ºä¿"],
                    creator_intention_score=0.85
                )
                constraints.append(constraint)
            
            # æ–°ã—ã„è©³ç´°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼šã‚¹ã‚¿ãƒƒãƒ•ã®ã‚·ãƒ•ãƒˆå¤šæ§˜æ€§åˆ†æ
            shift_variety_count = len(profile["shift_variety"])
            if shift_variety_count >= 3:  # 3ç¨®é¡ä»¥ä¸Šã§å¤šæ§˜æ€§
                constraint = MultiAxisConstraint(
                    id=f"STAFF-VARIETY-{len(constraints):03d}",
                    description=f"ã€Œ{staff}ã€ã¯å¤šæ§˜æ€§ã‚¹ã‚¿ãƒƒãƒ•ï¼ˆ{shift_variety_count}ç¨®é¡ã®ã‚·ãƒ•ãƒˆå¯¾å¿œï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=min(1.0, shift_variety_count / 5.0),
                    constraint_type="å¤šæ§˜æ€§åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"variety_count": shift_variety_count, "shift_types": profile["shift_variety"]},
                    implications=["æŸ”è»Ÿãªé…ç½®ã«é©ç”¨", "å¤šç›®çš„æ´»ç”¨å¯èƒ½"],
                    creator_intention_score=0.8
                )
                constraints.append(constraint)
            
            # ã‚¹ã‚¿ãƒƒãƒ•ã®å‹¤å‹™é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
            total_shifts = profile["total_shifts"]
            if total_shifts >= 5:  # 5å›ä»¥ä¸Šã§é »ç¹å‹¤å‹™
                constraint = MultiAxisConstraint(
                    id=f"STAFF-FREQ-{len(constraints):03d}",
                    description=f"ã€Œ{staff}ã€ã¯é »ç¹å‹¤å‹™è€…ï¼ˆ{total_shifts}å›å‹¤å‹™ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=min(1.0, total_shifts / 10.0),
                    constraint_type="å‹¤å‹™é »åº¦åˆ¶ç´„",
                    static_dynamic="DYNAMIC",
                    evidence={"total_shifts": total_shifts},
                    implications=["ä¸»åŠ›ã‚¹ã‚¿ãƒƒãƒ•ã¨ã—ã¦ã®ä½ç½®ã¥ã‘", "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«èª¿æ•´é‡è¦"],
                    creator_intention_score=0.75
                )
                constraints.append(constraint)
            elif total_shifts >= 2:  # 2å›ä»¥ä¸Šã§é€šå¸¸å‹¤å‹™
                constraint = MultiAxisConstraint(
                    id=f"STAFF-NORMAL-{len(constraints):03d}",
                    description=f"ã€Œ{staff}ã€ã¯é€šå¸¸å‹¤å‹™è€…ï¼ˆ{total_shifts}å›å‹¤å‹™ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.7,
                    constraint_type="é€šå¸¸å‹¤å‹™åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"total_shifts": total_shifts},
                    implications=["å®šæœŸé…ç½®å¯¾è±¡", "æ¨™æº–çš„ãªé‹ç”¨"],
                    creator_intention_score=0.6
                )
                constraints.append(constraint)
            else:  # 1å›ã®ã¿ã§ç¨€å°‘å‹¤å‹™
                constraint = MultiAxisConstraint(
                    id=f"STAFF-RARE-{len(constraints):03d}",
                    description=f"ã€Œ{staff}ã€ã¯ç¨€å°‘å‹¤å‹™è€…ï¼ˆ{total_shifts}å›ã®ã¿ï¼‰",
                    axes=[ConstraintAxis.STAFF],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.8,
                    constraint_type="ç¨€å°‘å‹¤å‹™åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"total_shifts": total_shifts},
                    implications=["ç‰¹åˆ¥ãªé…ç½®äº‹æƒ…", "ä¾‹å¤–çš„ãªé‹ç”¨"],
                    creator_intention_score=0.9
                )
                constraints.append(constraint)
        
        return constraints

class TimeAxisAnalyzer:
    """æ™‚é–“è»¸å°‚é–€åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def analyze_deep_temporal_patterns(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """æ™‚é–“è»¸æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        constraints = []
        
        # æ™‚é–“å¸¯åˆ¥äººå“¡é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆé–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹ï¼‰
        for time_point, records in multi_data["time_patterns"].items():
            staff_count = len(set(r["staff"] for r in records))
            shift_diversity = len(set(r["shift_code"] for r in records))
            
            # äººå“¡é…ç½®å¯†åº¦åˆ¶ç´„ï¼ˆ2åä»¥ä¸Šã«ä¸‹ã’ã‚‹ï¼‰
            if staff_count >= 2:
                constraint = MultiAxisConstraint(
                    id=f"TIME-DENSE-{len(constraints):03d}",
                    description=f"æ™‚ç‚¹{time_point}ã¯è¤‡æ•°é…ç½®ï¼ˆ{staff_count}åã€{shift_diversity}ç¨®é¡ï¼‰",
                    axes=[ConstraintAxis.TIME],
                    depth=ConstraintDepth.MEDIUM,
                    confidence=min(1.0, staff_count / 5.0),
                    constraint_type="æ™‚é–“å¯†åº¦åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"staff_count": staff_count, "shift_diversity": shift_diversity, "time_point": time_point},
                    implications=["é‡è¦æ™‚é–“å¸¯ã®æŒ‡å®š", "æ¥­å‹™é›†ä¸­ãƒã‚¤ãƒ³ãƒˆ"],
                    creator_intention_score=0.8
                )
                constraints.append(constraint)
            elif staff_count == 1:  # å˜ä¸€é…ç½®ã‚‚åˆ¶ç´„ã¨ã—ã¦èªè­˜
                staff_name = records[0]["staff"]
                shift_code = records[0]["shift_code"]
                constraint = MultiAxisConstraint(
                    id=f"TIME-SINGLE-{len(constraints):03d}",
                    description=f"æ™‚ç‚¹{time_point}ã¯ã€Œ{staff_name}ã€ã®å˜ç‹¬é…ç½®ï¼ˆ{shift_code}ï¼‰",
                    axes=[ConstraintAxis.TIME],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.9,
                    constraint_type="å˜ç‹¬é…ç½®åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"staff": staff_name, "shift_code": shift_code, "time_point": time_point},
                    implications=["å°‚ä»»æ™‚é–“å¸¯", "é›†ä¸­æ¥­å‹™æ™‚é–“"],
                    creator_intention_score=0.8
                )
                constraints.append(constraint)
            
            # ã‚·ãƒ•ãƒˆå¤šæ§˜æ€§åˆ¶ç´„
            if shift_diversity >= 2:  # 2ç¨®é¡ä»¥ä¸Šã§å¤šæ§˜
                constraint = MultiAxisConstraint(
                    id=f"TIME-DIVERSE-{len(constraints):03d}",
                    description=f"æ™‚ç‚¹{time_point}ã¯å¤šæ§˜ã‚·ãƒ•ãƒˆé…ç½®ï¼ˆ{shift_diversity}ç¨®é¡ã€{staff_count}åï¼‰",
                    axes=[ConstraintAxis.TIME],
                    depth=ConstraintDepth.SURFACE,
                    confidence=min(1.0, shift_diversity / 3.0),
                    constraint_type="æ™‚é–“å¤šæ§˜æ€§åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"shift_diversity": shift_diversity, "staff_count": staff_count, "time_point": time_point},
                    implications=["è¤‡åˆæ¥­å‹™æ™‚é–“å¸¯", "å¤šæ©Ÿèƒ½é‹ç”¨"],
                    creator_intention_score=0.75
                )
                constraints.append(constraint)
        
        # æ™‚é–“è»¸ã®çµ±è¨ˆçš„ãƒ‘ã‚¿ãƒ¼ãƒ³
        time_points = list(multi_data["time_patterns"].keys())
        if time_points:
            # æœ€ã‚‚æ—©ã„æ™‚é–“ã¨é…ã„æ™‚é–“
            min_time = min(time_points)
            max_time = max(time_points)
            
            constraint = MultiAxisConstraint(
                id=f"TIME-RANGE-{len(constraints):03d}",
                description=f"é‹ç”¨æ™‚é–“ç¯„å›²ï¼šæ™‚ç‚¹{min_time}ï½{max_time}ï¼ˆå…¨{len(time_points)}æ™‚ç‚¹ï¼‰",
                axes=[ConstraintAxis.TIME],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="æ™‚é–“ç¯„å›²åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"min_time": min_time, "max_time": max_time, "total_points": len(time_points)},
                implications=["é‹ç”¨æ™‚é–“ã®åˆ¶é™", "æ¥­å‹™æ™‚é–“æ ã®è¨­å®š"],
                creator_intention_score=0.9
            )
            constraints.append(constraint)
        
        return constraints

class TaskAxisAnalyzer:
    """ã‚¿ã‚¹ã‚¯è»¸å°‚é–€åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def analyze_deep_task_patterns(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """ã‚¿ã‚¹ã‚¯è»¸æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        constraints = []
        
        # å…¨ã‚¿ã‚¹ã‚¯ã®åŸºæœ¬åˆ†æï¼ˆå¤§å¹…æ‹¡å¼µï¼‰
        for task, complexity_data in multi_data["task_complexity"].items():
            complexity_score = complexity_data["complexity_score"]
            specialization_level = complexity_data["specialization_level"]
            usage_frequency = complexity_data["usage_frequency"]
            
            # é«˜è¤‡é›‘åº¦ã‚¿ã‚¹ã‚¯åˆ¶ç´„ï¼ˆé–¾å€¤ã‚’1.0ã«ä¸‹ã’ã‚‹ï¼‰
            if complexity_score > 1.0:
                constraint = MultiAxisConstraint(
                    id=f"TASK-COMPLEX-{len(constraints):03d}",
                    description=f"ã€Œ{task}ã€ã¯è¤‡é›‘ã‚¿ã‚¹ã‚¯ï¼ˆè¤‡é›‘åº¦{complexity_score:.2f}ï¼‰",
                    axes=[ConstraintAxis.TASK],
                    depth=ConstraintDepth.DEEP,
                    confidence=min(1.0, complexity_score / 3.0),
                    constraint_type="ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence=complexity_data,
                    implications=["å°‚é–€ã‚¹ã‚­ãƒ«è¦æ±‚", "ç ”ä¿®ãƒ»ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¿…è¦"],
                    creator_intention_score=0.9
                )
                constraints.append(constraint)
            
            # é«˜å°‚é–€æ€§ã‚¿ã‚¹ã‚¯åˆ¶ç´„
            if specialization_level > 0.5:  # 50%ä»¥ä¸Šã§å°‚é–€æ€§
                constraint = MultiAxisConstraint(
                    id=f"TASK-SPECIAL-{len(constraints):03d}",
                    description=f"ã€Œ{task}ã€ã¯å°‚é–€æ€§ã‚¿ã‚¹ã‚¯ï¼ˆå°‚é–€åº¦{specialization_level:.2f}ï¼‰",
                    axes=[ConstraintAxis.TASK],
                    depth=ConstraintDepth.MEDIUM,
                    confidence=specialization_level,
                    constraint_type="å°‚é–€æ€§åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"specialization_level": specialization_level, "task": task},
                    implications=["é™å®šã‚¹ã‚¿ãƒƒãƒ•ã§ã®å®Ÿè¡Œ", "å°‚é–€ç ”ä¿®å¿…è¦"],
                    creator_intention_score=0.85
                )
                constraints.append(constraint)
            
            # ã‚¿ã‚¹ã‚¯ä½¿ç”¨é »åº¦åˆ¶ç´„
            if usage_frequency >= 3:  # 3å›ä»¥ä¸Šã§é »ç¹
                constraint = MultiAxisConstraint(
                    id=f"TASK-FREQUENT-{len(constraints):03d}",
                    description=f"ã€Œ{task}ã€ã¯é »ç¹ã‚¿ã‚¹ã‚¯ï¼ˆ{usage_frequency}å›ä½¿ç”¨ï¼‰",
                    axes=[ConstraintAxis.TASK],
                    depth=ConstraintDepth.SURFACE,
                    confidence=min(1.0, usage_frequency / 10.0),
                    constraint_type="é »åº¦åˆ¶ç´„",
                    static_dynamic="DYNAMIC",
                    evidence={"usage_frequency": usage_frequency, "task": task},
                    implications=["å®šæœŸæ¥­å‹™", "æ¨™æº–é‹ç”¨æ‰‹é †"],
                    creator_intention_score=0.7
                )
                constraints.append(constraint)
            elif usage_frequency >= 2:  # 2å›ã§é€šå¸¸
                constraint = MultiAxisConstraint(
                    id=f"TASK-NORMAL-{len(constraints):03d}",
                    description=f"ã€Œ{task}ã€ã¯é€šå¸¸ã‚¿ã‚¹ã‚¯ï¼ˆ{usage_frequency}å›ä½¿ç”¨ï¼‰",
                    axes=[ConstraintAxis.TASK],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.6,
                    constraint_type="é€šå¸¸æ¥­å‹™åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"usage_frequency": usage_frequency, "task": task},
                    implications=["å¿…è¦æ™‚æ¥­å‹™", "å®šæœŸçš„ãªå®Ÿè¡Œ"],
                    creator_intention_score=0.6
                )
                constraints.append(constraint)
            else:  # 1å›ã®ã¿ã§ç¨€å°‘
                constraint = MultiAxisConstraint(
                    id=f"TASK-RARE-{len(constraints):03d}",
                    description=f"ã€Œ{task}ã€ã¯ç¨€å°‘ã‚¿ã‚¹ã‚¯ï¼ˆ{usage_frequency}å›ã®ã¿ï¼‰",
                    axes=[ConstraintAxis.TASK],
                    depth=ConstraintDepth.SURFACE,
                    confidence=0.8,
                    constraint_type="ç¨€å°‘æ¥­å‹™åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"usage_frequency": usage_frequency, "task": task},
                    implications=["ç‰¹åˆ¥æ¥­å‹™", "ä¾‹å¤–çš„ãªå®Ÿè¡Œ"],
                    creator_intention_score=0.9
                )
                constraints.append(constraint)
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†é¡åˆ¶ç´„
        task_types = multi_data["task_types"]
        
        # æ•°å€¤ç³»ã‚¿ã‚¹ã‚¯ã®æ¤œå‡º
        numeric_tasks = []
        text_tasks = []
        for task in task_types:
            try:
                float(task)
                numeric_tasks.append(task)
            except ValueError:
                text_tasks.append(task)
        
        if numeric_tasks:
            constraint = MultiAxisConstraint(
                id=f"TASK-NUMERIC-{len(constraints):03d}",
                description=f"æ•°å€¤ç³»ã‚¿ã‚¹ã‚¯ç¾¤ï¼ˆ{len(numeric_tasks)}ç¨®é¡ï¼š{', '.join(numeric_tasks[:3])}ãªã©ï¼‰",
                axes=[ConstraintAxis.TASK],
                depth=ConstraintDepth.SURFACE,
                confidence=0.9,
                constraint_type="ã‚¿ã‚¹ã‚¯åˆ†é¡åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"numeric_tasks": numeric_tasks, "count": len(numeric_tasks)},
                implications=["å®šé‡çš„æ¥­å‹™", "æ™‚é–“ç®¡ç†é‡è¦"],
                creator_intention_score=0.8
            )
            constraints.append(constraint)
        
        if text_tasks:
            constraint = MultiAxisConstraint(
                id=f"TASK-TEXT-{len(constraints):03d}",
                description=f"ãƒ†ã‚­ã‚¹ãƒˆç³»ã‚¿ã‚¹ã‚¯ç¾¤ï¼ˆ{len(text_tasks)}ç¨®é¡ï¼š{', '.join(text_tasks[:3])}ãªã©ï¼‰",
                axes=[ConstraintAxis.TASK],
                depth=ConstraintDepth.SURFACE,
                confidence=0.9,
                constraint_type="ã‚¿ã‚¹ã‚¯åˆ†é¡åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"text_tasks": text_tasks, "count": len(text_tasks)},
                implications=["å®šæ€§çš„æ¥­å‹™", "å†…å®¹ç†è§£é‡è¦"],
                creator_intention_score=0.8
            )
            constraints.append(constraint)
        
        return constraints

class RelationshipAxisAnalyzer:
    """é–¢ä¿‚è»¸å°‚é–€åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def analyze_deep_relationship_patterns(self, multi_data: Dict[str, Any]) -> List[MultiAxisConstraint]:
        """é–¢ä¿‚è»¸æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        constraints = []
        
        # å…¨é–¢ä¿‚æ€§ãƒšã‚¢ã®åˆ†æï¼ˆé–¾å€¤ã‚’å¤§å¹…ã«ä¸‹ã’ã‚‹ï¼‰
        for pair, relationship_data in multi_data["relationship_pairs"].items():
            compatibility_score = relationship_data["compatibility_score"]
            frequency = relationship_data["frequency"]
            
            # é«˜ç›¸æ€§ãƒšã‚¢ï¼ˆé–¾å€¤ã‚’0.7ã«ä¸‹ã’ã€é »åº¦ã‚‚2ã«ä¸‹ã’ã‚‹ï¼‰
            if compatibility_score > 0.7 and frequency >= 2:
                staff1, staff2 = pair
                constraint = MultiAxisConstraint(
                    id=f"REL-COMPAT-{len(constraints):03d}",
                    description=f"ã€Œ{staff1}ã€Ã—ã€Œ{staff2}ã€ã¯é«˜ç›¸æ€§ãƒšã‚¢ï¼ˆç›¸æ€§{compatibility_score:.2f}ã€{frequency}å›å”åŠ›ï¼‰",
                    axes=[ConstraintAxis.RELATIONSHIP],
                    depth=ConstraintDepth.DEEP,
                    confidence=compatibility_score,
                    constraint_type="é–¢ä¿‚æ€§åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence=relationship_data,
                    implications=["å„ªå…ˆçš„ãªåŒæ™‚é…ç½®", "ãƒãƒ¼ãƒ ç·¨æˆã§ã®æ´»ç”¨"],
                    creator_intention_score=0.95
                )
                constraints.append(constraint)
            
            # é€šå¸¸ç›¸æ€§ãƒšã‚¢
            elif compatibility_score > 0.5 and frequency >= 2:
                staff1, staff2 = pair
                constraint = MultiAxisConstraint(
                    id=f"REL-NORMAL-{len(constraints):03d}",
                    description=f"ã€Œ{staff1}ã€Ã—ã€Œ{staff2}ã€ã¯é€šå¸¸ãƒšã‚¢ï¼ˆç›¸æ€§{compatibility_score:.2f}ã€{frequency}å›å”åŠ›ï¼‰",
                    axes=[ConstraintAxis.RELATIONSHIP],
                    depth=ConstraintDepth.MEDIUM,
                    confidence=compatibility_score,
                    constraint_type="é€šå¸¸é–¢ä¿‚åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence=relationship_data,
                    implications=["é€šå¸¸ã®åŒæ™‚é…ç½®", "æ¨™æº–çš„ãªãƒãƒ¼ãƒ ç·¨æˆ"],
                    creator_intention_score=0.8
                )
                constraints.append(constraint)
            
            # é™å®šå”åŠ›ãƒšã‚¢ï¼ˆé »åº¦1å›ã®ã¿ã§ã‚‚è¨˜éŒ²ï¼‰
            elif frequency == 1:
                staff1, staff2 = pair
                constraint = MultiAxisConstraint(
                    id=f"REL-LIMITED-{len(constraints):03d}",
                    description=f"ã€Œ{staff1}ã€Ã—ã€Œ{staff2}ã€ã¯é™å®šå”åŠ›ãƒšã‚¢ï¼ˆç›¸æ€§{compatibility_score:.2f}ã€1å›ã®ã¿ï¼‰",
                    axes=[ConstraintAxis.RELATIONSHIP],
                    depth=ConstraintDepth.SURFACE,
                    confidence=compatibility_score * 0.5,  # é »åº¦ãŒä½ã„ã®ã§ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
                    constraint_type="é™å®šé–¢ä¿‚åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence=relationship_data,
                    implications=["ç‰¹åˆ¥ãªçµ„ã¿åˆã‚ã›", "ä¾‹å¤–çš„ãªå”åŠ›"],
                    creator_intention_score=0.7
                )
                constraints.append(constraint)
        
        # é–¢ä¿‚æ€§ã®çµ±è¨ˆçš„åˆ†æ
        if multi_data["relationship_pairs"]:
            total_pairs = len(multi_data["relationship_pairs"])
            avg_compatibility = sum(data["compatibility_score"] for data in multi_data["relationship_pairs"].values()) / total_pairs
            total_collaborations = sum(data["frequency"] for data in multi_data["relationship_pairs"].values())
            
            constraint = MultiAxisConstraint(
                id=f"REL-STATS-{len(constraints):03d}",
                description=f"é–¢ä¿‚æ€§çµ±è¨ˆï¼š{total_pairs}ãƒšã‚¢ã€å¹³å‡ç›¸æ€§{avg_compatibility:.2f}ã€ç·å”åŠ›{total_collaborations}å›",
                axes=[ConstraintAxis.RELATIONSHIP],
                depth=ConstraintDepth.SURFACE,
                confidence=1.0,
                constraint_type="é–¢ä¿‚æ€§çµ±è¨ˆåˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"total_pairs": total_pairs, "avg_compatibility": avg_compatibility, "total_collaborations": total_collaborations},
                implications=["çµ„ç¹”ã®å”åŠ›ãƒ¬ãƒ™ãƒ«", "ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æŒ‡æ¨™"],
                creator_intention_score=0.9
            )
            constraints.append(constraint)
        
        return constraints

class MultiDimensionalSynthesizer:
    """å¤šæ¬¡å…ƒçµ±åˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        pass

# è©³ç´°å¤šè»¸åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…
def _analyze_staff_time_interaction(self, multi_data):
    """ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•ã®æ™‚é–“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    for staff, profile in multi_data["staff_profiles"].items():
        temporal_patterns = profile["temporal_patterns"]
        if len(temporal_patterns) >= 3:
            # æ™‚é–“é›†ä¸­åº¦åˆ†æ
            time_points = [tp[0] for tp in temporal_patterns]
            time_concentration = len(set(time_points)) / len(time_points)
            
            if time_concentration < 0.5:  # ç‰¹å®šæ™‚é–“ã«é›†ä¸­
                most_common_time = Counter(time_points).most_common(1)[0]
                constraint = self._generate_constraint(
                    description=f"ã€Œ{staff}ã€ã¯æ™‚ç‚¹{most_common_time[0]}ã«{most_common_time[1]}å›é›†ä¸­é…ç½®ï¼ˆæ™‚é–“ç‰¹åŒ–åº¦{1-time_concentration:.2f}ï¼‰",
                    axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME],
                    depth=ConstraintDepth.MEDIUM,
                    confidence=1 - time_concentration,
                    constraint_type="ã‚¹ã‚¿ãƒƒãƒ•æ™‚é–“ç‰¹åŒ–åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"staff": staff, "time_concentration": time_concentration, "most_common_time": most_common_time},
                    implications=["ç‰¹å®šæ™‚é–“å¸¯ã¸ã®å„ªå…ˆé…ç½®", "æ™‚é–“å¸¯å°‚é–€æ€§ã®æ´»ç”¨"],
                    creator_intention_score=0.85
                )
                constraints.append(constraint)
    
    return constraints

def _analyze_staff_task_interaction(self, multi_data):
    """ã‚¹ã‚¿ãƒƒãƒ•Ã—ã‚¿ã‚¹ã‚¯è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•-ã‚¿ã‚¹ã‚¯è¦ªå’Œæ€§åˆ†æ
    for task, staff_usage in multi_data["task_staff_affinity"].items():
        if not staff_usage:
            continue
            
        # æœ€é«˜ä½¿ç”¨ã‚¹ã‚¿ãƒƒãƒ•ã®ç‰¹å®š
        top_staff = max(staff_usage.items(), key=lambda x: x[1])
        staff_name, usage_count = top_staff
        
        total_task_usage = sum(staff_usage.values())
        dominance_ratio = usage_count / total_task_usage
        
        if dominance_ratio > 0.4:  # 40%ä»¥ä¸Šã§æ”¯é…çš„ï¼ˆé–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼‰
            constraint = self._generate_constraint(
                description=f"ã€Œ{staff_name}ã€ã¯ã€Œ{task}ã€ã‚¿ã‚¹ã‚¯ã‚’{dominance_ratio:.0%}æ”¯é…ï¼ˆ{usage_count}/{total_task_usage}å›ï¼‰",
                axes=[ConstraintAxis.STAFF, ConstraintAxis.TASK],
                depth=ConstraintDepth.DEEP,
                confidence=dominance_ratio,
                constraint_type="ã‚¹ã‚¿ãƒƒãƒ•ã‚¿ã‚¹ã‚¯æ”¯é…åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"staff": staff_name, "task": task, "dominance_ratio": dominance_ratio, "usage_stats": staff_usage},
                implications=["ã‚¿ã‚¹ã‚¯å°‚é–€å®¶ã¨ã—ã¦ã®ãƒã‚¸ã‚·ãƒ§ãƒ³", "ä»£æ›¿è¦å“¡ã®è‚²æˆå¿…è¦"],
                creator_intention_score=0.9
            )
            constraints.append(constraint)
    
    return constraints

def _analyze_staff_relationship_interaction(self, multi_data):
    """ã‚¹ã‚¿ãƒƒãƒ•Ã—é–¢ä¿‚è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•ã®é–¢ä¿‚æ€§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
    staff_relationship_profiles = defaultdict(lambda: {"partnerships": 0, "avg_compatibility": 0.0, "leadership_role": 0})
    
    for pair, relationship_data in multi_data["relationship_pairs"].items():
        staff1, staff2 = pair
        compatibility = relationship_data["compatibility_score"]
        
        for staff in [staff1, staff2]:
            staff_relationship_profiles[staff]["partnerships"] += 1
            staff_relationship_profiles[staff]["avg_compatibility"] += compatibility
    
    # å¹³å‡ç›¸æ€§ã®è¨ˆç®—
    for staff, profile in staff_relationship_profiles.items():
        if profile["partnerships"] > 0:
            profile["avg_compatibility"] /= profile["partnerships"]
    
    # é–¢ä¿‚æ€§ãƒªãƒ¼ãƒ€ãƒ¼ã®ç‰¹å®š
    for staff, profile in staff_relationship_profiles.items():
        if profile["partnerships"] >= 3 and profile["avg_compatibility"] > 0.7:
            constraint = self._generate_constraint(
                description=f"ã€Œ{staff}ã€ã¯é–¢ä¿‚æ€§ãƒãƒ–ï¼ˆ{profile['partnerships']}é–¢ä¿‚ã€å¹³å‡ç›¸æ€§{profile['avg_compatibility']:.2f}ï¼‰",
                axes=[ConstraintAxis.STAFF, ConstraintAxis.RELATIONSHIP],
                depth=ConstraintDepth.DEEP,
                confidence=profile["avg_compatibility"],
                constraint_type="é–¢ä¿‚æ€§ãƒãƒ–åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"staff": staff, "relationship_profile": profile},
                implications=["ãƒãƒ¼ãƒ èª¿æ•´å½¹ã¨ã—ã¦ã®æ´»ç”¨", "äººé–“é–¢ä¿‚ã®å®‰å®šè¦å› "],
                creator_intention_score=0.95
            )
            constraints.append(constraint)
    
    return constraints

def _analyze_time_task_interaction(self, multi_data):
    """æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # æ™‚é–“å¸¯åˆ¥ã‚¿ã‚¹ã‚¯åˆ†å¸ƒåˆ†æ
    time_task_matrix = defaultdict(lambda: defaultdict(int))
    
    for record in multi_data["raw_shift_records"]:
        time_point = record["time_point"]
        task = record["shift_code"]
        time_task_matrix[time_point][task] += 1
    
    # æ™‚é–“å¸¯å°‚ç”¨ã‚¿ã‚¹ã‚¯ã®ç™ºè¦‹
    for time_point, task_counts in time_task_matrix.items():
        if not task_counts:
            continue
            
        total_tasks_at_time = sum(task_counts.values())
        dominant_task = max(task_counts.items(), key=lambda x: x[1])
        task_name, task_count = dominant_task
        
        task_dominance = task_count / total_tasks_at_time
        
        if task_dominance > 0.5:  # 50%ä»¥ä¸Šã§æ™‚é–“å°‚ç”¨ï¼ˆé–¾å€¤ã‚’ä¸‹ã’ã‚‹ï¼‰
            constraint = self._generate_constraint(
                description=f"æ™‚ç‚¹{time_point}ã¯ã€Œ{task_name}ã€å°‚ç”¨æ™‚é–“å¸¯ï¼ˆ{task_dominance:.0%}å æœ‰ã€{task_count}/{total_tasks_at_time}å›ï¼‰",
                axes=[ConstraintAxis.TIME, ConstraintAxis.TASK],
                depth=ConstraintDepth.MEDIUM,
                confidence=task_dominance,
                constraint_type="æ™‚é–“ã‚¿ã‚¹ã‚¯å°‚ç”¨åˆ¶ç´„",
                static_dynamic="STATIC",
                evidence={"time_point": time_point, "dominant_task": task_name, "dominance_ratio": task_dominance},
                implications=["æ™‚é–“å¸¯åˆ¥ã‚¿ã‚¹ã‚¯ç‰¹åŒ–", "åŠ¹ç‡çš„ãªæ¥­å‹™é…ç½®"],
                creator_intention_score=0.85
            )
            constraints.append(constraint)
    
    return constraints

def _analyze_time_relationship_interaction(self, multi_data):
    """æ™‚é–“Ã—é–¢ä¿‚è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # æ™‚é–“å¸¯åˆ¥å”åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    time_relationship_patterns = defaultdict(list)
    
    for pair, collaborations in multi_data["collaboration_patterns"].items():
        for collab in collaborations:
            time_point = collab["time_point"]
            time_relationship_patterns[time_point].append((pair, collab))
    
    # é«˜å”åŠ›æ™‚é–“å¸¯ã®ç‰¹å®š
    for time_point, relationships in time_relationship_patterns.items():
        if len(relationships) >= 2:  # 2çµ„ä»¥ä¸Šã®å”åŠ›
            unique_pairs = len(set(rel[0] for rel in relationships))
            avg_compatibility = sum(rel[1].get("compatibility_score", 0.7) for rel in relationships) / len(relationships)
            
            if avg_compatibility > 0.8:
                constraint = self._generate_constraint(
                    description=f"æ™‚ç‚¹{time_point}ã¯é«˜å”åŠ›æ™‚é–“å¸¯ï¼ˆ{unique_pairs}ãƒšã‚¢ã€å¹³å‡ç›¸æ€§{avg_compatibility:.2f}ï¼‰",
                    axes=[ConstraintAxis.TIME, ConstraintAxis.RELATIONSHIP],
                    depth=ConstraintDepth.MEDIUM,
                    confidence=avg_compatibility,
                    constraint_type="æ™‚é–“å”åŠ›å¼·åŒ–åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"time_point": time_point, "relationships": len(relationships), "avg_compatibility": avg_compatibility},
                    implications=["ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é‡è¦æ™‚é–“", "å”åŠ›æ¥­å‹™ã®é›†ä¸­é…ç½®"],
                    creator_intention_score=0.8
                )
                constraints.append(constraint)
    
    return constraints

def _analyze_task_relationship_interaction(self, multi_data):
    """ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¿ã‚¹ã‚¯åˆ¥å”åŠ›å¿…è¦åº¦åˆ†æ
    task_collaboration_requirements = defaultdict(list)
    
    for pair, collaborations in multi_data["collaboration_patterns"].items():
        for collab in collaborations:
            task1, task2 = collab["shift1"], collab["shift2"]
            collaboration_score = multi_data["relationship_pairs"][pair]["compatibility_score"]
            
            task_collaboration_requirements[task1].append(collaboration_score)
            task_collaboration_requirements[task2].append(collaboration_score)
    
    # é«˜å”åŠ›è¦æ±‚ã‚¿ã‚¹ã‚¯ã®ç‰¹å®š
    for task, collab_scores in task_collaboration_requirements.items():
        if len(collab_scores) >= 2:
            avg_collab_requirement = sum(collab_scores) / len(collab_scores)
            
            if avg_collab_requirement > 0.8:
                constraint = self._generate_constraint(
                    description=f"ã€Œ{task}ã€ã¯é«˜å”åŠ›è¦æ±‚ã‚¿ã‚¹ã‚¯ï¼ˆå¹³å‡å”åŠ›åº¦{avg_collab_requirement:.2f}ã€{len(collab_scores)}å›å®Ÿç¸¾ï¼‰",
                    axes=[ConstraintAxis.TASK, ConstraintAxis.RELATIONSHIP],
                    depth=ConstraintDepth.DEEP,
                    confidence=avg_collab_requirement,
                    constraint_type="ã‚¿ã‚¹ã‚¯å”åŠ›è¦æ±‚åˆ¶ç´„",
                    static_dynamic="STATIC",
                    evidence={"task": task, "avg_collaboration": avg_collab_requirement, "instances": len(collab_scores)},
                    implications=["ãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é‡è¦–é…ç½®", "ç›¸æ€§è‰¯å¥½ã‚¹ã‚¿ãƒƒãƒ•ã®åŒæ™‚é…ç½®"],
                    creator_intention_score=0.9
                )
                constraints.append(constraint)
    
    return constraints

def _analyze_staff_time_task_interaction(self, multi_data):
    """ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•ã®æ™‚é–“å¸¯åˆ¥ã‚¿ã‚¹ã‚¯ç‰¹åŒ–åˆ†æ
    staff_time_task_patterns = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))
    
    for record in multi_data["raw_shift_records"]:
        staff = record["staff"]
        time_point = record["time_point"]
        task = record["shift_code"]
        staff_time_task_patterns[staff][time_point][task] += 1
    
    # ä¸‰é‡ç‰¹åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºè¦‹
    for staff, time_patterns in staff_time_task_patterns.items():
        for time_point, task_counts in time_patterns.items():
            if not task_counts:
                continue
                
            total_tasks = sum(task_counts.values())
            if total_tasks >= 2:  # æœ€å°é »åº¦
                dominant_task = max(task_counts.items(), key=lambda x: x[1])
                task_name, task_count = dominant_task
                
                specialization_ratio = task_count / total_tasks
                if specialization_ratio >= 0.8:  # 80%ä»¥ä¸Šç‰¹åŒ–
                    constraint = self._generate_constraint(
                        description=f"ã€Œ{staff}ã€ã¯æ™‚ç‚¹{time_point}ã§ã€Œ{task_name}ã€ã«{specialization_ratio:.0%}ç‰¹åŒ–ï¼ˆ{task_count}/{total_tasks}å›ï¼‰",
                        axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME, ConstraintAxis.TASK],
                        depth=ConstraintDepth.DEEP,
                        confidence=specialization_ratio,
                        constraint_type="ä¸‰é‡ç‰¹åŒ–åˆ¶ç´„",
                        static_dynamic="STATIC",
                        evidence={"staff": staff, "time_point": time_point, "task": task_name, "specialization": specialization_ratio},
                        implications=["æ™‚ç©ºé–“ã‚¿ã‚¹ã‚¯å°‚é–€å®¶", "æ¥µåº¦ã®å°‚é–€æ€§æ´»ç”¨"],
                        creator_intention_score=0.95
                    )
                    constraints.append(constraint)
    
    return constraints

def _analyze_staff_time_relationship_interaction(self, multi_data):
    """ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—é–¢ä¿‚è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•ã®æ™‚é–“å¸¯åˆ¥é–¢ä¿‚æ€§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
    staff_time_relationships = defaultdict(lambda: defaultdict(list))
    
    for pair, collaborations in multi_data["collaboration_patterns"].items():
        staff1, staff2 = pair
        for collab in collaborations:
            time_point = collab["time_point"]
            compatibility = multi_data["relationship_pairs"][pair]["compatibility_score"]
            
            staff_time_relationships[staff1][time_point].append(compatibility)
            staff_time_relationships[staff2][time_point].append(compatibility)
    
    # æ™‚é–“å¸¯åˆ¥é–¢ä¿‚æ€§ã‚¨ãƒ¼ã‚¹ã®ç™ºè¦‹
    for staff, time_relationships in staff_time_relationships.items():
        for time_point, compatibilities in time_relationships.items():
            if len(compatibilities) >= 2:  # è¤‡æ•°é–¢ä¿‚
                avg_compatibility = sum(compatibilities) / len(compatibilities)
                
                if avg_compatibility > 0.85:
                    constraint = self._generate_constraint(
                        description=f"ã€Œ{staff}ã€ã¯æ™‚ç‚¹{time_point}ã®é–¢ä¿‚æ€§ã‚¨ãƒ¼ã‚¹ï¼ˆå¹³å‡ç›¸æ€§{avg_compatibility:.2f}ã€{len(compatibilities)}é–¢ä¿‚ï¼‰",
                        axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME, ConstraintAxis.RELATIONSHIP],
                        depth=ConstraintDepth.DEEP,
                        confidence=avg_compatibility,
                        constraint_type="æ™‚é–“å¸¯é–¢ä¿‚æ€§ã‚¨ãƒ¼ã‚¹åˆ¶ç´„",
                        static_dynamic="STATIC",
                        evidence={"staff": staff, "time_point": time_point, "avg_compatibility": avg_compatibility, "relationship_count": len(compatibilities)},
                        implications=["æ™‚é–“å¸¯ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼", "é–¢ä¿‚æ€§èª¿æ•´ã®è¦"],
                        creator_intention_score=0.92
                    )
                    constraints.append(constraint)
    
    return constraints

def _analyze_staff_task_relationship_interaction(self, multi_data):
    """ã‚¹ã‚¿ãƒƒãƒ•Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚¿ã‚¹ã‚¯åˆ¥é–¢ä¿‚æ€§ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—åˆ†æ
    staff_task_leadership = defaultdict(lambda: defaultdict(list))
    
    for record in multi_data["raw_shift_records"]:
        staff = record["staff"]
        task = record["shift_code"]
        
        # ã“ã®æ™‚ç‚¹ã§ã®é–¢ä¿‚æ€§ã‚’åˆ†æ
        for pair, relationship_data in multi_data["relationship_pairs"].items():
            if staff in pair:
                other_staff = pair[1] if pair[0] == staff else pair[0]
                compatibility = relationship_data["compatibility_score"]
                staff_task_leadership[staff][task].append(compatibility)
    
    # ã‚¿ã‚¹ã‚¯åˆ¥é–¢ä¿‚æ€§ãƒªãƒ¼ãƒ€ãƒ¼ã®ç‰¹å®š
    for staff, task_relationships in staff_task_leadership.items():
        for task, compatibilities in task_relationships.items():
            if len(compatibilities) >= 2:
                avg_compatibility = sum(compatibilities) / len(compatibilities)
                
                if avg_compatibility > 0.8:
                    constraint = self._generate_constraint(
                        description=f"ã€Œ{staff}ã€ã¯ã€Œ{task}ã€ã‚¿ã‚¹ã‚¯ã®é–¢ä¿‚æ€§ãƒªãƒ¼ãƒ€ãƒ¼ï¼ˆå¹³å‡ç›¸æ€§{avg_compatibility:.2f}ã€{len(compatibilities)}é–¢ä¿‚ï¼‰",
                        axes=[ConstraintAxis.STAFF, ConstraintAxis.TASK, ConstraintAxis.RELATIONSHIP],
                        depth=ConstraintDepth.DEEP,
                        confidence=avg_compatibility,
                        constraint_type="ã‚¿ã‚¹ã‚¯é–¢ä¿‚æ€§ãƒªãƒ¼ãƒ€ãƒ¼åˆ¶ç´„",
                        static_dynamic="STATIC",
                        evidence={"staff": staff, "task": task, "leadership_score": avg_compatibility, "relationships": len(compatibilities)},
                        implications=["ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒãƒ¼ãƒ ãƒªãƒ¼ãƒ€ãƒ¼", "å°‚é–€æ¥­å‹™ã§ã®äººé–“é–¢ä¿‚èª¿æ•´"],
                        creator_intention_score=0.93
                    )
                    constraints.append(constraint)
    
    return constraints

def _analyze_time_task_relationship_interaction(self, multi_data):
    """æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚è»¸ç›¸äº’ä½œç”¨åˆ†æ"""
    constraints = []
    
    # æ™‚é–“å¸¯Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚æ€§ã®ä¸‰é‡çµ±åˆåˆ†æ
    time_task_relationship_matrix = defaultdict(lambda: defaultdict(list))
    
    for pair, collaborations in multi_data["collaboration_patterns"].items():
        for collab in collaborations:
            time_point = collab["time_point"]
            task_combo = tuple(sorted([collab["shift1"], collab["shift2"]]))
            compatibility = multi_data["relationship_pairs"][pair]["compatibility_score"]
            
            time_task_relationship_matrix[time_point][task_combo].append(compatibility)
    
    # æœ€é©æ™‚é–“å¸¯Ã—ã‚¿ã‚¹ã‚¯çµ„ã¿åˆã‚ã›ã®ç™ºè¦‹
    for time_point, task_combinations in time_task_relationship_matrix.items():
        for task_combo, compatibilities in task_combinations.items():
            if len(compatibilities) >= 2:
                avg_compatibility = sum(compatibilities) / len(compatibilities)
                
                if avg_compatibility > 0.85:
                    task1, task2 = task_combo
                    constraint = self._generate_constraint(
                        description=f"æ™‚ç‚¹{time_point}ã§ã®ã€Œ{task1}ã€Ã—ã€Œ{task2}ã€ã¯æœ€é©çµ„ã¿åˆã‚ã›ï¼ˆç›¸æ€§{avg_compatibility:.2f}ã€{len(compatibilities)}å®Ÿç¸¾ï¼‰",
                        axes=[ConstraintAxis.TIME, ConstraintAxis.TASK, ConstraintAxis.RELATIONSHIP],
                        depth=ConstraintDepth.DEEP,
                        confidence=avg_compatibility,
                        constraint_type="ä¸‰é‡æœ€é©åŒ–åˆ¶ç´„",
                        static_dynamic="STATIC",
                        evidence={"time_point": time_point, "task_combination": task_combo, "optimal_compatibility": avg_compatibility},
                        implications=["æ™‚ç©ºé–“ã‚¿ã‚¹ã‚¯æœ€é©é…ç½®", "ä¸‰é‡è»¸çµ±åˆé‹ç”¨"],
                        creator_intention_score=0.95
                    )
                    constraints.append(constraint)
    
    return constraints

def _analyze_four_axis_ultra_deep_patterns(self, multi_data):
    """å››è»¸è¶…æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
    constraints = []
    
    # ã‚¹ã‚¿ãƒƒãƒ•Ã—æ™‚é–“Ã—ã‚¿ã‚¹ã‚¯Ã—é–¢ä¿‚ã®å››é‡çµ±åˆåˆ†æ
    ultra_deep_patterns = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(list))))
    
    # å…¨è¨˜éŒ²ã‚’å››æ¬¡å…ƒãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã«å±•é–‹
    for record in multi_data["raw_shift_records"]:
        staff = record["staff"]
        time_point = record["time_point"]
        task = record["shift_code"]
        
        # ã“ã®é…ç½®ã§ã®é–¢ä¿‚æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        relationship_scores = []
        for pair, relationship_data in multi_data["relationship_pairs"].items():
            if staff in pair:
                relationship_scores.append(relationship_data["compatibility_score"])
        
        if relationship_scores:
            avg_relationship = sum(relationship_scores) / len(relationship_scores)
            ultra_deep_patterns[staff][time_point][task]["relationships"].append(avg_relationship)
    
    # å››é‡å®Œå…¨ç‰¹åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºè¦‹
    ultra_constraint_count = 0
    for staff, time_data in ultra_deep_patterns.items():
        for time_point, task_data in time_data.items():
            for task, relationship_data in task_data.items():
                relationships = relationship_data.get("relationships", [])
                
                if len(relationships) >= 2:  # è¤‡æ•°é–¢ä¿‚æ€§å®Ÿç¸¾
                    avg_relationship = sum(relationships) / len(relationships)
                    pattern_strength = len(relationships) * avg_relationship
                    
                    if pattern_strength > 2.0:  # è¶…é«˜å¼·åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
                        constraint = self._generate_constraint(
                            description=f"ã€Œ{staff}ã€Ã—æ™‚ç‚¹{time_point}Ã—ã€Œ{task}ã€Ã—é–¢ä¿‚æ€§{avg_relationship:.2f}ã®å››é‡è¶…æœ€é©ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¼·åº¦{pattern_strength:.2f}ï¼‰",
                            axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME, ConstraintAxis.TASK, ConstraintAxis.RELATIONSHIP],
                            depth=ConstraintDepth.ULTRA_DEEP,
                            confidence=min(1.0, pattern_strength / 3.0),
                            constraint_type="å››é‡è¶…æœ€é©åŒ–åˆ¶ç´„",
                            static_dynamic="STATIC",
                            evidence={
                                "staff": staff, "time_point": time_point, "task": task,
                                "avg_relationship": avg_relationship, "pattern_strength": pattern_strength,
                                "relationship_instances": len(relationships)
                            },
                            implications=["ç©¶æ¥µã®æœ€é©é…ç½®", "å››æ¬¡å…ƒçµ±åˆé‹ç”¨", "ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æœ€æ·±å±¤æ„å›³"],
                            creator_intention_score=0.99
                        )
                        constraints.append(constraint)
                        ultra_constraint_count += 1
                        
                        if ultra_constraint_count >= 10:  # æœ€å¤§10å€‹ã®è¶…æ·±å±¤åˆ¶ç´„
                            break
            if ultra_constraint_count >= 10:
                break
        if ultra_constraint_count >= 10:
            break
    
    return constraints

def _analyze_dynamic_temporal_patterns(self, multi_data):
    """å‹•çš„æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
    constraints = []
    
    # æ™‚ç³»åˆ—ã§ã®å‹•çš„å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    temporal_evolution = defaultdict(list)
    
    # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆã—ãŸè¨˜éŒ²ã§å¤‰åŒ–ã‚’è¿½è·¡
    sorted_records = sorted(multi_data["raw_shift_records"], key=lambda x: x["time_point"])
    
    # ã‚¹ã‚¿ãƒƒãƒ•ã®å‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–
    staff_evolution = defaultdict(list)
    for record in sorted_records:
        staff = record["staff"]
        time_point = record["time_point"]
        task = record["shift_code"]
        staff_evolution[staff].append((time_point, task))
    
    # å‹•çš„åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç™ºè¦‹
    for staff, evolution in staff_evolution.items():
        if len(evolution) >= 4:  # æœ€ä½4æ™‚ç‚¹ã®ãƒ‡ãƒ¼ã‚¿
            # ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–ã®æ¤œå‡º
            task_sequences = [ev[1] for ev in evolution]
            unique_tasks = list(set(task_sequences))
            
            # å¾ªç’°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            for cycle_length in range(2, min(5, len(unique_tasks) + 1)):
                cycles_found = 0
                for i in range(len(task_sequences) - cycle_length + 1):
                    cycle = task_sequences[i:i + cycle_length]
                    # æ¬¡ã®åŒã˜é•·ã•ã®éƒ¨åˆ†ã¨æ¯”è¼ƒ
                    if i + cycle_length * 2 <= len(task_sequences):
                        next_cycle = task_sequences[i + cycle_length:i + cycle_length * 2]
                        if cycle == next_cycle:
                            cycles_found += 1
                
                if cycles_found >= 1:  # å¾ªç’°ç™ºè¦‹
                    cycle_pattern = task_sequences[:cycle_length]
                    constraint = self._generate_constraint(
                        description=f"ã€Œ{staff}ã€ã¯{cycle_length}å‘¨æœŸã®å‹•çš„å¾ªç’°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼š{' â†’ '.join(cycle_pattern)}ï¼ˆ{cycles_found}å›åå¾©ï¼‰",
                        axes=[ConstraintAxis.STAFF, ConstraintAxis.TIME, ConstraintAxis.TASK],
                        depth=ConstraintDepth.DEEP,
                        confidence=min(1.0, cycles_found / 2.0),
                        constraint_type="å‹•çš„å¾ªç’°åˆ¶ç´„",
                        static_dynamic="DYNAMIC",
                        evidence={"staff": staff, "cycle_pattern": cycle_pattern, "cycle_length": cycle_length, "repetitions": cycles_found},
                        implications=["äºˆæ¸¬å¯èƒ½ãªå‹•çš„ãƒ‘ã‚¿ãƒ¼ãƒ³", "å¾ªç’°å‹ã‚·ãƒ•ãƒˆè¨­è¨ˆ", "é•·æœŸè¨ˆç”»ã¸ã®æ´»ç”¨"],
                        creator_intention_score=0.88
                    )
                    constraints.append(constraint)
                    break  # æœ€åˆã®å¾ªç’°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿
    
    return constraints

# ã“ã‚Œã‚‰ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚¯ãƒ©ã‚¹ã«è¿½åŠ 
MultiAxisDeepConstraintDiscoverySystem._analyze_staff_time_interaction = _analyze_staff_time_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_staff_task_interaction = _analyze_staff_task_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_staff_relationship_interaction = _analyze_staff_relationship_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_time_task_interaction = _analyze_time_task_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_time_relationship_interaction = _analyze_time_relationship_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_task_relationship_interaction = _analyze_task_relationship_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_staff_time_task_interaction = _analyze_staff_time_task_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_staff_time_relationship_interaction = _analyze_staff_time_relationship_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_staff_task_relationship_interaction = _analyze_staff_task_relationship_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_time_task_relationship_interaction = _analyze_time_task_relationship_interaction
MultiAxisDeepConstraintDiscoverySystem._analyze_four_axis_ultra_deep_patterns = _analyze_four_axis_ultra_deep_patterns
MultiAxisDeepConstraintDiscoverySystem._analyze_dynamic_temporal_patterns = _analyze_dynamic_temporal_patterns

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    system = MultiAxisDeepConstraintDiscoverySystem()
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«
    test_file = "ãƒ‡ã‚¤_ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿_ä¼‘æ—¥ç²¾ç·».xlsx"
    
    if not Path(test_file).exists():
        print(f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {test_file}")
        return 1
    
    try:
        results = system.discover_revolutionary_constraints(test_file)
        
        total_constraints = results.get("system_metadata", {}).get("total_constraints", 0)
        achievement = results.get("system_metadata", {}).get("achievement_status", "UNKNOWN")
        
        print(f"\n{'='*100}")
        print("ã€é©æ–°çš„å¤šè»¸æ·±å±¤åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ  æœ€çµ‚åˆ¤å®šã€‘")
        print(f"{'='*100}")
        print(f"ç›®æ¨™: æ—¢å­˜263å€‹ã‚’å¤§å¹…è¶…è¶Šï¼ˆ500+å€‹ï¼‰")
        print(f"å®Ÿç¸¾: {total_constraints}å€‹ã®é©æ–°çš„åˆ¶ç´„ç™ºè¦‹")
        
        if achievement == "REVOLUTIONARY_SUCCESS":
            print("ğŸ‰ é©æ–°çš„æˆåŠŸï¼æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Œå…¨ã«è¶…è¶Šã™ã‚‹å¤šæ¬¡å…ƒåˆ¶ç´„ç™ºè¦‹ã‚’å®Ÿç¾ï¼")
            return 0
        elif achievement == "MAJOR_SUCCESS":
            print("ğŸš€ å¤§æˆåŠŸï¼æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’å¤§å¹…ã«æ”¹å–„ï¼")
            return 0
        elif achievement == "SUCCESS":
            print("âœ… æˆåŠŸï¼æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’å®Ÿç¾ï¼")
            return 0
        else:
            print("âš ï¸ éƒ¨åˆ†æˆåŠŸã€‚ã•ã‚‰ãªã‚‹é©æ–°ãŒå¿…è¦ã€‚")
            return 1
            
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())