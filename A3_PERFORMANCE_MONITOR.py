#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
Phase 2/3.1å‡¦ç†æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®åŒ…æ‹¬çš„ç›£è¦–
å…¨ä½“æœ€é©åŒ–ã®è¦³ç‚¹ã§SLOT_HOURSä¿®æ­£ã®æ€§èƒ½å½±éŸ¿ã‚’è©•ä¾¡
"""

import os
import sys
import time
import json
import psutil
import gc
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import subprocess
from dataclasses import dataclass

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™"""
    start_time: float
    end_time: float
    duration: float
    memory_before: float
    memory_after: float
    memory_peak: float
    cpu_percent: float
    status: str

class PerformanceMonitor:
    """Phase 2/3.1å°‚é–€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
    
    def __init__(self):
        self.monitoring_dir = Path("logs/monitoring")
        self.monitoring_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤
        self.performance_thresholds = {
            "phase2_max_seconds": 30.0,      # Phase 2æœ€å¤§å‡¦ç†æ™‚é–“
            "phase31_max_seconds": 20.0,     # Phase 3.1æœ€å¤§å‡¦ç†æ™‚é–“
            "memory_growth_mb": 100.0,       # ãƒ¡ãƒ¢ãƒªå¢—åŠ é‡ä¸Šé™
            "cpu_max_percent": 80.0,         # CPUä½¿ç”¨ç‡ä¸Šé™
            "response_time_seconds": 5.0     # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ä¸Šé™
        }
    
    def measure_system_baseline(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š"""
        
        print("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š...")
        
        baseline = {
            "timestamp": datetime.now().isoformat(),
            "memory": {},
            "cpu": {},
            "disk": {},
            "status": "ok"
        }
        
        try:
            # ãƒ¡ãƒ¢ãƒªæƒ…å ±
            memory = psutil.virtual_memory()
            baseline["memory"] = {
                "total_gb": round(memory.total / (1024**3), 2),
                "used_gb": round(memory.used / (1024**3), 2),
                "available_gb": round(memory.available / (1024**3), 2),
                "percent": memory.percent
            }
            
            # CPUæƒ…å ±
            cpu_percent = psutil.cpu_percent(interval=1)
            baseline["cpu"] = {
                "percent": cpu_percent,
                "count": psutil.cpu_count(),
                "frequency": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {}
            }
            
            # ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±
            disk = psutil.disk_usage('.')
            baseline["disk"] = {
                "total_gb": round(disk.total / (1024**3), 2),
                "used_gb": round(disk.used / (1024**3), 2),
                "free_gb": round(disk.free / (1024**3), 2),
                "percent": round((disk.used / disk.total) * 100, 1)
            }
            
            print(f"  âœ… ãƒ¡ãƒ¢ãƒª: {baseline['memory']['used_gb']:.1f}GB/{baseline['memory']['total_gb']:.1f}GB")
            print(f"  âœ… CPU: {baseline['cpu']['percent']:.1f}%")
            print(f"  âœ… ãƒ‡ã‚£ã‚¹ã‚¯: {baseline['disk']['percent']:.1f}%ä½¿ç”¨")
            
        except Exception as e:
            baseline["error"] = str(e)
            baseline["status"] = "error"
            print(f"  âŒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        return baseline
    
    def measure_component_performance(self, component_name: str, test_function) -> PerformanceMetrics:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ€§èƒ½æ¸¬å®š"""
        
        # æ¸¬å®šå‰ã®ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
        gc.collect()
        
        # æ¸¬å®šé–‹å§‹
        process = psutil.Process()
        memory_before = process.memory_info().rss / (1024**2)  # MB
        cpu_before = process.cpu_percent()
        start_time = time.time()
        
        status = "ok"
        error_message = None
        
        try:
            # æ¸¬å®šå¯¾è±¡å®Ÿè¡Œ
            test_function()
            
        except Exception as e:
            status = "error"
            error_message = str(e)
            print(f"    âŒ {component_name} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¸¬å®šçµ‚äº†
        end_time = time.time()
        memory_after = process.memory_info().rss / (1024**2)  # MB
        cpu_after = process.cpu_percent()
        
        duration = end_time - start_time
        memory_peak = max(memory_before, memory_after)
        cpu_percent = max(cpu_before, cpu_after)
        
        return PerformanceMetrics(
            start_time=start_time,
            end_time=end_time,
            duration=duration,
            memory_before=memory_before,
            memory_after=memory_after,
            memory_peak=memory_peak,
            cpu_percent=cpu_percent,
            status=status
        )
    
    def test_phase2_performance(self) -> Dict[str, Any]:
        """Phase 2ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        
        print("ğŸ” Phase 2 FactExtractorPrototypeæ€§èƒ½æ¸¬å®š...")
        
        results = {
            "component": "Phase 2 FactExtractorPrototype",
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "status": "ok"
        }
        
        def test_import_performance():
            """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
            try:
                import sys
                if 'shift_suite.tasks.fact_extractor_prototype' in sys.modules:
                    del sys.modules['shift_suite.tasks.fact_extractor_prototype']
                from shift_suite.tasks.fact_extractor_prototype import FactExtractorPrototype
                return True
            except Exception as e:
                print(f"      ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        def test_syntax_check_performance():
            """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
            try:
                result = subprocess.run([
                    sys.executable, "-m", "py_compile", 
                    "shift_suite/tasks/fact_extractor_prototype.py"
                ], capture_output=True, timeout=10)
                return result.returncode == 0
            except Exception as e:
                print(f"      æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        def test_slot_hours_calculation():
            """SLOT_HOURSè¨ˆç®—æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
            try:
                # åŸºæœ¬çš„ãªSLOT_HOURSè¨ˆç®—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                SLOT_HOURS = 0.5
                test_cases = [
                    (8, 4.0),    # 4æ™‚é–“å‹¤å‹™
                    (16, 8.0),   # 8æ™‚é–“å‹¤å‹™  
                    (320, 160.0) # æœˆ160æ™‚é–“å‹¤å‹™
                ]
                
                for slots, expected_hours in test_cases:
                    calculated = slots * SLOT_HOURS
                    if abs(calculated - expected_hours) > 0.01:
                        return False
                return True
            except Exception as e:
                print(f"      è¨ˆç®—ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_functions = [
            ("ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", test_import_performance),
            ("æ§‹æ–‡ãƒã‚§ãƒƒã‚¯", test_syntax_check_performance),
            ("SLOT_HOURSè¨ˆç®—", test_slot_hours_calculation)
        ]
        
        for test_name, test_func in test_functions:
            print(f"  ğŸ“Š {test_name}ãƒ†ã‚¹ãƒˆ: ", end="")
            metrics = self.measure_component_performance(f"Phase2_{test_name}", test_func)
            
            results["tests"][test_name] = {
                "duration_seconds": round(metrics.duration, 3),
                "memory_used_mb": round(metrics.memory_after - metrics.memory_before, 2),
                "memory_peak_mb": round(metrics.memory_peak, 2),
                "cpu_percent": round(metrics.cpu_percent, 1),
                "status": metrics.status,
                "threshold_check": {
                    "duration_ok": metrics.duration < self.performance_thresholds["phase2_max_seconds"],
                    "memory_ok": (metrics.memory_after - metrics.memory_before) < self.performance_thresholds["memory_growth_mb"],
                    "cpu_ok": metrics.cpu_percent < self.performance_thresholds["cpu_max_percent"]
                }
            }
            
            if metrics.status == "ok" and all(results["tests"][test_name]["threshold_check"].values()):
                print(f"âœ… {metrics.duration:.3f}s")
            else:
                print(f"âš ï¸ {metrics.duration:.3f}s")
                results["status"] = "warning"
        
        return results
    
    def test_phase31_performance(self) -> Dict[str, Any]:
        """Phase 3.1ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        
        print("ğŸ” Phase 3.1 LightweightAnomalyDetectoræ€§èƒ½æ¸¬å®š...")
        
        results = {
            "component": "Phase 3.1 LightweightAnomalyDetector",
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "status": "ok"
        }
        
        def test_import_performance():
            """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
            try:
                import sys
                if 'shift_suite.tasks.lightweight_anomaly_detector' in sys.modules:
                    del sys.modules['shift_suite.tasks.lightweight_anomaly_detector']
                from shift_suite.tasks.lightweight_anomaly_detector import LightweightAnomalyDetector
                return True
            except Exception as e:
                print(f"      ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        def test_syntax_check_performance():
            """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
            try:
                result = subprocess.run([
                    sys.executable, "-m", "py_compile",
                    "shift_suite/tasks/lightweight_anomaly_detector.py"
                ], capture_output=True, timeout=10)
                return result.returncode == 0
            except Exception as e:
                print(f"      æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        def test_anomaly_calculation():
            """ç•°å¸¸æ¤œçŸ¥è¨ˆç®—æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
            try:
                # ç•°å¸¸æ¤œçŸ¥è¨ˆç®—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                SLOT_HOURS = 0.5
                monthly_slots = [320, 340, 280, 360, 300]  # æœˆæ¬¡ã‚¹ãƒ­ãƒƒãƒˆæ•°
                monthly_hours = [slots * SLOT_HOURS for slots in monthly_slots]
                
                # ç°¡æ˜“ç•°å¸¸æ¤œçŸ¥
                avg_hours = sum(monthly_hours) / len(monthly_hours)
                for hours in monthly_hours:
                    if abs(hours - avg_hours) > avg_hours * 0.3:  # 30%ä»¥ä¸Šã®åå·®
                        pass  # ç•°å¸¸æ¤œçŸ¥ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                return True
            except Exception as e:
                print(f"      ç•°å¸¸æ¤œçŸ¥ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_functions = [
            ("ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", test_import_performance),
            ("æ§‹æ–‡ãƒã‚§ãƒƒã‚¯", test_syntax_check_performance),
            ("ç•°å¸¸æ¤œçŸ¥è¨ˆç®—", test_anomaly_calculation)
        ]
        
        for test_name, test_func in test_functions:
            print(f"  ğŸ“Š {test_name}ãƒ†ã‚¹ãƒˆ: ", end="")
            metrics = self.measure_component_performance(f"Phase31_{test_name}", test_func)
            
            results["tests"][test_name] = {
                "duration_seconds": round(metrics.duration, 3),
                "memory_used_mb": round(metrics.memory_after - metrics.memory_before, 2),
                "memory_peak_mb": round(metrics.memory_peak, 2),
                "cpu_percent": round(metrics.cpu_percent, 1),
                "status": metrics.status,
                "threshold_check": {
                    "duration_ok": metrics.duration < self.performance_thresholds["phase31_max_seconds"],
                    "memory_ok": (metrics.memory_after - metrics.memory_before) < self.performance_thresholds["memory_growth_mb"],
                    "cpu_ok": metrics.cpu_percent < self.performance_thresholds["cpu_max_percent"]
                }
            }
            
            if metrics.status == "ok" and all(results["tests"][test_name]["threshold_check"].values()):
                print(f"âœ… {metrics.duration:.3f}s")
            else:
                print(f"âš ï¸ {metrics.duration:.3f}s")
                results["status"] = "warning"
        
        return results
    
    def test_integration_performance(self) -> Dict[str, Any]:
        """çµ±åˆãƒã‚§ãƒ¼ãƒ³æ€§èƒ½æ¸¬å®š"""
        
        print("ğŸ” çµ±åˆãƒã‚§ãƒ¼ãƒ³æ€§èƒ½æ¸¬å®š...")
        
        results = {
            "component": "Integration Chain",
            "timestamp": datetime.now().isoformat(),
            "tests": {},
            "status": "ok"
        }
        
        def test_factbook_visualizer_import():
            """FactBookVisualizerçµ±åˆæ€§èƒ½"""
            try:
                import sys
                if 'shift_suite.tasks.fact_book_visualizer' in sys.modules:
                    del sys.modules['shift_suite.tasks.fact_book_visualizer']
                from shift_suite.tasks.fact_book_visualizer import FactBookVisualizer
                return True
            except Exception as e:
                print(f"      FactBookVisualizerã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        def test_dash_integration_import():
            """Dashçµ±åˆæ€§èƒ½"""
            try:
                import sys
                if 'shift_suite.tasks.dash_fact_book_integration' in sys.modules:
                    del sys.modules['shift_suite.tasks.dash_fact_book_integration']
                from shift_suite.tasks.dash_fact_book_integration import DashFactBookIntegration
                return True
            except Exception as e:
                print(f"      Dashçµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        def test_end_to_end_simulation():
            """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
            try:
                # æ•°å€¤æ•´åˆæ€§ç¢ºèªï¼ˆ670æ™‚é–“åŸºæº–ï¼‰
                baseline_hours = 670
                calculated_hours = baseline_hours  # å®Ÿéš›ã®è¨ˆç®—çµæœ
                
                # SLOT_HOURSå¤‰æ›ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                SLOT_HOURS = 0.5
                slots = int(calculated_hours / SLOT_HOURS)
                reconverted_hours = slots * SLOT_HOURS
                
                return abs(reconverted_hours - baseline_hours) < 1.0
            except Exception as e:
                print(f"      ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        
        # å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_functions = [
            ("FactBookVisualizer", test_factbook_visualizer_import),
            ("Dashçµ±åˆ", test_dash_integration_import),
            ("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰", test_end_to_end_simulation)
        ]
        
        for test_name, test_func in test_functions:
            print(f"  ğŸ“Š {test_name}ãƒ†ã‚¹ãƒˆ: ", end="")
            metrics = self.measure_component_performance(f"Integration_{test_name}", test_func)
            
            results["tests"][test_name] = {
                "duration_seconds": round(metrics.duration, 3),
                "memory_used_mb": round(metrics.memory_after - metrics.memory_before, 2),
                "memory_peak_mb": round(metrics.memory_peak, 2),
                "cpu_percent": round(metrics.cpu_percent, 1),
                "status": metrics.status,
                "threshold_check": {
                    "duration_ok": metrics.duration < self.performance_thresholds["response_time_seconds"],
                    "memory_ok": (metrics.memory_after - metrics.memory_before) < self.performance_thresholds["memory_growth_mb"],
                    "cpu_ok": metrics.cpu_percent < self.performance_thresholds["cpu_max_percent"]
                }
            }
            
            if metrics.status == "ok" and all(results["tests"][test_name]["threshold_check"].values()):
                print(f"âœ… {metrics.duration:.3f}s")
            else:
                print(f"âš ï¸ {metrics.duration:.3f}s")
                results["status"] = "warning"
        
        return results
    
    def analyze_performance_trends(self, all_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ"""
        
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "overall_performance": "excellent",
            "phase2_performance": "unknown",
            "phase31_performance": "unknown", 
            "integration_performance": "unknown",
            "bottlenecks": [],
            "recommendations": [],
            "thresholds_status": {
                "all_within_limits": True,
                "warnings": [],
                "errors": []
            }
        }
        
        total_duration = 0.0
        total_memory = 0.0
        test_count = 0
        
        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ†æ
        for result in all_results:
            if "tests" in result:
                component = result["component"]
                
                # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
                if "Phase 2" in component:
                    analysis["phase2_performance"] = result["status"]
                elif "Phase 3.1" in component:
                    analysis["phase31_performance"] = result["status"]
                elif "Integration" in component:
                    analysis["integration_performance"] = result["status"]
                
                # å€‹åˆ¥ãƒ†ã‚¹ãƒˆåˆ†æ
                for test_name, test_data in result["tests"].items():
                    total_duration += test_data["duration_seconds"]
                    total_memory += test_data["memory_used_mb"]
                    test_count += 1
                    
                    # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                    thresholds = test_data.get("threshold_check", {})
                    if not all(thresholds.values()):
                        analysis["thresholds_status"]["all_within_limits"] = False
                        
                        if test_data["status"] == "error":
                            analysis["thresholds_status"]["errors"].append(f"{component}:{test_name}")
                        else:
                            analysis["thresholds_status"]["warnings"].append(f"{component}:{test_name}")
                    
                    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
                    if test_data["duration_seconds"] > 2.0:
                        analysis["bottlenecks"].append({
                            "component": component,
                            "test": test_name,
                            "duration": test_data["duration_seconds"],
                            "type": "å‡¦ç†æ™‚é–“"
                        })
                    
                    if test_data["memory_used_mb"] > 50.0:
                        analysis["bottlenecks"].append({
                            "component": component,
                            "test": test_name,
                            "memory": test_data["memory_used_mb"],
                            "type": "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡"
                        })
        
        # å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        avg_duration = total_duration / test_count if test_count > 0 else 0
        avg_memory = total_memory / test_count if test_count > 0 else 0
        
        if analysis["thresholds_status"]["errors"]:
            analysis["overall_performance"] = "poor"
        elif analysis["thresholds_status"]["warnings"] or avg_duration > 1.0:
            analysis["overall_performance"] = "acceptable"
        elif avg_duration < 0.5 and avg_memory < 10.0:
            analysis["overall_performance"] = "excellent"
        else:
            analysis["overall_performance"] = "good"
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        if analysis["bottlenecks"]:
            analysis["recommendations"].append("ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è¦å› ã®è©³ç´°èª¿æŸ»")
        
        if analysis["overall_performance"] in ["poor", "acceptable"]:
            analysis["recommendations"].append("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ¤œè¨")
        
        if not analysis["thresholds_status"]["all_within_limits"]:
            analysis["recommendations"].append("é–¾å€¤èª¿æ•´ã¾ãŸã¯å‡¦ç†æ”¹å–„")
        
        if analysis["overall_performance"] == "excellent":
            analysis["recommendations"].append("ç¾è¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç¶­æŒ")
            analysis["recommendations"].append("A3.1.4ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã¸ã®é€²è¡Œ")
        
        return analysis
    
    def generate_performance_report(self, baseline: Dict[str, Any], all_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        performance_icons = {
            "excellent": "ğŸŸ¢",
            "good": "ğŸŸ¡",
            "acceptable": "ğŸŸ ",
            "poor": "ğŸ”´"
        }
        
        perf_icon = performance_icons.get(analysis["overall_performance"], "â“")
        
        report = f"""
âš¡ **A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ**
å®Ÿè¡Œæ—¥æ™‚: {analysis['timestamp']}
ç·åˆè©•ä¾¡: {perf_icon} {analysis['overall_performance'].upper()}

ğŸ“Š **ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**
- ãƒ¡ãƒ¢ãƒª: {baseline['memory']['used_gb']:.1f}GB/{baseline['memory']['total_gb']:.1f}GB ({baseline['memory']['percent']:.1f}%)
- CPU: {baseline['cpu']['percent']:.1f}%
- ãƒ‡ã‚£ã‚¹ã‚¯: {baseline['disk']['percent']:.1f}%ä½¿ç”¨

ğŸ¯ **ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥æ€§èƒ½**
- Phase 2: {analysis['phase2_performance']} 
- Phase 3.1: {analysis['phase31_performance']}
- çµ±åˆãƒã‚§ãƒ¼ãƒ³: {analysis['integration_performance']}

ğŸ“ˆ **è©³ç´°æ¸¬å®šçµæœ**"""

        for result in all_results:
            if "tests" in result:
                report += f"\n\n**{result['component']}**"
                for test_name, test_data in result["tests"].items():
                    status_icon = "âœ…" if test_data["status"] == "ok" and all(test_data["threshold_check"].values()) else "âš ï¸"
                    report += f"""
  {status_icon} {test_name}: {test_data['duration_seconds']:.3f}s, {test_data['memory_used_mb']:.1f}MB"""

        if analysis["bottlenecks"]:
            report += f"""

ğŸ” **æ¤œå‡ºã•ã‚ŒãŸãƒœãƒˆãƒ«ãƒãƒƒã‚¯**"""
            for bottleneck in analysis["bottlenecks"][:3]:
                if bottleneck["type"] == "å‡¦ç†æ™‚é–“":
                    report += f"\n- {bottleneck['component']}: {bottleneck['duration']:.3f}s (å‡¦ç†æ™‚é–“)"
                else:
                    report += f"\n- {bottleneck['component']}: {bottleneck['memory']:.1f}MB (ãƒ¡ãƒ¢ãƒª)"

        report += f"""

ğŸ’¡ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**"""
        
        if analysis["overall_performance"] == "poor":
            report += """
ğŸš¨ æ€§èƒ½å•é¡ŒãŒã‚ã‚Šã¾ã™:
  1. ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è¦å› ã®è©³ç´°èª¿æŸ»
  2. å‡¦ç†æœ€é©åŒ–ã®å®Ÿæ–½
  3. ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª"""
        elif analysis["overall_performance"] == "acceptable":
            report += """
âš ï¸ æ€§èƒ½æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™:
  1. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®æ¤œè¨
  2. é–¾å€¤è¨­å®šã®è¦‹ç›´ã—
  3. ç¶™ç¶šç›£è¦–ã®å¼·åŒ–"""
        else:
            report += """
âœ… å„ªç§€ãªæ€§èƒ½ã§ã™:
  1. ç¾è¡Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç¶­æŒ
  2. A3.1.4 ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šã¸ã®é€²è¡Œ
  3. å®šæœŸçš„ãªæ€§èƒ½ç›£è¦–ç¶™ç¶š"""

        # SLOT_HOURSä¿®æ­£ã®å½±éŸ¿è©•ä¾¡
        report += f"""

ğŸ¯ **SLOT_HOURSä¿®æ­£æ€§èƒ½å½±éŸ¿è©•ä¾¡**
Phase 2/3.1ã®SLOT_HOURSä¹—ç®—å‡¦ç†ã¯è»½é‡ã§ã€ã‚·ã‚¹ãƒ†ãƒ æ€§èƒ½ã¸ã®
è² è·ã¯æœ€å°é™ã§ã™ã€‚è¨ˆç®—ç²¾åº¦å‘ä¸Šã¨æ€§èƒ½ç¶­æŒã®ä¸¡ç«‹ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚"""
        
        return report
    
    def save_performance_results(self, baseline: Dict[str, Any], all_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–çµæœä¿å­˜"""
        
        result_file = self.monitoring_dir / f"performance_monitoring_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        monitoring_data = {
            "monitoring_version": "performance_1.0",
            "timestamp": datetime.now().isoformat(),
            "baseline": baseline,
            "component_results": all_results,
            "analysis": analysis,
            "thresholds": self.performance_thresholds,
            "metadata": {
                "monitoring_tool": "A3_PERFORMANCE_MONITOR",
                "focus": "Phase 2/3.1 SLOT_HOURSä¿®æ­£å½±éŸ¿",
                "optimization_perspective": "whole_system"
            }
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(monitoring_data, f, indent=2, ensure_ascii=False)
        
        return str(result_file)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    print("âš¡ A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
    print("ğŸ¯ Phase 2/3.1å‡¦ç†æ™‚é–“ãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–")
    print("ğŸ¨ å…¨ä½“æœ€é©åŒ–è¦³ç‚¹ã§ã®SLOT_HOURSä¿®æ­£æ€§èƒ½è©•ä¾¡")
    print("=" * 80)
    
    try:
        monitor = PerformanceMonitor()
        
        # 1. ã‚·ã‚¹ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®š
        print("\n" + "=" * 60)
        baseline = monitor.measure_system_baseline()
        
        # 2. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥æ€§èƒ½æ¸¬å®š
        all_results = []
        
        print("\n" + "=" * 60)
        phase2_results = monitor.test_phase2_performance()
        all_results.append(phase2_results)
        
        print("\n" + "=" * 60)
        phase31_results = monitor.test_phase31_performance()
        all_results.append(phase31_results)
        
        print("\n" + "=" * 60)
        integration_results = monitor.test_integration_performance()
        all_results.append(integration_results)
        
        # 3. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ
        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ...")
        analysis = monitor.analyze_performance_trends(all_results)
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ“‹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        
        report = monitor.generate_performance_report(baseline, all_results, analysis)
        print(report)
        
        # 5. çµæœä¿å­˜
        result_file = monitor.save_performance_results(baseline, all_results, analysis)
        print(f"\nğŸ“ ç›£è¦–çµæœä¿å­˜: {result_file}")
        
        # 6. æˆåŠŸåˆ¤å®š
        success = analysis["overall_performance"] in ["excellent", "good", "acceptable"]
        status_text = "âœ… å®Œäº†" if success else "âŒ è¦æ”¹å–„"
        print(f"\nğŸ¯ A3.1.3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–: {status_text}")
        
        return success
        
    except Exception as e:
        print(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)