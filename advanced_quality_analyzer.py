#!/usr/bin/env python3
"""
é«˜åº¦å“è³ªåˆ†æã‚·ã‚¹ãƒ†ãƒ 

MECEã‚·ã‚¹ãƒ†ãƒ ã®å“è³ªã‚’è©³ç´°ã«åˆ†æã—ã€å…·ä½“çš„ãªæ”¹å–„ç‚¹ã‚’ç‰¹å®š
"""

import pandas as pd
import numpy as np
import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
from collections import defaultdict, Counter
import re

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)


class AdvancedQualityAnalyzer:
    """é«˜åº¦å“è³ªåˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.quality_dimensions = {
            'completeness': 0.0,     # ç¶²ç¾…æ€§
            'specificity': 0.0,      # å…·ä½“æ€§
            'actionability': 0.0,    # å®Ÿè¡Œå¯èƒ½æ€§
            'consistency': 0.0,      # ä¸€è²«æ€§
            'verifiability': 0.0,    # æ¤œè¨¼å¯èƒ½æ€§
            'usability': 0.0         # ä½¿ã„ã‚„ã™ã•
        }
        
    def analyze_comprehensive_quality(self, mece_results: Dict[int, Dict]) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„å“è³ªåˆ†æ"""
        log.info("ğŸ” åŒ…æ‹¬çš„å“è³ªåˆ†æé–‹å§‹...")
        
        analysis = {
            'overall_score': 0.0,
            'dimension_scores': {},
            'critical_issues': [],
            'improvement_opportunities': [],
            'detailed_findings': {},
            'actionable_recommendations': []
        }
        
        # å„å“è³ªæ¬¡å…ƒã®åˆ†æ
        analysis['dimension_scores']['completeness'] = self._analyze_completeness(mece_results)
        analysis['dimension_scores']['specificity'] = self._analyze_specificity(mece_results)
        analysis['dimension_scores']['actionability'] = self._analyze_actionability(mece_results)
        analysis['dimension_scores']['consistency'] = self._analyze_consistency(mece_results)
        analysis['dimension_scores']['verifiability'] = self._analyze_verifiability(mece_results)
        analysis['dimension_scores']['usability'] = self._analyze_usability(mece_results)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = analysis['dimension_scores']
        analysis['overall_score'] = np.mean(list(scores.values()))
        
        # é‡è¦èª²é¡Œã®ç‰¹å®š
        analysis['critical_issues'] = self._identify_critical_issues(scores, mece_results)
        
        # æ”¹å–„æ©Ÿä¼šã®ç™ºè¦‹
        analysis['improvement_opportunities'] = self._find_improvement_opportunities(scores, mece_results)
        
        # è©³ç´°æ‰€è¦‹
        analysis['detailed_findings'] = self._generate_detailed_findings(scores, mece_results)
        
        # å®Ÿè¡Œå¯èƒ½ãªæ¨å¥¨äº‹é …
        analysis['actionable_recommendations'] = self._generate_actionable_recommendations(analysis)
        
        log.info(f"âœ… å“è³ªåˆ†æå®Œäº† - ç·åˆã‚¹ã‚³ã‚¢: {analysis['overall_score']:.1%}")
        return analysis
    
    def _analyze_completeness(self, mece_results: Dict[int, Dict]) -> float:
        """ç¶²ç¾…æ€§åˆ†æ"""
        log.info("ğŸ“Š ç¶²ç¾…æ€§åˆ†æä¸­...")
        
        # æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°
        expected_categories_per_axis = 8
        total_expected = len(mece_results) * expected_categories_per_axis
        
        # å®Ÿéš›ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°
        actual_categories = 0
        empty_categories = 0
        
        for axis_num, results in mece_results.items():
            if results and 'human_readable' in results:
                hr_data = results['human_readable']
                if 'MECEåˆ†è§£äº‹å®Ÿ' in hr_data:
                    mece_facts = hr_data['MECEåˆ†è§£äº‹å®Ÿ']
                    actual_categories += len(mece_facts)
                    
                    # ç©ºã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
                    for category, facts in mece_facts.items():
                        if isinstance(facts, list) and len(facts) == 0:
                            empty_categories += 1
                        elif isinstance(facts, dict) and len(facts) == 0:
                            empty_categories += 1
        
        # ç¶²ç¾…æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        category_coverage = actual_categories / total_expected if total_expected > 0 else 0
        content_coverage = (actual_categories - empty_categories) / actual_categories if actual_categories > 0 else 0
        
        completeness_score = (category_coverage * 0.6) + (content_coverage * 0.4)
        
        log.info(f"  ã‚«ãƒ†ã‚´ãƒªãƒ¼ç¶²ç¾…ç‡: {category_coverage:.1%}")
        log.info(f"  ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å……å®Ÿç‡: {content_coverage:.1%}")
        log.info(f"  ç¶²ç¾…æ€§ã‚¹ã‚³ã‚¢: {completeness_score:.1%}")
        
        return completeness_score
    
    def _analyze_specificity(self, mece_results: Dict[int, Dict]) -> float:
        """å…·ä½“æ€§åˆ†æ"""
        log.info("ğŸ¯ å…·ä½“æ€§åˆ†æä¸­...")
        
        total_constraints = 0
        specific_constraints = 0
        
        # å…·ä½“æ€§ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        specific_keywords = [
            'æ™‚é–“', 'äººæ•°', 'å', 'å›', 'æ—¥', 'é€±', 'æœˆ', 'å¹´',
            'ä»¥ä¸Š', 'ä»¥ä¸‹', 'æœªæº€', 'æœ€ä½', 'æœ€å¤§', 'å¹³å‡',
            '%', 'å‰²åˆ', 'æ¯”ç‡', 'IF', 'THEN', 'æ¡ä»¶'
        ]
        
        vague_keywords = [
            'é©åˆ‡', 'ååˆ†', 'å¿…è¦', 'é‡è¦', 'åŸºæœ¬', 'ä¸€èˆ¬',
            'é€šå¸¸', 'æ¨™æº–', 'æ¨å¥¨', 'æœ›ã¾ã—ã„'
        ]
        
        for axis_num, results in mece_results.items():
            if results and 'machine_readable' in results:
                mr_data = results['machine_readable']
                
                for constraint_type in ['hard_constraints', 'soft_constraints', 'preferences']:
                    constraints = mr_data.get(constraint_type, [])
                    for constraint in constraints:
                        if isinstance(constraint, dict):
                            total_constraints += 1
                            
                            constraint_text = str(constraint.get('rule', '')) + str(constraint.get('constraint', ''))
                            
                            # å…·ä½“æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
                            specific_count = sum(1 for keyword in specific_keywords if keyword in constraint_text)
                            vague_count = sum(1 for keyword in vague_keywords if keyword in constraint_text)
                            
                            # æ•°å€¤ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                            has_numbers = bool(re.search(r'\d+', constraint_text))
                            
                            # IF-THENæ§‹é€ ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                            has_if_then = 'execution_rule' in constraint and constraint['execution_rule'].get('condition')
                            
                            # å…·ä½“æ€§åˆ¤å®š
                            if (specific_count >= 2 or has_numbers or has_if_then) and vague_count <= 1:
                                specific_constraints += 1
        
        specificity_score = specific_constraints / total_constraints if total_constraints > 0 else 0
        
        log.info(f"  ç·åˆ¶ç´„æ•°: {total_constraints}")
        log.info(f"  å…·ä½“çš„åˆ¶ç´„æ•°: {specific_constraints}")
        log.info(f"  å…·ä½“æ€§ã‚¹ã‚³ã‚¢: {specificity_score:.1%}")
        
        return specificity_score
    
    def _analyze_actionability(self, mece_results: Dict[int, Dict]) -> float:
        """å®Ÿè¡Œå¯èƒ½æ€§åˆ†æ"""
        log.info("âš¡ å®Ÿè¡Œå¯èƒ½æ€§åˆ†æä¸­...")
        
        total_constraints = 0
        actionable_constraints = 0
        high_actionability = 0
        
        for axis_num, results in mece_results.items():
            if results and 'machine_readable' in results:
                mr_data = results['machine_readable']
                
                for constraint_type in ['hard_constraints', 'soft_constraints', 'preferences']:
                    constraints = mr_data.get(constraint_type, [])
                    for constraint in constraints:
                        if isinstance(constraint, dict):
                            total_constraints += 1
                            
                            # å®Ÿè¡Œå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
                            actionability_score = constraint.get('actionability_score', 0)
                            if actionability_score >= 0.5:
                                actionable_constraints += 1
                            if actionability_score >= 0.8:
                                high_actionability += 1
        
        actionability_rate = actionable_constraints / total_constraints if total_constraints > 0 else 0
        high_actionability_rate = high_actionability / total_constraints if total_constraints > 0 else 0
        
        # ç·åˆå®Ÿè¡Œå¯èƒ½æ€§ã‚¹ã‚³ã‚¢
        overall_actionability = (actionability_rate * 0.7) + (high_actionability_rate * 0.3)
        
        log.info(f"  å®Ÿè¡Œå¯èƒ½åˆ¶ç´„ç‡: {actionability_rate:.1%}")
        log.info(f"  é«˜å®Ÿè¡Œå¯èƒ½åˆ¶ç´„ç‡: {high_actionability_rate:.1%}")
        log.info(f"  å®Ÿè¡Œå¯èƒ½æ€§ã‚¹ã‚³ã‚¢: {overall_actionability:.1%}")
        
        return overall_actionability
    
    def _analyze_consistency(self, mece_results: Dict[int, Dict]) -> float:
        """ä¸€è²«æ€§åˆ†æ"""
        log.info("ğŸ”— ä¸€è²«æ€§åˆ†æä¸­...")
        
        # åˆ¶ç´„ã‚¿ã‚¤ãƒ—ã®ä¸€è²«æ€§
        constraint_types = defaultdict(int)
        rule_patterns = defaultdict(int)
        confidence_levels = []
        
        for axis_num, results in mece_results.items():
            if results and 'machine_readable' in results:
                mr_data = results['machine_readable']
                
                for constraint_type in ['hard_constraints', 'soft_constraints', 'preferences']:
                    constraints = mr_data.get(constraint_type, [])
                    for constraint in constraints:
                        if isinstance(constraint, dict):
                            # ã‚¿ã‚¤ãƒ—ã®ä¸€è²«æ€§
                            c_type = constraint.get('type', 'unknown')
                            constraint_types[c_type] += 1
                            
                            # ãƒ«ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¸€è²«æ€§
                            rule = constraint.get('rule', '')
                            if len(rule) > 10:
                                rule_pattern = rule[:20] + "..."
                                rule_patterns[rule_pattern] += 1
                            
                            # ä¿¡é ¼åº¦ã®ä¸€è²«æ€§
                            confidence = constraint.get('confidence', 0)
                            confidence_levels.append(confidence)
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        type_consistency = len(constraint_types) / max(sum(constraint_types.values()), 1)
        
        # ä¿¡é ¼åº¦ã®åˆ†æ•£ï¼ˆä½ã„ã»ã©ä¸€è²«æ€§ãŒé«˜ã„ï¼‰
        confidence_std = np.std(confidence_levels) if confidence_levels else 1.0
        confidence_consistency = 1.0 - min(confidence_std, 1.0)
        
        # æ§‹é€ ã®ä¸€è²«æ€§ï¼ˆåŒã˜ã‚­ãƒ¼ã‚’æŒã¤åˆ¶ç´„ã®å‰²åˆï¼‰
        structure_consistency = self._calculate_structure_consistency(mece_results)
        
        overall_consistency = (type_consistency * 0.3) + (confidence_consistency * 0.3) + (structure_consistency * 0.4)
        
        log.info(f"  ã‚¿ã‚¤ãƒ—ä¸€è²«æ€§: {type_consistency:.1%}")
        log.info(f"  ä¿¡é ¼åº¦ä¸€è²«æ€§: {confidence_consistency:.1%}")
        log.info(f"  æ§‹é€ ä¸€è²«æ€§: {structure_consistency:.1%}")
        log.info(f"  ä¸€è²«æ€§ã‚¹ã‚³ã‚¢: {overall_consistency:.1%}")
        
        return overall_consistency
    
    def _analyze_verifiability(self, mece_results: Dict[int, Dict]) -> float:
        """æ¤œè¨¼å¯èƒ½æ€§åˆ†æ"""
        log.info("âœ… æ¤œè¨¼å¯èƒ½æ€§åˆ†æä¸­...")
        
        total_constraints = 0
        verifiable_constraints = 0
        
        for axis_num, results in mece_results.items():
            if results and 'machine_readable' in results:
                mr_data = results['machine_readable']
                
                for constraint_type in ['hard_constraints', 'soft_constraints', 'preferences']:
                    constraints = mr_data.get(constraint_type, [])
                    for constraint in constraints:
                        if isinstance(constraint, dict):
                            total_constraints += 1
                            
                            # æ¤œè¨¼å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
                            has_verification = constraint.get('verification_method') is not None
                            has_quantified_criteria = constraint.get('quantified_criteria') is not None
                            has_confidence = constraint.get('confidence', 0) > 0
                            
                            if has_verification or (has_quantified_criteria and has_confidence):
                                verifiable_constraints += 1
        
        verifiability_score = verifiable_constraints / total_constraints if total_constraints > 0 else 0
        
        log.info(f"  æ¤œè¨¼å¯èƒ½åˆ¶ç´„æ•°: {verifiable_constraints}/{total_constraints}")
        log.info(f"  æ¤œè¨¼å¯èƒ½æ€§ã‚¹ã‚³ã‚¢: {verifiability_score:.1%}")
        
        return verifiability_score
    
    def _analyze_usability(self, mece_results: Dict[int, Dict]) -> float:
        """ä½¿ã„ã‚„ã™ã•åˆ†æ"""
        log.info("ğŸ‘¥ ä½¿ã„ã‚„ã™ã•åˆ†æä¸­...")
        
        # å¯èª­æ€§ã‚¹ã‚³ã‚¢
        readability_score = self._calculate_readability(mece_results)
        
        # æ§‹é€ åŒ–ã‚¹ã‚³ã‚¢
        structure_score = self._calculate_structure_quality(mece_results)
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã‚¹ã‚³ã‚¢
        documentation_score = self._calculate_documentation_quality(mece_results)
        
        usability_score = (readability_score * 0.4) + (structure_score * 0.3) + (documentation_score * 0.3)
        
        log.info(f"  å¯èª­æ€§: {readability_score:.1%}")
        log.info(f"  æ§‹é€ åŒ–: {structure_score:.1%}")
        log.info(f"  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–: {documentation_score:.1%}")
        log.info(f"  ä½¿ã„ã‚„ã™ã•ã‚¹ã‚³ã‚¢: {usability_score:.1%}")
        
        return usability_score
    
    def _calculate_structure_consistency(self, mece_results: Dict[int, Dict]) -> float:
        """æ§‹é€ ä¸€è²«æ€§ã®è¨ˆç®—"""
        required_keys = ['type', 'rule', 'confidence']
        total_constraints = 0
        consistent_constraints = 0
        
        for axis_num, results in mece_results.items():
            if results and 'machine_readable' in results:
                mr_data = results['machine_readable']
                
                for constraint_type in ['hard_constraints', 'soft_constraints', 'preferences']:
                    constraints = mr_data.get(constraint_type, [])
                    for constraint in constraints:
                        if isinstance(constraint, dict):
                            total_constraints += 1
                            
                            # å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                            has_required_keys = all(key in constraint for key in required_keys)
                            if has_required_keys:
                                consistent_constraints += 1
        
        return consistent_constraints / total_constraints if total_constraints > 0 else 0
    
    def _calculate_readability(self, mece_results: Dict[int, Dict]) -> float:
        """å¯èª­æ€§ã®è¨ˆç®—"""
        total_text_length = 0
        readable_text_count = 0
        
        for axis_num, results in mece_results.items():
            if results and 'human_readable' in results:
                hr_data = results['human_readable']
                
                # æ—¥æœ¬èªã®å¯èª­æ€§ãƒã‚§ãƒƒã‚¯
                text_content = str(hr_data)
                total_text_length += len(text_content)
                
                # é©åˆ‡ãªé•·ã•ï¼ˆçŸ­ã™ããšé•·ã™ããªã„ï¼‰
                if 50 <= len(text_content) <= 2000:
                    readable_text_count += 1
        
        return readable_text_count / len(mece_results) if mece_results else 0
    
    def _calculate_structure_quality(self, mece_results: Dict[int, Dict]) -> float:
        """æ§‹é€ å“è³ªã®è¨ˆç®—"""
        required_sections = ['human_readable', 'machine_readable', 'extraction_metadata']
        total_axes = len(mece_results)
        well_structured_axes = 0
        
        for axis_num, results in mece_results.items():
            if results and all(section in results for section in required_sections):
                well_structured_axes += 1
        
        return well_structured_axes / total_axes if total_axes > 0 else 0
    
    def _calculate_documentation_quality(self, mece_results: Dict[int, Dict]) -> float:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå“è³ªã®è¨ˆç®—"""
        total_axes = len(mece_results)
        documented_axes = 0
        
        for axis_num, results in mece_results.items():
            if results and 'extraction_metadata' in results:
                metadata = results['extraction_metadata']
                
                # åŸºæœ¬çš„ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
                has_timestamp = 'extraction_timestamp' in metadata
                has_quality_info = 'data_quality' in metadata
                
                if has_timestamp and has_quality_info:
                    documented_axes += 1
        
        return documented_axes / total_axes if total_axes > 0 else 0
    
    def _identify_critical_issues(self, scores: Dict[str, float], mece_results: Dict[int, Dict]) -> List[Dict]:
        """é‡è¦èª²é¡Œã®ç‰¹å®š"""
        critical_issues = []
        
        # ã—ãã„å€¤æœªæº€ã®æ¬¡å…ƒã‚’ç‰¹å®š
        threshold = 0.6
        
        for dimension, score in scores.items():
            if score < threshold:
                issue = {
                    'dimension': dimension,
                    'current_score': score,
                    'severity': 'critical' if score < 0.4 else 'high',
                    'impact': self._assess_impact(dimension, score),
                    'specific_problems': self._identify_specific_problems(dimension, mece_results)
                }
                critical_issues.append(issue)
        
        return critical_issues
    
    def _find_improvement_opportunities(self, scores: Dict[str, float], mece_results: Dict[int, Dict]) -> List[Dict]:
        """æ”¹å–„æ©Ÿä¼šã®ç™ºè¦‹"""
        opportunities = []
        
        # æ”¹å–„å¯èƒ½æ€§ã®é«˜ã„é ˜åŸŸã‚’ç‰¹å®š
        for dimension, score in scores.items():
            if 0.5 <= score < 0.8:  # æ”¹å–„ä½™åœ°ãŒã‚ã‚‹ç¯„å›²
                opportunity = {
                    'dimension': dimension,
                    'current_score': score,
                    'target_score': min(0.9, score + 0.2),
                    'effort_level': self._estimate_effort(dimension, score),
                    'quick_wins': self._identify_quick_wins(dimension, mece_results)
                }
                opportunities.append(opportunity)
        
        return opportunities
    
    def _generate_detailed_findings(self, scores: Dict[str, float], mece_results: Dict[int, Dict]) -> Dict[str, Any]:
        """è©³ç´°æ‰€è¦‹ã®ç”Ÿæˆ"""
        findings = {}
        
        for dimension, score in scores.items():
            findings[dimension] = {
                'score': score,
                'grade': self._score_to_grade(score),
                'strengths': self._identify_strengths(dimension, mece_results),
                'weaknesses': self._identify_weaknesses(dimension, mece_results),
                'benchmark': self._get_benchmark(dimension)
            }
        
        return findings
    
    def _generate_actionable_recommendations(self, analysis: Dict[str, Any]) -> List[Dict]:
        """å®Ÿè¡Œå¯èƒ½ãªæ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # é‡è¦èª²é¡Œã¸ã®å¯¾å¿œ
        for issue in analysis['critical_issues']:
            rec = {
                'type': 'critical_fix',
                'dimension': issue['dimension'],
                'action': self._get_fix_action(issue['dimension']),
                'priority': 'high',
                'estimated_effort': 'medium',
                'expected_improvement': 0.2
            }
            recommendations.append(rec)
        
        # æ”¹å–„æ©Ÿä¼šã¸ã®å¯¾å¿œ
        for opportunity in analysis['improvement_opportunities']:
            rec = {
                'type': 'enhancement',
                'dimension': opportunity['dimension'],
                'action': self._get_enhancement_action(opportunity['dimension']),
                'priority': 'medium',
                'estimated_effort': opportunity['effort_level'],
                'expected_improvement': 0.1
            }
            recommendations.append(rec)
        
        return recommendations
    
    def _assess_impact(self, dimension: str, score: float) -> str:
        """å½±éŸ¿åº¦è©•ä¾¡"""
        impact_map = {
            'completeness': 'ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¶²ç¾…æ€§ã«é‡å¤§ãªå½±éŸ¿',
            'specificity': 'åˆ¶ç´„ã®å®Ÿç”¨æ€§ã«å¤§ããªå½±éŸ¿',
            'actionability': 'AIå®Ÿè£…æ™‚ã®å®Ÿè¡Œå¯èƒ½æ€§ã«è‡´å‘½çš„å½±éŸ¿',
            'consistency': 'ã‚·ã‚¹ãƒ†ãƒ ã®ä¿¡é ¼æ€§ã«å½±éŸ¿',
            'verifiability': 'å“è³ªä¿è¨¼ã«å½±éŸ¿',
            'usability': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã«å½±éŸ¿'
        }
        return impact_map.get(dimension, 'ä¸æ˜ãªå½±éŸ¿')
    
    def _identify_specific_problems(self, dimension: str, mece_results: Dict[int, Dict]) -> List[str]:
        """å…·ä½“çš„å•é¡Œã®ç‰¹å®š"""
        problems = []
        
        if dimension == 'completeness':
            problems.append("ç©ºã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ãŒå¤šæ•°å­˜åœ¨")
            problems.append("è»¸4-12ã§ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸è¶³")
        elif dimension == 'actionability':
            problems.append("æŠ½è±¡çš„ãªåˆ¶ç´„ãŒå¤šã„")
            problems.append("IF-THENæ§‹é€ ãŒä¸å®Œå…¨")
        elif dimension == 'specificity':
            problems.append("æ›–æ˜§ãªè¡¨ç¾ãŒå¤šç”¨ã•ã‚Œã¦ã„ã‚‹")
            problems.append("æ•°å€¤åŸºæº–ãŒä¸æ˜ç¢º")
        
        return problems
    
    def _estimate_effort(self, dimension: str, score: float) -> str:
        """ä½œæ¥­é‡æ¨å®š"""
        if score < 0.3:
            return 'high'
        elif score < 0.6:
            return 'medium'
        else:
            return 'low'
    
    def _identify_quick_wins(self, dimension: str, mece_results: Dict[int, Dict]) -> List[str]:
        """ã‚¯ã‚¤ãƒƒã‚¯ã‚¦ã‚£ãƒ³ã®ç‰¹å®š"""
        quick_wins = []
        
        if dimension == 'actionability':
            quick_wins.append("æ—¢å­˜åˆ¶ç´„ã«IF-THENæ§‹é€ ã‚’è¿½åŠ ")
            quick_wins.append("æ•°å€¤åŸºæº–ã®æ˜ç¢ºåŒ–")
        elif dimension == 'completeness':
            quick_wins.append("ç©ºã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿è¿½åŠ ")
            quick_wins.append("é¡ä¼¼åˆ¶ç´„ã®è¤‡è£½ãƒ»èª¿æ•´")
        
        return quick_wins
    
    def _score_to_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‚’ç­‰ç´šã«å¤‰æ›"""
        if score >= 0.9:
            return 'A+'
        elif score >= 0.8:
            return 'A'
        elif score >= 0.7:
            return 'B+'
        elif score >= 0.6:
            return 'B'
        elif score >= 0.5:
            return 'C+'
        elif score >= 0.4:
            return 'C'
        else:
            return 'D'
    
    def _identify_strengths(self, dimension: str, mece_results: Dict[int, Dict]) -> List[str]:
        """å¼·ã¿ã®ç‰¹å®š"""
        strengths = []
        
        if dimension == 'actionability':
            strengths.append("åˆ¶ç´„å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ ãŒå®Ÿè£…æ¸ˆã¿")
        elif dimension == 'consistency':
            strengths.append("åŸºæœ¬æ§‹é€ ãŒçµ±ä¸€ã•ã‚Œã¦ã„ã‚‹")
        
        return strengths
    
    def _identify_weaknesses(self, dimension: str, mece_results: Dict[int, Dict]) -> List[str]:
        """å¼±ã¿ã®ç‰¹å®š"""
        weaknesses = []
        
        if dimension == 'completeness':
            weaknesses.append("ã‚«ãƒ†ã‚´ãƒªãƒ¼ä¸è¶³")
        elif dimension == 'specificity':
            weaknesses.append("æŠ½è±¡çš„è¡¨ç¾ãŒå¤šã„")
        
        return weaknesses
    
    def _get_benchmark(self, dimension: str) -> Dict[str, float]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å–å¾—"""
        benchmarks = {
            'completeness': {'industry_standard': 0.85, 'best_practice': 0.95},
            'specificity': {'industry_standard': 0.70, 'best_practice': 0.90},
            'actionability': {'industry_standard': 0.80, 'best_practice': 0.95},
            'consistency': {'industry_standard': 0.75, 'best_practice': 0.85},
            'verifiability': {'industry_standard': 0.70, 'best_practice': 0.85},
            'usability': {'industry_standard': 0.65, 'best_practice': 0.80}
        }
        return benchmarks.get(dimension, {'industry_standard': 0.70, 'best_practice': 0.85})
    
    def _get_fix_action(self, dimension: str) -> str:
        """ä¿®æ­£ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—"""
        actions = {
            'completeness': "ä¸è¶³ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å®Ÿè£…ã¨ç©ºã‚«ãƒ†ã‚´ãƒªãƒ¼ã®å……å®Ÿ",
            'specificity': "æŠ½è±¡çš„åˆ¶ç´„ã®å…·ä½“åŒ–ã¨æ•°å€¤åŸºæº–ã®æ˜ç¢ºåŒ–",
            'actionability': "IF-THENæ§‹é€ ã®å®Œå…¨å®Ÿè£…ã¨å®Ÿè¡Œå¯èƒ½æ€§å‘ä¸Š",
            'consistency': "åˆ¶ç´„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®çµ±ä¸€ã¨æ§‹é€ ã®æ¨™æº–åŒ–",
            'verifiability': "æ¤œè¨¼æ–¹æ³•ã®å®šç¾©ã¨å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½åŠ ",
            'usability': "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æ”¹å–„ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå……å®Ÿ"
        }
        return actions.get(dimension, "å…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œè¨")
    
    def _get_enhancement_action(self, dimension: str) -> str:
        """å¼·åŒ–ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å–å¾—"""
        actions = {
            'completeness': "è¿½åŠ ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®æ®µéšçš„å®Ÿè£…",
            'specificity': "åˆ¶ç´„ã®è©³ç´°åŒ–ã¨å…·ä½“ä¾‹ã®è¿½åŠ ",
            'actionability': "é«˜åº¦ãªIF-THENæ§‹é€ ã®å®Ÿè£…",
            'consistency': "åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¨™æº–åŒ–",
            'verifiability': "è‡ªå‹•æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŒ–",
            'usability': "ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®å®Ÿæ–½"
        }
        return actions.get(dimension, "æ®µéšçš„ãªæ”¹å–„ã®å®Ÿæ–½")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    analyzer = AdvancedQualityAnalyzer()
    log.info("é«˜åº¦å“è³ªåˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ")
    log.info("ä½¿ç”¨æ–¹æ³•: analyzer.analyze_comprehensive_quality(mece_results)")


if __name__ == "__main__":
    main()