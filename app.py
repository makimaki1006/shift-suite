#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  app.py  (Part 1 / 3)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Shift-Suite Streamlit GUI + å†…è”µãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰  v1.30.0 (ä¼‘æš‡åˆ†ææ©Ÿèƒ½è¿½åŠ )
# ==============================================================================
# å¤‰æ›´å±¥æ­´
#   â€¢ v1.30.0: ä¼‘æš‡åˆ†ææ©Ÿèƒ½ã‚’è¿½åŠ ã€‚leave_analyzer ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã®é€£æºã‚’å®Ÿè£…ã€‚
#   â€¢ v1.29.13: st.experimental_rerun() ã‚’ st.rerun() ã«ä¿®æ­£ã€‚
#   â€¢ v1.29.12: need_ref_start/end_date_widget ã® StreamlitAPIException å¯¾ç­–ã€‚
#               ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã®æ—¥ä»˜ç¯„å›²æ¨å®šçµæœã‚’ã€ãƒ•ãƒ©ã‚°ã‚’ç”¨ã„ã¦
#               æ¬¡å›ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã«ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦å®‰å…¨ã«åæ˜ ã™ã‚‹ã‚ˆã†ä¿®æ­£ã€‚
#   â€¢ v1.29.11: ãƒ­ã‚°ã§æŒ‡æ‘˜ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ç®‡æ‰€ã‚’ä¿®æ­£ã€‚
#               - shift_sheets_multiselect_widget ã® StreamlitAPIException å¯¾ç­–ã€‚
#               - param_penalty_per_lack ã® NameError ä¿®æ­£ã€‚
#               - progress_bar_exec_main_run ç­‰ã® NameError ä¿®æ­£ã€‚
#               - ãƒ­ã‚°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…ã®ã‚¿ã‚¤ãƒä¿®æ­£ã€‚
#   â€¢ v1.29.10: selectboxã®defaultå¼•æ•°ã‚¨ãƒ©ãƒ¼ã‚’indexã«çµ±ä¸€ã—ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰æ­£ã—ãå‚ç…§ã€‚
#               å…¨ã¦ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®å€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ç®¡ç†ã—ã€åˆæœŸåŒ–ã‚’å¾¹åº•ã€‚
#               ãƒ˜ãƒƒãƒ€ãƒ¼é–‹å§‹è¡ŒUIã®è¡¨ç¤ºã¨å€¤ã®åˆ©ç”¨ã‚’ç¢ºå®ŸåŒ–ã€‚
#               Excelæ—¥ä»˜ç¯„å›²æ¨å®šã®å®‰å®šåŒ–ã€‚
#               on_changeã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆç®¡ç†ã‚’ç›®æŒ‡ã™ã€‚
# ==============================================================================

from __future__ import annotations

import datetime
import io
import logging
import os

# Streamlit's watchdog can print repeated errors on Windows
# unless STREAMLIT_WATCHER_TYPE is set to "poll".
os.environ.setdefault("STREAMLIT_WATCHER_TYPE", "poll")

import re
import tempfile
import zipfile
import shutil
from pathlib import Path
from typing import IO, Optional
import json
import numpy as np

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import streamlit as st
import subprocess
from streamlit.runtime import exists as st_runtime_exists

try:
    from streamlit_plotly_events import plotly_events
except ModuleNotFoundError:  # pragma: no cover - optional dependency
    plotly_events = None
    logging.getLogger(__name__).warning(
        "streamlit-plotly-events not installed; interactive plots disabled"
    )
import datetime as dt

from shift_suite.i18n import translate as _
from shift_suite.logger_config import configure_logging
from shift_suite.tasks import (
    dashboard,
    leave_analyzer,  #  æ–°è¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
    over_shortage_log,
)
from shift_suite.tasks.utils import (
    safe_read_excel,
    safe_sheet,
    _parse_as_date,
    _valid_df,
    date_with_weekday,
)

# ğŸ¯ çµ±ä¸€åˆ†æçµæœç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
try:
    from shift_suite.tasks.unified_analysis_manager import UnifiedAnalysisManager
    UNIFIED_ANALYSIS_AVAILABLE = True
except ImportError:
    UNIFIED_ANALYSIS_AVAILABLE = False
    log.warning("çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

# ğŸ¯ å®Ÿè¡Œçµæœãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›æ©Ÿèƒ½è¿½åŠ 
try:
    from execution_logger import create_app_logger, ExecutionLogger
    EXECUTION_LOGGING_AVAILABLE = True
except ImportError:
    # create_app_logger, ExecutionLogger  # æœªä½¿ç”¨ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
    EXECUTION_LOGGING_AVAILABLE = False
    # log.warning("å®Ÿè¡Œãƒ­ã‚°æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")  # ä¸è¦ãªè­¦å‘Šã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

# ğŸ¤– AIå‘ã‘åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½è¿½åŠ 
try:
    from shift_suite.tasks.ai_comprehensive_report_generator import AIComprehensiveReportGenerator
    AI_REPORT_GENERATOR_AVAILABLE = True
except ImportError:
    AI_REPORT_GENERATOR_AVAILABLE = False

# ğŸ” é«˜åº¦ãªä¸è¶³åˆ†ææ©Ÿèƒ½è¿½åŠ 
try:
    from advanced_shortage_integration import display_advanced_shortage_tab
    ADVANCED_SHORTAGE_AVAILABLE = True
except ImportError:
    ADVANCED_SHORTAGE_AVAILABLE = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from shift_suite.tasks.analyzers import (
    AttendanceBehaviorAnalyzer,
    CombinedScoreCalculator,
    LowStaffLoadAnalyzer,
    RestTimeAnalyzer,
    WorkPatternAnalyzer,
)
# Anomaly detection import with fallback for sklearn issues
try:
    from shift_suite.tasks.anomaly import detect_anomaly
    _HAS_ANOMALY = True
except ImportError:
    _HAS_ANOMALY = False
    def detect_anomaly(*args, **kwargs):
        return {"error": "Anomaly detection not available due to sklearn dependency issues"}
from shift_suite.tasks.build_stats import build_stats
# Clustering import with fallback for sklearn issues
try:
    from shift_suite.tasks.cluster import cluster_staff
    _HAS_CLUSTER = True
except ImportError:
    _HAS_CLUSTER = False
    def cluster_staff(*args, **kwargs):
        return {"error": "Clustering not available due to sklearn dependency issues"}
from shift_suite.tasks.constants import SUMMARY5 as SUMMARY5_CONST
from shift_suite.tasks.cost_benefit import analyze_cost_benefit
from shift_suite.tasks.fairness import run_fairness
from shift_suite.tasks.fatigue import train_fatigue

# é«˜åº¦ä¸è¶³åˆ†ææ©Ÿèƒ½ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from advanced_shortage_integration import display_advanced_shortage_tab
from shift_suite.tasks.forecast import build_demand_series, forecast_need
from shift_suite.tasks.h2hire import build_hire_plan as build_hire_plan_from_kpi
from shift_suite.tasks.heatmap import build_heatmap
from shift_suite.tasks.hire_plan import build_hire_plan as build_hire_plan_standard

# â”€â”€ Shift-Suite task modules â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from shift_suite.tasks.io_excel import SHEET_COL_ALIAS, _normalize, ingest_excel
from shift_suite.tasks.leave_analyzer import (
    LEAVE_TYPE_PAID,
    LEAVE_TYPE_REQUESTED,
    LEAVE_TYPE_OTHER,
)
# from shift_suite.tasks.rl import learn_roster  # Temporarily disabled due to gymnasium dependency
from shift_suite.tasks.shortage import (
    merge_shortage_leave,
    shortage_and_brief,
    weekday_timeslot_summary,
)
# ShortageFactorAnalyzer import with fallback for sklearn issues
try:
    from shift_suite.tasks.shortage_factor_analyzer import ShortageFactorAnalyzer
    _HAS_SHORTAGE_ANALYZER = True
except ImportError:
    _HAS_SHORTAGE_ANALYZER = False
    class ShortageFactorAnalyzer:
        def __init__(self, *args, **kwargs):
            pass
        def analyze(self, *args, **kwargs):
            return {"error": "ShortageFactorAnalyzer not available due to sklearn dependency issues"}
# Skill NMF import with fallback for sklearn issues  
try:
    from shift_suite.tasks.skill_nmf import build_skill_matrix
    _HAS_SKILL_NMF = True
except ImportError:
    _HAS_SKILL_NMF = False
    def build_skill_matrix(*args, **kwargs):
        return {"error": "Skill matrix building not available due to sklearn dependency issues"}
from shift_suite.tasks.daily_cost import calculate_daily_cost  # <-- added
from shift_suite.tasks.optimal_hire_plan import create_optimal_hire_plan

# â˜…æ–°è¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from shift_suite.tasks.gap_analyzer import analyze_standards_gap

# 12è»¸è¶…é«˜æ¬¡å…ƒåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from ultra_dimensional_constraint_discovery_system import UltraDimensionalConstraintDiscoverySystem
    _HAS_ULTRA_CONSTRAINT_SYSTEM = True
except ImportError:
    _HAS_ULTRA_CONSTRAINT_SYSTEM = False
    class UltraDimensionalConstraintDiscoverySystem:
        def __init__(self, *args, **kwargs):
            pass
        def discover_constraints(self, *args, **kwargs):
            return {"error": "12è»¸åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}

# AdvancedBlueprintEngineV2 import with fallback for sklearn issues
try:
    from shift_suite.tasks.advanced_blueprint_engine_v2 import AdvancedBlueprintEngineV2
    _HAS_BLUEPRINT_V2 = True
except ImportError:
    _HAS_BLUEPRINT_V2 = False
    class AdvancedBlueprintEngineV2:
        def __init__(self, *args, **kwargs):
            pass
        def analyze(self, *args, **kwargs):
            return {"error": "AdvancedBlueprintEngineV2 not available due to sklearn dependency issues"}
# sklearn and matplotlib imports with fallback
try:
    # from sklearn.tree import plot_tree  # Commented out due to sklearn dependency
    import matplotlib.pyplot as plt
    _HAS_PLOT_TREE = False  # Disabled due to sklearn dependency
except ImportError:
    _HAS_PLOT_TREE = False
    plot_tree = None
    plt = None

# â˜…Truth-Driven Analysis System (å‘ä¸Šã—ãŸåˆ†ææ©Ÿèƒ½)
from shift_suite.tasks.enhanced_data_ingestion import (
    TruthAssuredDataIngestion, 
    ingest_excel_with_quality_assurance
)
from shift_suite.tasks.truth_assured_decomposer import TruthAssuredDecomposer
from shift_suite.tasks.hierarchical_truth_analyzer import HierarchicalTruthAnalyzer
from shift_suite.tasks.weekday_role_need_visualizer import WeekdayRoleNeedAnalyzer


def create_comprehensive_analysis_log(output_dir: Path, analysis_type: str = "FULL") -> Path:
    """åŒ…æ‹¬çš„ãªåˆ†æãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆapp.pyç”¨ï¼‰- è©³ç´°ç‰ˆ"""
    timestamp = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥%Hæ™‚%Måˆ†")
    log_filename = f"{timestamp}_åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ_{analysis_type}.txt"
    log_filepath = output_dir / log_filename
    
    try:
        with open(log_filepath, 'w', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write(f"                           åŒ…æ‹¬åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆ\n")
            f.write("=" * 100 + "\n\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {timestamp}\n")
            f.write(f"åˆ†æã‚¿ã‚¤ãƒ—: {analysis_type}\n")
            f.write(f"åˆ†æãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}\n")
            f.write(f"åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³: Shift-Suite v1.30.0\n")
            f.write("=" * 100 + "\n\n")
            
            # 1. å®Ÿè¡Œã‚µãƒãƒªãƒ¼ï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("ã€1. å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã€‘\n")
            f.write("-" * 80 + "\n")
            summary_stats = collect_execution_summary(output_dir)
            f.write(f"  å®Ÿè¡Œé–‹å§‹æ™‚åˆ»: {summary_stats.get('start_time', 'N/A')}\n")
            f.write(f"  å®Ÿè¡Œçµ‚äº†æ™‚åˆ»: {summary_stats.get('end_time', 'N/A')}\n")
            f.write(f"  å‡¦ç†æ™‚é–“: {summary_stats.get('duration', 'N/A')}\n")
            f.write(f"  å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—æ•°: {summary_stats.get('total_steps', 0)}\n")
            f.write(f"  æˆåŠŸã‚¹ãƒ†ãƒƒãƒ—æ•°: {summary_stats.get('successful_steps', 0)}\n")
            f.write(f"  ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—æ•°: {summary_stats.get('failed_steps', 0)}\n")
            f.write(f"  è­¦å‘Šæ•°: {summary_stats.get('warnings', 0)}\n")
            f.write(f"  ä½¿ç”¨ãƒ¡ãƒ¢ãƒª: {summary_stats.get('memory_usage', 'N/A')}\n")
            f.write(f"  å®Ÿè¡Œç’°å¢ƒ: {summary_stats.get('environment', 'N/A')}\n\n")
            
            # 2. ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("ã€2. ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ã€‘\n")
            f.write("-" * 80 + "\n")
            data_stats = collect_data_overview(output_dir)
            f.write(f"  å¯¾è±¡æœŸé–“: {data_stats.get('date_range', 'N/A')}\n")
            f.write(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {data_stats.get('total_records', 0):,}ä»¶\n")
            f.write(f"  ã‚¹ã‚¿ãƒƒãƒ•æ•°: {data_stats.get('total_staff', 0)}å\n")
            f.write(f"  è·ç¨®æ•°: {data_stats.get('total_roles', 0)}ç¨®é¡\n")
            f.write(f"  é›‡ç”¨å½¢æ…‹æ•°: {data_stats.get('total_employments', 0)}ç¨®é¡\n")
            f.write(f"  å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {data_stats.get('total_patterns', 0)}ç¨®é¡\n")
            f.write(f"  ä¼‘æ¥­æ—¥æ•°: {data_stats.get('holiday_count', 0)}æ—¥\n")
            f.write(f"  ã‚¹ãƒ­ãƒƒãƒˆé–“éš”: {data_stats.get('slot_minutes', 30)}åˆ†\n")
            f.write(f"  æ™‚é–“è»¸æ•°: {data_stats.get('time_slots', 48)}ã‚¹ãƒ­ãƒƒãƒˆ/æ—¥\n\n")
            
            # 3. ä¸»è¦KPIï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("ã€3. ä¸»è¦KPIã€‘\n")
            f.write("-" * 80 + "\n")
            kpi_stats = collect_main_kpis(output_dir)
            f.write(f"  â–  ä¸è¶³ãƒ»éå‰°åˆ†æ\n")
            f.write(f"    ç·ä¸è¶³æ™‚é–“: {kpi_stats.get('total_shortage_hours', 0):.2f}æ™‚é–“\n")
            f.write(f"    ç·éå‰°æ™‚é–“: {kpi_stats.get('total_excess_hours', 0):.2f}æ™‚é–“\n")
            f.write(f"    ä¸è¶³ç‡: {kpi_stats.get('shortage_ratio', 0):.2%}\n")
            f.write(f"    éå‰°ç‡: {kpi_stats.get('excess_ratio', 0):.2%}\n")
            f.write(f"    æœ€å¤§ä¸è¶³æ™‚é–“ï¼ˆ1æ—¥ï¼‰: {kpi_stats.get('max_daily_shortage', 0):.1f}æ™‚é–“\n")
            f.write(f"    å¹³å‡ä¸è¶³æ™‚é–“ï¼ˆ1æ—¥ï¼‰: {kpi_stats.get('avg_daily_shortage', 0):.1f}æ™‚é–“\n")
            f.write(f"\n  â–  åŠ´å‹™æŒ‡æ¨™\n")
            f.write(f"    å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢: {kpi_stats.get('avg_fatigue_score', 0):.2f}\n")
            f.write(f"    å…¬å¹³æ€§ã‚¹ã‚³ã‚¢: {kpi_stats.get('fairness_score', 0):.2f}\n")
            f.write(f"    æœ€é©åŒ–ã‚¹ã‚³ã‚¢: {kpi_stats.get('optimization_score', 0):.2f}\n")
            f.write(f"    ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢: {kpi_stats.get('risk_score', 0):.2f}\n")
            f.write(f"\n  â–  ã‚³ã‚¹ãƒˆæŒ‡æ¨™\n")
            f.write(f"    ç·äººä»¶è²»: Â¥{kpi_stats.get('total_cost', 0):,.0f}\n")
            f.write(f"    æœˆå¹³å‡äººä»¶è²»: Â¥{kpi_stats.get('monthly_avg_cost', 0):,.0f}\n")
            f.write(f"    æ™‚é–“å˜ä¾¡å¹³å‡: Â¥{kpi_stats.get('avg_hourly_rate', 0):.0f}\n")
            f.write(f"    ä¸è¶³æ™‚é–“ã®æ©Ÿä¼šæå¤±: Â¥{kpi_stats.get('shortage_opportunity_cost', 0):,.0f}\n\n")
            
            # 4. è·ç¨®åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("ã€4. è·ç¨®åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‘\n")
            f.write("-" * 80 + "\n")
            role_performance = collect_role_performance_detailed(output_dir)
            if role_performance:
                for i, role in enumerate(role_performance, 1):
                    f.write(f"\n  â—† {i}. {role.get('role', 'N/A')}\n")
                    f.write(f"    â”œâ”€ ä¸è¶³æ™‚é–“: {role.get('shortage_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ éå‰°æ™‚é–“: {role.get('excess_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ å¿…è¦äººå“¡: {role.get('required_staff', 0):.1f}äºº\n")
                    f.write(f"    â”œâ”€ å®Ÿç¸¾äººå“¡: {role.get('actual_staff', 0):.1f}äºº\n")
                    f.write(f"    â”œâ”€ å……è¶³ç‡: {role.get('fulfillment_rate', 0):.1%}\n")
                    f.write(f"    â”œâ”€ å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢: {role.get('avg_fatigue', 0):.2f}\n")
                    f.write(f"    â”œâ”€ å…¬å¹³æ€§ã‚¹ã‚³ã‚¢: {role.get('fairness_score', 0):.2f}\n")
                    f.write(f"    â”œâ”€ äººä»¶è²»: Â¥{role.get('total_cost', 0):,.0f}\n")
                    f.write(f"    â”œâ”€ ã‚³ã‚¹ãƒˆæ¯”ç‡: {role.get('cost_ratio', 0):.1%}\n")
                    f.write(f"    â””â”€ ä¸»è¦èª²é¡Œ: {role.get('main_issue', 'N/A')}\n")
            else:
                f.write("  è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            
            # 5. é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("\nã€5. é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æã€‘\n")
            f.write("-" * 80 + "\n")
            emp_analysis = collect_employment_analysis_detailed(output_dir)
            if emp_analysis:
                for i, emp in enumerate(emp_analysis, 1):
                    f.write(f"\n  â—† {i}. {emp.get('employment', 'N/A')}\n")
                    f.write(f"    â”œâ”€ ç·äººæ•°: {emp.get('staff_count', 0)}äºº\n")
                    f.write(f"    â”œâ”€ ä¸è¶³æ™‚é–“: {emp.get('shortage_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ éå‰°æ™‚é–“: {emp.get('excess_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ å¹³å‡å‹¤å‹™æ™‚é–“: {emp.get('avg_work_hours', 0):.1f}æ™‚é–“/æœˆ\n")
                    f.write(f"    â”œâ”€ æ™‚çµ¦å¹³å‡: Â¥{emp.get('avg_hourly_wage', 0):.0f}\n")
                    f.write(f"    â”œâ”€ ç·äººä»¶è²»: Â¥{emp.get('total_cost', 0):,.0f}\n")
                    f.write(f"    â”œâ”€ åŠ¹ç‡æ€§æŒ‡æ¨™: {emp.get('efficiency', 0):.2f}\n")
                    f.write(f"    â””â”€ æ´»ç”¨åº¦: {emp.get('utilization', 0):.1%}\n")
            
            # 6. æœˆåˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("\nã€6. æœˆåˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã€‘\n")
            f.write("-" * 80 + "\n")
            monthly_trends = collect_monthly_trends_detailed(output_dir)
            if monthly_trends:
                for month_data in monthly_trends:
                    month = month_data.get('month', 'N/A')
                    f.write(f"\n  â–  {month}\n")
                    f.write(f"    â”œâ”€ ä¸è¶³æ™‚é–“: {month_data.get('shortage_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ éå‰°æ™‚é–“: {month_data.get('excess_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ ä¼‘æš‡æ—¥æ•°: {month_data.get('leave_days', 0):.0f}æ—¥\n")
                    f.write(f"    â”œâ”€ æœ‰çµ¦å–å¾—: {month_data.get('paid_leave', 0):.0f}æ—¥\n")
                    f.write(f"    â”œâ”€ ã‚¢ãƒ©ãƒ¼ãƒˆæ•°: {month_data.get('alerts', 0)}ä»¶\n")
                    f.write(f"    â”œâ”€ ç–²åŠ´åº¦å¹³å‡: {month_data.get('avg_fatigue', 0):.2f}\n")
                    f.write(f"    â”œâ”€ äººä»¶è²»: Â¥{month_data.get('cost', 0):,.0f}\n")
                    f.write(f"    â””â”€ ç‰¹è¨˜äº‹é …: {month_data.get('notes', 'ãªã—')}\n")
            
            # 7. æ™‚é–“å¸¯åˆ¥åˆ†æï¼ˆæ–°è¦è¿½åŠ ï¼‰
            f.write("\nã€7. æ™‚é–“å¸¯åˆ¥åˆ†æã€‘\n")
            f.write("-" * 80 + "\n")
            time_analysis = collect_time_slot_analysis(output_dir)
            if time_analysis:
                f.write("  æ™‚é–“å¸¯     | å¹³å‡å¿…è¦äººå“¡ | å¹³å‡å®Ÿç¸¾äººå“¡ | ä¸è¶³ç‡ | ä¸»è¦è·ç¨®\n")
                f.write("  " + "-" * 65 + "\n")
                for slot in time_analysis[:10]:  # ä¸Šä½10æ™‚é–“å¸¯
                    time_slot = str(slot.get('time_slot', 'N/A')).ljust(10)
                    avg_need = slot.get('avg_need', 0)
                    avg_actual = slot.get('avg_actual', 0)
                    shortage_rate = slot.get('shortage_rate', 0)
                    main_role = slot.get('main_role', 'N/A')[:10]
                    f.write(f"  {time_slot} | {avg_need:12.1f} | {avg_actual:12.1f} | {shortage_rate:6.1%} | {main_role}\n")
            
            # 8. å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼ˆæ–°è¦è¿½åŠ ï¼‰
            f.write("\nã€8. å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã€‘\n")
            f.write("-" * 80 + "\n")
            pattern_analysis = collect_work_pattern_analysis(output_dir)
            if pattern_analysis:
                for i, pattern in enumerate(pattern_analysis[:15], 1):  # ä¸Šä½15ãƒ‘ã‚¿ãƒ¼ãƒ³
                    f.write(f"\n  â—† ãƒ‘ã‚¿ãƒ¼ãƒ³{i}: {pattern.get('pattern_name', 'N/A')}\n")
                    f.write(f"    â”œâ”€ ä½¿ç”¨é »åº¦: {pattern.get('frequency', 0)}å›\n")
                    f.write(f"    â”œâ”€ ã‚¹ã‚¿ãƒƒãƒ•æ•°: {pattern.get('staff_count', 0)}äºº\n")
                    f.write(f"    â”œâ”€ 1æ—¥å‹¤å‹™æ™‚é–“: {pattern.get('daily_hours', 0):.1f}æ™‚é–“\n")
                    f.write(f"    â”œâ”€ é€£ç¶šå‹¤å‹™: {pattern.get('consecutive_days', 0)}æ—¥\n")
                    f.write(f"    â”œâ”€ ç–²åŠ´å½±éŸ¿: {pattern.get('fatigue_impact', 'N/A')}\n")
                    f.write(f"    â””â”€ æ¨å¥¨åº¦: {pattern.get('recommendation_score', 0):.1f}/10\n")
            
            # 9. ä¼‘æš‡åˆ†æï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("\nã€9. ä¼‘æš‡åˆ†æã€‘\n")
            f.write("-" * 80 + "\n")
            leave_analysis = collect_leave_analysis_detailed(output_dir)
            if leave_analysis:
                f.write(f"  â–  å…¨ä½“çµ±è¨ˆ\n")
                f.write(f"    ç·ä¼‘æš‡æ—¥æ•°: {leave_analysis.get('total_leave_days', 0):.0f}æ—¥\n")
                f.write(f"    æœ‰çµ¦å–å¾—ç‡: {leave_analysis.get('paid_leave_ratio', 0):.1%}\n")
                f.write(f"    å¸Œæœ›ä¼‘å–å¾—ç‡: {leave_analysis.get('requested_leave_ratio', 0):.1%}\n")
                f.write(f"    é›†ä¸­æ—¥æ•°: {leave_analysis.get('concentration_days', 0)}æ—¥\n")
                f.write(f"\n  â–  ä¼‘æš‡ã‚¿ã‚¤ãƒ—åˆ¥å†…è¨³\n")
                for leave_type in leave_analysis.get('by_type', []):
                    f.write(f"    {leave_type['type']}: {leave_type['days']}æ—¥ ({leave_type['ratio']:.1%})\n")
                f.write(f"\n  â–  æœˆåˆ¥ä¼‘æš‡å‚¾å‘\n")
                for month_leave in leave_analysis.get('monthly', [])[:6]:
                    f.write(f"    {month_leave['month']}: {month_leave['days']}æ—¥\n")
            
            # 10. ç•°å¸¸å€¤ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆåˆ†æï¼ˆæ–°è¦è¿½åŠ ï¼‰
            f.write("\nã€10. ç•°å¸¸å€¤ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆåˆ†æã€‘\n")
            f.write("-" * 80 + "\n")
            anomaly_analysis = collect_anomaly_analysis(output_dir)
            if anomaly_analysis:
                f.write(f"  â–  æ¤œå‡ºã•ã‚ŒãŸç•°å¸¸å€¤\n")
                for i, anomaly in enumerate(anomaly_analysis.get('anomalies', [])[:10], 1):
                    f.write(f"    {i}. {anomaly['date']} {anomaly['type']}: {anomaly['description']}\n")
                f.write(f"\n  â–  ã‚¢ãƒ©ãƒ¼ãƒˆçµ±è¨ˆ\n")
                f.write(f"    é«˜ãƒªã‚¹ã‚¯ã‚¢ãƒ©ãƒ¼ãƒˆ: {anomaly_analysis.get('high_risk_count', 0)}ä»¶\n")
                f.write(f"    ä¸­ãƒªã‚¹ã‚¯ã‚¢ãƒ©ãƒ¼ãƒˆ: {anomaly_analysis.get('medium_risk_count', 0)}ä»¶\n")
                f.write(f"    ä½ãƒªã‚¹ã‚¯ã‚¢ãƒ©ãƒ¼ãƒˆ: {anomaly_analysis.get('low_risk_count', 0)}ä»¶\n")
            
            # 11. ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã‚µãƒãƒªãƒ¼ï¼ˆæ–°è¦è¿½åŠ ï¼‰
            f.write("\nã€11. ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã‚µãƒãƒªãƒ¼ã€‘\n")
            f.write("-" * 80 + "\n")
            blueprint_summary = collect_blueprint_summary(output_dir)
            if blueprint_summary:
                f.write(f"  â–  ç™ºè¦‹ã•ã‚ŒãŸæš—é»™çŸ¥\n")
                for i, knowledge in enumerate(blueprint_summary.get('implicit_knowledge', [])[:20], 1):
                    f.write(f"    {i}. {knowledge}\n")
                f.write(f"\n  â–  åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³\n")
                for i, constraint in enumerate(blueprint_summary.get('constraints', [])[:15], 1):
                    f.write(f"    {i}. {constraint}\n")
                f.write(f"\n  â–  æœ€é©åŒ–ææ¡ˆ\n")
                for i, suggestion in enumerate(blueprint_summary.get('optimization_suggestions', [])[:10], 1):
                    f.write(f"    {i}. {suggestion}\n")
            
            # 12. ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("\nã€12. ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã€‘\n")
            f.write("-" * 80 + "\n")
            generated_files = collect_generated_files_detailed(output_dir)
            if generated_files:
                for category, files in generated_files.items():
                    f.write(f"\n  â–  {category}\n")
                    for file_info in files:
                        f.write(f"    {file_info}\n")
            
            # 13. ã‚·ã‚¹ãƒ†ãƒ æ¨å¥¨äº‹é …ï¼ˆè©³ç´°ç‰ˆï¼‰
            f.write("\nã€13. ã‚·ã‚¹ãƒ†ãƒ æ¨å¥¨äº‹é …ã€‘\n")
            f.write("-" * 80 + "\n")
            recommendations = generate_comprehensive_recommendations(output_dir)
            if recommendations:
                f.write("  â–  ç·Šæ€¥å¯¾å¿œäº‹é …\n")
                for i, rec in enumerate(recommendations.get('urgent', []), 1):
                    f.write(f"    {i}. {rec}\n")
                f.write("\n  â–  çŸ­æœŸæ”¹å–„äº‹é …ï¼ˆ1-3ãƒ¶æœˆï¼‰\n")
                for i, rec in enumerate(recommendations.get('short_term', []), 1):
                    f.write(f"    {i}. {rec}\n")
                f.write("\n  â–  ä¸­é•·æœŸæ”¹å–„äº‹é …ï¼ˆ3-6ãƒ¶æœˆï¼‰\n")
                for i, rec in enumerate(recommendations.get('long_term', []), 1):
                    f.write(f"    {i}. {rec}\n")
            
            # 14. è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            if analysis_type == "FULL":
                f.write("\nã€14. è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ€ãƒ³ãƒ—ã€‘\n")
                f.write("-" * 80 + "\n")
                f.write("  â€» è©³ç´°ãƒ‡ãƒ¼ã‚¿ã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã•ã‚Œã¦ã„ã¾ã™ã€‚\n")
                f.write(f"    - {timestamp}_è©³ç´°ãƒ‡ãƒ¼ã‚¿_è·ç¨®åˆ¥.csv\n")
                f.write(f"    - {timestamp}_è©³ç´°ãƒ‡ãƒ¼ã‚¿_æ™‚ç³»åˆ—.csv\n")
                f.write(f"    - {timestamp}_è©³ç´°ãƒ‡ãƒ¼ã‚¿_ãƒ‘ã‚¿ãƒ¼ãƒ³.csv\n")
            
            f.write("\n" + "=" * 100 + "\n")
            f.write(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {timestamp}\n")
            f.write("ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯æ©Ÿå¯†æƒ…å ±ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å–ã‚Šæ‰±ã„ã«ã”æ³¨æ„ãã ã•ã„ã€‚\n")
            f.write("=" * 100 + "\n")
            
        logging.info(f"[app] åŒ…æ‹¬åˆ†æãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {log_filepath}")
        return log_filepath
        
    except Exception as e:
        logging.error(f"[app] åŒ…æ‹¬åˆ†æãƒ­ã‚°ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def collect_execution_summary(output_dir: Path) -> dict:
    """å®Ÿè¡Œã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’åé›†"""
    try:
        # å®Ÿè¡Œæ™‚é–“ã®æ¨å®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ™‚åˆ»ã‹ã‚‰ï¼‰
        parquet_files = list(output_dir.glob("*.parquet"))
        if parquet_files:
            timestamps = [f.stat().st_mtime for f in parquet_files]
            start_time = datetime.datetime.fromtimestamp(min(timestamps)).strftime("%H:%M:%S")
            end_time = datetime.datetime.fromtimestamp(max(timestamps)).strftime("%H:%M:%S")
            duration_sec = max(timestamps) - min(timestamps)
            duration = f"{duration_sec:.1f}ç§’"
        else:
            start_time = "N/A"
            end_time = "N/A"
            duration = "N/A"
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‹ã‚‰ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’æ¨å®š
        all_files = list(output_dir.glob("*"))
        successful_steps = len([f for f in all_files if f.suffix in ['.parquet', '.xlsx', '.json']])
        total_steps = successful_steps  # ã‚¨ãƒ©ãƒ¼ã‚¹ãƒ†ãƒƒãƒ—ã®æ­£ç¢ºãªè¨ˆæ¸¬ã¯å›°é›£
        
        return {
            'start_time': start_time,
            'end_time': end_time,
            'duration': duration,
            'total_steps': total_steps,
            'successful_steps': successful_steps,
            'failed_steps': 0
        }
    except:
        return {}


def collect_data_overview(output_dir: Path) -> dict:
    """ãƒ‡ãƒ¼ã‚¿æ¦‚è¦æƒ…å ±ã‚’åé›†"""
    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
        meta_file = output_dir / "heatmap.meta.json"
        if meta_file.exists():
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
            
            dates = meta_data.get('dates', [])
            roles = meta_data.get('roles', [])
            employments = meta_data.get('employments', [])
            
            date_range = f"{dates[0]} ï½ {dates[-1]}" if dates else "N/A"
            
            # heat_ALL.parquetã‹ã‚‰ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’å–å¾—
            heat_all_file = output_dir / "heat_ALL.parquet"
            total_records = 0
            if heat_all_file.exists():
                try:
                    df = pd.read_parquet(heat_all_file)
                    total_records = df.shape[0] * df.shape[1] if not df.empty else 0
                except:
                    pass
            
            return {
                'date_range': date_range,
                'total_records': total_records,
                'total_staff': 0,  # ã‚¹ã‚¿ãƒƒãƒ•æ•°ã¯ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ãŒå¿…è¦
                'total_roles': len(roles),
                'total_employments': len(employments),
                'total_patterns': 0  # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚‚ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ãŒå¿…è¦
            }
        else:
            return {}
    except:
        return {}


def collect_main_kpis(output_dir: Path) -> dict:
    """ä¸»è¦KPIæƒ…å ±ã‚’åé›†"""
    try:
        kpis = {}
        
        # shortage_role_summary.parquetã‹ã‚‰ä¸è¶³ãƒ»éå‰°æ™‚é–“
        shortage_role_file = output_dir / "shortage_role_summary.parquet"
        if shortage_role_file.exists():
            try:
                df = pd.read_parquet(shortage_role_file)
                kpis['total_shortage_hours'] = df.get('lack_h', pd.Series()).sum()
                kpis['total_excess_hours'] = df.get('excess_h', pd.Series()).sum()
            except:
                pass
        
        # fatigue_score.parquetã‹ã‚‰ç–²åŠ´ã‚¹ã‚³ã‚¢
        fatigue_file = output_dir / "fatigue_score.parquet"
        if fatigue_file.exists():
            try:
                df = pd.read_parquet(fatigue_file)
                kpis['avg_fatigue_score'] = df.get('fatigue_score', pd.Series()).mean()
            except:
                pass
        
        # fairness_after.parquetã‹ã‚‰å…¬å¹³æ€§ã‚¹ã‚³ã‚¢
        fairness_file = output_dir / "fairness_after.parquet"
        if fairness_file.exists():
            try:
                df = pd.read_parquet(fairness_file)
                kpis['fairness_score'] = df.get('fairness_score', pd.Series()).mean()
            except:
                pass
        
        # ãã®ä»–ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        kpis.setdefault('total_shortage_hours', 0)
        kpis.setdefault('total_excess_hours', 0)
        kpis.setdefault('avg_fatigue_score', 0)
        kpis.setdefault('fairness_score', 0)
        kpis.setdefault('optimization_score', 0)
        kpis.setdefault('total_cost', 0)
        
        return kpis
    except:
        return {}


def collect_role_performance(output_dir: Path) -> list:
    """è·ç¨®åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã‚’åé›†"""
    try:
        shortage_role_file = output_dir / "shortage_role_summary.parquet"
        if shortage_role_file.exists():
            df = pd.read_parquet(shortage_role_file)
            return [
                {
                    'role': row.get('role', 'N/A'),
                    'shortage_hours': row.get('lack_h', 0),
                    'excess_hours': row.get('excess_h', 0),
                    'avg_fatigue': 0,  # ç–²åŠ´åº¦ã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®çµåˆãŒå¿…è¦
                    'total_cost': 0    # ã‚³ã‚¹ãƒˆã‚‚åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®çµåˆãŒå¿…è¦
                }
                for _, row in df.iterrows()
            ]
        else:
            return []
    except:
        return []


def collect_monthly_trends(output_dir: Path) -> list:
    """æœˆåˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’åé›†"""
    try:
        monthly_role_file = output_dir / "shortage_role_monthly.parquet"
        if monthly_role_file.exists():
            df = pd.read_parquet(monthly_role_file)
            monthly_summary = df.groupby('month').agg({
                'lack_h': 'sum',
                'excess_h': 'sum'
            }).reset_index()
            
            return [
                {
                    'month': row['month'],
                    'shortage_hours': row.get('lack_h', 0),
                    'excess_hours': row.get('excess_h', 0),
                    'leave_days': 0,  # ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ã¯åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰
                    'alerts': 0       # ã‚¢ãƒ©ãƒ¼ãƒˆæ•°ã‚‚åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰
                }
                for _, row in monthly_summary.iterrows()
            ]
        else:
            return []
    except:
        return []


def collect_generated_files(output_dir: Path) -> dict:
    """ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’åé›†"""
    try:
        files_by_category = {
            'ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—': [],
            'ä¸è¶³åˆ†æ': [],
            'ç–²åŠ´åˆ†æ': [],
            'å…¬å¹³æ€§åˆ†æ': [],
            'ãã®ä»–': []
        }
        
        for file_path in output_dir.glob("*"):
            if file_path.is_file():
                file_size = file_path.stat().st_size
                file_info = f"{file_path.name} ({file_size} bytes)"
                
                if 'heat_' in file_path.name:
                    files_by_category['ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—'].append(file_info)
                elif 'shortage_' in file_path.name:
                    files_by_category['ä¸è¶³åˆ†æ'].append(file_info)
                elif 'fatigue_' in file_path.name:
                    files_by_category['ç–²åŠ´åˆ†æ'].append(file_info)
                elif 'fairness_' in file_path.name:
                    files_by_category['å…¬å¹³æ€§åˆ†æ'].append(file_info)
                else:
                    files_by_category['ãã®ä»–'].append(file_info)
        
        return files_by_category
    except:
        return {}


def generate_recommendations(kpi_stats: dict, role_performance: list) -> list:
    """KPIã¨è·ç¨®åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
    recommendations = []
    
    try:
        # ä¸è¶³æ™‚é–“ã«åŸºã¥ãæ¨å¥¨
        total_shortage = kpi_stats.get('total_shortage_hours', 0)
        if total_shortage > 100:
            recommendations.append(f"ç·ä¸è¶³æ™‚é–“ãŒ{total_shortage:.1f}æ™‚é–“ã¨é«˜ã„æ°´æº–ã§ã™ã€‚è¿½åŠ æ¡ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        elif total_shortage > 50:
            recommendations.append(f"ç·ä¸è¶³æ™‚é–“ãŒ{total_shortage:.1f}æ™‚é–“ã§ã™ã€‚ã‚·ãƒ•ãƒˆèª¿æ•´ã§æ”¹å–„å¯èƒ½ã§ã™ã€‚")
        
        # éå‰°æ™‚é–“ã«åŸºã¥ãæ¨å¥¨
        total_excess = kpi_stats.get('total_excess_hours', 0)
        if total_excess > 50:
            recommendations.append(f"ç·éå‰°æ™‚é–“ãŒ{total_excess:.1f}æ™‚é–“ã§ã™ã€‚ã‚·ãƒ•ãƒˆæœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚")
        
        # è·ç¨®åˆ¥ã®æ¨å¥¨
        if role_performance:
            high_shortage_roles = [r for r in role_performance if r.get('shortage_hours', 0) > 20]
            if high_shortage_roles:
                role_names = [r['role'] for r in high_shortage_roles[:3]]
                recommendations.append(f"è·ç¨®ã€Œ{', '.join(role_names)}ã€ã§ä¸è¶³ãŒé¡•è‘—ã§ã™ã€‚å„ªå…ˆçš„ãªå¯¾å¿œãŒå¿…è¦ã§ã™ã€‚")
        
        # ç–²åŠ´ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨
        avg_fatigue = kpi_stats.get('avg_fatigue_score', 0)
        if avg_fatigue > 55:
            recommendations.append("å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢ãŒé«˜ã„æ°´æº–ã§ã™ã€‚å‹¤å‹™é–“éš”ã¨é€£å‹¤å›æ•°ã®è¦‹ç›´ã—ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        # å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨
        fairness_score = kpi_stats.get('fairness_score', 0)
        if fairness_score < 0.5:
            recommendations.append("å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ãŒä½ã„æ°´æº–ã§ã™ã€‚ã‚¹ã‚¿ãƒƒãƒ•é–“ã®å‹¤å‹™æ™‚é–“ãƒãƒ©ãƒ³ã‚¹ã®èª¿æ•´ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        return recommendations
    except:
        return ["æ¨å¥¨äº‹é …ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"]


def _patch_streamlit_watcher() -> None:
    """Patch Streamlit's module watcher to ignore failing modules."""
    try:
        from streamlit.watcher import local_sources_watcher as _lsw  # type: ignore
    except Exception:
        return

    if not hasattr(_lsw, "extract_paths"):
        return

    if getattr(_lsw.extract_paths, "_patched", False):
        return

    _orig = _lsw.extract_paths

    def _safe_extract_paths(module):
        try:
            return _orig(module)
        except Exception as e:  # pragma: no cover - optional
            logging.getLogger(__name__).debug(
                "streamlit watcher skipped %s due to %s",
                getattr(module, "__name__", repr(module)),
                e,
            )
            return []

    _safe_extract_paths._patched = True  # type: ignore[attr-defined]
    _lsw.extract_paths = _safe_extract_paths


# Call the watcher patch before configuring logging so that any modules
# imported during setup won't trigger errors.
_patch_streamlit_watcher()
# â”€â”€ ãƒ­ã‚¬ãƒ¼è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
configure_logging()
log = logging.getLogger(__name__)

analysis_logger = logging.getLogger('analysis')
analysis_logger.setLevel(logging.INFO)
analysis_logger.propagate = False
try:
    file_handler = logging.FileHandler('analysis_log.log', mode='a', encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(module)s.%(funcName)s] - %(message)s')
    file_handler.setFormatter(formatter)
    if not analysis_logger.handlers:
        analysis_logger.addHandler(file_handler)
except Exception as e:
    logging.error(f"\u5206\u6790\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u8a2d\u5b9a\u306b\u5931\u6557\u3057\u307e\u3057\u305f: {e}")


def log_environment_info() -> None:
    """Log basic environment details to help with debugging."""
    import os
    import platform
    import sys

    log.info("Python: %s", sys.version.replace("\n", " "))
    log.info("Platform: %s", platform.platform())
    log.info("Working dir: %s", Path.cwd())
    log.info("Streamlit: %s", st.__version__)
    try:
        import torch

        log.info("PyTorch: %s", torch.__version__)
    except Exception as e:  # pragma: no cover - optional
        log.info("PyTorch not available: %s", e)
    log.debug("Environment PATH: %s", os.getenv("PATH", ""))


log_environment_info()


# â”€â”€ Utility: log error to terminal and show in Streamlit â”€â”€
def log_and_display_error(msg: str, exc: Exception | None) -> None:
    """Log an error and also show it in the Streamlit interface.

    Parameters
    ----------
    msg : str
        User-facing error message.
    exc : Exception | None
        Exception instance, if available. ``None`` is allowed to avoid
        displaying ``NoneType: None`` when an exception object is absent.
    """

    if exc is not None:
        log.error(f"{msg}: {exc}", exc_info=True)
        st.error(f"{msg}: {exc}")
    else:
        log.error(msg)
        st.error(msg)


def log_and_display_error_enhanced(step: str, msg: str, exc: Exception) -> None:
    """Show user friendly error messages with technical details in logs."""
    log.error(f"[{step}] {msg}: {exc}", exc_info=True)

    user_messages = {
        "å¹´æœˆã‚»ãƒ«èª­ã¿è¾¼ã¿": "å¹´æœˆæƒ…å ±ã‚»ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚»ãƒ«ä½ç½®ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "å‹¤å‹™åŒºåˆ†èª­ã¿è¾¼ã¿": "å‹¤å‹™åŒºåˆ†ã‚·ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆåã¨å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "æ™‚åˆ»å¤‰æ›": "å‹¤å‹™åŒºåˆ†ã®æ™‚åˆ»ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸã€‚é–‹å§‹ãƒ»çµ‚äº†æ™‚åˆ»ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "ã‚·ãƒ¼ãƒˆèª­ã¿è¾¼ã¿": "å®Ÿç¸¾ã‚·ãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆåã¨åˆ—æ§‹é€ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
        "æ—¥ä»˜åˆ—è§£æ": "æ—¥ä»˜åˆ—ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ—¥ä»˜ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
    }

    user_msg = user_messages.get(step, f"{step}ã®å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    st.error(f"âŒ {user_msg}")
    with st.expander("æŠ€è¡“çš„ãªè©³ç´°æƒ…å ±"):
        st.code(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(exc).__name__}\nãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(exc)}")


# â”€â”€ æ—¥æœ¬èªãƒ©ãƒ™ãƒ«è¾æ›¸ã¯ resources/strings_ja.json ã§ç®¡ç† â”€â”€


def _file_mtime(path: Path) -> float:
    """Return the modification time of a file for cache keys."""
    try:
        return path.stat().st_mtime
    except OSError:
        return 0.0




def load_shortage_meta(data_dir: Path) -> tuple[list[str], list[str]]:
    """Return role and employment lists from ``shortage.meta.json`` if present."""
    roles: list[str] = []
    employments: list[str] = []
    meta_fp = data_dir / "shortage.meta.json"
    if meta_fp.exists():
        try:
            meta = json.loads(meta_fp.read_text(encoding="utf-8"))
            roles = meta.get("roles", []) or []
            employments = meta.get("employments", []) or []
        except Exception as e:  # noqa: BLE001
            log.debug("failed to load shortage meta: %s", e)
    return roles, employments


@st.cache_data
def calc_ratio_from_heatmap_simple(df: pd.DataFrame) -> pd.DataFrame:
    """Return shortage ratio DataFrame calculated from heatmap data (simple version)."""
    if df is None or df.empty or "need" not in df.columns:
        return pd.DataFrame()
    date_cols = [c for c in df.columns if _parse_as_date(str(c)) is not None]
    if not date_cols:
        return pd.DataFrame()
    need_series = df["need"].fillna(0)
    need_df = pd.DataFrame(
        np.repeat(need_series.values[:, np.newaxis], len(date_cols), axis=1),
        index=need_series.index,
        columns=date_cols,
    )
    staff_df = df[date_cols].fillna(0)
    ratio_df = (
        ((need_df - staff_df) / need_df.replace(0, np.nan)).clip(lower=0).fillna(0)
    )
    return ratio_df


@st.cache_data
def calc_opt_score_from_heatmap(
    df: pd.DataFrame, w_lack: float = 0.6, w_excess: float = 0.4
) -> pd.DataFrame:
    """Return optimization score heatmap from raw heatmap data."""
    if df is None or df.empty or "need" not in df.columns:
        return pd.DataFrame()
    date_cols = [c for c in df.columns if _parse_as_date(str(c)) is not None]
    if not date_cols:
        return pd.DataFrame()
    need_series = df["need"].fillna(0)
    need_df = pd.DataFrame(
        np.repeat(need_series.values[:, np.newaxis], len(date_cols), axis=1),
        index=need_series.index,
        columns=date_cols,
    )
    staff_df = df[date_cols].fillna(0)
    lack_ratio = (
        ((need_df - staff_df) / need_df.replace(0, np.nan)).clip(lower=0).fillna(0)
    )
    if "upper" in df.columns:
        upper_series = df["upper"].fillna(0)
        upper_df = pd.DataFrame(
            np.repeat(upper_series.values[:, np.newaxis], len(date_cols), axis=1),
            index=upper_series.index,
            columns=date_cols,
        )
        excess_ratio = (
            ((staff_df - upper_df) / upper_df.replace(0, np.nan))
            .clip(lower=0)
            .fillna(0)
        )
    else:
        excess_ratio = staff_df * 0
    score_df = 1 - (w_lack * lack_ratio + w_excess * excess_ratio)
    return score_df.clip(lower=0, upper=1)


def excel_cell_to_row_col(cell: str) -> tuple[int, int] | None:
    """Convert Excel style cell like 'A1' to 0-based row/column indexes."""
    m = re.match(r"^([A-Za-z]+)(\d+)$", cell.strip())
    if not m:
        return None
    col_txt, row_txt = m.groups()
    col = 0
    for ch in col_txt.upper():
        col = col * 26 + (ord(ch) - 64)
    return int(row_txt) - 1, col - 1


def estimate_date_range_from_excel(excel_path: Path, sheet_name: str, header_row: int) -> list[datetime.date] | None:
    """ğŸ“… Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‚ç…§æœŸé–“ã‚’è‡ªå‹•æ¨å®š"""
    try:
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’èª­ã¿è¾¼ã‚“ã§æ—¥ä»˜åˆ—ã‚’æ¤œå‡º
        df_header = pd.read_excel(excel_path, sheet_name=sheet_name, header=header_row, nrows=1)
        
        date_columns = []
        for col in df_header.columns:
            col_str = str(col)
            # æ—¥ä»˜ã‚‰ã—ã„åˆ—åã‚’æ¤œå‡º (ä¾‹: "1/1", "2024/1/1", "01/01"ãªã©)
            if re.search(r'\d{1,4}[/-]\d{1,2}([/-]\d{1,4})?', col_str):
                try:
                    # æ—¥ä»˜ã¨ã—ã¦è§£æã‚’è©¦è¡Œ
                    parsed_date = pd.to_datetime(col_str, errors='coerce')
                    if pd.notna(parsed_date):
                        date_columns.append(parsed_date.date())
                except:
                    continue
        
        if len(date_columns) >= 2:
            date_columns.sort()
            start_date = date_columns[0]
            end_date = date_columns[-1]
            return [start_date, end_date]
        
        return None
        
    except Exception as e:
        log.warning(f"æ—¥ä»˜ç¯„å›²è‡ªå‹•æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
        return None


def run_import_wizard() -> None:
    """Show step-by-step wizard for Excel ingest."""
    step = st.session_state.get("wizard_step", 1)
    st.header("ğŸ“¥ Excel Import Wizard")
    
    # ğŸ“Š ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
    progress_value = (step - 1) / 3  # 4ã‚¹ãƒ†ãƒƒãƒ—æƒ³å®š
    st.progress(progress_value, text=f"ã‚¹ãƒ†ãƒƒãƒ— {step}/4")
    
    # ğŸ”™ æˆ»ã‚‹ãƒœã‚¿ãƒ³ã®å®Ÿè£… (Step 2ä»¥é™ã§è¡¨ç¤º)
    if step > 1:
        col_back, col_spacer = st.columns([1, 4])
        with col_back:
            if st.button("ğŸ”™ å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã«æˆ»ã‚‹", key=f"back_step_{step}"):
                st.session_state.wizard_step = step - 1
                st.rerun()

    if step == 1:
        default_excel = os.getenv("SHIFT_SUITE_DEFAULT_EXCEL")
        if default_excel and not st.session_state.get("wizard_excel_path"):
            try:
                xls = pd.ExcelFile(default_excel)
            except Exception as e:  # noqa: BLE001
                log_and_display_error(
                    "Excel\u30d5\u30a1\u30a4\u30eb\u306e\u81ea\u52d5\u8aad\u307f\u8fbc\u307f\u306b\u5931\u6557\u3057\u307e\u3057\u305f",
                    e,
                )
            else:
                default_path = Path(default_excel)
                st.session_state.wizard_excel_path = str(default_path)
                st.session_state.wizard_file_size = default_path.stat().st_size
                st.session_state.wizard_sheet_names = xls.sheet_names
                st.session_state.work_root_path_str = str(default_path.parent)

        uploaded = st.file_uploader("Excel file", type=["xlsx"], key="wiz_upload")
        if uploaded is not None:
            if st.session_state.get("wizard_file_size") != uploaded.size:
                tmp = tempfile.mkdtemp(prefix="ShiftSuiteWizard_")
                path = Path(tmp) / uploaded.name
                with open(path, "wb") as f:
                    f.write(uploaded.getbuffer())
                st.session_state.wizard_excel_path = str(path)
                st.session_state.wizard_file_size = uploaded.size
                st.session_state.work_root_path_str = str(tmp)
                xls = pd.ExcelFile(path)
                st.session_state.wizard_sheet_names = xls.sheet_names
        if st.session_state.wizard_excel_path:
            master = st.selectbox(
                "å‹¤å‹™åŒºåˆ†ã‚·ãƒ¼ãƒˆ", st.session_state.wizard_sheet_names, key="wiz_master"
            )
            opts = [s for s in st.session_state.wizard_sheet_names if s != master]
            st.multiselect("ã‚·ãƒ•ãƒˆå®Ÿç¸¾ã‚·ãƒ¼ãƒˆ", opts, key="wiz_shift_sheets")
            if st.button("Next", disabled=not st.session_state.get("wiz_shift_sheets")):
                st.session_state.shift_sheets_multiselect_widget = (
                    st.session_state.wiz_shift_sheets[:]
                )
                st.session_state.candidate_sheet_list_for_ui = (
                    st.session_state.wiz_shift_sheets[:]
                )
                st.session_state.uploaded_files_info = {
                    uploaded.name: {
                        "path": st.session_state.wizard_excel_path,
                        "size": uploaded.size,
                    }
                }
                st.session_state.year_month_cell_input_widget = "D1"
                st.session_state.header_row_input_widget = 1
                st.session_state.data_start_row_input_widget = 3
                st.session_state.wizard_step = 2
                st.rerun()
        return

    if step == 2:
        for sheet in st.session_state.shift_sheets_multiselect_widget:
            st.subheader(sheet)
            ym = st.text_input(
                "å¹´æœˆæƒ…å ±ã‚»ãƒ«ä½ç½®", value="D1", key=f"ym_{sheet}", help="ä¾‹: D1"
            )
            hdr = st.number_input(
                "åˆ—åãƒ˜ãƒƒãƒ€ãƒ¼è¡Œç•ªå·", 1, 20, value=1, key=f"hdr_{sheet}"
            )
            data_start = st.number_input(
                "ãƒ‡ãƒ¼ã‚¿é–‹å§‹è¡Œç•ªå·", 1, 20, value=3, key=f"data_{sheet}", help="ãƒ‡ãƒ¼ã‚¿ãŒé–‹å§‹ã•ã‚Œã‚‹è¡Œç•ªå·"
            )
            df_prev = pd.read_excel(
                st.session_state.wizard_excel_path,
                sheet_name=sheet,
                header=int(hdr) - 1,
                nrows=10,
            )
            st.dataframe(df_prev, use_container_width=True)
            
            # ğŸ“… å‚ç…§æœŸé–“ã®è‡ªå‹•æ¨å®šã‚’å®Ÿè¡Œ
            auto_date_range = estimate_date_range_from_excel(
                st.session_state.wizard_excel_path, sheet, int(hdr) - 1
            )
            if auto_date_range and len(auto_date_range) == 2:
                if f"auto_start_{sheet}" not in st.session_state:
                    st.session_state[f"auto_start_{sheet}"] = auto_date_range[0]
                    st.session_state[f"auto_end_{sheet}"] = auto_date_range[1]
                    st.success(f"ğŸ“… å‚ç…§æœŸé–“ã‚’è‡ªå‹•æ¨å®š: {auto_date_range[0]} ï½ {auto_date_range[1]}")
                    # å®Ÿéš›ã®å‚ç…§æœŸé–“ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã«åæ˜ 
                    if "need_ref_start_date_widget" not in st.session_state:
                        st.session_state.need_ref_start_date_widget = auto_date_range[0]
                        st.session_state.need_ref_end_date_widget = auto_date_range[1]
            rc = excel_cell_to_row_col(ym)
            ym_text = "N/A"
            if rc is not None:
                r, c = rc
                try:
                    cell_df = pd.read_excel(
                        st.session_state.wizard_excel_path,
                        sheet_name=sheet,
                        header=None,
                        skiprows=r,
                        nrows=1,
                        usecols=[c],
                        dtype=str,
                    )
                    ym_text = str(cell_df.iloc[0, 0])
                except Exception:
                    pass
            st.caption(f"æŠ½å‡ºå¹´æœˆ: {ym_text}")
            st.caption(f"èªè­˜åˆ—å: {df_prev.columns.tolist()}")
        if st.button("Next", key="wiz_next2"):
            first = st.session_state.shift_sheets_multiselect_widget[0]
            st.session_state.year_month_cell_input_widget = st.session_state[
                f"ym_{first}"
            ]
            st.session_state.header_row_input_widget = st.session_state[f"hdr_{first}"]
            st.session_state.wizard_step = 3
            st.rerun()
        return

    if step == 3:
        first = st.session_state.shift_sheets_multiselect_widget[0]
        hdr = st.session_state.get(
            f"hdr_{first}", st.session_state.get("header_row_input_widget", 1)
        )
        df_cols = pd.read_excel(
            st.session_state.wizard_excel_path,
            sheet_name=first,
            header=int(hdr) - 1,
            nrows=1,
        )
        cols = list(df_cols.columns)
        guessed: dict[str, str] = {}
        for c in cols:
            canon = SHEET_COL_ALIAS.get(_normalize(str(c)))
            if canon and canon not in guessed:
                guessed[canon] = c
        st.selectbox(
            "æ°ååˆ—",
            cols,
            index=cols.index(guessed.get("staff", cols[0])),
            key="map_staff",
        )
        st.selectbox(
            "è·ç¨®åˆ—",
            cols,
            index=cols.index(guessed.get("role", cols[0])),
            key="map_role",
        )
        st.selectbox(
            "é›‡ç”¨å½¢æ…‹åˆ—",
            cols,
            index=cols.index(guessed.get("employment", cols[0])),
            key="map_emp",
        )
        date_cols = [c for c in cols if re.search(r"\d", str(c))]
        if date_cols:
            st.caption(f"æœ€åˆã®æ—¥ä»˜åˆ—å€™è£œ: {date_cols[0]}")
        if st.button("Next", key="wiz_next3"):
            st.session_state.wizard_mapping = {
                "staff": st.session_state.map_staff,
                "role": st.session_state.map_role,
                "employment": st.session_state.map_emp,
            }
            st.session_state.wizard_step = 4
            st.rerun()
        return

    if step == 4:
        st.write("### è¨­å®šå†…å®¹ç¢ºèª")
        st.write("å®Ÿç¸¾ã‚·ãƒ¼ãƒˆ:", st.session_state.shift_sheets_multiselect_widget)
        st.write("å¹´æœˆã‚»ãƒ«:", st.session_state.year_month_cell_input_widget)
        st.write("ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ:", st.session_state.header_row_input_widget)
        st.write("åˆ—ãƒãƒƒãƒ”ãƒ³ã‚°:", st.session_state.wizard_mapping)
        if st.button("å–ã‚Šè¾¼ã¿é–‹å§‹", key="wiz_ingest"):
            try:
                long_df, _, unknown_codes = ingest_excel(
                    Path(st.session_state.wizard_excel_path),
                    shift_sheets=st.session_state.shift_sheets_multiselect_widget,
                    header_row=int(st.session_state.header_row_input_widget) - 1,  # Convert from 1-indexed to 0-indexed
                    slot_minutes=int(st.session_state.slot_input_widget),
                    year_month_cell_location=st.session_state.year_month_cell_input_widget,
                )
            except Exception as e:  # noqa: BLE001
                st.error(f"èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                log.error(f"Wizard ingest failed: {e}", exc_info=True)
                return
            st.session_state.analysis_results = {"preview": long_df.head()}
            st.session_state.long_df = long_df
            if not long_df.empty:
                if "role" in long_df.columns:
                    st.session_state.available_roles = (
                        sorted(long_df["role"].astype(str).dropna().unique().tolist())
                    )
                if "employment" in long_df.columns:
                    st.session_state.available_employments = (
                        sorted(long_df["employment"].astype(str).dropna().unique().tolist())
                    )
            if unknown_codes:
                st.warning("æœªçŸ¥ã®å‹¤å‹™ã‚³ãƒ¼ãƒ‰: " + ", ".join(sorted(unknown_codes)))
            st.success("å–ã‚Šè¾¼ã¿å®Œäº†")
            st.dataframe(long_df.head(), use_container_width=True)


@st.cache_data(show_spinner=False, ttl=3600)
def load_data_cached(
    file_path: str,
    *,
    file_mtime: float | None = None,
    is_parquet: bool = False,
    **kwargs,
) -> pd.DataFrame:
    """Load an Excel or Parquet file with caching."""
    p = Path(file_path)
    if not p.exists():
        return pd.DataFrame()

    if is_parquet:
        return pd.read_parquet(p)

    return safe_read_excel(file_path, **kwargs)


@st.cache_data(show_spinner=False, ttl=1800)
def compute_heatmap_ratio_cached(
    heat_df: pd.DataFrame, need_series: pd.Series
) -> pd.DataFrame:
    """Cache expensive ratio calculations for heatmaps."""
    if heat_df.empty or need_series.empty:
        return pd.DataFrame()

    clean_df = heat_df.drop(
        columns=[c for c in SUMMARY5_CONST if c in heat_df.columns], errors="ignore"
    )
    need_series_safe = need_series.replace(0, np.nan)
    return clean_df.div(need_series_safe, axis=0).clip(lower=0, upper=2)


@st.cache_data(show_spinner=False, ttl=1800)
def prepare_heatmap_display_data(df_heat: pd.DataFrame, mode: str) -> pd.DataFrame:
    """Cache heatmap display data preparation."""
    if df_heat.empty:
        return pd.DataFrame()

    if mode == "Ratio":
        need_series = df_heat.get("need", pd.Series())
        return compute_heatmap_ratio_cached(df_heat, need_series)
    else:
        return df_heat.drop(
            columns=[c for c in SUMMARY5_CONST if c in df_heat.columns],
            errors="ignore",
        )


@st.cache_data(show_spinner=False, ttl=1800)
def optimize_large_heatmap_display(
    df: pd.DataFrame, max_cells: int = 10000
) -> pd.DataFrame:
    """Optimize large heatmap display by intelligent sampling."""
    if df.empty or df.shape[0] * df.shape[1] <= max_cells:
        return df

    sample_step_rows = max(1, len(df) // 100)
    sample_step_cols = max(1, len(df.columns) // 50)
    return df.iloc[::sample_step_rows, ::sample_step_cols]


@st.cache_data(show_spinner=False, ttl=1800)
def load_all_heatmap_files(data_dir: Path) -> dict:
    """Load all available heatmap parquet files dynamically."""
    heatmap_data: dict[str, pd.DataFrame] = {}

    base_files = {"heat_all": "heat_ALL.parquet"}

    for key, filename in base_files.items():
        fp = data_dir / filename
        if fp.exists():
            try:
                heatmap_data[key] = pd.read_parquet(fp)
                log.info(
                    "'%s'ã‚’èª­ã¿è¾¼ã¿ã€heatmap_data['%s']ã«æ ¼ç´ã—ã¾ã—ãŸã€‚", filename, key
                )
            except Exception as e:  # noqa: BLE001
                log.warning("%s ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", filename, e)

    for pattern in ["heat_role_*.parquet", "heat_emp_*.parquet"]:
        for fp in data_dir.glob(pattern):
            try:
                if pattern.startswith("heat_role_"):
                    key = f"heat_role_{fp.stem.replace('heat_role_', '')}"
                else:
                    key = f"heat_emp_{fp.stem.replace('heat_emp_', '')}"
                heatmap_data[key] = pd.read_parquet(fp)
                log.info(
                    "'%s'ã‚’èª­ã¿è¾¼ã¿ã€heatmap_data['%s']ã«æ ¼ç´ã—ã¾ã—ãŸã€‚", fp.name, key
                )
            except Exception as e:  # noqa: BLE001
                log.warning("%s ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", fp.name, e)

    return heatmap_data


def load_and_sum_heatmaps(data_dir: Path, keys: list[str]) -> pd.DataFrame:
    """Load multiple heatmap files and return their aggregated sum."""
    dfs = []
    for key in keys:
        df = st.session_state.display_data.get(key)
        if df is None:
            fp = data_dir / f"{key}.parquet"
            if fp.exists():
                try:
                    df = pd.read_parquet(fp)
                    st.session_state.display_data[key] = df
                except Exception as e:  # noqa: BLE001
                    log.warning("%s ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", fp.name, e)
                    df = None
        if isinstance(df, pd.DataFrame) and not df.empty:
            dfs.append(df)
    if not dfs:
        return pd.DataFrame()

    total = dfs[0].copy()
    for df in dfs[1:]:
        total = total.add(df, fill_value=0)

    if {"need", "staff"}.issubset(total.columns):
        total["lack"] = (total["need"] - total["staff"]).clip(lower=0)
    if {"staff", "upper"}.issubset(total.columns):
        total["excess"] = (total["staff"] - total["upper"]).clip(lower=0)
    return total


def update_display_data_with_heatmaps(out_dir: Path) -> None:
    """Update ``display_data`` including dynamically loaded heatmap files."""
    log.info(f"è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã®æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™ã€‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {out_dir}")

    st.session_state.display_data = {}

    files_to_load = {
        "shortage_role": "shortage_role_summary.parquet",
        "shortage_emp": "shortage_employment_summary.parquet",
        "shortage_time": "shortage_time.parquet",
        "excess_time": "excess_time.parquet",
        "shortage_ratio": "shortage_ratio.parquet",
        "excess_ratio": "excess_ratio.parquet",
        "shortage_freq": "shortage_freq.parquet",
        "excess_freq": "excess_freq.parquet",
        "shortage_leave": "shortage_leave.parquet",
        "fatigue_score": "fatigue_score.parquet",
        "fairness_before": "fairness_before.parquet",
        "fairness_after": "fairness_after.parquet",
        "forecast": "forecast.parquet",
        "hire_plan": "hire_plan.parquet",
        "optimal_hire_plan": "optimal_hire_plan.parquet",
        "cost_benefit": "cost_benefit.parquet",
        "daily_cost": "daily_cost.parquet",
        "staff_stats": "staff_stats.parquet",
        "stats_alerts": "stats_alerts.parquet",
        "demand_series": "demand_series.csv",
    }

    initial_loaded = 0
    for key, filename in files_to_load.items():
        fp = out_dir / filename
        if fp.exists():
            try:
                if filename.endswith(".csv"):
                    st.session_state.display_data[key] = pd.read_csv(fp)
                else:
                    st.session_state.display_data[key] = pd.read_parquet(fp)
                initial_loaded += 1
                log.info(
                    "'%s'ã‚’èª­ã¿è¾¼ã¿ã€display_data['%s']ã«æ ¼ç´ã—ã¾ã—ãŸã€‚", filename, key
                )
            except Exception as e:  # noqa: BLE001
                log.warning("%s ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", filename, e)
        else:
            log.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {fp}")

    # --- ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¨æ´¾ç”Ÿãƒ‡ãƒ¼ã‚¿ã®äº‹å‰è¨ˆç®—ã‚’è¿½åŠ  ---

    # 1. å…¨ã¦ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä¸€æ‹¬ã§èª­ã¿è¾¼ã‚€
    heatmap_data = load_all_heatmap_files(out_dir)
    st.session_state.display_data.update(heatmap_data)

    # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆè·ç¨®ãƒ»é›‡ç”¨å½¢æ…‹ãƒªã‚¹ãƒˆï¼‰ã‚’èª­ã¿è¾¼ã‚€
    roles, employments = load_shortage_meta(out_dir)
    st.session_state.available_roles = roles
    st.session_state.available_employments = employments

    # 3. èª­ã¿è¾¼ã‚“ã å„ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‹ã‚‰æ´¾ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ã—ã¦ä¿å­˜
    for key, df_heat in heatmap_data.items():
        if not isinstance(df_heat, pd.DataFrame) or df_heat.empty:
            continue

        # ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        ratio_key = key.replace("heat_", "ratio_")
        st.session_state.display_data[ratio_key] = calc_ratio_from_heatmap_simple(df_heat)

        # æœ€é©åŒ–ã‚¹ã‚³ã‚¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        score_key = key.replace("heat_", "score_")
        st.session_state.display_data[score_key] = calc_opt_score_from_heatmap(df_heat)

        # æœ€é©åŒ–åˆ†æã‚¿ãƒ–ç”¨ã®ãƒ‡ãƒ¼ã‚¿ (Surplus, Margin)
        date_cols = [c for c in df_heat.columns if _parse_as_date(str(c)) is not None]
        if not date_cols:
            continue

        staff_df = df_heat[date_cols].fillna(0)
        need_df = pd.DataFrame(
            np.repeat(df_heat["need"].values[:, np.newaxis], len(date_cols), axis=1),
            index=df_heat.index,
            columns=date_cols,
        )
        upper_df = pd.DataFrame(
            np.repeat(df_heat["upper"].values[:, np.newaxis], len(date_cols), axis=1),
            index=df_heat.index,
            columns=date_cols,
        )

        surplus_key = key.replace("heat_", "surplus_")
        st.session_state.display_data[surplus_key] = (staff_df - need_df).clip(lower=0).fillna(0).astype(int)

        margin_key = key.replace("heat_", "margin_")
        st.session_state.display_data[margin_key] = (upper_df - staff_df).clip(lower=0).fillna(0).astype(int)

    loaded_count = len(st.session_state.display_data)
    log.info(
        "display_dataæ›´æ–°å®Œäº†: %då€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã«èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚", loaded_count
    )


@st.cache_data(show_spinner=False, ttl=900)
def get_optimized_display_data(
    df_heat: pd.DataFrame, mode: str, max_display_cells: int = 15000
) -> pd.DataFrame:
    """Get optimized display data with intelligent sampling for large datasets."""
    if df_heat.empty:
        return pd.DataFrame()

    prepared_data = prepare_heatmap_display_data(df_heat, mode)

    if prepared_data.shape[0] * prepared_data.shape[1] > max_display_cells:
        return optimize_large_heatmap_display(prepared_data, max_display_cells)

    return prepared_data


@st.cache_data(show_spinner=False, ttl=600)
def create_progress_indicator():
    """Create reusable progress indicator components."""
    return st.empty(), st.empty()


@st.cache_data(show_spinner=False, ttl=3600)
def generate_heatmap_figure(df_heat, mode, scope_info, max_display_cells=15000):
    """Generate and cache heatmap figure."""
    if not isinstance(df_heat, pd.DataFrame) or df_heat.empty:
        return px.imshow(pd.DataFrame(), title=_("Data not available or empty"))

    prepared_data = prepare_heatmap_display_data(df_heat, mode)

    if prepared_data.shape[0] * prepared_data.shape[1] > max_display_cells:
        display_df = optimize_large_heatmap_display(prepared_data, max_display_cells)
        st.info("å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãŸã‚ã€è¡¨ç¤ºã‚’æœ€é©åŒ–ã—ã¦ã„ã¾ã™...")
    else:
        display_df = prepared_data

    color_scale = "RdBu_r" if mode == "Ratio" else "Blues"
    title = f"{scope_info} - {_('Heatmap') if mode != 'Ratio' else _('Ratio (staff Ã· need)')}"

    fig = px.imshow(
        display_df,
        aspect="auto",
        color_continuous_scale=color_scale,
        title=title,
        labels={"x": _("Date"), "y": _("Time"), "color": _(mode)},
    )
    fig.update_xaxes(
        tickvals=list(range(len(display_df.columns))),
        ticktext=[date_with_weekday(c) for c in display_df.columns],
    )
    return fig


st.set_page_config(
    page_title="Shift-Suite", layout="wide", initial_sidebar_state="expanded"
)
st.title("ğŸ—‚ï¸ Shift-Suite : å‹¤å‹™ã‚·ãƒ•ãƒˆåˆ†æãƒ„ãƒ¼ãƒ«")

master_sheet_keyword = "å‹¤å‹™åŒºåˆ†"

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ– (ä¸€åº¦ã ã‘å®Ÿè¡Œ) ---
if "app_initialized" not in st.session_state:
    st.session_state.app_initialized = True
    st.session_state.analysis_done = False
    st.session_state.work_root_path_str = None
    st.session_state.out_dir_path_str = None
    st.session_state.current_step_for_progress = 0

    # ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨ã®è¾æ›¸ã‚’å®‰å…¨ã«åˆæœŸåŒ–
    if 'display_data' not in st.session_state:
        st.session_state.display_data = {}
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = {}
    if 'analysis_status' not in st.session_state:
        st.session_state.analysis_status = {}
    if 'leave_analysis_results' not in st.session_state:
        st.session_state.leave_analysis_results = {}
    
    # ğŸ¯ çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    if UNIFIED_ANALYSIS_AVAILABLE and 'unified_analysis_manager' not in st.session_state:
        st.session_state.unified_analysis_manager = UnifiedAnalysisManager()
        log.info("çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¢é€£ã®åˆæœŸåŒ–ã‚’ç¢ºå®Ÿã«å®Ÿè¡Œ
    if "heatmap_cache_key" not in st.session_state:
        st.session_state.heatmap_cache_key = None

    if "performance_mode" not in st.session_state:
        st.session_state.performance_mode = "auto"

    log.info("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿æ ¼ç´ç”¨è¾æ›¸ã‚’åˆæœŸåŒ–ã€‚")

    today_val = datetime.date.today()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®ã‚­ãƒ¼ã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«åˆæœŸè¨­å®š
    st.session_state.slot_input_widget = 30
    st.session_state.header_row_input_widget = 1
    st.session_state.year_month_cell_input_widget = "D1"
    st.session_state.data_start_row_input_widget = 3
    st.session_state.candidate_sheet_list_for_ui = []
    st.session_state.shift_sheets_multiselect_widget = []
    st.session_state._force_update_multiselect_flag = False

    st.session_state.need_ref_start_date_widget = today_val - datetime.timedelta(
        days=59
    )  # åˆæœŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    st.session_state.need_ref_end_date_widget = today_val - datetime.timedelta(
        days=1
    )  # åˆæœŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    st.session_state._force_update_need_ref_dates_flag = False
    st.session_state._intended_need_ref_start_date = None
    st.session_state._intended_need_ref_end_date = None

    st.session_state.need_stat_method_options_widget = [
        "10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«",
        "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«",
        "ä¸­å¤®å€¤",
        "å¹³å‡å€¤",
    ]
    st.session_state.need_stat_method_widget = "ä¸­å¤®å€¤"
    st.session_state.need_remove_outliers_widget = True
    st.session_state.need_adjustment_factor_widget = 1.0

    st.session_state.min_method_for_upper_options_widget = ["mean-1s", "p25", "mode"]
    st.session_state.min_method_for_upper_widget = "p25"
    st.session_state.max_method_for_upper_options_widget = ["mean+1s", "p75"]
    st.session_state.max_method_for_upper_widget = "p75"

    #  ä¼‘æš‡åˆ†æã‚’å«ã‚€è¿½åŠ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒªã‚¹ãƒˆ
    st.session_state.available_ext_opts_widget = [
        "Stats",
        "Anomaly",
        "Fatigue",
        "Cluster",
        "Skill",
        "Fairness",
        "Rest Time Analysis",
        "Work Pattern Analysis",
        "Attendance Analysis",
        "Combined Score",
        "Low Staff Load",
        _("Leave Analysis"),
        "Need forecast",
        "RL roster (PPO)",
        "RL roster (model)",
        "Hire plan",
        "Cost / Benefit",
        "æœ€é©æ¡ç”¨è¨ˆç”»",
    ]
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¼‘æš‡åˆ†æã‚‚é¸æŠçŠ¶æ…‹ã«ã™ã‚‹ã‹ã¯ãŠå¥½ã¿ã§
    st.session_state.ext_opts_multiselect_widget = (
        st.session_state.available_ext_opts_widget[:]
    )

    st.session_state.save_mode_selectbox_options_widget = [
        _("ZIP Download"),
        _("Save to folder"),
    ]
    st.session_state.save_mode_selectbox_widget = _("ZIP Download")

    st.session_state.std_work_hours_widget = 160
    st.session_state.safety_factor_widget = 0.0
    st.session_state.target_coverage_widget = 0.95
    st.session_state.wage_direct_widget = 1500
    st.session_state.wage_temp_widget = 2200
    st.session_state.hiring_cost_once_widget = 180000
    st.session_state.penalty_per_lack_widget = 4000
    st.session_state.forecast_period_widget = 30

    #  ä¼‘æš‡åˆ†æç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸåŒ–
    st.session_state.leave_analysis_target_types_widget = [
        LEAVE_TYPE_REQUESTED,
        LEAVE_TYPE_PAID,
        LEAVE_TYPE_OTHER,
    ]  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ä¸¡æ–¹
    st.session_state.leave_concentration_threshold_widget = (
        3  # å¸Œæœ›ä¼‘é›†ä¸­åº¦é–¾å€¤ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    )

    #  ä¼‘æš‡åˆ†æçµæœæ ¼ç´ç”¨
    st.session_state.leave_analysis_results = {}

    st.session_state.rest_time_results = None
    st.session_state.rest_time_monthly = None
    st.session_state.work_pattern_results = None
    st.session_state.work_pattern_monthly = None
    st.session_state.attendance_results = None
    st.session_state.combined_score_results = None
    st.session_state.low_staff_load_results = None
    st.session_state.uploaded_files_info = {}
    st.session_state.file_options = {}
    st.session_state.analysis_results = {}
    st.session_state.analysis_status = {}
    st.session_state.wizard_step = 1
    st.session_state.wizard_excel_path = None
    st.session_state.wizard_sheet_names = []
    st.session_state.wizard_shift_sheets = []
    st.session_state.wizard_mapping = {}
    st.session_state.long_df = pd.DataFrame()
    log.info("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸã€‚")

run_import_wizard()

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®UIè¦ç´  ---
with st.sidebar:
    st.header("ğŸ› ï¸ è§£æè¨­å®š")

    with st.expander("åŸºæœ¬è¨­å®š", expanded=True):
        st.number_input(
            _("Slot (min)"),
            5,
            120,
            key="slot_input_widget",
            help="åˆ†æã®æ™‚é–“é–“éš”ï¼ˆåˆ†ï¼‰",
        )

    with st.expander("ğŸ“„ ã‚·ãƒ¼ãƒˆé¸æŠã¨ãƒ˜ãƒƒãƒ€ãƒ¼", expanded=True):
        if st.session_state.get("wizard_step", 1) <= 2:
            st.info("Use the Excel Import Wizard to select sheets")
        else:
            if st.session_state.get("_force_update_multiselect_flag", False):
                new_options = st.session_state.candidate_sheet_list_for_ui
                current_selection = st.session_state.get(
                    "shift_sheets_multiselect_widget", []
                )
                valid_selection = [s for s in current_selection if s in new_options]
                if not valid_selection and new_options:
                    st.session_state.shift_sheets_multiselect_widget = new_options[:]
                elif valid_selection:
                    st.session_state.shift_sheets_multiselect_widget = valid_selection
                else:
                    st.session_state.shift_sheets_multiselect_widget = []
                st.session_state._force_update_multiselect_flag = False

            st.multiselect(
                _("Select shift sheets to analyze (multiple)"),
                options=st.session_state.candidate_sheet_list_for_ui,
                default=st.session_state.shift_sheets_multiselect_widget,
                key="shift_sheets_multiselect_widget",
                help="è§£æå¯¾è±¡ã¨ã™ã‚‹ã‚·ãƒ¼ãƒˆã‚’é¸æŠã—ã¾ã™ã€‚",
            )
            # Force reset session state if it has invalid value
            if st.session_state.get("header_row_input_widget", 1) < 1:
                st.session_state.header_row_input_widget = 1
            st.number_input(
                _("Header row number (1-indexed)"),
                1,
                20,
                value=1,
                key="header_row_input_widget",
                help="ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¾‹ã® 'No' ãªã©åˆ—åãŒã‚ã‚‹è¡Œç•ªå·",
            )
            st.text_input(
                _("Year-Month cell location"),
                key="year_month_cell_input_widget",
                help="å¹´æœˆæƒ…å ±ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‚»ãƒ«ä½ç½® (ä¾‹: A1)",
            )

    st.subheader("åˆ†æåŸºæº–è¨­å®š")
    need_calc_method = st.radio(
        _("æœ€é©ã‚¾ãƒ¼ãƒ³ã®ä¸‹é™å€¤(Need)ã®ç®—å‡ºæ–¹æ³•"),
        options=[
            _("éå»ã®å®Ÿç¸¾ã‹ã‚‰çµ±è¨ˆçš„ã«æ¨å®šã™ã‚‹"),
            _("äººå“¡é…ç½®åŸºæº–ã«åŸºã¥ãè¨­å®šã™ã‚‹"),
        ],
        key="need_calc_method_widget",
        horizontal=True,
    )

    if need_calc_method == _("éå»ã®å®Ÿç¸¾ã‹ã‚‰çµ±è¨ˆçš„ã«æ¨å®šã™ã‚‹"):
        st.date_input(_("å‚ç…§æœŸé–“ é–‹å§‹æ—¥"), key="need_ref_start_date_widget")
        st.date_input(_("å‚ç…§æœŸé–“ çµ‚äº†æ—¥"), key="need_ref_end_date_widget")
        st.selectbox(
            _("çµ±è¨ˆçš„æŒ‡æ¨™"),
            options=["ä¸­å¤®å€¤", "å¹³å‡å€¤", "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«", "10ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«"],
            key="need_stat_method_widget",
        )
        st.checkbox(
            _("å¤–ã‚Œå€¤ã‚’é™¤å»ã—ã¦Needã‚’ç®—å‡º"),
            value=True,
            key="need_remove_outliers_widget",
        )
        st.slider(
            "å¿…è¦äººæ•° èª¿æ•´ä¿‚æ•°",
            min_value=0.1,
            max_value=1.0,
            value=st.session_state.get("need_adjustment_factor_widget", 1.0),
            step=0.05,
            key="need_adjustment_factor_widget",
        )
    else:
        st.number_input(
            _("åˆ†æå¯¾è±¡ã®å¹³å‡åˆ©ç”¨è€…æ•°"), min_value=0, key="avg_users_widget"
        )
        st.write("è·ç¨®ã”ã¨ã®æœ€ä½å¿…è¦äººæ•°ï¼ˆé…ç½®åŸºæº–ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:")
        if "long_df" in st.session_state and not st.session_state.long_df.empty:
            roles = sorted(st.session_state.long_df["role"].unique())
            manual_need_values = {}
            for role in roles:
                manual_need_values[role] = st.number_input(
                    f"{role}",
                    min_value=0,
                    key=f"manual_need_{role}",
                )
            st.session_state.manual_need_values_widget = manual_need_values

    st.divider()
    upper_calc_method = st.selectbox(
        _("æœ€é©ã‚¾ãƒ¼ãƒ³ã®ä¸Šé™å€¤(Upper)ã®ç®—å‡ºæ–¹æ³•"),
        options=[
            _("ä¸‹é™å€¤(Need) + å›ºå®šå€¤"),
            _("ä¸‹é™å€¤(Need) * å›ºå®šä¿‚æ•°"),
            _("éå»å®Ÿç¸¾ã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«"),
        ],
        key="upper_calc_method_widget",
    )

    if upper_calc_method == _("ä¸‹é™å€¤(Need) + å›ºå®šå€¤"):
        st.number_input(
            _("åŠ ç®—ã™ã‚‹äººæ•°"), min_value=0, step=1, key="upper_param_fixed_val"
        )
    elif upper_calc_method == _("ä¸‹é™å€¤(Need) * å›ºå®šä¿‚æ•°"):
        st.slider(
            _("ä¹—ç®—ã™ã‚‹ä¿‚æ•°"),
            min_value=1.0,
            max_value=2.0,
            value=1.2,
            step=0.05,
            key="upper_param_factor_val",
        )
    else:
        st.selectbox(
            _("ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«"),
            options=[75, 80, 85, 90, 95],
            index=3,
            key="upper_param_percentile_val",
        )

    st.divider()
    with st.expander("è¿½åŠ åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"):
        if "ext_opts_multiselect_widget" not in st.session_state:
            st.session_state.ext_opts_multiselect_widget = st.session_state.get(
                "available_ext_opts_widget", []
            )

        st.multiselect(
            _("Extra modules"),
            st.session_state.available_ext_opts_widget,
            default=st.session_state.ext_opts_multiselect_widget,
            key="ext_opts_multiselect_widget",
            help="å®Ÿè¡Œã™ã‚‹è¿½åŠ ã®åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚",
        )

        if _("Leave Analysis") in st.session_state.ext_opts_multiselect_widget:
            # Nested expanders trigger StreamlitAPIException, so use a heading
            # instead of an inner st.expander here.
            st.markdown("### ğŸ“Š " + _("Leave Analysis") + " è¨­å®š")
            st.multiselect(
                "åˆ†æå¯¾è±¡ã®ä¼‘æš‡ã‚¿ã‚¤ãƒ—",
                options=[
                    LEAVE_TYPE_REQUESTED,
                    LEAVE_TYPE_PAID,
                    LEAVE_TYPE_OTHER,
                ],
                key="leave_analysis_target_types_widget",
                help="åˆ†æã™ã‚‹ä¼‘æš‡ã®ç¨®é¡ã‚’é¸æŠã—ã¾ã™ã€‚",
            )
            if (
                LEAVE_TYPE_REQUESTED
                in st.session_state.leave_analysis_target_types_widget
            ):
                st.number_input(
                    "å¸Œæœ›ä¼‘ é›†ä¸­åº¦åˆ¤å®šé–¾å€¤ (äºº)",
                    min_value=1,
                    step=1,
                    key="leave_concentration_threshold_widget",
                    help="åŒæ—¥ã«ã“ã®äººæ•°ä»¥ä¸Šã®å¸Œæœ›ä¼‘ãŒã‚ã£ãŸå ´åˆã«ã€é›†ä¸­ã€ã¨ã¿ãªã—ã¾ã™ã€‚",
                )

    current_save_mode_idx_val = 0
    try:
        current_save_mode_idx_val = (
            st.session_state.save_mode_selectbox_options_widget.index(
                st.session_state.save_mode_selectbox_widget
            )
        )
    except (ValueError, AttributeError):
        current_save_mode_idx_val = 0
    st.selectbox(
        _("Save method"),
        options=st.session_state.save_mode_selectbox_options_widget,
        index=current_save_mode_idx_val,
        key="save_mode_selectbox_widget",
        help="è§£æçµæœã®ä¿å­˜æ–¹æ³•ã‚’é¸æŠã—ã¾ã™ã€‚",
    )

    with st.expander("ç–²åŠ´ã‚¹ã‚³ã‚¢é‡ã¿è¨­å®š"):
        st.slider(
            "â‘  å‹¤å‹™é–‹å§‹æ™‚åˆ»ãƒ©ãƒ³ãƒ€ãƒ æ€§",
            0.0,
            2.0,
            value=1.0,
            step=0.1,
            key="weight_start_var_widget",
        )
        st.slider(
            "â‘¡ æ¥­å‹™å¤šæ§˜æ€§",
            0.0,
            2.0,
            value=1.0,
            step=0.1,
            key="weight_diversity_widget",
        )
        st.slider(
            "â‘¢ åŠ´åƒæ™‚é–“ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§",
            0.0,
            2.0,
            value=1.0,
            step=0.1,
            key="weight_worktime_var_widget",
        )
        st.slider(
            "â‘£ å¤œå‹¤é–“ã®ä¼‘æ¯ä¸è¶³",
            0.0,
            2.0,
            value=1.0,
            step=0.1,
            key="weight_short_rest_widget",
        )
        st.slider(
            "â‘¤ é€£å‹¤æ—¥æ•°",
            0.0,
            2.0,
            value=1.0,
            step=0.1,
            key="weight_consecutive_widget",
        )
        st.slider(
            "â‘¥ å¤œå‹¤æ¯”ç‡",
            0.0,
            2.0,
            value=1.0,
            step=0.1,
            key="weight_night_ratio_widget",
        )

    with st.expander(_("Cost & Hire Parameters")):
        st.subheader("äººä»¶è²»è¨ˆç®— è¨­å®š")
        cost_by_key = st.radio(
            _("Unit Price Standard"),
            options=["role", "employment", "staff"],
            captions=[_("Role"), _("Employment"), _("Staff")],
            index=0,
            horizontal=True,
            key="cost_by_widget",
        )
        if "long_df" in st.session_state and not st.session_state.long_df.empty:
            if cost_by_key in st.session_state.long_df.columns:
                unique_keys = sorted(st.session_state.long_df[cost_by_key].unique())
                if "wage_config" not in st.session_state:
                    st.session_state.wage_config = {}
                for key in unique_keys:
                    wage_key = f"wage_{cost_by_key}_{key}"
                    if wage_key not in st.session_state:
                        st.session_state[wage_key] = st.session_state.get(
                            "default_wage", 1000
                        )
                    wage_val = st.number_input(
                        f"{_('Hourly Wage')}: {key}",
                        value=st.session_state[wage_key],
                        key=wage_key,
                    )
                    st.session_state.wage_config[key] = wage_val
        st.divider()
        st.subheader("æ¡ç”¨ãƒ»ã‚³ã‚¹ãƒˆè©¦ç®— è¨­å®š")
        st.number_input(
            _("Standard work hours (h/month)"), 100, 300, key="std_work_hours_widget"
        )
        st.slider(
            _("Safety factor (shortage h multiplier)"),
            0.00,
            2.00,
            key="safety_factor_widget",
            help="ä¸è¶³æ™‚é–“ã«ä¹—ç®—ã™ã‚‹å€ç‡ (ä¾‹: 1.10 ã¯ 10% ä¸Šä¹—ã›)",
        )
        st.slider(_("Target coverage rate"), 0.50, 1.00, key="target_coverage_widget")
        st.number_input(
            _("Direct employee labor cost (Â¥/h)"), 500, 10000, key="wage_direct_widget"
        )
        st.number_input(
            _("Temporary staff labor cost (Â¥/h)"), 800, 12000, key="wage_temp_widget"
        )
        st.number_input(
            _("One-time hiring cost (Â¥/person)"),
            0,
            1000000,
            key="hiring_cost_once_widget",
        )
        st.number_input(
            _("Penalty for shortage (Â¥/h)"), 0, 20000, key="penalty_per_lack_widget"
        )

    with st.expander("ä¸Šç´šè¨­å®š"):
        st.number_input(
            _("Forecast days"),
            1,
            365,
            key="forecast_period_widget",
            help="Need forecast ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§å…ˆèª­ã¿ã™ã‚‹æ—¥æ•°",
        )

    st.markdown("---")
    st.subheader("ğŸš¨ ç·Šæ€¥å¯¾å‡¦")
    if st.button("ğŸ”„ å…¨ãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ", type="secondary"):
        for key in list(st.session_state.keys()):
            if key not in ["app_initialized"]:
                del st.session_state[key]
        try:
            st.cache_data.clear()
        except Exception:
            pass
        st.rerun()

    if st.button("ğŸ“Š display_dataå¼·åˆ¶å†èª­ã¿è¾¼ã¿", type="secondary"):
        st.session_state.display_data = {}
        st.rerun()

# --- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ ---
holiday_file_global_uploaded = st.file_uploader(
    _("Global holiday file (CSV or JSON)"),
    type=["csv", "json"],
    key="holiday_file_global_widget",
    help="å…¨å›½å…±é€šã®ç¥æ—¥ãªã© (YYYY-MM-DD)",
)
holiday_file_local_uploaded = st.file_uploader(
    _("Local holiday file (CSV or JSON)"),
    type=["csv", "json"],
    key="holiday_file_local_widget",
    help="æ–½è¨­å›ºæœ‰ã®ä¼‘æ¥­æ—¥ (YYYY-MM-DD)",
)

run_button_disabled_status = (
    not st.session_state.uploaded_files_info
    or not st.session_state.get("shift_sheets_multiselect_widget", [])
)
run_button_clicked = st.button(
    _("Run Analysis"),
    key="run_analysis_button_final_trigger",
    use_container_width=True,
    type="primary",
    disabled=run_button_disabled_status,
)

if (
    st.session_state.get("shift_sheets_multiselect_widget")
    and st.session_state.uploaded_files_info
):
    try:
        first_file_path = next(iter(st.session_state.uploaded_files_info.values()))[
            "path"
        ]
        preview_df_sidebar = pd.read_excel(
            first_file_path,
            sheet_name=st.session_state.shift_sheets_multiselect_widget[0],
            nrows=5,
            header=None,
        )
        st.caption(
            _("Preview")
            + f": {st.session_state.shift_sheets_multiselect_widget[0]} (first 5 rows)"
        )
        st.dataframe(preview_df_sidebar.astype(str), use_container_width=True)
    except Exception as e_prev:
        st.warning(_("Error during preview display") + f": {e_prev}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  app.py  (Part 2 / 3)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if run_button_clicked:
    # å®Œå…¨ãªãƒªã‚»ãƒƒãƒˆå‡¦ç†
    st.session_state.analysis_done = False
    
    # analysis_resultsã¯å®‰å…¨ã«ãƒªã‚»ãƒƒãƒˆï¼ˆæ–°ã—ã„åˆ†æçµæœç”¨ã¯ä¿è­·ï¼‰
    if 'analysis_results' not in st.session_state:
        st.session_state.analysis_results = {}
    else:
        # æ—¢å­˜ã®analysis_resultsã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ãŒã€æ–°ã—ã„ã‚­ãƒ¼ã®ãŸã‚ã®è¾æ›¸æ§‹é€ ã¯ç¶­æŒ
        st.session_state.analysis_results.clear()
    
    st.session_state.analysis_status = {}

    # è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã‚‚å®Œå…¨ã«ã‚¯ãƒªã‚¢
    st.session_state.display_data = {}

    # ä¼‘æš‡åˆ†æçµæœã‚‚ã‚¯ãƒªã‚¢
    st.session_state.leave_analysis_results = {}

    # ãã®ä»–ã®åˆ†æçµæœã‚‚ã‚¯ãƒªã‚¢
    st.session_state.rest_time_results = None
    st.session_state.rest_time_monthly = None
    st.session_state.work_pattern_results = None
    st.session_state.work_pattern_monthly = None
    st.session_state.attendance_results = None
    st.session_state.combined_score_results = None
    st.session_state.low_staff_load_results = None

    # 1. æ¯”è¼ƒã—ãŸã„åˆ†æã‚·ãƒŠãƒªã‚ªã‚’å®šç¾©
    analysis_scenarios = {
        "median_based": {"name": "ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹", "need_stat_method": "ä¸­å¤®å€¤"},
        "mean_based": {"name": "å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹", "need_stat_method": "å¹³å‡å€¤"},
        "p25_based": {"name": "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹", "need_stat_method": "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«"},
    }

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
    try:
        if hasattr(st, "cache_data"):
            st.cache_data.clear()
        log.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")
    except Exception as e:
        log.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ã«å¤±æ•—: {e}")

    log.info("æ–°ã—ã„åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚ã™ã¹ã¦ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")

    holiday_dates_global_for_run = None
    holiday_dates_local_for_run = None

    def _read_holiday_upload(uploaded_file: IO[str | bytes]) -> list[dt.date]:
        """Return dates from an uploaded CSV or JSON file."""
        if uploaded_file.name.lower().endswith(".json"):
            import json

            return [pd.to_datetime(d).date() for d in json.load(uploaded_file)]
        df_h = pd.read_csv(uploaded_file, header=None)
        return [pd.to_datetime(x).date() for x in df_h.iloc[:, 0].dropna().unique()]

    if holiday_file_global_uploaded is not None:
        try:
            holiday_dates_global_for_run = _read_holiday_upload(
                holiday_file_global_uploaded
            )
        except Exception as e_hread:
            st.warning(_("Holiday file parse error") + f": {e_hread}")
            log.warning(f"Holiday file parse error: {e_hread}")

    if holiday_file_local_uploaded is not None:
        try:
            holiday_dates_local_for_run = _read_holiday_upload(
                holiday_file_local_uploaded
            )
        except Exception as e_hread:
            st.warning(_("Holiday file parse error") + f": {e_hread}")
            log.warning(f"Holiday file parse error: {e_hread}")

    for file_name, file_info in st.session_state.uploaded_files_info.items():
        st.session_state.current_step_for_progress = 0

        excel_path_to_use = Path(file_info["path"])
        if (
            st.session_state.work_root_path_str is None
            or not excel_path_to_use.exists()
        ):
            log_and_display_error(
                "Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚",
                FileNotFoundError(excel_path_to_use),
            )
            st.stop()

        work_root_exec = excel_path_to_use.parent
        st.session_state.work_root_path_str = str(work_root_exec)
        st.session_state.out_dir_path_str = str(work_root_exec / "out")
        out_dir_exec = Path(st.session_state.out_dir_path_str)
        out_dir_exec.mkdir(parents=True, exist_ok=True)
        log.info(f"è§£æå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {out_dir_exec} (file: {file_name})")
        base_work_dir = Path(st.session_state.work_root_path_str)

        # --- å®Ÿè¡Œæ™‚ã®UIã®å€¤ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã‹ã‚‰å–å¾— ---
        param_selected_sheets = st.session_state.shift_sheets_multiselect_widget
        param_header_row = st.session_state.header_row_input_widget - 1  # Convert from 1-indexed to 0-indexed
        param_year_month_cell = st.session_state.year_month_cell_input_widget
        param_slot = st.session_state.slot_input_widget
        param_need_calc_method = st.session_state.need_calc_method_widget
        param_need_ref_start = st.session_state.get("need_ref_start_date_widget")
        param_need_ref_end = st.session_state.get("need_ref_end_date_widget")
        param_need_stat_method = st.session_state.get("need_stat_method_widget")
        param_need_manual = st.session_state.get("manual_need_values_widget")
        param_need_remove_outliers = st.session_state.get(
            "need_remove_outliers_widget",
            True,
        )
        param_upper_method = st.session_state.upper_calc_method_widget
        param_upper_param = {
            "fixed_value": st.session_state.get("upper_param_fixed_val"),
            "factor": st.session_state.get("upper_param_factor_val"),
            "percentile": st.session_state.get("upper_param_percentile_val"),
        }
        param_ext_opts = st.session_state.ext_opts_multiselect_widget
        param_save_mode = st.session_state.save_mode_selectbox_widget
        param_std_work_hours = st.session_state.std_work_hours_widget
        param_safety_factor = st.session_state.safety_factor_widget
        param_target_coverage = st.session_state.target_coverage_widget
        param_wage_direct = st.session_state.wage_direct_widget
        param_wage_temp = st.session_state.wage_temp_widget
        param_hiring_cost = st.session_state.hiring_cost_once_widget
        param_penalty_lack = st.session_state.penalty_per_lack_widget
        param_forecast_period = st.session_state.forecast_period_widget

        #  ä¼‘æš‡åˆ†æç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å–å¾—
        param_leave_target_types = st.session_state.leave_analysis_target_types_widget
        param_leave_concentration_threshold = (
            st.session_state.leave_concentration_threshold_widget
        )

        #  ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆå†…ã®å‰å›çµæœã‚’ã‚¯ãƒªã‚¢
        st.session_state.leave_analysis_results = {}
        # --- UIå€¤å–å¾—ã“ã“ã¾ã§ ---

        st.session_state.rest_time_results = None
        st.session_state.work_pattern_results = None
        st.session_state.attendance_results = None
        st.session_state.combined_score_results = None
        st.session_state.low_staff_load_results = None
        progress_status = st.empty()
        progress_text_area = st.empty()
        progress_bar_val = st.progress(0)
        total_steps_exec_run = 3 + len(param_ext_opts)

        def update_progress_exec_run(step_name_key_exec: str):
            st.session_state.current_step_for_progress += 1
            progress_percentage_exec_run = int(
                (st.session_state.current_step_for_progress / total_steps_exec_run)
                * 100
            )
            progress_percentage_exec_run = min(progress_percentage_exec_run, 100)
            try:
                progress_bar_val.progress(progress_percentage_exec_run)
                progress_text_area.info(
                    f"âš™ï¸ {st.session_state.current_step_for_progress}/{total_steps_exec_run} - {_(step_name_key_exec)}"
                )
                progress_status.write(_(step_name_key_exec))
            except Exception as e_prog_exec_run:
                log.warning(f"é€²æ—è¡¨ç¤ºã®æ›´æ–°ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_prog_exec_run}")

        st.markdown("---")
        st.header("2. è§£æå‡¦ç†")
        try:
            if param_selected_sheets and excel_path_to_use:
                update_progress_exec_run("File Preview (first 8 rows)")
                st.subheader(_("File Preview (first 8 rows)"))
                try:
                    preview_df_exec_run = pd.read_excel(
                        excel_path_to_use,
                        sheet_name=param_selected_sheets[0],
                        header=None,
                        nrows=8,
                    )
                    st.dataframe(
                        preview_df_exec_run.astype(str), use_container_width=True
                    )
                except Exception as e_prev_exec_run:
                    st.warning(
                        _("Error during preview display") + f": {e_prev_exec_run}"
                    )
                    log.warning(
                        f"ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e_prev_exec_run}", exc_info=True
                    )
            else:
                st.warning(
                    "ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹ã‚·ãƒ¼ãƒˆãŒé¸æŠã•ã‚Œã¦ã„ãªã„ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒç„¡åŠ¹ã§ã™ã€‚"
                )

            long_df = None
            try:
                update_progress_exec_run("Ingest: Reading Excel data...")
                long_df, wt_df, unknown_codes = ingest_excel(
                    excel_path_to_use,
                    shift_sheets=param_selected_sheets,
                    header_row=param_header_row,
                    slot_minutes=param_slot,
                    year_month_cell_location=param_year_month_cell,
                )
                intermediate_parquet_path = work_root_exec / "intermediate_data.parquet"
                long_df.to_parquet(intermediate_parquet_path)
                if wt_df is not None and not wt_df.empty:
                    wt_df.to_parquet(work_root_exec / "work_patterns.parquet", index=False)
                    log.info("å‹¤å‹™åŒºåˆ†æƒ…å ±ã‚’ work_patterns.parquet ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
                st.session_state["intermediate_parquet_path"] = str(intermediate_parquet_path)
                st.session_state.analysis_status["ingest"] = "success"
                log.info(
                    f"Ingestå®Œäº†. long_df shape: {long_df.shape}, wt_df shape: {wt_df.shape if wt_df is not None else 'N/A'}"
                )
                if unknown_codes:
                    st.warning("æœªçŸ¥ã®å‹¤å‹™ã‚³ãƒ¼ãƒ‰: " + ", ".join(sorted(unknown_codes)))
                    log.warning(
                        f"Unknown shift codes encountered: {sorted(unknown_codes)}"
                    )
                st.success("âœ… Excelãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                st.session_state.long_df = long_df
                if not long_df.empty:
                    if "role" in long_df.columns:
                        st.session_state.available_roles = (
                            sorted(long_df["role"].astype(str).dropna().unique().tolist())
                        )
                    if "employment" in long_df.columns:
                        st.session_state.available_employments = (
                            sorted(long_df["employment"].astype(str).dropna().unique().tolist())
                        )
            except Exception as e:
                st.session_state.analysis_status["ingest"] = "failure"
                log_and_display_error(
                    "Excelãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", e
                )

            base_out_dir = Path(st.session_state.work_root_path_str) / "out"
            base_out_dir.mkdir(parents=True, exist_ok=True)
            
            # long_df (intermediate_data.parquet) ã‚’outãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            if intermediate_parquet_path.exists():
                shutil.copy(intermediate_parquet_path, base_out_dir / "intermediate_data.parquet")
                log.info("intermediate_data.parquet ã‚’ out ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")

            # --- å…±é€šåˆ†æã‚’ã‚·ãƒŠãƒªã‚ªãƒ«ãƒ¼ãƒ—ã®å‰ã«å®Ÿè¡Œ ---
            try:
                if "Fairness" in param_ext_opts:
                    run_fairness(long_df, base_out_dir)
                    fairness_xlsx = base_out_dir / "fairness_after.xlsx"
                    if fairness_xlsx.exists():
                        try:
                            fairness_df = pd.read_excel(fairness_xlsx)
                            fairness_df.to_parquet(base_out_dir / "fairness_after.parquet")
                            
                            # ğŸ¯ çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹å…¬å¹³æ€§åˆ†æçµæœä¿å­˜
                            if UNIFIED_ANALYSIS_AVAILABLE:
                                try:
                                    # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã§å…¬å¹³æ€§åˆ†æçµæœã‚’ä½œæˆ
                                    fairness_results = st.session_state.unified_analysis_manager.create_fairness_analysis(
                                        "common_fairness", scenario_key, fairness_df
                                    )
                                    
                                    # å¾“æ¥ã®session_stateå½¢å¼ã«ã‚‚ä¿å­˜ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                                    if not hasattr(st.session_state, 'fairness_analysis_results'):
                                        st.session_state.fairness_analysis_results = {}
                                    
                                    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã«çµ±ä¸€çµæœã‚’é©ç”¨
                                    for fairness_result in fairness_results:
                                        legacy_data = {
                                            "avg_fairness_score": fairness_result.core_metrics.get("avg_fairness_score", {}).get("value", 0.8),
                                            "low_fairness_staff_count": fairness_result.core_metrics.get("low_fairness_staff_count", {}).get("value", 0),
                                            "total_staff_analyzed": fairness_result.core_metrics.get("total_staff_analyzed", {}).get("value", 0),
                                            "data_integrity": fairness_result.data_integrity,
                                            "analysis_key": fairness_result.analysis_key,
                                            "score_column_used": fairness_result.extended_data.get("score_column_used", "unknown")
                                        }
                                        
                                        # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã«é©ç”¨
                                        for fname in file_names:
                                            st.session_state.fairness_analysis_results[fname] = legacy_data.copy()
                                    
                                    log.info(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã§å…¬å¹³æ€§åˆ†æå®Œäº†: {len(fairness_results)}çµæœä½œæˆ")
                                    
                                except Exception as e:
                                    log.error(f"ğŸš¨ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ å…¬å¹³æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ 
                                    if not hasattr(st.session_state, 'fairness_analysis_results'):
                                        st.session_state.fairness_analysis_results = {}
                                    for fname in file_names:
                                        st.session_state.fairness_analysis_results[fname] = {
                                            "avg_fairness_score": 0.8,
                                            "low_fairness_staff_count": 0,
                                            "total_staff_analyzed": 0,
                                            "data_integrity": "error",
                                            "error_message": str(e)[:100]
                                        }
                                
                        except Exception as e_conv:
                            log.warning(f"fairness_after.xlsx conversion failed: {e_conv}")
                # if "Fatigue" in param_ext_opts:
                #     fatigue_weights = {
                #         "start_var": st.session_state.get("weight_start_var_widget", 1.0),
                #         "diversity": st.session_state.get("weight_diversity_widget", 1.0),
                #         "worktime_var": st.session_state.get("weight_worktime_var_widget", 1.0),
                #         "short_rest": st.session_state.get("weight_short_rest_widget", 1.0),
                #         "consecutive": st.session_state.get("weight_consecutive_widget", 1.0),
                #         "night_ratio": st.session_state.get("weight_night_ratio_widget", 1.0),
                #     }
                #     train_fatigue(long_df, base_out_dir, weights=fatigue_weights)
                #     
                #     # Copy fatigue analysis files to all scenarios
                #     try:
                #         fatigue_xlsx = base_out_dir / "fatigue_score.xlsx"
                #         fatigue_parquet = base_out_dir / "fatigue_score.parquet"
                #         
                #         if fatigue_xlsx.exists() or fatigue_parquet.exists():
                #             for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                #                 if fatigue_xlsx.exists():
                #                     target_xlsx = Path(scenario_path) / "fatigue_score.xlsx"
                #                     shutil.copy2(fatigue_xlsx, target_xlsx)
                #                 if fatigue_parquet.exists():
                #                     target_parquet = Path(scenario_path) / "fatigue_score.parquet"
                #                     shutil.copy2(fatigue_parquet, target_parquet)
                #                 
                #                 log.info(f"ç–²åŠ´åº¦åˆ†æçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                #         else:
                #             log.warning("ç–²åŠ´åº¦åˆ†æãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                #     except Exception as e_fatigue_copy:
                #         log.warning(f"ç–²åŠ´åº¦åˆ†æçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_fatigue_copy}")
                if _("Leave Analysis") in param_ext_opts:
                    daily_leave_df = leave_analyzer.get_daily_leave_counts(long_df, target_leave_types=param_leave_target_types)
                    if not daily_leave_df.empty:
                        staff_balance = (
                            long_df[long_df["parsed_slots_count"] > 0]
                            .assign(date=lambda df: pd.to_datetime(df["ds"]).dt.normalize())
                            .groupby("date")["staff"]
                            .nunique()
                            .reset_index(name="total_staff")
                        )
                        leave_counts = (
                            leave_analyzer.summarize_leave_by_day_count(daily_leave_df.copy(), period="date")
                            .groupby("date")["total_leave_days"]
                            .sum()
                            .reset_index(name="leave_applicants_count")
                        )
                        staff_balance = staff_balance.merge(leave_counts, on="date", how="left")
                        staff_balance["leave_applicants_count"] = staff_balance["leave_applicants_count"].fillna(0).astype(int)
                        staff_balance["non_leave_staff"] = staff_balance["total_staff"] - staff_balance["leave_applicants_count"]
                        staff_balance["leave_ratio"] = staff_balance["leave_applicants_count"] / staff_balance["total_staff"]
                        staff_balance.to_csv(base_out_dir / "staff_balance_daily.csv", index=False)

                        summary = leave_analyzer.summarize_leave_by_day_count(daily_leave_df.copy(), period="date")
                        summary.to_csv(base_out_dir / "leave_analysis.csv", index=False)
                        ratio_df = leave_analyzer.leave_ratio_by_period_and_weekday(summary.copy())
                        ratio_df.to_csv(base_out_dir / "leave_ratio_breakdown.csv", index=False)

                        if LEAVE_TYPE_REQUESTED in param_leave_target_types:
                            req_daily = daily_leave_df[daily_leave_df["leave_type"] == LEAVE_TYPE_REQUESTED]
                            if not req_daily.empty:
                                applicants = leave_analyzer.summarize_leave_by_day_count(req_daily.copy(), period="date")
                                conc = leave_analyzer.analyze_leave_concentration(
                                    applicants,
                                    leave_type_to_analyze=LEAVE_TYPE_REQUESTED,
                                    concentration_threshold=param_leave_concentration_threshold,
                                    daily_leave_df=req_daily.copy(),
                                )
                                conc.to_csv(base_out_dir / "concentration_requested.csv", index=False)
                if "Cluster" in param_ext_opts:
                    cluster_staff(long_df, base_out_dir)
                if "Skill" in param_ext_opts:
                    build_skill_matrix(long_df, base_out_dir)
            except Exception as e_common:
                log.warning(f"common analysis failed: {e_common}")

            # Store scenario directories for file copying
            st.session_state.current_scenario_dirs = {}
            
            for scenario_key, scenario_params in analysis_scenarios.items():
                st.info(f"ã‚·ãƒŠãƒªã‚ª '{scenario_params['name']}' ã®åˆ†æã‚’é–‹å§‹...")
                scenario_out_dir = base_out_dir / f"out_{scenario_key}"
                scenario_out_dir.mkdir(parents=True, exist_ok=True)
                
                # Store scenario directory mapping
                st.session_state.current_scenario_dirs[scenario_key] = scenario_out_dir

                try:
                    shutil.copy(intermediate_parquet_path, scenario_out_dir / "intermediate_data.parquet")
                    log.info(f"Copied intermediate_data.parquet to {scenario_out_dir}")
                    if (work_root_exec / "work_patterns.parquet").exists():
                        shutil.copy(
                            work_root_exec / "work_patterns.parquet",
                            scenario_out_dir / "work_patterns.parquet",
                        )
                except Exception as e:
                    log_and_display_error(
                        f"Failed to copy intermediate files to {scenario_out_dir}",
                        e,
                    )
                    continue

                try:
                    update_progress_exec_run("Heatmap: Generating heatmap...")
                    build_heatmap(
                        long_df,
                        scenario_out_dir,
                        param_slot,
                        include_zero_days=True,
                        need_calc_method=param_need_calc_method,
                        ref_start_date_for_need=param_need_ref_start,
                        ref_end_date_for_need=param_need_ref_end,
                        need_stat_method=scenario_params["need_stat_method"],
                        need_manual_values=param_need_manual,
                        need_remove_outliers=param_need_remove_outliers,
                        upper_calc_method=param_upper_method,
                        upper_calc_param=param_upper_param,
                    )
                    if _("åŸºæº–ä¹–é›¢åˆ†æ") in param_ext_opts and param_need_calc_method == _(
                        "äººå“¡é…ç½®åŸºæº–ã«åŸºã¥ãè¨­å®šã™ã‚‹"
                    ):
                        heat_all_df = pd.read_parquet(scenario_out_dir / "heat_ALL.parquet")
                        gap_results = analyze_standards_gap(heat_all_df, param_need_manual)
                        st.session_state.gap_analysis_results = gap_results
                        gap_results["gap_summary"].to_excel(
                            scenario_out_dir / "gap_summary.xlsx", index=False
                        )
                        gap_results["gap_heatmap"].to_excel(
                            scenario_out_dir / "gap_heatmap.xlsx"
                        )
                    st.session_state.analysis_status["heatmap"] = "success"
                    st.success(f"âœ… Heatmapç”Ÿæˆå®Œäº† ({scenario_key})")
                except Exception as e:
                    st.session_state.analysis_status["heatmap"] = "failure"
                    log_and_display_error("Heatmapã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", e)
                    continue

                try:
                    update_progress_exec_run("Shortage: Analyzing shortage...")
                    shortage_result_exec_run = shortage_and_brief(
                        scenario_out_dir,
                        param_slot,
                        holidays=(holiday_dates_global_for_run or [])
                        + (holiday_dates_local_for_run or []),
                        include_zero_days=True,
                        wage_direct=param_wage_direct,
                        wage_temp=param_wage_temp,
                        penalty_per_lack=param_penalty_lack,
                    )
                    
                    # ğŸ¯ çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹ä¸è¶³åˆ†æçµæœä¿å­˜
                    if shortage_result_exec_run and UNIFIED_ANALYSIS_AVAILABLE:
                        try:
                            role_summary_path = scenario_out_dir / "shortage_role_summary.parquet"
                            if role_summary_path.exists():
                                role_df = pd.read_parquet(role_summary_path)
                                
                                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã§åˆ†æçµæœã‚’ä½œæˆ
                                shortage_result = st.session_state.unified_analysis_manager.create_shortage_analysis(
                                    file_name, scenario_key, role_df
                                )
                                
                                # å¾“æ¥ã®session_stateå½¢å¼ã«ã‚‚ä¿å­˜ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                                if not hasattr(st.session_state, 'shortage_analysis_results'):
                                    st.session_state.shortage_analysis_results = {}
                                
                                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å¾“æ¥å½¢å¼ã«å¤‰æ›
                                legacy_data = {
                                    "total_shortage_hours": shortage_result.core_metrics.get("total_shortage_hours", {}).get("value", 0.0),
                                    "total_shortage_events": shortage_result.core_metrics.get("shortage_events_count", {}).get("value", 0),
                                    "role_count": shortage_result.extended_data.get("role_count", 0),
                                    "top_shortage_roles": shortage_result.extended_data.get("top_shortage_roles", []),
                                    "severity": shortage_result.extended_data.get("severity_level", "unknown"),
                                    "data_integrity": shortage_result.data_integrity,
                                    "analysis_key": shortage_result.analysis_key
                                }
                                
                                st.session_state.shortage_analysis_results[file_name] = legacy_data
                                log.info(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã§ä¸è¶³åˆ†æå®Œäº†: {shortage_result.analysis_key}")
                                
                        except Exception as e:
                            log.error(f"ğŸš¨ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ä¸è¶³åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ 
                            if not hasattr(st.session_state, 'shortage_analysis_results'):
                                st.session_state.shortage_analysis_results = {}
                            st.session_state.shortage_analysis_results[file_name] = {
                                "total_shortage_hours": 0.0,
                                "total_shortage_events": 0,
                                "role_count": 0,
                                "severity": "error",
                                "data_integrity": "error",
                                "error_message": str(e)[:100]
                            }
                    
                    st.session_state.analysis_status["shortage"] = "success"
                    
                    # ğŸ”§ ä¿®æ­£: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ä¸è¶³åˆ†æçµæœç™»éŒ²
                    if UNIFIED_ANALYSIS_AVAILABLE and hasattr(st.session_state, 'unified_analysis_manager'):
                        try:
                            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
                            shortage_role_file = scenario_out_dir / "shortage_role_summary.parquet"
                            if shortage_role_file.exists():
                                role_df = pd.read_parquet(shortage_role_file)
                                
                                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã«ç™»éŒ²
                                shortage_result = st.session_state.unified_analysis_manager.create_shortage_analysis(
                                    file_name, scenario_key, role_df
                                )
                                
                                log.info(f"âœ… çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ä¸è¶³åˆ†æçµæœç™»éŒ²å®Œäº†: {shortage_result.analysis_key}")
                            else:
                                log.warning("âš ï¸ shortage_role_summary.parquetãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        except Exception as e:
                            log.error(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµæœç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")
                    if shortage_result_exec_run is None:
                        st.warning(
                            "Shortage (ä¸è¶³åˆ†æ) ã®ä¸€éƒ¨ã¾ãŸã¯å…¨ã¦ãŒå®Œäº†ã—ã¾ã›ã‚“ã§ã—ãŸã€‚"
                        )
                    else:
                        st.success(f"âœ… Shortage (ä¸è¶³åˆ†æ) å®Œäº† ({scenario_key})")
                        if "Hire plan" in param_ext_opts:
                            try:
                                build_hire_plan_from_kpi(
                                    scenario_out_dir,
                                    monthly_hours_fte=param_std_work_hours,
                                    hourly_wage=param_wage_direct,
                                    recruit_cost=param_hiring_cost,
                                    safety_factor=param_safety_factor,
                                )
                            except Exception as e:
                                log.warning(f"hire_plan generation error: {e}")
                except Exception as e:
                    st.session_state.analysis_status["shortage"] = "failure"
                    log_and_display_error("ä¸è¶³åˆ†æã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", e)

                # 4. dash_app.pyç”¨ã®ã€Œä¸­é–“ã‚µãƒãƒªãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã€ã‚’ç”Ÿæˆ
                # å…¨æ—¥ä»˜ãƒ»æ™‚é–“å¸¯ãƒ»è·ç¨®ãƒ»é›‡ç”¨å½¢æ…‹ã®çµ„ã¿åˆã‚ã›ã‚’ç¶²ç¾…ã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
                # ğŸ¯ é‡è¦ï¼šä¼‘æ—¥é™¤å¤–æ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‹ã‚‰çµ„ã¿åˆã‚ã›ã‚’ä½œæˆ
                working_long_df = long_df[
                    (long_df.get("parsed_slots_count", 0) > 0) & 
                    (long_df.get("holiday_type", "é€šå¸¸å‹¤å‹™") == "é€šå¸¸å‹¤å‹™")
                ]
                all_combinations_from_long_df = working_long_df[[
                    "ds",
                    "role",
                    "employment",
                ]].drop_duplicates().copy()

                all_combinations_from_long_df["date_lbl"] = all_combinations_from_long_df[
                    "ds"
                ].dt.strftime("%Y-%m-%d")
                all_combinations_from_long_df["time"] = all_combinations_from_long_df[
                    "ds"
                ].dt.strftime("%H:%M")

                # parsed_slots_count > 0 ã®ã‚¹ãƒ­ãƒƒãƒˆã®ã¿ã‚¹ã‚¿ãƒƒãƒ•æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                working_staff_data = long_df[long_df.get("parsed_slots_count", 0) > 0].copy()
                working_staff_data["date_lbl"] = working_staff_data["ds"].dt.strftime("%Y-%m-%d")
                working_staff_data["time"] = working_staff_data["ds"].dt.strftime("%H:%M")

                staff_counts_actual = (
                    working_staff_data.groupby([
                        "date_lbl",
                        "time",
                        "role",
                        "employment",
                    ])["staff"]
                    .nunique()
                    .reset_index()
                    .rename(columns={"staff": "staff_count"})
                )

                # ã™ã¹ã¦ã®çµ„ã¿åˆã‚ã›ã«å®Ÿéš›ã®ã‚¹ã‚¿ãƒƒãƒ•æ•°ã‚’çµåˆã—ã€ç¨¼åƒãŒãªã„å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
                pre_aggregated_df = pd.merge(
                    all_combinations_from_long_df,
                    staff_counts_actual,
                    on=["date_lbl", "time", "role", "employment"],
                    how="left",
                )
                pre_aggregated_df["staff_count"] = pre_aggregated_df["staff_count"].fillna(0).astype(int)

                pre_aggregated_df.to_parquet(
                    scenario_out_dir / "pre_aggregated_data.parquet",
                    index=False,
                )
                log.info(f"ã‚·ãƒŠãƒªã‚ª '{scenario_params['name']}' ç”¨ã®ä¸­é–“ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚")
                st.success(f"ã‚·ãƒŠãƒªã‚ª '{scenario_params['name']}' ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


            # ----- ä¼‘æš‡åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Ÿè¡Œ -----
            # "ä¼‘æš‡åˆ†æ" (æ—¥æœ¬èª) ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            if _("Leave Analysis") in param_ext_opts:
                update_progress_exec_run("Leave Analysis: Processing...")
                st.info(f"{_('Leave Analysis')} å‡¦ç†ä¸­â€¦")
                try:
                    if "long_df" in locals() and not long_df.empty:
                        # 1. æ—¥æ¬¡ãƒ»è·å“¡åˆ¥ã®ä¼‘æš‡å–å¾—ãƒ•ãƒ©ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
                        daily_leave_df = leave_analyzer.get_daily_leave_counts(
                            long_df, target_leave_types=param_leave_target_types
                        )
                        st.session_state.leave_analysis_results["daily_leave_df"] = (
                            daily_leave_df
                        )

                        if not daily_leave_df.empty:
                            leave_results_temp = {}  # ä¸€æ™‚çš„ãªçµæœæ ¼ç´ç”¨

                            # 2. å¸Œæœ›ä¼‘é–¢é€£ã®é›†è¨ˆã¨åˆ†æ
                            if LEAVE_TYPE_REQUESTED in param_leave_target_types:
                                requested_leave_daily = daily_leave_df[
                                    daily_leave_df["leave_type"] == LEAVE_TYPE_REQUESTED
                                ]
                                if not requested_leave_daily.empty:
                                    leave_results_temp["summary_dow_requested"] = (
                                        leave_analyzer.summarize_leave_by_day_count(
                                            requested_leave_daily.copy(),
                                            period="dayofweek",
                                        )
                                    )
                                    leave_results_temp[
                                        "summary_month_period_requested"
                                    ] = leave_analyzer.summarize_leave_by_day_count(
                                        requested_leave_daily.copy(),
                                        period="month_period",
                                    )
                                    leave_results_temp["summary_month_requested"] = (
                                        leave_analyzer.summarize_leave_by_day_count(
                                            requested_leave_daily.copy(), period="month"
                                        )
                                    )

                                    daily_requested_applicants_counts = (
                                        leave_analyzer.summarize_leave_by_day_count(
                                            requested_leave_daily.copy(), period="date"
                                        )
                                    )
                                    leave_results_temp["concentration_requested"] = (
                                        leave_analyzer.analyze_leave_concentration(
                                            daily_requested_applicants_counts,
                                            leave_type_to_analyze=LEAVE_TYPE_REQUESTED,
                                            concentration_threshold=param_leave_concentration_threshold,
                                            daily_leave_df=requested_leave_daily.copy(),
                                        )
                                    )
                                else:
                                    log.info(
                                        f"{LEAVE_TYPE_REQUESTED} ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€é–¢é€£ã™ã‚‹é›†è¨ˆãƒ»åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
                                    )
                                    leave_results_temp["summary_dow_requested"] = (
                                        pd.DataFrame()
                                    )
                                    leave_results_temp[
                                        "summary_month_period_requested"
                                    ] = pd.DataFrame()
                                    leave_results_temp["summary_month_requested"] = (
                                        pd.DataFrame()
                                    )
                                    leave_results_temp["concentration_requested"] = (
                                        pd.DataFrame()
                                    )

                            # å‹¤å‹™äºˆå®šäººæ•°ã¨ã®æ¯”è¼ƒãƒ‡ãƒ¼ã‚¿ä½œæˆ (å…¨ä¼‘æš‡ã‚¿ã‚¤ãƒ—)
                            try:
                                total_staff_per_day = (
                                    long_df[long_df["parsed_slots_count"] > 0]
                                    .assign(
                                        date=lambda df: pd.to_datetime(
                                            df["ds"]
                                        ).dt.normalize()
                                    )
                                    .groupby("date")["staff"]
                                    .nunique()
                                    .reset_index(name="total_staff")
                                )
                                all_leave_counts = (
                                    leave_analyzer.summarize_leave_by_day_count(
                                        daily_leave_df.copy(),
                                        period="date",
                                    )
                                    .groupby("date")["total_leave_days"]
                                    .sum()
                                    .reset_index(name="leave_applicants_count")
                                )
                                staff_balance = total_staff_per_day.merge(
                                    all_leave_counts,
                                    on="date",
                                    how="left",
                                )
                                staff_balance["leave_applicants_count"] = (
                                    staff_balance["leave_applicants_count"]
                                    .fillna(0)
                                    .astype(int)
                                )
                                staff_balance["non_leave_staff"] = (
                                    staff_balance["total_staff"]
                                    - staff_balance["leave_applicants_count"]
                                )
                                staff_balance["leave_ratio"] = (
                                    staff_balance["leave_applicants_count"]
                                    / staff_balance["total_staff"]
                                )
                                leave_results_temp["staff_balance_daily"] = (
                                    staff_balance
                                )
                            except Exception as e:
                                log.error(f"å‹¤å‹™äºˆå®šäººæ•°ã®è¨ˆç®—ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

                            # 3. æœ‰çµ¦ä¼‘æš‡é–¢é€£ã®é›†è¨ˆ
                            if LEAVE_TYPE_PAID in param_leave_target_types:
                                paid_leave_daily = daily_leave_df[
                                    daily_leave_df["leave_type"] == LEAVE_TYPE_PAID
                                ]
                                if not paid_leave_daily.empty:
                                    leave_results_temp["summary_dow_paid"] = (
                                        leave_analyzer.summarize_leave_by_day_count(
                                            paid_leave_daily.copy(), period="dayofweek"
                                        )
                                    )
                                    leave_results_temp["summary_month_paid"] = (
                                        leave_analyzer.summarize_leave_by_day_count(
                                            paid_leave_daily.copy(), period="month"
                                        )
                                    )
                                else:
                                    log.info(
                                        f"{LEAVE_TYPE_PAID} ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€é–¢é€£ã™ã‚‹é›†è¨ˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
                                    )
                                    leave_results_temp["summary_dow_paid"] = (
                                        pd.DataFrame()
                                    )
                                    leave_results_temp["summary_month_paid"] = (
                                        pd.DataFrame()
                                    )

                            # 4. è·å“¡åˆ¥ä¼‘æš‡ãƒªã‚¹ãƒˆ (çµ‚æ—¥ã®ã¿)
                            leave_results_temp["staff_leave_list"] = (
                                leave_analyzer.get_staff_leave_list(
                                    long_df, target_leave_types=param_leave_target_types
                                )
                            )

                            st.session_state.leave_analysis_results.update(
                                leave_results_temp
                            )

                            # Save each analysis output as individual CSV files
                            if "staff_balance_daily" in leave_results_temp:
                                leave_results_temp["staff_balance_daily"].to_csv(
                                    scenario_out_dir / "staff_balance_daily.csv",
                                    index=False,
                                )
                            if "concentration_requested" in leave_results_temp:
                                leave_results_temp[
                                    "concentration_requested"
                                ].to_csv(
                                    scenario_out_dir / "concentration_requested.csv",
                                    index=False,
                                )

                            # Save summary by date for external use
                            try:
                                daily_summary = (
                                    leave_analyzer.summarize_leave_by_day_count(
                                        daily_leave_df.copy(), period="date"
                                    )
                                )
                                st.session_state.leave_analysis_results[
                                    "daily_summary"
                                ] = daily_summary
                                ratio_df = (
                                    leave_analyzer.leave_ratio_by_period_and_weekday(
                                        daily_summary.copy()
                                    )
                                )
                                st.session_state.leave_analysis_results[
                                    "leave_ratio_breakdown"
                                ] = ratio_df
                                try:
                                    ratio_df.to_csv(
                                        scenario_out_dir / "leave_ratio_breakdown.csv",
                                        index=False,
                                    )
                                except Exception as e_ratio:
                                    log.warning(
                                        f"leave_ratio_breakdown.csv write error: {e_ratio}"
                                    )
                                try:
                                    both_conc = leave_analyzer.analyze_both_leave_concentration(
                                        daily_summary.copy(),
                                        concentration_threshold=param_leave_concentration_threshold,
                                    )
                                    st.session_state.leave_analysis_results[
                                        "concentration_both"
                                    ] = both_conc
                                except Exception as e_both:
                                    log.warning(
                                        f"concentration_both generation error: {e_both}"
                                    )
                                leave_csv = scenario_out_dir / "leave_analysis.csv"
                                daily_summary.to_csv(leave_csv, index=False)

                                # Copy leave analysis files to all scenarios
                                try:
                                    for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                        if scenario_path != scenario_out_dir:  # Don't copy to itself
                                            target_leave_csv = Path(scenario_path) / "leave_analysis.csv"
                                            daily_summary.to_csv(target_leave_csv, index=False)
                                            
                                            # Also copy leave_ratio_breakdown.csv if it exists
                                            if "leave_ratio_breakdown" in st.session_state.leave_analysis_results:
                                                ratio_target = Path(scenario_path) / "leave_ratio_breakdown.csv"
                                                st.session_state.leave_analysis_results["leave_ratio_breakdown"].to_csv(
                                                    ratio_target, index=False
                                                )
                                            
                                            log.info(f"ä¼‘æš‡åˆ†æçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                                except Exception as e_copy:
                                    log.warning(f"ä¼‘æš‡åˆ†æçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")

                                # Also generate shortage_leave.xlsx for the Shortage tab
                                merge_shortage_leave(scenario_out_dir, leave_csv=leave_csv)
                            except FileNotFoundError:
                                log.warning("shortage_time.parquet not found for merge, skipping.")
                            except Exception as e_save:  # noqa: BLE001
                                log.warning(
                                    f"leave_analysis.csv æ›¸ãå‡ºã—ã¾ãŸã¯ shortage_leave.xlsx ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_save}"
                                )

                            st.success(f"âœ… {_('Leave Analysis')} å®Œäº†")
                        else:
                            st.info(
                                f"{_('Leave Analysis')}: åˆ†æå¯¾è±¡ã¨ãªã‚‹ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                            )
                    else:
                        st.warning(
                            f"{_('Leave Analysis')}: å‰æã¨ãªã‚‹ long_df ãŒå­˜åœ¨ã—ãªã„ã‹ç©ºã®ãŸã‚ã€å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
                        )
                except Exception as e_leave:
                    log_and_display_error(
                        f"{_('Leave Analysis')} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", e_leave
                    )
            # ----- ä¼‘æš‡åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Ÿè¡Œã“ã“ã¾ã§ -----

            # ä»–ã®è¿½åŠ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®Ÿè¡Œ

            skip_opts = {"Fairness", "Cluster", "Skill"}
            for opt_module_name_exec_run in st.session_state.available_ext_opts_widget:
                if (
                    opt_module_name_exec_run in param_ext_opts
                    and opt_module_name_exec_run not in skip_opts
                    and opt_module_name_exec_run != _("Leave Analysis")
                ):
                    progress_key_exec_run = f"{opt_module_name_exec_run}: Processing..."
                    if opt_module_name_exec_run != "Stats":
                        update_progress_exec_run(progress_key_exec_run)
                    st.info(f"{_(opt_module_name_exec_run)} å‡¦ç†ä¸­â€¦")
                    try:
                        if opt_module_name_exec_run == "Stats":
                            if (
                                st.session_state.analysis_status.get("heatmap")
                                == "success"
                            ):
                                update_progress_exec_run("Stats: Processing...")
                                build_stats(
                                    scenario_out_dir,
                                    slot_minutes=param_slot,
                                    holidays=(holiday_dates_global_for_run or [])
                                    + (holiday_dates_local_for_run or []),
                                    wage_direct=param_wage_direct,
                                    wage_temp=param_wage_temp,
                                    penalty_per_lack=param_penalty_lack,
                                )
                                st.session_state.analysis_status["stats"] = "success"
                                st.success("âœ… Stats (çµ±è¨ˆæƒ…å ±) ç”Ÿæˆå®Œäº†")
                                
                                # Copy Stats files to all scenarios
                                try:
                                    for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                        if scenario_path != scenario_out_dir:  # Don't copy to itself
                                            for file_name in ["stats_alerts.parquet", "stats_daily_metrics_raw.parquet", 
                                                            "stats_monthly_summary.parquet", "stats_overall_summary.parquet", 
                                                            "stats_summary.txt"]:
                                                source_file = scenario_out_dir / file_name
                                                if source_file.exists():
                                                    target_file = Path(scenario_path) / file_name
                                                    shutil.copy2(source_file, target_file)
                                                    log.info(f"Statsçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                                except Exception as e_copy:
                                    log.warning(f"Statsçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                            else:
                                st.session_state.analysis_status["stats"] = "skipped"
                                st.warning(
                                    "Heatmapç”ŸæˆãŒå¤±æ•—ã—ãŸãŸã‚ã€Statså‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚"
                                )
                        elif opt_module_name_exec_run == "Anomaly":
                            detect_anomaly(scenario_out_dir)
                            
                            # Copy Anomaly files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        source_file = scenario_out_dir / "anomaly_days.parquet"
                                        if source_file.exists():
                                            target_file = Path(scenario_path) / "anomaly_days.parquet"
                                            shutil.copy2(source_file, target_file)
                                            log.info(f"Anomalyçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                            except Exception as e_copy:
                                log.warning(f"Anomalyçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Fatigue":
                            fatigue_weights = {
                                "start_var": st.session_state.get("weight_start_var_widget", 1.0),
                                "diversity": st.session_state.get("weight_diversity_widget", 1.0),
                                "worktime_var": st.session_state.get("weight_worktime_var_widget", 1.0),
                                "short_rest": st.session_state.get("weight_short_rest_widget", 1.0),
                                "consecutive": st.session_state.get("weight_consecutive_widget", 1.0),
                                "night_ratio": st.session_state.get("weight_night_ratio_widget", 1.0),
                            }
                            # train_fatigueã¯ãƒ¢ãƒ‡ãƒ«ã‚’è¿”ã™ãŒã€å†…éƒ¨ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã™ã‚‹
                            model = train_fatigue(
                                long_df, scenario_out_dir, weights=fatigue_weights, slot_minutes=param_slot
                            )
                            
                            # ğŸ¯ çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚‹ç–²åŠ´åˆ†æçµæœä¿å­˜
                            if UNIFIED_ANALYSIS_AVAILABLE:
                                try:
                                    fatigue_score_path = scenario_out_dir / "fatigue_score.parquet"
                                    combined_score_path = scenario_out_dir / "combined_score.csv"
                                    
                                    # å‹•çš„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
                                    fatigue_df = None
                                    combined_df = None
                                    
                                    if fatigue_score_path.exists():
                                        fatigue_df = pd.read_parquet(fatigue_score_path)
                                    elif combined_score_path.exists():
                                        combined_df = pd.read_csv(combined_score_path)
                                    
                                    # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã§ç–²åŠ´åˆ†æçµæœã‚’ä½œæˆ
                                    fatigue_result = st.session_state.unified_analysis_manager.create_fatigue_analysis(
                                        file_name, scenario_key, fatigue_df, combined_df
                                    )
                                    
                                    # å¾“æ¥ã®session_stateå½¢å¼ã«ã‚‚ä¿å­˜ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
                                    if not hasattr(st.session_state, 'fatigue_analysis_results'):
                                        st.session_state.fatigue_analysis_results = {}
                                    
                                    # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å¾“æ¥å½¢å¼ã«å¤‰æ›
                                    legacy_data = {
                                        "avg_fatigue_score": fatigue_result.core_metrics.get("avg_fatigue_score", {}).get("value", 0.5),
                                        "high_fatigue_staff_count": fatigue_result.core_metrics.get("high_fatigue_staff_count", {}).get("value", 0),
                                        "total_staff_analyzed": fatigue_result.core_metrics.get("total_staff_analyzed", {}).get("value", 0),
                                        "data_source": fatigue_result.extended_data.get("data_source_type", "unknown"),
                                        "data_integrity": fatigue_result.data_integrity,
                                        "analysis_key": fatigue_result.analysis_key
                                    }
                                    
                                    st.session_state.fatigue_analysis_results[file_name] = legacy_data
                                    log.info(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã§ç–²åŠ´åˆ†æå®Œäº†: {fatigue_result.analysis_key}")
                                    
                                except Exception as e:
                                    log.error(f"ğŸš¨ çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ç–²åŠ´åˆ†æã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ 
                                    if not hasattr(st.session_state, 'fatigue_analysis_results'):
                                        st.session_state.fatigue_analysis_results = {}
                                    st.session_state.fatigue_analysis_results[file_name] = {
                                        "avg_fatigue_score": 0.5,
                                        "high_fatigue_staff_count": 0,
                                        "total_staff_analyzed": 0,
                                        "data_source": "error_fallback",
                                        "data_integrity": "error",
                                        "error_message": str(e)[:100]
                                    }
                            
                            # Copy Fatigue files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        for file_name in ["fatigue_score.xlsx", "fatigue_score.parquet"]:
                                            source_file = scenario_out_dir / file_name
                                            if source_file.exists():
                                                target_file = Path(scenario_path) / file_name
                                                shutil.copy2(source_file, target_file)
                                                log.info(f"ç–²åŠ´åº¦åˆ†æçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                            except Exception as e_copy:
                                log.warning(f"ç–²åŠ´åº¦åˆ†æçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Cluster":
                            cluster_staff(long_df, scenario_out_dir)
                        elif opt_module_name_exec_run == "Skill":
                            build_skill_matrix(long_df, scenario_out_dir)
                        elif opt_module_name_exec_run == "Fairness":
                            run_fairness(long_df, scenario_out_dir)
                            fairness_xlsx = scenario_out_dir / "fairness_after.xlsx"
                            if fairness_xlsx.exists():
                                try:
                                    fairness_df = pd.read_excel(fairness_xlsx)
                                    fairness_df.to_parquet(
                                        scenario_out_dir / "fairness_after.parquet"
                                    )
                                except Exception as e_conv:
                                    log.warning(f"fairness_after.xlsx conversion failed: {e_conv}")
                        elif opt_module_name_exec_run == "Rest Time Analysis":
                            rta = RestTimeAnalyzer()
                            st.session_state.rest_time_results = rta.analyze(
                                long_df, slot_minutes=param_slot
                            )
                            st.session_state.rest_time_results.to_csv(
                                scenario_out_dir / "rest_time.csv", index=False
                            )
                            st.session_state.rest_time_monthly = rta.monthly(
                                st.session_state.rest_time_results
                            )
                            if st.session_state.rest_time_monthly is not None:
                                st.session_state.rest_time_monthly.to_csv(
                                    scenario_out_dir / "rest_time_monthly.csv", index=False
                                )
                            
                            # Copy Rest Time Analysis files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        for file_name in ["rest_time.csv", "rest_time_monthly.csv"]:
                                            source_file = scenario_out_dir / file_name
                                            if source_file.exists():
                                                target_file = Path(scenario_path) / file_name
                                                shutil.copy2(source_file, target_file)
                                                log.info(f"Rest Time Analysisçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                            except Exception as e_copy:
                                log.warning(f"Rest Time Analysisçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Work Pattern Analysis":
                            wpa = WorkPatternAnalyzer()
                            st.session_state.work_pattern_results = wpa.analyze(long_df)
                            st.session_state.work_pattern_results.to_csv(
                                scenario_out_dir / "work_patterns.csv", index=False
                            )
                            st.session_state.work_pattern_monthly = wpa.analyze_monthly(
                                long_df
                            )
                            if st.session_state.work_pattern_monthly is not None:
                                st.session_state.work_pattern_monthly.to_csv(
                                    scenario_out_dir / "work_pattern_monthly.csv",
                                    index=False,
                                )
                            
                            # Copy Work Pattern Analysis files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        for file_name in ["work_patterns.csv", "work_pattern_monthly.csv"]:
                                            source_file = scenario_out_dir / file_name
                                            if source_file.exists():
                                                target_file = Path(scenario_path) / file_name
                                                shutil.copy2(source_file, target_file)
                                                log.info(f"Work Pattern Analysisçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                            except Exception as e_copy:
                                log.warning(f"Work Pattern Analysisçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Attendance Analysis":
                            st.session_state.attendance_results = (
                                AttendanceBehaviorAnalyzer().analyze(long_df)
                            )
                            st.session_state.attendance_results.to_csv(
                                scenario_out_dir / "attendance.csv", index=False
                            )
                            
                            # Copy Attendance Analysis files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        source_file = scenario_out_dir / "attendance.csv"
                                        if source_file.exists():
                                            target_file = Path(scenario_path) / "attendance.csv"
                                            shutil.copy2(source_file, target_file)
                                            log.info(f"Attendance Analysisçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                            except Exception as e_copy:
                                log.warning(f"Attendance Analysisçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Low Staff Load":
                            lsl = LowStaffLoadAnalyzer()
                            st.session_state.low_staff_load_results = lsl.analyze(
                                long_df, threshold=0.25
                            )
                            st.session_state.low_staff_load_results.to_csv(
                                scenario_out_dir / "low_staff_load.csv", index=False
                            )
                            
                            # Copy Low Staff Load files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        source_file = scenario_out_dir / "low_staff_load.csv"
                                        if source_file.exists():
                                            target_file = Path(scenario_path) / "low_staff_load.csv"
                                            shutil.copy2(source_file, target_file)
                                            log.info(f"Low Staff Loadçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                            except Exception as e_copy:
                                log.warning(f"Low Staff Loadçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Combined Score":
                            rest_df = (
                                st.session_state.rest_time_results
                                if st.session_state.rest_time_results is not None
                                else pd.DataFrame()
                            )
                            work_df = (
                                st.session_state.work_pattern_results
                                if st.session_state.work_pattern_results is not None
                                else pd.DataFrame()
                            )
                            att_df = (
                                st.session_state.attendance_results
                                if st.session_state.attendance_results is not None
                                else pd.DataFrame()
                            )
                            st.session_state.combined_score_results = (
                                CombinedScoreCalculator().calculate(
                                    rest_df, work_df, att_df
                                )
                            )
                            st.session_state.combined_score_results.to_csv(
                                scenario_out_dir / "combined_score.csv", index=False
                            )
                            
                            # Copy Combined Score files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        source_file = scenario_out_dir / "combined_score.csv"
                                        if source_file.exists():
                                            target_file = Path(scenario_path) / "combined_score.csv"
                                            shutil.copy2(source_file, target_file)
                                            log.info(f"Combined Scoreçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ")
                            except Exception as e_copy:
                                log.warning(f"Combined Scoreçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "Need forecast":
                            demand_csv_exec_run_fc = scenario_out_dir / "demand_series.csv"
                            forecast_xls_exec_run_fc = (
                                scenario_out_dir / "forecast.parquet"
                            )  # å‡ºåŠ›ã‚‚parquetã«
                            heat_all_for_fc_exec_run_fc = (
                                scenario_out_dir / "heat_ALL.parquet"
                            )  # å…¥åŠ›ã‚’parquetã«
                            if not heat_all_for_fc_exec_run_fc.exists():
                                st.warning(
                                    _("Need forecast")
                                    + f": å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ« {heat_all_for_fc_exec_run_fc.name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                                )
                            else:
                                build_demand_series(
                                    heat_all_for_fc_exec_run_fc,
                                    demand_csv_exec_run_fc,
                                    leave_csv=scenario_out_dir / "leave_analysis.csv"
                                    if (scenario_out_dir / "leave_analysis.csv").exists()
                                    else None,
                                )
                                if demand_csv_exec_run_fc.exists():
                                    fc_leave = scenario_out_dir / "leave_analysis.csv"
                                    forecast_need(
                                        demand_csv_exec_run_fc,
                                        forecast_xls_exec_run_fc,
                                        periods=param_forecast_period,
                                        leave_csv=fc_leave
                                        if fc_leave.exists()
                                        else None,
                                        holidays=(holiday_dates_global_for_run or [])
                                        + (holiday_dates_local_for_run or []),
                                        log_csv=scenario_out_dir / "forecast_history.csv",
                                    )
                                    if forecast_xls_exec_run_fc.exists() and forecast_xls_exec_run_fc.suffix.lower() in {".xlsx", ".xls"}:
                                        try:
                                            forecast_df = pd.read_excel(forecast_xls_exec_run_fc)
                                            forecast_df.to_parquet(
                                                scenario_out_dir / "forecast.parquet"
                                            )
                                        except Exception as e_conv:
                                            log.warning(f"forecast parquet conversion error: {e_conv}")
                                    
                                    # Copy Need forecast files to all scenarios
                                    try:
                                        for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                            if scenario_path != scenario_out_dir:  # Don't copy to itself
                                                for file_name in ["demand_series.csv", "demand_series.meta.json", "forecast.parquet", "forecast.json", "forecast.summary.txt", "forecast_history.csv"]:
                                                    source_file = scenario_out_dir / file_name
                                                    if source_file.exists():
                                                        target_file = Path(scenario_path) / file_name
                                                        shutil.copy2(source_file, target_file)
                                                        log.info(f"Need forecastçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                                    except Exception as e_copy:
                                        log.warning(f"Need forecastçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                                else:
                                    st.warning(
                                        _("Need forecast")
                                        + ": demand_series.csv ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
                                    )
                        elif opt_module_name_exec_run == "RL roster (PPO)":
                            demand_csv_rl_exec_run_rl = (
                                scenario_out_dir / "demand_series.csv"
                            )
                            rl_roster_xls_exec_run_rl = scenario_out_dir / "rl_roster.xlsx"
                            model_zip_rl = scenario_out_dir / "ppo_model.zip"
                            fc_xls = scenario_out_dir / "forecast.xlsx"
                            shortage_xlsx = scenario_out_dir / "shortage_time.xlsx"
                            if demand_csv_rl_exec_run_rl.exists():
                                # learn_roster temporarily disabled due to gymnasium dependency
                                # learn_roster(
                                #     demand_csv_rl_exec_run_rl,
                                #     rl_roster_xls_exec_run_rl,
                                #     forecast_csv=fc_xls if fc_xls.exists() else None,
                                #     shortage_csv=shortage_xlsx
                                #     if shortage_xlsx.exists()
                                #     else None,
                                #     model_path=model_zip_rl,
                                # )
                                pass
                            else:
                                st.warning(
                                    _("RL Roster")
                                    + ": éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ (demand_series.csv) ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                                )
                        elif opt_module_name_exec_run == "RL roster (model)":
                            demand_csv_rl_exec_run_rl = (
                                scenario_out_dir / "demand_series.csv"
                            )
                            rl_roster_xls_use = scenario_out_dir / "rl_roster.xlsx"
                            model_zip_rl = scenario_out_dir / "ppo_model.zip"
                            fc_xls = scenario_out_dir / "forecast.xlsx"
                            shortage_xlsx = scenario_out_dir / "shortage_time.xlsx"
                            if model_zip_rl.exists() and fc_xls.exists():
                                # learn_roster temporarily disabled due to gymnasium dependency
                                # learn_roster(
                                #     demand_csv_rl_exec_run_rl
                                #     if demand_csv_rl_exec_run_rl.exists()
                                #     else fc_xls,
                                #     rl_roster_xls_use,
                                #     forecast_csv=fc_xls,
                                #     shortage_csv=shortage_xlsx
                                #     if shortage_xlsx.exists()
                                #     else None,
                                #     model_path=model_zip_rl,
                                #     use_saved_model=True,
                                # )
                                pass
                            else:
                                st.warning(
                                    _("RL Roster")
                                    + ": å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯ forecast.xlsx ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                                )
                        elif opt_module_name_exec_run == "Hire plan":
                            demand_csv_hp_exec_run_hp = (
                                scenario_out_dir / "demand_series.csv"
                            )
                            hire_xls_exec_run_hp = scenario_out_dir / "hire_plan.parquet"
                            if demand_csv_hp_exec_run_hp.exists():
                                build_hire_plan_standard(
                                    demand_csv_hp_exec_run_hp,
                                    hire_xls_exec_run_hp,
                                    param_std_work_hours,
                                    param_safety_factor,
                                    param_target_coverage,
                                )
                                
                                # Copy Hire plan files to all scenarios
                                try:
                                    for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                        if scenario_path != scenario_out_dir:  # Don't copy to itself
                                            for file_name in ["hire_plan.parquet", "hire_plan.txt", "hire_plan_meta.parquet"]:
                                                source_file = scenario_out_dir / file_name
                                                if source_file.exists():
                                                    target_file = Path(scenario_path) / file_name
                                                    shutil.copy2(source_file, target_file)
                                                    log.info(f"Hire plançµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                                except Exception as e_copy:
                                    log.warning(f"Hire plançµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                            else:
                                st.warning(
                                    _("Hire Plan")
                                    + ": éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ (demand_series.csv) ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
                                )
                        elif opt_module_name_exec_run == "Cost / Benefit":
                            analyze_cost_benefit(
                                scenario_out_dir,
                                param_wage_direct,
                                param_wage_temp,
                                param_hiring_cost,
                                param_penalty_lack,
                            )
                            
                            # Copy Cost/Benefit files to all scenarios
                            try:
                                for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                    if scenario_path != scenario_out_dir:  # Don't copy to itself
                                        for file_name in ["cost_benefit.parquet", "cost_benefit_summary.txt"]:
                                            source_file = scenario_out_dir / file_name
                                            if source_file.exists():
                                                target_file = Path(scenario_path) / file_name
                                                shutil.copy2(source_file, target_file)
                                                log.info(f"Cost/Benefitçµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                            except Exception as e_copy:
                                log.warning(f"Cost/Benefitçµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                        elif opt_module_name_exec_run == "æœ€é©æ¡ç”¨è¨ˆç”»":
                            if (
                                st.session_state.analysis_status.get("shortage")
                                == "success"
                            ):
                                try:
                                    log.info(
                                        "æœ€é©æ¡ç”¨è¨ˆç”»ã®ãŸã‚ã®ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
                                    )
                                    weekday_summary_df = weekday_timeslot_summary(
                                        scenario_out_dir
                                    )
                                    summary_fp = (
                                        scenario_out_dir
                                        / "shortage_weekday_timeslot_summary.xlsx"
                                    )
                                    weekday_summary_df.to_excel(summary_fp, index=False)
                                    log.info(
                                        f"ä¸è¶³åˆ†æã®æ›œæ—¥ãƒ»æ™‚é–“å¸¯ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {summary_fp}"
                                    )

                                    original_excel_path = Path(
                                        next(
                                            iter(
                                                st.session_state.uploaded_files_info.values()
                                            )
                                        )["path"]
                                    )
                                    create_optimal_hire_plan(
                                        scenario_out_dir, 
                                        original_excel_path=original_excel_path
                                    )
                                    st.success("âœ… æœ€é©æ¡ç”¨è¨ˆç”» ç”Ÿæˆå®Œäº†")
                                    
                                    # Copy æœ€é©æ¡ç”¨è¨ˆç”» files to all scenarios
                                    try:
                                        for scenario_name, scenario_path in st.session_state.current_scenario_dirs.items():
                                            if scenario_path != scenario_out_dir:  # Don't copy to itself
                                                for file_name in ["optimal_hire_plan.parquet", "shortage_weekday_timeslot_summary.xlsx"]:
                                                    source_file = scenario_out_dir / file_name
                                                    if source_file.exists():
                                                        target_file = Path(scenario_path) / file_name
                                                        shutil.copy2(source_file, target_file)
                                                        log.info(f"æœ€é©æ¡ç”¨è¨ˆç”»çµæœã‚’ {scenario_name} ã«ã‚³ãƒ”ãƒ¼ã—ã¾ã—ãŸ: {file_name}")
                                    except Exception as e_copy:
                                        log.warning(f"æœ€é©æ¡ç”¨è¨ˆç”»çµæœã®ã‚³ãƒ”ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_copy}")
                                except Exception as e_opt_hire:
                                    log.error(
                                        f"æœ€é©æ¡ç”¨è¨ˆç”»ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e_opt_hire}",
                                        exc_info=True,
                                    )
                                    st.warning(
                                        "æœ€é©æ¡ç”¨è¨ˆç”»ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                                    )
                            else:
                                st.warning(
                                    "æœ€é©æ¡ç”¨è¨ˆç”»ã®ç”Ÿæˆã«ã¯ã€ä¸è¶³åˆ†æãŒå…ˆã«å®Œäº†ã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
                                )
                        st.success(f"âœ… {_(opt_module_name_exec_run)} å®Œäº†")
                    except FileNotFoundError as fe_opt_exec_run_loop:
                        log_and_display_error(
                            f"{_(opt_module_name_exec_run)} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ (ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡º)",
                            fe_opt_exec_run_loop,
                        )
                        log.error(
                            f"{opt_module_name_exec_run} å‡¦ç†ã‚¨ãƒ©ãƒ¼ (FileNotFoundError): {fe_opt_exec_run_loop}",
                            exc_info=True,
                        )
                    except Exception as e_opt_exec_run_loop:
                        log_and_display_error(
                            f"{_(opt_module_name_exec_run)} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                            e_opt_exec_run_loop,
                        )
                        log.error(
                            f"{opt_module_name_exec_run} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e_opt_exec_run_loop}",
                            exc_info=True,
                        )

            # --- æ—¥åˆ¥äººä»¶è²»ã®è¨ˆç®—ã¨ä¿å­˜ ---
            if (
                "wage_config" in st.session_state
                and st.session_state.analysis_status.get("ingest") == "success"
                and not long_df.empty
            ):
                try:
                    daily_cost_df = calculate_daily_cost(
                        long_df,
                        st.session_state.wage_config,
                        by=st.session_state.get("cost_by_widget", "role"),
                        slot_minutes=param_slot,
                    )
                    daily_cost_df.to_excel(
                        out_dir_exec / "daily_cost.xlsx", index=False
                    )
                    daily_cost_df.to_parquet(
                        out_dir_exec / "daily_cost.parquet", index=False
                    )
                except Exception as e_cost:
                    log.warning(f"daily cost calculation failed: {e_cost}")

            progress_bar_val.progress(100)
            progress_text_area.success("âœ¨ å…¨å·¥ç¨‹å®Œäº†ï¼")
            st.balloons()
            st.success(_("All processes complete!"))
            st.session_state.analysis_done = True
            
            # ğŸ¯ çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if UNIFIED_ANALYSIS_AVAILABLE and hasattr(st.session_state, 'unified_analysis_manager'):
                try:
                    st.session_state.unified_analysis_manager.cleanup_old_results(max_age_hours=24)
                    log.info("çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
                except Exception as e:
                    log.warning(f"çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            
            # ğŸ¤– AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆã®å³åº§ç”Ÿæˆï¼ˆåˆ†æå®Œäº†ç›´å¾Œï¼‰
            if AI_REPORT_GENERATOR_AVAILABLE:
                try:
                    log.info("åˆ†æå®Œäº†ç›´å¾Œã®AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’é–‹å§‹...")
                    ai_generator = AIComprehensiveReportGenerator()
                    
                    # åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ã‚’åé›†
                    analysis_results = {}
                    if hasattr(st.session_state, 'analysis_results'):
                        analysis_results = st.session_state.analysis_results
                    
                    # çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®å®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆ
                    if UNIFIED_ANALYSIS_AVAILABLE and hasattr(st.session_state, 'unified_analysis_manager'):
                        try:
                            unified_results = st.session_state.unified_analysis_manager.get_ai_compatible_results(
                                scenario="median_based"
                            )
                            analysis_results.update(unified_results)
                        except Exception as e:
                            log.warning(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    # AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                    input_file_path = getattr(st.session_state, 'uploaded_excel_path', 'unknown.xlsx')
                    zip_base = Path(st.session_state.work_root_path_str) / "out"
                    
                    comprehensive_report = ai_generator.generate_comprehensive_report(
                        analysis_results=analysis_results,
                        input_file_path=input_file_path,
                        output_dir=str(zip_base),
                        analysis_params={"scenario": "median_based"}
                    )
                    
                    log.info("åˆ†æå®Œäº†ç›´å¾Œã®AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
                    
                except Exception as e:
                    log.error(f"AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆå³åº§ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            
            # ğŸ¤– 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º
            if AI_REPORT_GENERATOR_AVAILABLE:
                st.header("ğŸ§  AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆ (18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆã‚·ã‚¹ãƒ†ãƒ )")
                
                try:
                    # æœ€æ–°ã®AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                    ai_report_files = list(Path(st.session_state.work_root_path_str).glob("out/ai_comprehensive_report_*.json"))
                    if ai_report_files:
                        latest_report = max(ai_report_files, key=lambda x: x.stat().st_mtime)
                        
                        with open(latest_report, 'r', encoding='utf-8') as f:
                            comprehensive_report = json.load(f)
                        
                        st.success(f"ğŸ“Š 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº† ({latest_report.name})")
                        
                        # 18ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®è¡¨ç¤º
                        sections = comprehensive_report.get('sections', [])
                        st.info(f"ğŸ¯ **å“è³ªå‘ä¸Šé”æˆ**: åŸºæœ¬12ã‚»ã‚¯ã‚·ãƒ§ãƒ³ â†’ **{len(sections)}ã‚»ã‚¯ã‚·ãƒ§ãƒ³**çµ±åˆã‚·ã‚¹ãƒ†ãƒ ")
                        
                        # æ–°ã—ã„6ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ13-18ï¼‰ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆè¡¨ç¤º
                        for i, section in enumerate(sections, 1):
                            section_title = section.get('title', f'ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i}')
                            
                            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³13-18ã‚’ç‰¹åˆ¥è¡¨ç¤º
                            if i >= 13:
                                st.subheader(f"ğŸ†• **æ–°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i}**: {section_title}")
                                st.success("âœ… **æ·±åº¦æ‹¡å¼µå®Œäº†** - ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµ±åˆ")
                            else:
                                st.subheader(f"ğŸ“‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i}: {section_title}")
                            
                            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…å®¹ã®è¡¨ç¤º
                            content = section.get('content', {})
                            if isinstance(content, dict):
                                for key, value in content.items():
                                    if isinstance(value, dict) and value:
                                        st.write(f"**{key}**:")
                                        for sub_key, sub_value in list(value.items())[:3]:  # æœ€åˆã®3é …ç›®ã®ã¿è¡¨ç¤º
                                            if isinstance(sub_value, (str, int, float)):
                                                st.write(f"  â€¢ {sub_key}: {sub_value}")
                                            else:
                                                st.write(f"  â€¢ {sub_key}: [åˆ†æãƒ‡ãƒ¼ã‚¿]")
                                    elif isinstance(value, (str, int, float)):
                                        st.write(f"**{key}**: {value}")
                            
                            st.write("---")
                        
                        # å“è³ªå‘ä¸Šã®è¨¼æ‹ è¡¨ç¤º
                        metadata = comprehensive_report.get('metadata', {})
                        st.success("ğŸ‰ **ç¾çŠ¶æœ€é©åŒ–ç¶™ç¶šæˆ¦ç•¥ã«ã‚ˆã‚‹æ·±åº¦æ‹¡å¼µé”æˆ**")
                        st.info(f"ğŸ“ˆ **ç†è«–çµ±åˆ**: {metadata.get('total_theories_integrated', 0)}ã®ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’çµ±åˆ")
                        st.info(f"ğŸ”¬ **åˆ†ææ·±åº¦**: 91.9% â†’ 100% â†’ **110%** å“è³ªå‘ä¸Šå®Œäº†")
                        
                    else:
                        st.warning("âš ï¸ AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        
                except Exception as e:
                    st.error(f"âŒ AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                    log.error(f"AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                
                st.write("---")
            
            st.header("ã‚¹ãƒ†ãƒƒãƒ—1: åˆ†æçµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            st.write(
                "ä»¥ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€åˆ†æçµæœãŒã™ã¹ã¦å…¥ã£ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ‰‹å…ƒã®PCã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚"
            )
            st.info(
                "ğŸ¤– **AIå‘ã‘åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆæ©Ÿèƒ½**: ZIPãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯å¾“æ¥ã®åˆ†æçµæœã«åŠ ãˆã¦ã€"
                "ä»–ã®AIã‚·ã‚¹ãƒ†ãƒ ã§ã®åˆ†æã‚¢ã‚¦ãƒˆã‚½ãƒ¼ã‚·ãƒ³ã‚°ã«æœ€é©åŒ–ã•ã‚ŒãŸåŒ…æ‹¬çš„ãªJSONå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ"
                "(`ai_comprehensive_report_*.json`)ãŒè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™ã€‚"
            )
            
            # ğŸ”§ æ–°æ©Ÿèƒ½: ã‚·ãƒŠãƒªã‚ªé¸æŠUI
            if UNIFIED_ANALYSIS_AVAILABLE and hasattr(st.session_state, 'unified_analysis_manager'):
                st.subheader("ğŸ“Š AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š")
                
                # ã‚·ãƒŠãƒªã‚ªé¸æŠ
                selected_scenario = st.selectbox(
                    "åˆ†æã‚·ãƒŠãƒªã‚ªé¸æŠ",
                    ["median_based", "mean_based", "p25_based"],
                    index=0,  # median_based ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    help="**æ¨å¥¨: median_based** - çµ±è¨ˆçš„ã«æœ€ã‚‚å®‰å®šã—ãŸåˆ†æçµæœã‚’æä¾›ã—ã¾ã™ã€‚"
                         "\n- median_based: ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰"
                         "\n- mean_based: å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆæ¨™æº–çš„ï¼‰"
                         "\n- p25_based: 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼ˆä¿å®ˆçš„è¦‹ç©ã‚‚ã‚Šï¼‰"
                )
                
                # ã‚·ãƒŠãƒªã‚ªæƒ…å ±ã®è¡¨ç¤º
                scenario_info = {
                    "median_based": "ğŸ“Š ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ - æœ€ã‚‚å®‰å®šã—ãŸåˆ†æçµæœ",
                    "mean_based": "ğŸ“ˆ å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹ - æ¨™æº–çš„ãªåˆ†æçµæœ",
                    "p25_based": "ğŸ›¡ï¸ 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ« - ä¿å®ˆçš„ãªåˆ†æçµæœ"
                }
                st.info(f"é¸æŠä¸­: {scenario_info.get(selected_scenario, selected_scenario)}")
            else:
                selected_scenario = "median_based"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            zip_buffer = io.BytesIO()
            zip_base = Path(st.session_state.work_root_path_str) / "out"
            if zip_base.exists():
                # ğŸ¤– AIå‘ã‘åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                if AI_REPORT_GENERATOR_AVAILABLE:
                    try:
                        log.info("AIå‘ã‘åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚’é–‹å§‹...")
                        ai_generator = AIComprehensiveReportGenerator()
                        
                        # åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ã‚’åé›†
                        analysis_results = {}
                        if hasattr(st.session_state, 'analysis_results'):
                            analysis_results = st.session_state.analysis_results
                        
                        # ğŸ¯ çµ±ä¸€åˆ†æç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®å®Ÿãƒ‡ãƒ¼ã‚¿çµ±åˆï¼ˆã‚·ãƒŠãƒªã‚ªå¯¾å¿œï¼‰
                        try:
                            if UNIFIED_ANALYSIS_AVAILABLE and hasattr(st.session_state, 'unified_analysis_manager'):
                                # çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰AIäº’æ›å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                                # ğŸ”§ ä¿®æ­£: ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚¹ãƒ†ãƒ ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’ä½¿ç”¨
                                file_stem = Path(file_name).stem
                                log.info(f"[AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ] ãƒ•ã‚¡ã‚¤ãƒ«å: {file_name} â†’ ã‚¹ãƒ†ãƒ : {file_stem}")
                                log.info(f"[AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ] é¸æŠã‚·ãƒŠãƒªã‚ª: {selected_scenario}")
                                
                                # ğŸ”§ æ–°æ©Ÿèƒ½: ã‚·ãƒŠãƒªã‚ªå¯¾å¿œã®çµæœå–å¾—
                                unified_results = st.session_state.unified_analysis_manager.get_scenario_compatible_results(
                                    file_stem, 
                                    scenario=selected_scenario
                                )
                                
                                # çµæœãŒç©ºã®å ´åˆã®è©³ç´°è¨ºæ–­
                                if not unified_results:
                                    log.warning(f"[AIãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ] çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰çµæœãŒå–å¾—ã§ãã¾ã›ã‚“")
                                    log.warning(f"  æ¤œç´¢ã‚­ãƒ¼: {file_stem}")
                                    log.warning(f"  é¸æŠã‚·ãƒŠãƒªã‚ª: {selected_scenario}")
                                    log.warning(f"  çµ±åˆãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚µã‚¤ã‚º: {len(st.session_state.unified_analysis_manager.results_registry)}")
                                    
                                    # ã‚·ãƒŠãƒªã‚ªåˆ¥ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®è©³ç´°è¨ºæ–­
                                    for scenario, registry in st.session_state.unified_analysis_manager.scenario_registries.items():
                                        log.warning(f"  {scenario}ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‚µã‚¤ã‚º: {len(registry)}")
                                else:
                                    log.info(f"âœ… çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰{len(unified_results)}ç¨®é¡ã®åˆ†æçµæœã‚’å–å¾— (ã‚·ãƒŠãƒªã‚ª: {selected_scenario})")
                                
                                # AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”¨ã«çµ±åˆ
                                analysis_results.update(unified_results)
                                log.info(f"çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰{len(unified_results)}ç¨®é¡ã®åˆ†æçµæœã‚’çµ±åˆ")
                                
                                # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æƒ…å ±ã‚‚è¿½åŠ 
                                analysis_results["data_integrity_summary"] = {
                                    "total_analyses": len(unified_results),
                                    "valid_analyses": len([r for r in unified_results.values() if r.get("data_integrity") == "valid"]),
                                    "error_analyses": len([r for r in unified_results.values() if r.get("data_integrity") == "error"]),
                                    "generation_method": "unified_analysis_manager"
                                }
                            else:
                                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã®çµ±åˆ
                                if hasattr(st.session_state, 'shortage_analysis_results') and file_name in st.session_state.shortage_analysis_results:
                                    analysis_results["shortage_analysis"] = st.session_state.shortage_analysis_results[file_name]
                                
                                if hasattr(st.session_state, 'fatigue_analysis_results') and file_name in st.session_state.fatigue_analysis_results:
                                    analysis_results["fatigue_analysis"] = st.session_state.fatigue_analysis_results[file_name]
                                
                                if hasattr(st.session_state, 'fairness_analysis_results') and file_name in st.session_state.fairness_analysis_results:
                                    analysis_results["fairness_analysis"] = st.session_state.fairness_analysis_results[file_name]
                                
                                analysis_results["data_integrity_summary"] = {
                                    "generation_method": "legacy_fallback",
                                    "warning": "çµ±ä¸€åˆ†æã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã§ã—ãŸ"
                                }
                                log.warning("çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã§åˆ†æçµæœã‚’çµ±åˆ")
                                
                        except Exception as e:
                            log.error(f"ğŸš¨ åˆ†æçµæœçµ±åˆã§é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                            analysis_results["data_integrity_summary"] = {
                                "generation_method": "error_fallback",
                                "error_message": str(e)[:200]
                            }
                        
                        # åˆ†æãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åé›†
                        analysis_params = {
                            "slot_minutes": getattr(st.session_state, 'slot_minutes', 30),
                            "need_calculation_method": "statistical_estimation",
                            "statistical_method": getattr(st.session_state, 'need_ref_method', 'median'),
                            "outlier_removal_enabled": getattr(st.session_state, 'need_ref_outlier_removal', True),
                            "analysis_start_date": str(getattr(st.session_state, 'need_ref_start_date_widget', param_need_ref_start)),
                            "analysis_end_date": str(getattr(st.session_state, 'need_ref_end_date_widget', param_need_ref_end)),
                            "enabled_modules": ["Shortage", "Fatigue", "Fairness", "Leave Analysis"]
                        }
                        
                        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—
                        input_file_path = getattr(st.session_state, 'uploaded_excel_path', 'unknown.xlsx')
                        
                        # AIåŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆæ—¢ã«åˆ†æå®Œäº†ç›´å¾Œã«å®Ÿè¡Œæ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                        log.info("AIå‘ã‘åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆåˆ†æå®Œäº†ç›´å¾Œã«å®Ÿè¡Œæ¸ˆã¿ï¼‰")
                        
                        # ğŸ”§ ä¿®æ­£: çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ24æ™‚é–“è¨­å®šã¨çµ±ä¸€ï¼‰
                        if UNIFIED_ANALYSIS_AVAILABLE and hasattr(st.session_state, 'unified_analysis_manager'):
                            try:
                                st.session_state.unified_analysis_manager.cleanup_old_results(max_age_hours=24)
                                log.info("âœ… çµ±ä¸€ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†ï¼ˆ24æ™‚é–“è¨­å®šï¼‰")
                            except Exception as e:
                                log.warning(f"ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
                        
                    except Exception as e:
                        log.warning(f"AIå‘ã‘ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™: {e}")
                
                # æ´å¯Ÿæ¤œå‡ºçµæœã‚‚è¿½åŠ ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
                try:
                    from shift_suite.tasks.insight_detection_service import InsightDetectionService
                    insight_service = InsightDetectionService()
                    insight_report = insight_service.analyze_directory(zip_base)
                    
                    # æ´å¯Ÿæ¤œå‡ºçµæœã¯æ—¢ã«JSON/HTML/CSVã¨ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹
                    # (insights_detected.json, insights_report.html, insights_table.csv)
                    log.info(f"æ´å¯Ÿæ¤œå‡ºã‚’å®Ÿè¡Œ: {len(insight_report.insights)}å€‹ã®æ´å¯Ÿã‚’ç™ºè¦‹")
                    
                except Exception as e:
                    log.debug(f"æ´å¯Ÿæ¤œå‡ºã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ: {e}")
                
                # æ–°è¦çµ±åˆæ©Ÿèƒ½ã®çµæœã‚‚è¿½åŠ ï¼ˆ2025-08-29è¿½åŠ ï¼‰
                if st.session_state.get("long_df") is not None and not st.session_state.long_df.empty:
                    try:
                        log.info("é«˜åº¦ãªåˆ†ææ©Ÿèƒ½ã®å®Ÿè¡Œã‚’é–‹å§‹...")
                        
                        # 1. é€£ç¶šå‹¤å‹™æ¤œå‡º
                        try:
                            from shift_suite.tasks.continuous_shift_detector import ContinuousShiftDetector
                            detector = ContinuousShiftDetector()
                            continuous_shifts = detector.detect_continuous_shifts(st.session_state.long_df)
                            
                            # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
                            continuous_results = {
                                "detection_time": dt.datetime.now().isoformat(),
                                "total_violations": len(continuous_shifts),
                                "violations": [
                                    {
                                        "staff": shift.staff,
                                        "start": f"{shift.start_date} {shift.start_time}",
                                        "end": f"{shift.end_date} {shift.end_time}",
                                        "total_hours": shift.total_duration_hours,
                                        "is_overnight": shift.is_overnight,
                                        "start_code": shift.start_code,
                                        "end_code": shift.end_code
                                    }
                                    for shift in continuous_shifts[:20]  # æœ€åˆã®20ä»¶ã®ã¿
                                ],
                                "summary": detector.get_continuous_shift_summary() if continuous_shifts else {}
                            }
                            
                            continuous_file = zip_base / "continuous_shifts_detected.json"
                            with open(continuous_file, "w", encoding="utf-8") as f:
                                json.dump(continuous_results, f, ensure_ascii=False, indent=2)
                            
                            log.info(f"é€£ç¶šå‹¤å‹™æ¤œå‡ºå®Œäº†: {len(continuous_shifts)}ä»¶ã®é•åã‚’æ¤œå‡º")
                            
                        except Exception as e:
                            log.debug(f"é€£ç¶šå‹¤å‹™æ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 2. é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬
                        try:
                            from shift_suite.tasks.turnover_prediction import TurnoverPredictionEngine
                            
                            engine = TurnoverPredictionEngine(
                                model_type='ensemble',
                                lookback_months=3,
                                enable_early_warning=True
                            )
                            
                            # ç‰¹å¾´é‡æŠ½å‡ºã¨äºˆæ¸¬
                            features_df = engine.extract_turnover_features(st.session_state.long_df)
                            features_df = engine.generate_synthetic_labels(features_df)
                            engine.train_models(features_df)
                            predictions_df = engine.predict_turnover_risk(features_df)
                            
                            # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
                            if not predictions_df.empty:
                                turnover_results = {
                                    "prediction_time": dt.datetime.now().isoformat(),
                                    "total_staff": len(predictions_df),
                                    "high_risk": len(predictions_df[predictions_df['risk_level'] == 'high']),
                                    "medium_risk": len(predictions_df[predictions_df['risk_level'] == 'medium']),
                                    "low_risk": len(predictions_df[predictions_df['risk_level'].isin(['low', 'very_low'])]),
                                    "staff_risks": predictions_df.to_dict('records')
                                }
                                
                                turnover_file = zip_base / "turnover_predictions.json"
                                with open(turnover_file, "w", encoding="utf-8") as f:
                                    json.dump(turnover_results, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬å®Œäº†: {len(predictions_df)}åã‚’åˆ†æ")
                            
                        except Exception as e:
                            log.debug(f"é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 3. ä½œæˆè€…æ„å›³åˆ†æ
                        try:
                            from shift_suite.tasks.shift_mind_reader_lite import ShiftMindReaderLite
                            
                            reader = ShiftMindReaderLite()
                            mind_results = reader.read_creator_mind(st.session_state.long_df)
                            
                            # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
                            mind_file = zip_base / "creator_mind_analysis.json"
                            with open(mind_file, "w", encoding="utf-8") as f:
                                json.dump(mind_results, f, ensure_ascii=False, indent=2)
                            
                            log.info("ä½œæˆè€…æ„å›³åˆ†æå®Œäº†")
                            
                        except Exception as e:
                            log.debug(f"ä½œæˆè€…æ„å›³åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 4. éœ€è¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ« (2025-08-29è¿½åŠ )
                        try:
                            from shift_suite.tasks.demand_prediction_model import DemandPredictionModel
                            
                            model = DemandPredictionModel()
                            
                            # å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆéå»ã®ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰éœ€è¦ã‚’æŠ½å‡ºï¼‰
                            training_data = []
                            for _, row in st.session_state.long_df.iterrows():
                                training_data.append({
                                    'timestamp': row['ds'],
                                    'demand': 1  # å„ã‚·ãƒ•ãƒˆã‚¹ãƒ­ãƒƒãƒˆã‚’éœ€è¦1ã¨ã—ã¦æ‰±ã†
                                })
                            
                            if training_data:
                                # ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
                                training_result = model.train_model(training_data)
                                
                                # 7æ—¥å¾Œã®äºˆæ¸¬
                                from datetime import datetime, timedelta
                                target_date = datetime.now() + timedelta(days=7)
                                prediction_result = model.predict_demand(
                                    target_date=target_date,
                                    days_ahead=7
                                )
                                
                                # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
                                demand_file = zip_base / "demand_prediction.json"
                                with open(demand_file, "w", encoding="utf-8") as f:
                                    json.dump({
                                        'training_result': training_result,
                                        'prediction': prediction_result,
                                        'model_info': model.get_model_info()
                                    }, f, ensure_ascii=False, indent=2, default=str)
                                
                                log.info("éœ€è¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œå®Œäº†")
                        
                        except Exception as e:
                            log.debug(f"éœ€è¦äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 5. è»½é‡ç•°å¸¸æ¤œå‡ºå™¨ (2025-08-29è¿½åŠ )
                        try:
                            from shift_suite.tasks.lightweight_anomaly_detector import LightweightAnomalyDetector
                            
                            detector = LightweightAnomalyDetector()
                            
                            # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèªã¨è¿½åŠ 
                            if 'parsed_slots_count' not in st.session_state.long_df.columns:
                                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦1ã‚’è¨­å®šï¼ˆå„ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒ1ã‚¹ãƒ­ãƒƒãƒˆï¼‰
                                st.session_state.long_df['parsed_slots_count'] = 1
                            
                            if 'code' not in st.session_state.long_df.columns:
                                # taskã‚«ãƒ©ãƒ ã‹ã‚‰æ¨å®š
                                st.session_state.long_df['code'] = st.session_state.long_df['task'].apply(
                                    lambda x: 'å¤œ' if 'å¤œ' in str(x) else 'æ—¥'
                                )
                            
                            # ç•°å¸¸æ¤œå‡ºã®å®Ÿè¡Œ
                            anomalies = detector.detect_anomalies(st.session_state.long_df)
                            
                            # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
                            anomaly_results = []
                            for anomaly in anomalies:
                                anomaly_results.append({
                                    'type': anomaly.anomaly_type,
                                    'severity': anomaly.severity,
                                    'score': anomaly.anomaly_score,
                                    'description': anomaly.description,
                                    'affected_count': len(anomaly.affected_records) if anomaly.affected_records else 0,
                                    'timestamp': anomaly.detection_timestamp.isoformat() if hasattr(anomaly, 'detection_timestamp') else None
                                })
                            
                            anomaly_file = zip_base / "anomaly_detection.json"
                            with open(anomaly_file, "w", encoding="utf-8") as f:
                                json.dump({
                                    'total_anomalies': len(anomalies),
                                    'anomalies': anomaly_results,
                                    'detection_config': {
                                        'sensitivity': detector.sensitivity if hasattr(detector, 'sensitivity') else 'medium',
                                        'threshold': detector.threshold if hasattr(detector, 'threshold') else None
                                    }
                                }, f, ensure_ascii=False, indent=2)
                            
                            log.info(f"ç•°å¸¸æ¤œå‡ºå®Œäº†: {len(anomalies)}ä»¶ã®ç•°å¸¸ã‚’æ¤œå‡º")
                        
                        except Exception as e:
                            log.debug(f"ç•°å¸¸æ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 6. å…¬å¹³æ€§è©•ä¾¡ (Phase 3: 2025-08-29è¿½åŠ )
                        try:
                            from shift_suite.tasks.fairness import run_fairness, calculate_jain_index
                            
                            # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèªã¨è¿½åŠ 
                            if 'holiday_type' not in st.session_state.long_df.columns:
                                st.session_state.long_df['holiday_type'] = 'å¹³æ—¥'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                            
                            # å…¬å¹³æ€§è©•ä¾¡ã®å®Ÿè¡Œ
                            fairness_result = run_fairness(st.session_state.long_df.copy(), Path(zip_base))
                            
                            if fairness_result and Path(fairness_result).exists():
                                fairness_df = pd.read_parquet(fairness_result)
                                
                                # JainæŒ‡æ•°ã®è¨ˆç®—ï¼ˆå…¨ä½“çš„ãªå…¬å¹³æ€§ï¼‰
                                if 'workload' in fairness_df.columns:
                                    workloads = fairness_df['workload'].values
                                    jain_index = calculate_jain_index(workloads)
                                else:
                                    jain_index = None
                                
                                # çµæœã‚’JSONã¨ã—ã¦ä¿å­˜
                                fairness_json = {
                                    'jain_index': jain_index,
                                    'evaluation_type': 'Jain Fairness Index',
                                    'description': 'åŠ´åƒè² è·ã®å…¬å¹³æ€§æŒ‡æ¨™ (1.0ãŒå®Œå…¨å…¬å¹³)',
                                    'staff_count': len(fairness_df) if not fairness_df.empty else 0,
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                fairness_file = zip_base / "fairness_evaluation.json"
                                with open(fairness_file, "w", encoding="utf-8") as f:
                                    json.dump(fairness_json, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"å…¬å¹³æ€§è©•ä¾¡å®Œäº†: JainæŒ‡æ•° = {jain_index}")
                            
                        except Exception as e:
                            log.debug(f"å…¬å¹³æ€§è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 7. ç–²åŠ´åº¦è©•ä¾¡ (Phase 3: 2025-08-29è¿½åŠ )
                        try:
                            from shift_suite.tasks.fatigue import train_fatigue
                            
                            # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèªã¨è¿½åŠ 
                            if 'code' not in st.session_state.long_df.columns:
                                st.session_state.long_df['code'] = st.session_state.long_df['task'].apply(
                                    lambda x: 'å¤œ' if 'å¤œ' in str(x) else 'æ—¥'
                                )
                            
                            if 'start_time' not in st.session_state.long_df.columns:
                                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é–‹å§‹æ™‚åˆ»ã‚’è¨­å®š
                                st.session_state.long_df['start_time'] = '09:00'
                            
                            if 'end_time' not in st.session_state.long_df.columns:
                                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®çµ‚äº†æ™‚åˆ»ã‚’è¨­å®š
                                st.session_state.long_df['end_time'] = '17:00'
                            
                            # ç–²åŠ´åº¦è©•ä¾¡ã®å®Ÿè¡Œ
                            fatigue_result = train_fatigue(st.session_state.long_df.copy(), Path(zip_base))
                            
                            if fatigue_result and Path(fatigue_result).exists():
                                fatigue_df = pd.read_parquet(fatigue_result)
                                
                                # çµæœã®é›†è¨ˆ
                                fatigue_summary = {
                                    'total_staff': len(fatigue_df),
                                    'average_fatigue_score': float(fatigue_df['fatigue_score'].mean()) if 'fatigue_score' in fatigue_df.columns else None,
                                    'max_fatigue_score': float(fatigue_df['fatigue_score'].max()) if 'fatigue_score' in fatigue_df.columns else None,
                                    'high_risk_staff': int((fatigue_df['fatigue_score'] > 55).sum()) if 'fatigue_score' in fatigue_df.columns else 0,
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                # å€‹åˆ¥ã‚¹ã‚¿ãƒƒãƒ•ã®ç–²åŠ´åº¦è©³ç´°
                                if 'fatigue_score' in fatigue_df.columns:
                                    staff_details = []
                                    for _, row in fatigue_df.iterrows():
                                        staff_details.append({
                                            'staff': row.get('staff', 'Unknown'),
                                            'fatigue_score': float(row.get('fatigue_score', 0)),
                                            'risk_level': 'High' if row.get('fatigue_score', 0) > 55 else 'Medium' if row.get('fatigue_score', 0) > 35 else 'Low'
                                        })
                                    fatigue_summary['staff_details'] = staff_details
                                
                                fatigue_file = zip_base / "fatigue_evaluation.json"
                                with open(fatigue_file, "w", encoding="utf-8") as f:
                                    json.dump(fatigue_summary, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"ç–²åŠ´åº¦è©•ä¾¡å®Œäº†: å¹³å‡ã‚¹ã‚³ã‚¢ = {fatigue_summary.get('average_fatigue_score', 0):.3f}")
                            
                        except Exception as e:
                            log.debug(f"ç–²åŠ´åº¦è©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 8. ã‚¹ã‚­ãƒ«åˆ†æ (Phase 3: 2025-08-29è¿½åŠ )
                        try:
                            from shift_suite.tasks.skill_nmf import build_skill_matrix
                            
                            # ã‚¹ã‚­ãƒ«è¡Œåˆ—ã®æ§‹ç¯‰
                            skill_matrix = build_skill_matrix(st.session_state.long_df.copy(), Path(zip_base))
                            
                            if isinstance(skill_matrix, pd.DataFrame) and not skill_matrix.empty:
                                # ã‚¹ã‚­ãƒ«åˆ†æçµæœã®é›†è¨ˆ
                                skill_summary = {
                                    'total_staff': skill_matrix.shape[0],
                                    'total_tasks': skill_matrix.shape[1],
                                    'matrix_shape': list(skill_matrix.shape),
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                # ã‚¹ã‚¿ãƒƒãƒ•ã”ã¨ã®ä¸»è¦ã‚¹ã‚­ãƒ«
                                staff_skills = []
                                for staff in skill_matrix.index:
                                    top_skills = skill_matrix.loc[staff].nlargest(3)
                                    staff_skills.append({
                                        'staff': str(staff),
                                        'top_skills': [
                                            {'task': str(task), 'score': float(score)}
                                            for task, score in top_skills.items()
                                        ]
                                    })
                                
                                skill_summary['staff_skills'] = staff_skills
                                
                                # ã‚¿ã‚¹ã‚¯ã”ã¨ã®å°‚é–€ã‚¹ã‚¿ãƒƒãƒ•
                                task_specialists = []
                                for task in skill_matrix.columns:
                                    top_staff = skill_matrix[task].nlargest(3)
                                    task_specialists.append({
                                        'task': str(task),
                                        'specialists': [
                                            {'staff': str(staff), 'score': float(score)}
                                            for staff, score in top_staff.items()
                                        ]
                                    })
                                
                                skill_summary['task_specialists'] = task_specialists
                                
                                skill_file = zip_base / "skill_analysis.json"
                                with open(skill_file, "w", encoding="utf-8") as f:
                                    json.dump(skill_summary, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"ã‚¹ã‚­ãƒ«åˆ†æå®Œäº†: {skill_summary['total_staff']}äººÃ—{skill_summary['total_tasks']}ã‚¿ã‚¹ã‚¯")
                            
                        except Exception as e:
                            log.debug(f"ã‚¹ã‚­ãƒ«åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 7. é€£ç¶šå‹¤å‹™æ¤œå‡º (Phase 4: 2025-08-30è¿½åŠ )
                        try:
                            from shift_suite.tasks.continuous_shift_detector import ContinuousShiftDetector
                            
                            detector = ContinuousShiftDetector()
                            violations = detector.detect_continuous_shifts(st.session_state.long_df.copy())
                            
                            if violations:
                                continuous_shifts_summary = {
                                    'total_violations': len(violations),
                                    'violations': [
                                        {
                                            'staff': v.staff_name,
                                            'start_date': v.start_date.isoformat() if hasattr(v.start_date, 'isoformat') else str(v.start_date),
                                            'end_date': v.end_date.isoformat() if hasattr(v.end_date, 'isoformat') else str(v.end_date),
                                            'consecutive_days': v.consecutive_days,
                                            'total_hours': v.total_duration_hours,
                                            'violation_type': v.violation_type
                                        }
                                        for v in violations[:10]  # æœ€åˆã®10ä»¶ã®ã¿
                                    ],
                                    'summary': {
                                        'max_consecutive_days': max(v.consecutive_days for v in violations) if violations else 0,
                                        'affected_staff_count': len(set(v.staff_name for v in violations))
                                    },
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                continuous_file = Path(zip_base) / "continuous_shifts.json"
                                with open(continuous_file, "w", encoding="utf-8") as f:
                                    json.dump(continuous_shifts_summary, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"é€£ç¶šå‹¤å‹™æ¤œå‡ºå®Œäº†: {len(violations)}ä»¶ã®é•åæ¤œå‡º")
                            else:
                                log.info("é€£ç¶šå‹¤å‹™é•åãªã—")
                            
                        except Exception as e:
                            log.debug(f"é€£ç¶šå‹¤å‹™æ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 8. é›¢è·äºˆæ¸¬ (Phase 4: MLç‰ˆ - é«˜ç²¾åº¦äºˆæ¸¬)
                        try:
                            from shift_suite.tasks.turnover_prediction import TurnoverPredictionEngine
                            import pickle
                            
                            # é«˜ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ï¼ˆXGBoost/LightGBMï¼‰ã‚’ä½¿ç”¨
                            predictor = TurnoverPredictionEngine(
                                model_type='ensemble',  # å…¨ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
                                enable_early_warning=True
                            )
                            
                            # ç‰¹å¾´é‡æŠ½å‡º
                            features_df = predictor.extract_turnover_features(st.session_state.long_df.copy())
                            risk_scores = {}
                            
                            if not features_df.empty:
                                # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ‘ã‚¹ã®ç¢ºèª
                                model_path = Path('models/turnover_model.pkl')
                                
                                # ä¿å­˜æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
                                if model_path.exists():
                                    try:
                                        with open(model_path, 'rb') as f:
                                            predictor.models = pickle.load(f)
                                        log.info("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                                        
                                        # äºˆæ¸¬å®Ÿè¡Œ
                                        risk_df = predictor.predict_turnover_risk(features_df)
                                        if not risk_df.empty and 'risk_score' in risk_df.columns:
                                            risk_scores = dict(zip(risk_df['staff'], risk_df['risk_score']))
                                    except Exception as e:
                                        log.debug(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ã€æ–°è¦å­¦ç¿’ã—ã¾ã™: {e}")
                                
                                # ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯æ–°è¦å­¦ç¿’
                                if not risk_scores and len(features_df) >= 5:  # æœ€ä½5äººåˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                                    try:
                                        # åˆæˆãƒ©ãƒ™ãƒ«ç”Ÿæˆ
                                        features_df = predictor.generate_synthetic_labels(features_df)
                                        
                                        # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
                                        X, y, feature_names = predictor.prepare_model_data(features_df)
                                        if X is not None and len(X) > 0:
                                            predictor.train_models(X, y, feature_names)
                                            
                                            # äºˆæ¸¬å®Ÿè¡Œ
                                            risk_df = predictor.predict_turnover_risk(features_df)
                                            if not risk_df.empty and 'risk_score' in risk_df.columns:
                                                risk_scores = dict(zip(risk_df['staff'], risk_df['risk_score']))
                                            
                                            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆæ¬¡å›ä½¿ç”¨ã®ãŸã‚ï¼‰
                                            try:
                                                model_path.parent.mkdir(exist_ok=True)
                                                with open(model_path, 'wb') as f:
                                                    pickle.dump(predictor.models, f)
                                                log.info("å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
                                            except Exception as e:
                                                log.debug(f"ãƒ¢ãƒ‡ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
                                    except Exception as e:
                                        log.debug(f"MLå­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
                                
                                # MLãŒå¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
                                if not risk_scores:
                                    for _, row in features_df.iterrows():
                                        risk = 0.0
                                        if 'max_consecutive_days' in row and row['max_consecutive_days'] > 7:
                                            risk += 0.3
                                        if 'night_shift_ratio' in row and row['night_shift_ratio'] > 0.5:
                                            risk += 0.3
                                        if 'total_hours' in row and row['total_hours'] > 200:
                                            risk += 0.2
                                        risk_scores[row['staff']] = min(risk, 1.0)
                            
                            if risk_scores:
                                # ãƒªã‚¹ã‚¯ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆï¼ˆé™é †ï¼‰
                                sorted_risks = sorted(risk_scores.items(), key=lambda x: x[1], reverse=True)
                                
                                turnover_summary = {
                                    'total_staff_analyzed': len(risk_scores),
                                    'high_risk_staff': [
                                        {'name': name, 'risk_score': round(score, 3)}
                                        for name, score in sorted_risks[:10]
                                        if score > 0.7  # é«˜ãƒªã‚¹ã‚¯é–¾å€¤
                                    ],
                                    'risk_distribution': {
                                        'high': sum(1 for _, score in risk_scores.items() if score > 0.7),
                                        'medium': sum(1 for _, score in risk_scores.items() if 0.4 <= score <= 0.7),
                                        'low': sum(1 for _, score in risk_scores.items() if score < 0.4)
                                    },
                                    'average_risk': round(sum(risk_scores.values()) / len(risk_scores), 3) if risk_scores else 0,
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                turnover_file = Path(zip_base) / "turnover_prediction.json"
                                with open(turnover_file, "w", encoding="utf-8") as f:
                                    json.dump(turnover_summary, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"é›¢è·äºˆæ¸¬å®Œäº†: {len(risk_scores)}äººåˆ†æ, é«˜ãƒªã‚¹ã‚¯{turnover_summary['risk_distribution']['high']}äºº")
                            
                        except Exception as e:
                            log.debug(f"é›¢è·äºˆæ¸¬ã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        # 9. ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ (Phase 4: 2025-08-30è¿½åŠ )
                        try:
                            from shift_suite.tasks.shift_mind_reader_lite import ShiftMindReaderLite
                            
                            reader = ShiftMindReaderLite()
                            analysis_result = reader.read_creator_mind(st.session_state.long_df.copy())
                            
                            if analysis_result:
                                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦ç´„ã‚’ä½œæˆ
                                basic_analysis = analysis_result.get('basic_analysis', {})
                                decision_patterns = analysis_result.get('decision_patterns', {})
                                
                                pattern_summary = {
                                    'analysis_type': analysis_result.get('analysis_type', 'lightweight'),
                                    'basic_statistics': basic_analysis,
                                    'decision_patterns': decision_patterns,
                                    'insights': reader.get_simplified_insights() if hasattr(reader, 'get_simplified_insights') else {},
                                    'timestamp': datetime.now().isoformat()
                                }
                                
                                patterns_file = Path(zip_base) / "shift_patterns.json"
                                with open(patterns_file, "w", encoding="utf-8") as f:
                                    json.dump(pattern_summary, f, ensure_ascii=False, indent=2)
                                
                                log.info(f"ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†: {len(decision_patterns)}ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹")
                            
                        except Exception as e:
                            log.debug(f"ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
                        
                        log.info("é«˜åº¦ãªåˆ†ææ©Ÿèƒ½ã®å®Ÿè¡ŒãŒå®Œäº†ã—ã¾ã—ãŸ")
                        
                    except Exception as e:
                        log.warning(f"é«˜åº¦ãªåˆ†ææ©Ÿèƒ½ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€å‡¦ç†ã‚’ç¶™ç¶šã—ã¾ã™: {e}")
                
                with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zf:
                    for f_path in zip_base.glob("**/*"):
                        if f_path.is_file():
                            zf.write(f_path, f_path.relative_to(zip_base))

                st.download_button(
                    label="ğŸ“¥ analysis_results.zip ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=zip_buffer.getvalue(),
                    file_name="analysis_results.zip",
                    mime="application/zip",
                    type="primary",
                    help="åˆ†æçµæœã®å…¨ãƒ•ã‚¡ã‚¤ãƒ« + AIå‘ã‘åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆ(JSON) + æ´å¯Ÿæ¤œå‡ºçµæœ(JSON/HTML/CSV) + é€£ç¶šå‹¤å‹™æ¤œå‡º + é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬ + ä½œæˆè€…æ„å›³åˆ†æ + éœ€è¦äºˆæ¸¬(NEW) + ç•°å¸¸æ¤œå‡º(NEW)ãŒå«ã¾ã‚Œã¦ã„ã¾ã™"
                )
            else:
                st.error("åˆ†æçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            st.header("ã‚¹ãƒ†ãƒƒãƒ—2: é«˜é€Ÿãƒ“ãƒ¥ãƒ¼ã‚¢ã§çµæœã‚’ç¢ºèª")
            st.write(
                "çµæœã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã€ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã‹ã‚‰ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚’é–‹ãã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
            )

            DASH_APP_URL = "http://127.0.0.1:8050"
            st.markdown(
                f"### [ğŸ“ˆ åˆ†æçµæœã‚’é«˜é€Ÿãƒ“ãƒ¥ãƒ¼ã‚¢ã§è¡¨ç¤ºã™ã‚‹]({DASH_APP_URL})",
                unsafe_allow_html=True,
            )
            
            # åŒ…æ‹¬åˆ†æãƒ­ã‚°ã®ç”Ÿæˆ
            if st.session_state.analysis_done and st.session_state.out_dir_path_str:
                try:
                    output_dir = Path(st.session_state.out_dir_path_str)
                    log_file = create_comprehensive_analysis_log(output_dir, analysis_type="FULL")
                    if log_file:
                        st.success(f"ğŸ“‹ åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {log_file.name}")
                        logging.info(f"[app] åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {log_file}")
                    else:
                        st.warning("âš ï¸ åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                except Exception as e_log:
                    st.warning(f"âš ï¸ åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e_log}")
                    logging.error(f"[app] åŒ…æ‹¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e_log}")
        except ValueError as ve_exec_run_main:
            log_and_display_error(
                _("Error during analysis (ValueError)"), ve_exec_run_main
            )
            log.error(f"è§£æã‚¨ãƒ©ãƒ¼ (ValueError): {ve_exec_run_main}", exc_info=True)
            st.session_state.analysis_done = False
        except FileNotFoundError as fe_exec_run_main:
            log_and_display_error(_("Required file not found"), fe_exec_run_main)
            log.error(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {fe_exec_run_main}", exc_info=True)
            st.session_state.analysis_done = False
        except Exception as e_exec_run_main:
            log_and_display_error(_("Unexpected error occurred"), e_exec_run_main)
            log.error(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e_exec_run_main}", exc_info=True)
            st.session_state.analysis_done = False
        finally:
            if "progress_bar_val" in locals() and progress_bar_val is not None:
                progress_bar_val.empty()
            if "progress_text_area" in locals() and progress_text_area is not None:
                progress_text_area.empty()
            if "progress_status" in locals() and progress_status is not None:
                progress_status.empty()

        if st.session_state.analysis_done and st.session_state.out_dir_path_str:
            out_dir_to_save_exec_main_run = Path(st.session_state.out_dir_path_str)
            if out_dir_to_save_exec_main_run.exists():
                st.markdown("---")
                st.header("3. " + _("Save Analysis Results"))
                with st.expander(_("Run Parameters")):
                    st.json(
                        {
                            "sheets": param_selected_sheets,
                            "slot": param_slot,
                            "need_calc": param_need_calc_method,
                            "stat": param_need_stat_method,
                            "ext_modules": param_ext_opts,
                        }
                    )
                current_save_mode_exec_main_run = (
                    st.session_state.save_mode_selectbox_widget
                )
                if current_save_mode_exec_main_run == _("Save to folder"):
                    st.info(_("Output folder") + f": `{out_dir_to_save_exec_main_run}`")
                    st.markdown(_("Open the above path in Explorer."))
                else:  # ZIP Download
                    zip_base_name_exec_main_run = f"ShiftSuite_Analysis_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
                    if st.session_state.work_root_path_str is None:
                        log_and_display_error(
                            "ä¸€æ™‚ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚",
                            FileNotFoundError("work_root"),
                        )
                    else:
                        work_root_for_zip_dl_exec_main_run = Path(
                            st.session_state.work_root_path_str
                        )
                        zip_path_obj_to_download_exec_main_run = (
                            work_root_for_zip_dl_exec_main_run
                            / f"{zip_base_name_exec_main_run}.zip"
                        )
                        try:
                            with zipfile.ZipFile(
                                zip_path_obj_to_download_exec_main_run,
                                "w",
                                zipfile.ZIP_DEFLATED,
                            ) as zf_dl_exec_main_run:
                                base_dir = Path(st.session_state.work_root_path_str)
                                scenario_dirs = sorted(base_dir.glob("out_*"))
                                if not scenario_dirs:
                                    scenario_dirs = [out_dir_to_save_exec_main_run]
                                for s_dir in scenario_dirs:
                                    for file_to_zip_dl_exec_main_run in s_dir.rglob("*"):
                                        if file_to_zip_dl_exec_main_run.is_file():
                                            zf_dl_exec_main_run.write(
                                                file_to_zip_dl_exec_main_run,
                                                file_to_zip_dl_exec_main_run.relative_to(base_dir),
                                            )
                            with open(
                                zip_path_obj_to_download_exec_main_run, "rb"
                            ) as fp_zip_data_to_download_dl_exec_main_run:
                                st.download_button(
                                    label=_("Download analysis results as ZIP"),
                                    data=fp_zip_data_to_download_dl_exec_main_run,
                                    file_name=f"{zip_base_name_exec_main_run}.zip",
                                    mime="application/zip",
                                    use_container_width=True,
                                )
                            log.info(
                                f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã—ãŸ: {zip_path_obj_to_download_exec_main_run}"
                            )
                        except Exception as e_zip_final_exec_run_main_ex_v3:
                            log_and_display_error(
                                _("Error creating ZIP file"),
                                e_zip_final_exec_run_main_ex_v3,
                            )
                            log.error(
                                f"ZIPä½œæˆã‚¨ãƒ©ãƒ¼ (æœ€çµ‚æ®µéš): {e_zip_final_exec_run_main_ex_v3}",
                                exc_info=True,
                            )
        else:
            log.warning(
                f"è§£æã¯å®Œäº†ã—ã¾ã—ãŸãŒã€å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{st.session_state.out_dir_path_str}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
            )

        # ğŸ¤– AIå‘ã‘åŒ…æ‹¬çš„åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ã‚’åé›†ãƒ»ä¿å­˜
        analysis_results_comprehensive = {
            "out_dir_path_str": st.session_state.out_dir_path_str,
            "leave_analysis_results": st.session_state.leave_analysis_results,
            "rest_time_results": st.session_state.rest_time_results,
            "work_pattern_results": st.session_state.work_pattern_results,
            "attendance_results": st.session_state.attendance_results,
            "combined_score_results": st.session_state.combined_score_results,
            "low_staff_load_results": st.session_state.low_staff_load_results,
        }
        
        # è¿½åŠ ã®åˆ†æçµæœãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆAIå‘ã‘ãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
        out_dir = Path(st.session_state.out_dir_path_str) if st.session_state.out_dir_path_str else None
        if out_dir and out_dir.exists():
            try:
                # session_stateã‹ã‚‰å®Ÿéš›ã®åˆ†æçµæœã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
                if hasattr(st.session_state, 'shortage_analysis_results') and file_name in st.session_state.shortage_analysis_results:
                    analysis_results_comprehensive["shortage_analysis"] = st.session_state.shortage_analysis_results[file_name]
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä¸è¶³æ™‚é–“ãƒ‡ãƒ¼ã‚¿ã®åé›†
                    shortage_files = list(out_dir.glob("*shortage*.parquet"))
                    if shortage_files:
                        shortage_df = pd.read_parquet(shortage_files[0])
                        analysis_results_comprehensive["shortage_analysis"] = {
                            "total_shortage_hours": float(shortage_df.get("shortage", pd.Series([0])).sum()),
                            "total_excess_hours": float(shortage_df.get("excess", pd.Series([0])).sum()),
                            "avg_shortage_per_slot": float(shortage_df.get("shortage", pd.Series([0])).mean())
                        }
                
                if hasattr(st.session_state, 'fatigue_analysis_results') and file_name in st.session_state.fatigue_analysis_results:
                    analysis_results_comprehensive["fatigue_analysis"] = st.session_state.fatigue_analysis_results[file_name]
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ã®åé›†
                    fatigue_files = list(out_dir.glob("*fatigue*.parquet"))
                    if fatigue_files:
                        fatigue_df = pd.read_parquet(fatigue_files[0])
                        analysis_results_comprehensive["fatigue_analysis"] = {
                            "avg_fatigue_score": float(fatigue_df.get("fatigue_score", pd.Series([0.5])).mean()),
                            "high_fatigue_staff_count": int((fatigue_df.get("fatigue_score", pd.Series([0])) > 55).sum()),
                            "staff_fatigue": fatigue_df.to_dict('index') if not fatigue_df.empty else {}
                        }
                
                if hasattr(st.session_state, 'fairness_analysis_results') and file_name in st.session_state.fairness_analysis_results:
                    analysis_results_comprehensive["fairness_analysis"] = st.session_state.fairness_analysis_results[file_name]
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¬å¹³æ€§åˆ†æãƒ‡ãƒ¼ã‚¿ã®åé›†
                    fairness_files = list(out_dir.glob("*fairness*.parquet"))
                    if fairness_files:
                        fairness_df = pd.read_parquet(fairness_files[0])
                        analysis_results_comprehensive["fairness_analysis"] = {
                            "avg_fairness_score": float(fairness_df.get("fairness_score", pd.Series([0.8])).mean()),
                            "low_fairness_staff_count": int((fairness_df.get("fairness_score", pd.Series([1])) < 0.7).sum())
                        }
                
                # ãƒ‡ãƒ¼ã‚¿å“è³ªã‚µãƒãƒªãƒ¼
                analysis_results_comprehensive["data_summary"] = {
                    "total_records": len(shortage_df) if 'shortage_df' in locals() and not shortage_df.empty else 0,
                    "analysis_period": f"{getattr(st.session_state, 'need_ref_start_date_widget', param_need_ref_start)} to {getattr(st.session_state, 'need_ref_end_date_widget', param_need_ref_end)}",
                    "generated_files_count": len(list(out_dir.glob("*.parquet")))
                }
                
            except Exception as e:
                log.warning(f"AIå‘ã‘åˆ†æçµæœãƒ‡ãƒ¼ã‚¿åé›†ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        st.session_state.analysis_results[file_name] = analysis_results_comprehensive


# åˆ†æãŒå®Œäº†ã—ã¦ã„ã‚‹å ´åˆã¯å¸¸ã«display_dataã‚’æ›´æ–°
if st.session_state.get("analysis_done"):
    out_dir_path = st.session_state.get("out_dir_path_str")
    if out_dir_path:
        out_dir = Path(out_dir_path)
        if out_dir.exists():
            update_display_data_with_heatmaps(out_dir)
        else:
            log.warning(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {out_dir}")
            st.warning(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {out_dir}")
    else:
        log.warning("å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.warning("åˆ†æçµæœã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

# å®Œå…¨ä¿®æ­£ç‰ˆ - ä¼‘æš‡åˆ†æçµæœè¡¨ç¤ºã‚³ãƒ¼ãƒ‰å…¨ä½“

# Plotlyã®å…¨ä½“å•é¡Œã‚’ä¿®æ­£ã—ãŸä¼‘æš‡åˆ†æã‚³ãƒ¼ãƒ‰


#  æ–°ã—ã„ã€Œä¼‘æš‡åˆ†æã€ã‚¿ãƒ–ã®è¡¨ç¤º (è§£æãŒå®Œäº†ã—ã€ä¼‘æš‡åˆ†æãŒé¸æŠã•ã‚Œã¦ã„ã‚‹å ´åˆ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  app.py  (Part 3 / 3)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def display_overview_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("Overview"))
        display_data = st.session_state.get("display_data", {})
        df_sh_role = display_data.get("shortage_role")
        lack_h = 0.0
        excess_cost = lack_temp_cost = lack_penalty_cost = 0.0
        if isinstance(df_sh_role, pd.DataFrame):
            try:
                lack_h = df_sh_role["lack_h"].sum() if "lack_h" in df_sh_role else 0.0
                excess_cost = float(
                    df_sh_role.get("estimated_excess_cost", pd.Series()).sum()
                )
                lack_temp_cost = float(
                    df_sh_role.get(
                        "estimated_lack_cost_if_temporary_staff", pd.Series()
                    ).sum()
                )
                lack_penalty_cost = float(
                    df_sh_role.get("estimated_lack_penalty_cost", pd.Series()).sum()
                )
            except Exception as e:
                st.warning(f"shortage_role_summary.parquet èª­è¾¼/é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                excess_cost = lack_temp_cost = lack_penalty_cost = 0.0

        meta_df = display_data.get("fairness_before")
        jain_display = "N/A"
        if isinstance(meta_df, pd.DataFrame):
            try:
                jain_row = meta_df[meta_df["metric"] == "jain_index"]
                if not jain_row.empty:
                    jain_display = f"{float(jain_row['value'].iloc[0]):.3f}"
            except Exception:
                pass

        staff_count = 0
        avg_night_ratio = 0.0
        df_staff = display_data.get("staff_stats")
        if isinstance(df_staff, pd.DataFrame):
            try:
                staff_count = len(df_staff)
                if (
                    "night_ratio" in df_staff.columns
                    and not df_staff["night_ratio"].empty
                ):
                    avg_night_ratio = float(df_staff["night_ratio"].mean())
            except Exception:
                pass

        alerts_count = 0
        df_alerts = display_data.get("stats_alerts")
        if isinstance(df_alerts, pd.DataFrame):
            try:
                if _valid_df(df_alerts):
                    alerts_count = len(df_alerts)
            except Exception:
                pass

        c1, c2, c3, c4, c5, c6, c7, c8 = st.columns(8)
        c1.metric(_("ç·ä¸è¶³æ™‚é–“(h) (è·ç¨®åˆ¥åˆè¨ˆ)"), f"{lack_h:.1f}")
        c2.metric("å¤œå‹¤ JainæŒ‡æ•°", jain_display)
        c3.metric(_("Total Staff"), staff_count)
        c4.metric(_("Avg. Night Ratio"), f"{avg_night_ratio:.3f}")
        c5.metric(_("Alerts Count"), alerts_count)
        c6.metric(_("ç·éå‰°ã‚³ã‚¹ãƒˆè©¦ç®—(Â¥)"), f"{excess_cost:,.0f}")
        c7.metric(_("ç·ä¸è¶³ã‚³ã‚¹ãƒˆè©¦ç®—(æ´¾é£è£œå¡«æ™‚)(Â¥)"), f"{lack_temp_cost:,.0f}")
        c8.metric(_("ç·ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£è©¦ç®—(Â¥)"), f"{lack_penalty_cost:,.0f}")


def display_heatmap_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("Heatmap"))

        # é¸æŠè‚¢ãŠã‚ˆã³ãƒ‡ãƒ¼ã‚¿ã¯äº‹å‰ã« st.session_state ã«æ ¼ç´æ¸ˆã¿
        roles = st.session_state.get("available_roles", [])
        employments = st.session_state.get("available_employments", [])

        scope_opts = {"overall": _("Overall")}
        if roles:
            scope_opts["role"] = _("Role")
        if employments:
            scope_opts["employment"] = _("Employment")

        # --- UIã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ« ---
        with st.form(key="heatmap_controls_form"):
            st.write(
                "è¡¨ç¤ºã™ã‚‹ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ç¯„å›²ã¨ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã€æ›´æ–°ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"
            )

            c1, c2 = st.columns(2)
            with c1:
                scope_lbl = st.selectbox(
                    "è¡¨ç¤ºç¯„å›²", list(scope_opts.values()), key="heat_scope_form"
                )
            scope = [k for k, v in scope_opts.items() if v == scope_lbl][0]

            sel_item = []
            with c2:
                if scope == "role":
                    sel_item = st.multiselect(
                        _("Role"), roles, key="heat_scope_role_form"
                    )
                elif scope == "employment":
                    sel_item = st.multiselect(
                        _("Employment"), employments, key="heat_scope_emp_form"
                    )

            mode_opts = {"Raw": _("Raw Count"), "Ratio": _("Ratio (staff Ã· need)")}
            mode_lbl = st.radio(
                _("Display Mode"),
                list(mode_opts.values()),
                horizontal=True,
                key="dash_heat_mode_radio_form",
            )

            submitted = st.form_submit_button("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æ›´æ–°")

        # --- ãƒ•ã‚©ãƒ¼ãƒ é€ä¿¡å¾Œã«ã‚°ãƒ©ãƒ•ã‚’æç”» ---
        if submitted:
            scope_info_for_title = scope_lbl
            heat_keys = ["heat_all"]
            if scope == "role" and sel_item:
                heat_keys = [f"heat_role_{safe_sheet(x, for_path=True)}" for x in sel_item]
                scope_info_for_title += f" ({', '.join(sel_item)})"
            elif scope == "employment" and sel_item:
                heat_keys = [f"heat_emp_{safe_sheet(x, for_path=True)}" for x in sel_item]
                scope_info_for_title += f" ({', '.join(sel_item)})"

            df_heat = load_and_sum_heatmaps(data_dir, heat_keys)
            mode = [k for k, v in mode_opts.items() if v == mode_lbl][0]

            if df_heat is not None and not df_heat.empty:
                fig = generate_heatmap_figure(df_heat, mode, scope_info_for_title)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def display_shortage_tab(tab_container, data_dir):
    with tab_container:
        if st.session_state.analysis_status.get("shortage") != "success":
            st.warning("ä¸è¶³åˆ†æãŒæ­£å¸¸ã«å®Œäº†ã—ã¦ã„ãªã„ãŸã‚ã€çµæœã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")
            return
        st.subheader(_("Shortage"))
        st.info(
            "\n".join(
                [
                    "### è¨ˆç®—ã«ä½¿ç”¨ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿",
                    f"- Needç®—å‡ºæ–¹æ³•: {st.session_state.get('need_calc_method_widget')}",
                    f"- Upperç®—å‡ºæ–¹æ³•: {st.session_state.get('upper_calc_method_widget')}",
                    f"- ç›´æ¥é›‡ç”¨å˜ä¾¡: Â¥{st.session_state.get('wage_direct_widget', 0):,.0f}/h",
                    f"- æ´¾é£å˜ä¾¡: Â¥{st.session_state.get('wage_temp_widget', 0):,.0f}/h",
                    f"- æ¡ç”¨ã‚³ã‚¹ãƒˆ: Â¥{st.session_state.get('hiring_cost_once_widget', 0):,}/äºº",
                    f"- ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£: Â¥{st.session_state.get('penalty_per_lack_widget', 0):,.0f}/h",
                ]
            )
        )
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯ãƒ¡ãƒ¢ãƒªä¸Šã® session_state ã‹ã‚‰å–å¾—
        roles = st.session_state.get("available_roles", [])
        employments = st.session_state.get("available_employments", [])
        display_data = st.session_state.get("display_data", {})
        scenario_df = st.session_state.get("shortage_scenarios")
        if isinstance(scenario_df, pd.DataFrame) and {"shortage_mean", "shortage_median", "shortage_p25"}.issubset(scenario_df.columns):
            st.write("ã‚·ãƒŠãƒªã‚ªæ¯”è¼ƒ")
            tab_mean, tab_median, tab_p25 = st.tabs(["å¹³å‡å€¤åŸºæº–", "ä¸­å¤®å€¤åŸºæº–", "25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åŸºæº–"])
            for tab, col in zip(
                [tab_mean, tab_median, tab_p25],
                ["shortage_mean", "shortage_median", "shortage_p25"],
            ):
                with tab:
                    st.dataframe(
                        scenario_df[["time_group", "è·ç¨®", "é›‡ç”¨å½¢æ…‹", "actual_count", col]].drop_duplicates(),
                        use_container_width=True,
                    )
        df_s_role = display_data.get("shortage_role")
        if isinstance(df_s_role, pd.DataFrame):
            if not _valid_df(df_s_role):
                st.info("Data not available")
                return
                display_role_df = df_s_role.rename(
                    columns={
                        "role": _("Role"),
                        "need_h": _("Need Hours"),
                        "staff_h": _("Staff Hours"),
                        "lack_h": _("Shortage Hours"),
                        "excess_h": _("Excess Hours"),
                        "estimated_excess_cost": _("Excess Cost Est.(Â¥)"),
                        "estimated_lack_cost_if_temporary_staff": _(
                            "Lack Cost if Temp(Â¥)"
                        ),
                        "estimated_lack_penalty_cost": _("Lack Penalty Est.(Â¥)"),
                        "working_days_considered": _("Working Days"),
                        "note": _("Note"),
                    }
                )
                total_lack = float(df_s_role.get("lack_h", pd.Series()).sum())
                if total_lack:
                    cols = st.columns(4)
                    cols[0].metric(_("Total Shortage Hours"), f"{total_lack:.1f}")
                    if {"role", "lack_h"}.issubset(df_s_role.columns):
                        top_roles = df_s_role.nlargest(3, "lack_h")[["role", "lack_h"]]
                        for i, row in enumerate(
                            top_roles.itertuples(index=False), start=1
                        ):
                            cols[i].metric(
                                _("Top shortage role {n}").format(n=i),
                                f"{row.role}: {row.lack_h:.1f}h",
                            )
                st.dataframe(display_role_df, use_container_width=True, hide_index=True)
                if "role" in df_s_role and "lack_h" in df_s_role:
                    fig_role = px.bar(
                        df_s_role,
                        x="role",
                        y="lack_h",
                        labels={"role": _("Role"), "lack_h": _("Shortage Hours")},
                        color_discrete_sequence=["#FFA500"],
                    )
                    st.plotly_chart(
                        fig_role, use_container_width=True, key="short_role_chart"
                    )
                    st.caption(_("Shortage by role caption"))
                if "role" in df_s_role and "excess_h" in df_s_role:
                    fig_role_ex = px.bar(
                        df_s_role,
                        x="role",
                        y="excess_h",
                        labels={"role": _("Role"), "excess_h": _("Excess Hours")},
                        color_discrete_sequence=["#00BFFF"],
                    )
                    st.plotly_chart(
                        fig_role_ex, use_container_width=True, key="excess_role_chart"
                    )
                cost_cols = [
                    "estimated_excess_cost",
                    "estimated_lack_cost_if_temporary_staff",
                    "estimated_lack_penalty_cost",
                ]
                if {"role"}.issubset(df_s_role.columns) and any(
                    c in df_s_role.columns for c in cost_cols
                ):
                    cost_df = df_s_role[
                        ["role"] + [c for c in cost_cols if c in df_s_role.columns]
                    ]
                    cost_long = cost_df.melt(
                        id_vars="role", var_name="type", value_name="cost"
                    )
                    fig_cost = px.bar(
                        cost_long,
                        x="role",
                        y="cost",
                        color="type",
                        barmode="group",
                        labels={
                            "role": _("Role"),
                            "cost": _("Cost (Â¥)"),
                            "type": _("Type"),
                        },
                    )
                    st.plotly_chart(
                        fig_cost, use_container_width=True, key="cost_role_chart"
                    )

                fp_hire = data_dir / "hire_plan.parquet"
                if fp_hire.exists():
                    try:
                        df_hire = load_data_cached(
                            str(fp_hire),
                            is_parquet=True,
                            file_mtime=_file_mtime(fp_hire),
                        )
                        if not _valid_df(df_hire):
                            st.info("Data not available")
                            return
                        if {"role", "hire_fte"}.issubset(df_hire.columns):
                            st.markdown(_("Required FTE per Role"))
                            display_hire_df = df_hire[["role", "hire_fte"]].rename(
                                columns={"role": _("Role"), "hire_fte": _("hire_fte")}
                            )
                            st.dataframe(
                                display_hire_df,
                                use_container_width=True,
                                hide_index=True,
                            )
                            fig_hire = px.bar(
                                df_hire,
                                x="role",
                                y="hire_fte",
                                labels={"role": _("Role"), "hire_fte": _("hire_fte")},
                                color_discrete_sequence=["#1f77b4"],
                            )
                            st.plotly_chart(
                                fig_hire,
                                use_container_width=True,
                                key="short_hire_chart",
                            )
                    except Exception as e:
                        log_and_display_error("hire_plan.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)

                role_month_fp = data_dir / "shortage_role_monthly.parquet"
                if role_month_fp.exists():
                    df_month = load_data_cached(
                        str(role_month_fp),
                        is_parquet=True,
                        file_mtime=_file_mtime(role_month_fp),
                    )
                    if not _valid_df(df_month):
                        st.info("Data not available")
                        return
                    if not df_month.empty and {"month", "role", "lack_h"}.issubset(
                        df_month.columns
                    ):
                        fig_m = px.bar(
                            df_month,
                            x="month",
                            y="lack_h",
                            color="role",
                            barmode="stack",
                            title=_("Monthly Shortage Hours by Role"),
                            labels={
                                "month": _("Month"),
                                "lack_h": _("Shortage Hours"),
                                "role": _("Role"),
                            },
                        )
                        st.plotly_chart(
                            fig_m, use_container_width=True, key="short_month_chart"
                        )
                        with st.expander(_("Monthly shortage data")):
                            st.dataframe(
                                df_month, use_container_width=True, hide_index=True
                            )
        else:
            st.info(
                _("Shortage")
                + " (shortage_role_summary.parquet) "
                + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        fp_s_emp = data_dir / "shortage_employment_summary.parquet"
        if fp_s_emp.exists():
            try:
                df_s_emp = load_data_cached(
                    str(fp_s_emp),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_s_emp),
                )
                if _valid_df(df_s_emp):
                    display_emp_df = df_s_emp.rename(
                        columns={
                            "employment": _("Employment"),
                            "need_h": _("Need Hours"),
                            "staff_h": _("Staff Hours"),
                            "lack_h": _("Shortage Hours"),
                            "excess_h": _("Excess Hours"),
                            "estimated_excess_cost": _("Excess Cost Est.(Â¥)"),
                            "estimated_lack_cost_if_temporary_staff": _(
                                "Lack Cost if Temp(Â¥)"
                            ),
                            "estimated_lack_penalty_cost": _("Lack Penalty Est.(Â¥)"),
                            "working_days_considered": _("Working Days"),
                            "note": _("Note"),
                        }
                    )
                    st.dataframe(
                        display_emp_df, use_container_width=True, hide_index=True
                    )
                    if "employment" in df_s_emp and "lack_h" in df_s_emp:
                        fig_emp = px.bar(
                            df_s_emp,
                            x="employment",
                            y="lack_h",
                            labels={
                                "employment": _("Employment"),
                                "lack_h": _("Shortage Hours"),
                            },
                            color_discrete_sequence=["#2ca02c"],
                        )
                        st.plotly_chart(
                            fig_emp, use_container_width=True, key="short_emp_chart"
                        )
                    if "employment" in df_s_emp and "excess_h" in df_s_emp:
                        fig_emp_ex = px.bar(
                            df_s_emp,
                            x="employment",
                            y="excess_h",
                            labels={
                                "employment": _("Employment"),
                                "excess_h": _("Excess Hours"),
                            },
                            color_discrete_sequence=["#00BFFF"],
                        )
                        st.plotly_chart(
                            fig_emp_ex, use_container_width=True, key="excess_emp_chart"
                        )
                    cost_cols_emp = [
                        "estimated_excess_cost",
                        "estimated_lack_cost_if_temporary_staff",
                        "estimated_lack_penalty_cost",
                    ]
                    if {"employment"}.issubset(df_s_emp.columns) and any(
                        c in df_s_emp.columns for c in cost_cols_emp
                    ):
                        emp_cost_df = df_s_emp[
                            ["employment"]
                            + [c for c in cost_cols_emp if c in df_s_emp.columns]
                        ]
                        emp_cost_long = emp_cost_df.melt(
                            id_vars="employment", var_name="type", value_name="cost"
                        )
                        fig_emp_cost = px.bar(
                            emp_cost_long,
                            x="employment",
                            y="cost",
                            color="type",
                            barmode="group",
                            labels={
                                "employment": _("Employment"),
                                "cost": _("Cost (Â¥)"),
                                "type": _("Type"),
                            },
                        )
                        st.plotly_chart(
                            fig_emp_cost,
                            use_container_width=True,
                            key="cost_emp_chart",
                        )
                emp_month_fp = data_dir / "shortage_employment_monthly.parquet"
                if emp_month_fp.exists():
                    df_emp_month = load_data_cached(
                        str(emp_month_fp),
                        is_parquet=True,
                        file_mtime=_file_mtime(emp_month_fp),
                    )
                    if _valid_df(df_emp_month) and {
                        "month",
                        "employment",
                        "lack_h",
                    }.issubset(df_emp_month.columns):
                        fig_emp_m = px.bar(
                            df_emp_month,
                            x="month",
                            y="lack_h",
                            color="employment",
                            barmode="stack",
                            title=_("Monthly Shortage Hours by Employment"),
                            labels={
                                "month": _("Month"),
                                "lack_h": _("Shortage Hours"),
                                "employment": _("Employment"),
                            },
                        )
                        st.plotly_chart(
                            fig_emp_m,
                            use_container_width=True,
                            key="short_emp_month_chart",
                        )
                        with st.expander(_("Monthly shortage data")):
                            st.dataframe(
                                df_emp_month, use_container_width=True, hide_index=True
                            )
            except Exception as e:
                log_and_display_error(
                    "shortage_employment_summary.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e
                )
        else:
            st.info(
                _("Shortage")
                + " (shortage_employment_summary.parquet) "
                + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )
        st.markdown("---")
        fp_s_time = data_dir / "shortage_time.parquet"
        if fp_s_time.exists():
            try:
                df_s_time = load_data_cached(
                    str(fp_s_time),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_s_time),
                )
                if not _valid_df(df_s_time):
                    st.info("Data not available")
                    return
                st.write(_("Shortage by Time (count per day)"))
                avail_dates = df_s_time.columns.tolist()
                if avail_dates:
                    sel_date = st.selectbox(
                        _("Select date to display"),
                        avail_dates,
                        key="dash_short_time_date",
                    )
                    if sel_date:
                        fig_time = px.bar(
                            df_s_time[sel_date].reset_index(),
                            x=df_s_time.index.name or "index",
                            y=sel_date,
                            labels={
                                df_s_time.index.name or "index": _("Time"),
                                sel_date: _("Shortage Hours"),
                            },
                            color_discrete_sequence=["#FFA500"],
                            title=f"{sel_date} ã®æ™‚é–“å¸¯åˆ¥ä¸è¶³æ™‚é–“",
                        )
                        st.plotly_chart(
                            fig_time, use_container_width=True, key="short_time_chart"
                        )
                        st.caption(_("Shortage by time caption"))
                else:
                    st.info(_("No date columns in shortage data."))
                with st.expander(_("Display all time-slot shortage data")):
                    st.dataframe(df_s_time, use_container_width=True)
            except Exception as e:
                log_and_display_error("shortage_time.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Shortage") + " (shortage_time.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        fp_e_time = data_dir / "excess_time.parquet"
        if fp_e_time.exists():
            try:
                df_e_time = load_data_cached(
                    str(fp_e_time),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_e_time),
                )
                if not _valid_df(df_e_time):
                    st.info("Data not available")
                    return
                st.write(_("Excess by Time (count per day)"))
                avail_e_dates = df_e_time.columns.tolist()
                if avail_e_dates:
                    sel_e_date = st.selectbox(
                        _("Select date to display"),
                        avail_e_dates,
                        key="excess_time_date",
                    )
                    if sel_e_date:
                        fig_e_time = px.bar(
                            df_e_time[sel_e_date].reset_index(),
                            x=df_e_time.index.name or "index",
                            y=sel_e_date,
                            labels={
                                df_e_time.index.name or "index": _("Time"),
                                sel_e_date: _("Excess Hours"),
                            },
                            color_discrete_sequence=["#00BFFF"],
                            title=f"{sel_e_date} ã®æ™‚é–“å¸¯åˆ¥éå‰°æ™‚é–“",
                        )
                        st.plotly_chart(
                            fig_e_time,
                            use_container_width=True,
                            key="excess_time_chart",
                        )
                else:
                    st.info(_("No date columns in excess data."))
                with st.expander(_("Display all time-slot excess data")):
                    st.dataframe(df_e_time, use_container_width=True)
            except Exception as e:
                log_and_display_error("excess_time.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Excess by Time (count per day)")
                + " (excess_time.parquet) "
                + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        st.markdown("##### ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
        st.info(
            """
            ã“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ã€å„æ™‚é–“å¸¯ã§**å¿…è¦äººæ•°ã«å¯¾ã—ã¦ã©ã‚Œãã‚‰ã„ã®å‰²åˆã§äººå“¡ãŒä¸è¶³ã—ã¦ã„ãŸã‹**ã‚’ç¤ºã—ã¾ã™ã€‚
            - **è‰²ãŒæ¿ƒã„ï¼ˆèµ¤ã«è¿‘ã„ï¼‰**: ä¸è¶³ã®å‰²åˆãŒé«˜ãã€äººå“¡é…ç½®ãŒç‰¹ã«æ‰‹è–„ã ã£ãŸæ™‚é–“å¸¯ã§ã™ã€‚
            - **è‰²ãŒè–„ã„ï¼ˆç™½ã«è¿‘ã„ï¼‰**: ä¸è¶³ãŒãªã‹ã£ãŸã€ã¾ãŸã¯å°‘ãªã‹ã£ãŸæ™‚é–“å¸¯ã§ã™ã€‚
            """
        )

        roles = st.session_state.get("available_roles", [])
        employments = st.session_state.get("available_employments", [])
        scope_opts_shortage = {"overall": _("Overall")}
        if roles:
            scope_opts_shortage["role"] = _("Role")
        if employments:
            scope_opts_shortage["employment"] = _("Employment")

        c1_short, c2_short, c3_short = st.columns(3)
        with c1_short:
            scope_lbl_s = st.selectbox(
                "è¡¨ç¤ºç¯„å›²",
                list(scope_opts_shortage.values()),
                key="shortage_heat_scope",
            )
            scope_s = [k for k, v in scope_opts_shortage.items() if v == scope_lbl_s][0]

        sel_item_s = None
        with c2_short:
            if scope_s == "role":
                sel_item_s = st.selectbox(
                    _("Role"), roles, key="shortage_heat_scope_role"
                )
            elif scope_s == "employment":
                sel_item_s = st.selectbox(
                    _("Employment"), employments, key="shortage_heat_scope_emp"
                )

        df_ratio = pd.DataFrame()
        ratio_key = None
        if scope_s == "overall":
            ratio_key = "ratio_all"
        elif scope_s == "role" and sel_item_s:
            ratio_key = f"ratio_role_{safe_sheet(sel_item_s, for_path=True)}"
        elif scope_s == "employment" and sel_item_s:
            ratio_key = f"ratio_emp_{safe_sheet(sel_item_s, for_path=True)}"

        if ratio_key:
            df_ratio = st.session_state.display_data.get(ratio_key, pd.DataFrame())

        if _valid_df(df_ratio):
            fig_ratio_heat = dashboard.shortage_heatmap(df_ratio)
            fig_ratio_heat.update_layout(
                title=f"ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— ({scope_lbl_s}{': ' + sel_item_s if sel_item_s else ''})"
            )
            st.plotly_chart(
                fig_ratio_heat,
                use_container_width=True,
                key="shortage_tab_ratio_heatmap_dynamic",
            )
        else:
            st.info("é¸æŠã•ã‚ŒãŸã‚¹ã‚³ãƒ¼ãƒ—ã®ä¸è¶³ç‡ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚")

        fp_e_ratio = data_dir / "excess_ratio.parquet"
        if fp_e_ratio.exists():
            try:
                df_e_ratio = load_data_cached(
                    str(fp_e_ratio),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_e_ratio),
                )
                if not _valid_df(df_e_ratio):
                    st.info("Data not available")
                    return
                st.write(_("Excess Ratio by Time"))
                avail_er_dates = df_e_ratio.columns.tolist()
                if avail_er_dates:
                    sel_er_date = st.selectbox(
                        _("Select date for ratio"),
                        avail_er_dates,
                        key="excess_ratio_date",
                    )
                    if sel_er_date:
                        fig_er = px.bar(
                            df_e_ratio[sel_er_date].reset_index(),
                            x=df_e_ratio.index.name or "index",
                            y=sel_er_date,
                            labels={
                                df_e_ratio.index.name or "index": _("Time"),
                                sel_er_date: _("Excess Hours"),
                            },
                            color_discrete_sequence=["#00BFFF"],
                            title=f"{sel_er_date} ã®æ™‚é–“å¸¯åˆ¥éå‰°ç‡",
                        )
                        st.plotly_chart(
                            fig_er, use_container_width=True, key="excess_ratio_chart"
                        )
                else:
                    st.info(_("No date columns in excess data."))
                with st.expander(_("Display all ratio data")):
                    st.dataframe(df_e_ratio, use_container_width=True)
            except Exception as e:
                log_and_display_error("excess_ratio.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Excess Ratio by Time")
                + " (excess_ratio.parquet) "
                + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        fp_s_freq = data_dir / "shortage_freq.parquet"
        if fp_s_freq.exists():
            try:
                df_freq = load_data_cached(
                    str(fp_s_freq),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_s_freq),
                )
                if not _valid_df(df_freq):
                    st.info("Data not available")
                    return
                st.write(_("Shortage Frequency (days)"))
                fig_freq = px.bar(
                    df_freq.reset_index(),
                    x=df_freq.index.name or "index",
                    y="shortage_days",
                    labels={
                        df_freq.index.name or "index": _("Time"),
                        "shortage_days": _("Shortage Frequency (days)"),
                    },
                    color_discrete_sequence=["#708090"],
                    title="æ™‚é–“å¸¯åˆ¥ä¸è¶³æ—¥æ•°",
                )
                st.plotly_chart(
                    fig_freq, use_container_width=True, key="short_freq_chart"
                )
                with st.expander(_("Data")):
                    st.dataframe(df_freq, use_container_width=True)
            except Exception as e:
                log_and_display_error("shortage_freq.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Shortage") + " (shortage_freq.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        fp_e_freq = data_dir / "excess_freq.parquet"
        if fp_e_freq.exists():
            try:
                df_e_freq = load_data_cached(
                    str(fp_e_freq),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_e_freq),
                )
                if not _valid_df(df_e_freq):
                    st.info("Data not available")
                    return
                st.write(_("Excess Frequency (days)"))
                fig_efreq = px.bar(
                    df_e_freq.reset_index(),
                    x=df_e_freq.index.name or "index",
                    y="excess_days",
                    labels={
                        df_e_freq.index.name or "index": _("Time"),
                        "excess_days": _("Excess Frequency (days)"),
                    },
                    color_discrete_sequence=["#00BFFF"],
                    title="æ™‚é–“å¸¯åˆ¥éå‰°æ—¥æ•°",
                )
                st.plotly_chart(
                    fig_efreq, use_container_width=True, key="excess_freq_chart"
                )
                with st.expander(_("Data")):
                    st.dataframe(df_e_freq, use_container_width=True)
            except Exception as e:
                log_and_display_error("excess_freq.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Excess Frequency (days)")
                + " (excess_freq.parquet) "
                + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        fp_s_leave = data_dir / "shortage_leave.parquet"
        if fp_s_leave.exists():
            try:
                df_sl = load_data_cached(
                    str(fp_s_leave),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_s_leave),
                )
                if not _valid_df(df_sl):
                    st.info("Data not available")
                    return
                st.write(_("Shortage with Leave"))
                display_sl = df_sl.rename(
                    columns={
                        "time": _("Time"),
                        "date": _("Date"),
                        "lack": _("Shortage Hours"),
                        "leave_applicants": _("Total Leave Days"),
                        "net_shortage": _("Net Shortage"),
                    }
                )
                st.dataframe(display_sl, use_container_width=True, hide_index=True)
            except Exception as e:
                log_and_display_error("shortage_leave.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Shortage") + " (shortage_leave.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        fp_cost = data_dir / "cost_benefit.parquet"
        if fp_cost.exists():
            st.markdown("---")
            try:
                df_cost = load_data_cached(
                    str(fp_cost),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_cost),
                )
                if not _valid_df(df_cost):
                    st.info("Data not available")
                    return
                st.write(_("Estimated Cost Impact (Million Â¥)"))
                if "Cost_Million" in df_cost:
                    fig_cost = px.bar(
                        df_cost.reset_index(),
                        x=df_cost.index.name or "index",
                        y="Cost_Million",
                        labels={"Cost_Million": _("Estimated Cost Impact (Million Â¥)")},
                        title="æ¨å®šã‚³ã‚¹ãƒˆå½±éŸ¿",
                    )
                    st.plotly_chart(
                        fig_cost, use_container_width=True, key="short_cost_chart"
                    )
                st.dataframe(df_cost, use_container_width=True)
            except Exception as e:
                log_and_display_error("cost_benefit.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)

        fp_stats = data_dir / "stats_alerts.parquet"
        if fp_stats.exists():
            try:
                df_alerts = load_data_cached(
                    str(fp_stats),
                    is_parquet=True,
                    file_mtime=_file_mtime(fp_stats),
                )
                if not _valid_df(df_alerts):
                    st.info("Data not available")
                    return
                if not df_alerts.empty:
                    st.markdown("---")
                    st.subheader(_("Alerts"))
                    st.dataframe(df_alerts, use_container_width=True, hide_index=True)
            except Exception as e:
                log_and_display_error("stats_alerts.parquet alertsè¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)

        display_shortage_factor_section(data_dir)


def display_shortage_factor_section(data_dir: Path) -> None:
    """Train and display shortage factor model."""
    st.markdown("---")
    st.subheader(_("Factor Analysis (AI)"))

    train_key = "train_factor_model_button"
    if st.button(_("Train factor model"), key=train_key, use_container_width=True):
        try:
            heat_df = load_data_cached(
                str(data_dir / "heat_ALL.parquet"),
                is_parquet=True,
                file_mtime=_file_mtime(data_dir / "heat_ALL.parquet"),
            )
            short_df = load_data_cached(
                str(data_dir / "shortage_time.parquet"),
                is_parquet=True,
                file_mtime=_file_mtime(data_dir / "shortage_time.parquet"),
            )
            leave_fp = data_dir / "leave_analysis.csv"
            leave_df = (
                pd.read_csv(leave_fp, parse_dates=["date"])
                if leave_fp.exists()
                else pd.DataFrame()
            )
            analyzer = ShortageFactorAnalyzer()
            features = analyzer.generate_features(
                pd.DataFrame(), heat_df, short_df, leave_df, set()
            )
            model, fi_df = analyzer.train_and_get_feature_importance(features)
            st.session_state.factor_features = features
            st.session_state.factor_model = model
            st.session_state.factor_importance_df = fi_df
            st.success("Model trained")
        except Exception as e:
            log_and_display_error("factor model training error", e)

    fi_df = st.session_state.get("factor_importance_df")
    feat_df = st.session_state.get("factor_features")
    if fi_df is not None and feat_df is not None and not feat_df.empty:
        dates = sorted({idx[0] for idx in feat_df.index})
        slots = sorted({idx[1] for idx in feat_df.index})
        sel_date = st.selectbox(
            _("Select date for factor analysis"),
            dates,
            key="factor_date_select",
        )
        sel_slot = st.selectbox(
            _("Select time slot for factor analysis"),
            slots,
            key="factor_slot_select",
        )
        if (sel_date, sel_slot) in feat_df.index:
            row = feat_df.loc[(sel_date, sel_slot)]
            top = fi_df.head(5)
            st.write(_("Top factors"))
            st.dataframe(top, hide_index=True)
            with st.expander("Feature values"):
                st.dataframe(row[top["feature"].tolist()].to_frame("value"))


def display_over_shortage_log_section(data_dir: Path) -> None:
    """Display editable over/shortage log."""
    st.markdown("---")
    st.subheader(_("Over/Short Log"))

    events = over_shortage_log.list_events(data_dir)
    if events.empty:
        st.info("No shortage/excess data.")
        return

    staff_options: list[str] = []
    fp_staff = data_dir / "staff_stats.parquet"
    if fp_staff.exists():
        try:
            staff_df = pd.read_parquet(fp_staff)
            if "staff" in staff_df.columns:
                staff_options = staff_df["staff"].astype(str).dropna().unique().tolist()
        except Exception:
            staff_options = []

    log_fp = data_dir / "over_shortage_log.csv"
    existing = over_shortage_log.load_log(log_fp)
    merged = events.merge(
        existing,
        on=["date", "time", "type"],
        how="left",
        suffixes=("", "_log"),
    )

    updated_rows = []
    reason_opts = [
        _("Sudden absence"),
        _("Planned leave"),
        _("Training/Meeting"),
        _("Resident response"),
        _("Hiring delay"),
        _("Other"),
    ]

    for idx, row in merged.iterrows():
        st.write(f"{row['date']} {row['time']} [{row['type']}] ({row['count']})")
        reason = st.selectbox(
            _("Reason Category"),
            reason_opts,
            index=reason_opts.index(row["reason"])
            if pd.notna(row.get("reason")) and row["reason"] in reason_opts
            else 0,
            key=f"reason_{idx}",
        )
        staff_sel = st.multiselect(
            _("Related Staff"),
            staff_options,
            default=str(row.get("staff", "")).split(";")
            if pd.notna(row.get("staff")) and str(row.get("staff"))
            else [],
            key=f"staff_{idx}",
        )
        memo = st.text_area(
            _("Memo"),
            value=str(row.get("memo", "")),
            key=f"memo_{idx}",
        )
        updated_rows.append(
            {
                "date": row["date"],
                "time": row["time"],
                "type": row["type"],
                "count": row["count"],
                "reason": reason,
                "staff": ";".join(staff_sel),
                "memo": memo,
            }
        )

    mode = st.radio(_("Save method"), [_("Append"), _("Overwrite")], horizontal=True)
    if st.button(_("Save log")):
        df_save = pd.DataFrame(updated_rows)
        over_shortage_log.save_log(
            df_save,
            log_fp,
            mode="append" if mode == _("Append") else "overwrite",
        )
        st.success(_("Save log"))

    if not existing.empty:
        summary = existing.groupby("reason")["count"].sum().reset_index()
        st.subheader(_("Reason stats"))
        fig = px.bar(
            summary,
            x="reason",
            y="count",
            labels={"reason": _("Reason Category"), "count": _("Count")},
            title="ä¸è¶³ç†ç”±åˆ¥ä»¶æ•°",
        )
        st.plotly_chart(fig, use_container_width=True)


def display_optimization_tab(tab_container, data_dir):
    """Display staffing optimization metrics."""
    with tab_container:
        st.subheader(_("Optimization Analysis"))

        roles = st.session_state.get("available_roles", [])
        employments = st.session_state.get("available_employments", [])

        # --- UI Controls at the top ---
        c1, c2, c3 = st.columns(3)
        with c1:
            scope_opts = {
                "overall": _("Overall"),
                "role": _("Role"),
                "employment": _("Employment"),
            }
            scope_lbl = st.selectbox(
                "ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®å¯¾è±¡ç¯„å›²",
                list(scope_opts.values()),
                key="opt_scope",
            )
            scope = [k for k, v in scope_opts.items() if v == scope_lbl][0]

        sel_role = None
        with c2:
            if scope == "role" and roles:
                sel_role = st.selectbox(_("Role"), roles, key="opt_scope_role")

        sel_emp = None
        with c3:
            if scope == "employment" and employments:
                sel_emp = st.selectbox(
                    _("Employment"), employments, key="opt_scope_emp"
                )

        # --- Display data retrieval ---
        base_key = None
        if scope == "overall":
            base_key = "heat_all"
        elif scope == "role" and sel_role:
            base_key = f"heat_role_{safe_sheet(sel_role, for_path=True)}"
        elif scope == "employment" and sel_emp:
            base_key = f"heat_emp_{safe_sheet(sel_emp, for_path=True)}"

        df_surplus = pd.DataFrame()
        df_margin = pd.DataFrame()
        df_score = pd.DataFrame()

        if base_key:
            df_surplus = st.session_state.display_data.get(base_key.replace("heat_", "surplus_"))
            df_margin = st.session_state.display_data.get(base_key.replace("heat_", "margin_"))
            df_score = st.session_state.display_data.get(base_key.replace("heat_", "score_"))

        if not (_valid_df(df_surplus) and _valid_df(df_margin) and _valid_df(df_score)):
            st.warning("é¸æŠã•ã‚ŒãŸã‚¹ã‚³ãƒ¼ãƒ—ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        st.divider()

        # --- Display Heatmaps Vertically ---

        st.markdown("##### 1. å¿…è¦äººæ•°ã«å¯¾ã™ã‚‹ä½™å‰° (Surplus vs Need)")
        st.info(
            """
            ã“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ã€å„æ™‚é–“å¸¯ã§**å¿…è¦äººæ•°ï¼ˆneedï¼‰ã«å¯¾ã—ã¦ä½•äººå¤šãã‚¹ã‚¿ãƒƒãƒ•ãŒã„ãŸã‹**ã‚’ç¤ºã—ã¾ã™ã€‚
            - **å€¤ãŒé«˜ã„ï¼ˆè‰²ãŒæ¿ƒã„ï¼‰**: å¿…è¦äººæ•°ã‚’å¤§å¹…ã«è¶…ãˆã‚‹äººå“¡ãŒé…ç½®ã•ã‚Œã¦ãŠã‚Šã€éå‰°äººå“¡ï¼ˆã‚³ã‚¹ãƒˆå¢—ï¼‰ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
            - **å€¤ãŒ0**: å¿…è¦äººæ•°ã¡ã‚‡ã†ã©ã‹ã€ãã‚Œä»¥ä¸‹ã®äººå“¡ã—ã‹é…ç½®ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚
            """
        )
        fig_surplus = px.imshow(
            df_surplus,
            aspect="auto",
            color_continuous_scale="Blues",
            labels={"x": _("Date"), "y": _("Time"), "color": _("Surplus vs Need")},
            x=[date_with_weekday(c) for c in df_surplus.columns],
            title="å¿…è¦äººæ•°ã«å¯¾ã™ã‚‹ä½™å‰°äººå“¡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
        )
        st.plotly_chart(fig_surplus, use_container_width=True, key="surplus_need_heat")

        st.markdown("##### 2. ä¸Šé™ã«å¯¾ã™ã‚‹ä½™ç™½ (Margin to Upper)")
        st.info(
            """
            ã“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ã€å„æ™‚é–“å¸¯ã§**é…ç½®äººæ•°ã®ä¸Šé™ï¼ˆupperï¼‰ã¾ã§ã‚ã¨ä½•äººã®ä½™è£•ãŒã‚ã£ãŸã‹**ã‚’ç¤ºã—ã¾ã™ã€‚
            - **å€¤ãŒé«˜ã„ï¼ˆè‰²ãŒæ¿ƒã„ï¼‰**: ä¸Šé™ã¾ã§ã¾ã ä½™è£•ãŒã‚ã‚Šã€è¿½åŠ ã®äººå“¡ã‚’å—ã‘å…¥ã‚Œã‚‰ã‚Œã‚‹ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãŒã‚ã£ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
            - **å€¤ãŒ0ã«è¿‘ã„**: ä¸Šé™ã‚®ãƒªã‚®ãƒªã§ç¨¼åƒã—ã¦ãŠã‚Šã€çªç™ºçš„ãªäº‹æ…‹ã«å¯¾å¿œã™ã‚‹ä½™è£•ãŒå°‘ãªã‹ã£ãŸã“ã¨ã‚’ç¤ºå”†ã—ã¾ã™ã€‚
            """
        )
        fig_margin = px.imshow(
            df_margin,
            aspect="auto",
            color_continuous_scale="Greens",
            labels={"x": _("Date"), "y": _("Time"), "color": _("Margin vs Upper")},
            x=[date_with_weekday(c) for c in df_margin.columns],
            title="ä¸Šé™äººæ•°ã¾ã§ã®ä½™ç™½ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
        )
        st.plotly_chart(fig_margin, use_container_width=True, key="margin_upper_heat")
        st.info(
            "æ³¨: ã“ã®ä½™ç™½ã¯ã€éå»ã®å®Ÿç¸¾ã‹ã‚‰ç®—å‡ºã•ã‚ŒãŸä¸Šé™äººæ•°ã¨å®Ÿéš›ã®é…ç½®äººæ•°ã®å·®ã‚’ç¤ºã—ã¾ã™ã€‚"
            "éœ€è¦ãŒä½ã„æ—¥ã‚„ä¼‘æ¥­æ—¥ï¼ˆä¾‹: æ—¥æ›œæ—¥ï¼‰ã¯ã€éå»ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãä¸Šé™å€¤ãŒé«˜ã‚ã«ç®—å‡ºã•ã‚Œã‚‹ã“ã¨ã§ã€"
            "è¦‹ã‹ã‘ä¸Šã®ä½™ç™½ãŒå¤§ãããªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€æ½œåœ¨çš„ãªéå‰°äººå“¡ã‚„ã‚³ã‚¹ãƒˆç™ºç”Ÿã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚"
        )

        st.markdown("##### 3. äººå“¡é…ç½® æœ€é©åŒ–ã‚¹ã‚³ã‚¢")
        st.info(
            """
            ã“ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¯ã€**äººå“¡é…ç½®ã®åŠ¹ç‡æ€§**ã‚’0ã‹ã‚‰1ã®ã‚¹ã‚³ã‚¢ã§ç¤ºã—ã¾ã™ï¼ˆ1ãŒæœ€ã‚‚è‰¯ã„ï¼‰ã€‚
            - **ã‚¹ã‚³ã‚¢ãŒé«˜ã„ï¼ˆç·‘è‰²ã«è¿‘ã„ï¼‰**: å¿…è¦äººæ•°ï¼ˆneedï¼‰ã‚’æº€ãŸã—ã¤ã¤ã€ä¸Šé™ï¼ˆupperï¼‰ã‚’è¶…ãˆãªã„ã€åŠ¹ç‡çš„ãªäººå“¡é…ç½®ãŒã§ãã¦ã„ã¾ã™ã€‚
            - **ã‚¹ã‚³ã‚¢ãŒä½ã„ï¼ˆèµ¤è‰²ã«è¿‘ã„ï¼‰**: äººå“¡ä¸è¶³ã€ã¾ãŸã¯éå‰°äººå“¡ãŒç™ºç”Ÿã—ã¦ãŠã‚Šã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
            """
        )
        fig_score = px.imshow(
            df_score,
            aspect="auto",
            color_continuous_scale="RdYlGn",
            zmin=0,
            zmax=1,
            labels={"x": _("Date"), "y": _("Time"), "color": _("Optimization Score")},
            x=[date_with_weekday(c) for c in df_score.columns],
            title="æœ€é©åŒ–ã‚¹ã‚³ã‚¢ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
        )
        st.plotly_chart(fig_score, use_container_width=True, key="optimization_heat")


def display_fatigue_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("Fatigue Score per Staff"))
        display_data = st.session_state.get("display_data", {})
        df = display_data.get("fatigue_score")
        if isinstance(df, pd.DataFrame):
            if not _valid_df(df):
                st.info(_("Data not available or empty"))
                return

            try:
                display_df = df.rename(
                    columns={"staff": _("Staff"), "fatigue_score": _("Score")}
                )
                st.dataframe(display_df, use_container_width=True, hide_index=True)
                if "fatigue_score" in df and "staff" in df:
                    fig_fatigue = px.bar(
                        df,
                        x="staff",
                        y="fatigue_score",
                        labels={"staff": _("Staff"), "fatigue_score": _("Score")},
                        color_discrete_sequence=["#FF8C00"],
                        title="ã‚¹ã‚¿ãƒƒãƒ•åˆ¥ç–²åŠ´ã‚¹ã‚³ã‚¢",
                    )
                    st.plotly_chart(
                        fig_fatigue, use_container_width=True, key="fatigue_chart"
                    )
                    fig_fatigue_hist = dashboard.fatigue_distribution(df)
                    fig_fatigue_hist.update_layout(title="ç–²åŠ´ã‚¹ã‚³ã‚¢åˆ†å¸ƒ")
                    st.plotly_chart(
                        fig_fatigue_hist,
                        use_container_width=True,
                        key="fatigue_hist",
                    )
            except AttributeError as e:
                log_and_display_error("Invalid data format in fatigue_score.parquet", e)
            except Exception as e:
                log_and_display_error("fatigue_score.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Fatigue") + " (fatigue_score.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )


def display_forecast_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("Demand Forecast (yhat)"))
        display_data = st.session_state.get("display_data", {})
        df_fc = display_data.get("forecast")
        df_actual = display_data.get("demand_series")
        if isinstance(df_fc, pd.DataFrame):
            if not _valid_df(df_fc):
                st.info(_("Forecast data not available or empty"))
                return

            try:
                fig = go.Figure()
                if "ds" in df_fc and "yhat" in df_fc:
                    fig.add_trace(
                        go.Scatter(
                            x=df_fc["ds"],
                            y=df_fc["yhat"],
                            mode="lines+markers",
                            name=_("Demand Forecast (yhat)"),
                        )
                    )
                if isinstance(df_actual, pd.DataFrame):
                    if _valid_df(df_actual) and "ds" in df_actual and "y" in df_actual:
                        fig.add_trace(
                            go.Scatter(
                                x=df_actual["ds"],
                                y=df_actual["y"],
                                mode="lines",
                                name=_("Actual (y)"),
                                line=dict(dash="dash"),
                            )
                        )
                fig.update_layout(
                    title=_("Demand Forecast vs Actual"),
                    xaxis_title=_("Date"),
                    yaxis_title=_("Demand"),
                )
                st.plotly_chart(fig, use_container_width=True, key="forecast_chart")
                with st.expander(_("Display forecast data")):
                    st.dataframe(df_fc, use_container_width=True, hide_index=True)
            except AttributeError as e:
                log_and_display_error("Invalid data format in forecast.parquet", e)
            except Exception as e:
                log_and_display_error("forecast.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(_("Forecast") + " (forecast.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"))


def display_fairness_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("Fairness (Unfairness Score)"))
        display_data = st.session_state.get("display_data", {})
        df = display_data.get("fairness_after")
        if isinstance(df, pd.DataFrame):
            if not _valid_df(df):
                st.info(_("Fairness data not available or empty"))
                return

            try:
                rename_map = {
                    "staff": _("Staff"),
                    "night_ratio": _("Night Shift Ratio"),
                }
                if "unfairness_score" in df.columns:
                    rename_map["unfairness_score"] = _("Unfairness Score")
                if "fairness_score" in df.columns:
                    rename_map["fairness_score"] = _("Fairness Score")
                display_df = df.rename(columns=rename_map)
                st.dataframe(display_df, use_container_width=True, hide_index=True)
                metric_col = (
                    "unfairness_score"
                    if "unfairness_score" in df.columns
                    else (
                        "fairness_score" if "fairness_score" in df.columns else "night_ratio"
                    )
                )
                if "staff" in df and metric_col in df:
                    fig_fair = px.bar(
                        df,
                        x="staff",
                        y=metric_col,
                        labels={
                            "staff": _("Staff"),
                            metric_col: _("Unfairness Score")
                            if metric_col == "unfairness_score"
                            else _("Fairness Score")
                            if metric_col == "fairness_score"
                            else _("Night Shift Ratio"),
                        },
                        color_discrete_sequence=["#FF8C00"],
                    )
                    avg_val = df[metric_col].mean()
                    fig_fair.add_hline(y=avg_val, line_dash="dash", line_color="red")
                    st.plotly_chart(
                        fig_fair, use_container_width=True, key="fairness_chart"
                    )
                    fig_hist = dashboard.fairness_histogram(df, metric=metric_col)
                    fig_hist.add_vline(x=avg_val, line_dash="dash", line_color="red")
                    st.plotly_chart(
                        fig_hist,
                        use_container_width=True,
                        key="fairness_hist",
                    )
                if "unfairness_score" in df.columns:
                    ranking = df.sort_values("unfairness_score", ascending=False)[[
                        "staff",
                        "unfairness_score",
                    ]]
                    ranking.index += 1
                    st.subheader(_("Unfairness Ranking"))
                    st.dataframe(ranking, use_container_width=True)
            except AttributeError as e:
                log_and_display_error(
                    "Invalid data format in fairness_after.parquet", e
                )
            except Exception as e:
                log_and_display_error("fairness_after.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info(
                _("Fairness") + " (fairness_after.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )


def display_constraint_discovery_tab(tab_container, data_dir):
    """12è»¸è¶…é«˜æ¬¡å…ƒåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ã‚¿ãƒ–"""
    with tab_container:
        st.header("ğŸ” 12è»¸è¶…é«˜æ¬¡å…ƒåˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ")
        st.markdown("""
        **ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ„å›³ã‚ã¶ã‚Šå‡ºã—ã‚·ã‚¹ãƒ†ãƒ **
        
        ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯12å€‹ã®åˆ†æè»¸ã‚’ä½¿ç”¨ã—ã¦ã€ã‚·ãƒ•ãƒˆä½œæˆè€…ãŒç„¡æ„è­˜ã«å¾“ã£ã¦ã„ã‚‹åˆ¶ç´„ã‚„ãƒ«ãƒ¼ãƒ«ã‚’è‡ªå‹•ç™ºè¦‹ã—ã¾ã™ã€‚
        
        **12ã®åˆ†æè»¸:**
        - ğŸ‘¥ ã‚¹ã‚¿ãƒƒãƒ•è»¸: å€‹äººç‰¹æ€§ãƒ»èƒ½åŠ›ãƒ»åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³
        - â° æ™‚é–“è»¸: æ™‚ç³»åˆ—ãƒ»å‘¨æœŸæ€§ãƒ»å‹•çš„å¤‰åŒ–
        - ğŸ’¼ ã‚¿ã‚¹ã‚¯è»¸: æ¥­å‹™ç‰¹æ€§ãƒ»è¤‡é›‘åº¦ãƒ»ç›¸äº’ä¾å­˜
        - ğŸ¤ é–¢ä¿‚è»¸: äººé–“é–¢ä¿‚ãƒ»å”åŠ›ãƒ»çµ„ç¹”åŠ›å­¦
        - ğŸ“ ç©ºé–“è»¸: å ´æ‰€ãƒ»ã‚¨ãƒªã‚¢ãƒ»ç‰©ç†é…ç½®
        - ğŸ”‘ æ¨©é™è»¸: æ¨©é™ãƒ»è²¬ä»»ãƒ»éšå±¤æ§‹é€ 
        - ğŸ“ çµŒé¨“è»¸: ç¿’ç†Ÿåº¦ãƒ»å­¦ç¿’ãƒ»æˆé•·
        - âš™ï¸ è² è·è»¸: æ¥­å‹™è² è·ãƒ»ç–²åŠ´ãƒ»ã‚¹ãƒˆãƒ¬ã‚¹
        - âœ¨ å“è³ªè»¸: å“è³ªè¦æ±‚ãƒ»æ¨™æº–ãƒ»è©•ä¾¡
        - ğŸ’° ã‚³ã‚¹ãƒˆè»¸: è²»ç”¨ãƒ»åŠ¹ç‡ãƒ»ãƒªã‚½ãƒ¼ã‚¹
        - âš ï¸ ãƒªã‚¹ã‚¯è»¸: å®‰å…¨ãƒ»å±é™ºãƒ»äºˆé˜²
        - ğŸ¯ æˆ¦ç•¥è»¸: ç›®æ¨™ãƒ»æ–¹é‡ãƒ»æ„å›³
        """)
        
        # å®Ÿè¡Œãƒœã‚¿ãƒ³
        if st.button("ğŸš€ åˆ¶ç´„ç™ºè¦‹åˆ†æã‚’å®Ÿè¡Œ", key="run_constraint_discovery"):
            if not _HAS_ULTRA_CONSTRAINT_SYSTEM:
                st.error("ğŸ˜¨ 12è»¸åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
                return
                
            # ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
            if st.session_state.get("long_df") is None or st.session_state.long_df.empty:
                st.warning("âš ï¸ å…ˆã«ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                return
                
            with st.spinner("ğŸ” 12è»¸åˆ†æã§åˆ¶ç´„ã‚’ç™ºè¦‹ä¸­..."):
                try:
                    # 12è»¸åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
                    discovery_system = UltraDimensionalConstraintDiscoverySystem()
                    
                    # åˆ¶ç´„ç™ºè¦‹å®Ÿè¡Œ
                    results = discovery_system.discover_constraints(
                        st.session_state.long_df,
                        "/tmp/test_data.xlsx"  # ä»®ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
                    )
                    
                    # çµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
                    st.session_state.constraint_discovery_results = results
                    
                    st.success(f"âœ… åˆ¶ç´„ç™ºè¦‹å®Œäº†ï¼ {len(results.get('discovered_constraints', []))}å€‹ã®åˆ¶ç´„ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚")
                    
                except Exception as e:
                    st.error(f"â— ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    import traceback
                    st.error(traceback.format_exc())
        
        # çµæœã®è¡¨ç¤º
        if "constraint_discovery_results" in st.session_state:
            display_constraint_discovery_results(st.session_state.constraint_discovery_results)
        
        # æ´å¯Ÿæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        st.markdown("---")
        st.subheader("ğŸ¯ é«˜åº¦ãªåˆ†ææ©Ÿèƒ½")
        
        # ã‚¿ãƒ–ã§åˆ†ææ©Ÿèƒ½ã‚’æ•´ç†
        analysis_tabs = st.tabs([
            "ğŸ” æ·±ã„æ´å¯Ÿ",
            "ğŸš¨ é€£ç¶šå‹¤å‹™ãƒã‚§ãƒƒã‚¯", 
            "ğŸ“Š é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬",
            "ğŸ§  ä½œæˆè€…ã®æ„å›³åˆ†æ"
        ])
        
        # æ·±ã„æ´å¯Ÿã‚¿ãƒ–
        with analysis_tabs[0]:
            col_insight1, col_insight2 = st.columns([3, 1])
            
            with col_insight1:
                enable_insights = st.checkbox(
                    "æ·±ã„æ´å¯Ÿã‚’è‡ªå‹•æ¤œå‡º",
                    value=True,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹
                    help="ã‚·ãƒ•ãƒˆã®å•é¡Œç‚¹ã‚„æ”¹å–„æ©Ÿä¼šã‚’è‡ªå‹•çš„ã«ç™ºè¦‹ã—ã¾ã™ï¼ˆZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«ã‚‚å«ã¾ã‚Œã¾ã™ï¼‰",
                    key="enable_insights"
                )
            
            if enable_insights:
                with col_insight2:
                    if st.button("æ´å¯Ÿã‚’æ¤œå‡º", key="detect_insights"):
                        run_insight_detection()
        
        # é€£ç¶šå‹¤å‹™ãƒã‚§ãƒƒã‚¯ã‚¿ãƒ–
        with analysis_tabs[1]:
            if st.checkbox("é€£ç¶šå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º", value=True, key="enable_continuous"):
                if st.button("æ¤œå‡ºå®Ÿè¡Œ", key="detect_continuous"):
                    run_continuous_shift_detection()
        
        # é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬ã‚¿ãƒ–
        with analysis_tabs[2]:
            if st.checkbox("ã‚¹ã‚¿ãƒƒãƒ•ã®é›¢è·ãƒªã‚¹ã‚¯ã‚’äºˆæ¸¬", value=True, key="enable_turnover"):
                if st.button("äºˆæ¸¬å®Ÿè¡Œ", key="predict_turnover"):
                    run_turnover_prediction()
        
        # ä½œæˆè€…ã®æ„å›³åˆ†æã‚¿ãƒ–
        with analysis_tabs[3]:
            if st.checkbox("ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ„å›³ã‚’åˆ†æ", value=False, key="enable_mind_reader"):
                if st.button("åˆ†æå®Ÿè¡Œ", key="read_mind"):
                    run_mind_reader_analysis()

def run_insight_detection():
    """æ´å¯Ÿæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè¡Œ"""
    # åˆ†æçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª
    possible_dirs = [
        Path("analysis_results_final_extracted/out_p25_based"),
        Path("extracted_results/out_p25_based"),
        Path("output/analysis_results"),
    ]
    
    analysis_dir = None
    for dir_path in possible_dirs:
        if dir_path.exists():
            analysis_dir = dir_path
            break
    
    if not analysis_dir:
        st.warning("âš ï¸ åˆ†æçµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return
    
    with st.spinner("ğŸ” æ·±ã„æ´å¯Ÿã‚’æ¤œå‡ºä¸­..."):
        try:
            from shift_suite.tasks.insight_detection_service import InsightDetectionService
            
            service = InsightDetectionService()
            report = service.analyze_directory(analysis_dir)
            
            if report.insights:
                st.success(f"âœ… {len(report.insights)}å€‹ã®é‡è¦ãªæ´å¯Ÿã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
                
                # æ´å¯Ÿã‚’é‡è¦åº¦é †ã«åˆ†é¡
                critical = [i for i in report.insights if i.severity == "critical"]
                high = [i for i in report.insights if i.severity == "high"]
                medium = [i for i in report.insights if i.severity == "medium"]
                
                # ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªå•é¡Œ
                if critical:
                    st.error("âš ï¸ **ç·Šæ€¥å¯¾å¿œãŒå¿…è¦ãªå•é¡Œ**")
                    for insight in critical:
                        with st.expander(f"ğŸš¨ {insight.title}", expanded=True):
                            st.write(insight.description)
                            if hasattr(insight, 'evidence') and insight.evidence:
                                st.write("**æ ¹æ‹ ãƒ‡ãƒ¼ã‚¿:**")
                                for key, value in insight.evidence.items():
                                    if isinstance(value, (int, float)):
                                        st.write(f"  â€¢ {key}: {value:.1f}")
                                    else:
                                        st.write(f"  â€¢ {key}: {value}")
                            st.write(f"ğŸ’° **è²¡å‹™å½±éŸ¿:** {insight.impact:.1f}ä¸‡å††/æœˆ")
                            st.write(f"ğŸ“ **æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:** {insight.recommendation}")
                
                # æ”¹å–„æ¨å¥¨äº‹é …
                if high:
                    st.warning("ğŸ“Š **æ”¹å–„æ¨å¥¨äº‹é …**")
                    for insight in high:
                        with st.expander(f"âš ï¸ {insight.title}"):
                            st.write(insight.description)
                            st.write(f"ğŸ’° **è²¡å‹™å½±éŸ¿:** {insight.impact:.1f}ä¸‡å††/æœˆ")
                            st.write(f"ğŸ“ **æ¨å¥¨:** {insight.recommendation}")
                
                # ãã®ä»–ã®æ´å¯Ÿ
                if medium:
                    st.info("ğŸ’¡ **ãã®ä»–ã®æ´å¯Ÿ**")
                    for insight in medium:
                        st.write(f"â€¢ {insight.title}: {insight.description}")
                
                # ç·è²¡å‹™å½±éŸ¿
                total_impact = sum(i.impact for i in report.insights)
                st.metric("ğŸ’° ç·è²¡å‹™å½±éŸ¿", f"{total_impact:.1f}ä¸‡å††/æœˆ")
                
            else:
                st.info("âœ¨ é‡å¤§ãªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except ImportError:
            st.error("æ´å¯Ÿæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.debug(f"Insight detection error: {e}")

def run_continuous_shift_detection():
    """é€£ç¶šå‹¤å‹™æ¤œå‡ºã‚’å®Ÿè¡Œ"""
    if st.session_state.get("long_df") is None or st.session_state.long_df.empty:
        st.warning("âš ï¸ å…ˆã«ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return
    
    with st.spinner("ğŸ” é€£ç¶šå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºä¸­..."):
        try:
            from shift_suite.tasks.continuous_shift_detector import ContinuousShiftDetector
            
            detector = ContinuousShiftDetector()
            continuous_shifts = detector.detect_continuous_shifts(st.session_state.long_df)
            
            if continuous_shifts:
                st.error(f"âš ï¸ {len(continuous_shifts)}ä»¶ã®é€£ç¶šå‹¤å‹™ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                
                # é‡å¤§åº¦åˆ¥ã«åˆ†é¡
                violations = []  # æ³•ä»¤é•åã®å¯èƒ½æ€§
                warnings = []    # æ³¨æ„ãŒå¿…è¦
                
                for shift in continuous_shifts:
                    if shift.total_hours > 16:  # 16æ™‚é–“è¶…ã¯é‡å¤§
                        violations.append(shift)
                    else:
                        warnings.append(shift)
                
                # æ³•ä»¤é•åãƒªã‚¹ã‚¯
                if violations:
                    st.error("ğŸš¨ **æ³•ä»¤é•åãƒªã‚¹ã‚¯ã®ã‚ã‚‹é€£ç¶šå‹¤å‹™**")
                    for shift in violations[:5]:  # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
                        with st.expander(f"âš ï¸ {shift.staff} - {shift.total_hours:.1f}æ™‚é–“é€£ç¶š", expanded=True):
                            st.write(f"**æœŸé–“:** {shift.start_date} {shift.start_time} ~ {shift.end_date} {shift.end_time}")
                            st.write(f"**é€£ç¶šæ™‚é–“:** {shift.total_hours:.1f}æ™‚é–“")
                            st.write(f"**ä¼‘æ†©æ™‚é–“:** {shift.break_minutes}åˆ†")
                            if shift.crosses_midnight:
                                st.write("ğŸŒ™ **æ·±å¤œåŠ´åƒã‚’å«ã‚€**")
                            st.error("âš ï¸ åŠ´åƒåŸºæº–æ³•é•åã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å³åº§ã®æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚")
                
                # æ³¨æ„ãŒå¿…è¦ãªé€£ç¶šå‹¤å‹™
                if warnings:
                    st.warning(f"âš ï¸ **æ³¨æ„ãŒå¿…è¦ãªé€£ç¶šå‹¤å‹™** ({len(warnings)}ä»¶)")
                    for shift in warnings[:3]:  # æœ€åˆã®3ä»¶ã‚’è¡¨ç¤º
                        st.write(f"â€¢ {shift.staff}: {shift.total_hours:.1f}æ™‚é–“ ({shift.start_date} ~ {shift.end_date})")
                
                # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
                summary = detector.get_continuous_shift_summary()
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("é€£ç¶šå‹¤å‹™ç·æ•°", summary['total_continuous_shifts'])
                with col2:
                    st.metric("å½±éŸ¿ã‚¹ã‚¿ãƒƒãƒ•æ•°", summary['affected_staff_count'])
                with col3:
                    st.metric("å¹³å‡é€£ç¶šæ™‚é–“", f"{summary['avg_continuous_hours']:.1f}æ™‚é–“")
                
            else:
                st.success("âœ… é€£ç¶šå‹¤å‹™ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except ImportError:
            st.error("é€£ç¶šå‹¤å‹™æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.debug(f"Continuous shift detection error: {e}")

def run_turnover_prediction():
    """é›¢è·ãƒªã‚¹ã‚¯äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    if st.session_state.get("long_df") is None or st.session_state.long_df.empty:
        st.warning("âš ï¸ å…ˆã«ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return
    
    with st.spinner("ğŸ“Š é›¢è·ãƒªã‚¹ã‚¯ã‚’äºˆæ¸¬ä¸­..."):
        try:
            # æ”¹å–„ç‰ˆã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from shift_suite.tasks.improved_turnover_predictor import (
                analyze_turnover_risk,
                generate_turnover_report
            )
            from pathlib import Path
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®è¨­å®š
            model_path = Path('models/turnover/improved_predictor.pkl')
            
            # ãƒªã‚¹ã‚¯åˆ†æã‚’å®Ÿè¡Œï¼ˆåˆå›ã¯è¨“ç·´ã‚‚å®Ÿè¡Œï¼‰
            predictions_df = analyze_turnover_risk(
                st.session_state.long_df,
                train_model=not model_path.exists(),  # ãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°è¨“ç·´
                model_path=model_path
            )
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = generate_turnover_report(predictions_df)
            
            if not predictions_df.empty:
                # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¥ã«åˆ†é¡
                high_risk = predictions_df[predictions_df['risk_level'] == 'é«˜ãƒªã‚¹ã‚¯']
                medium_risk = predictions_df[predictions_df['risk_level'] == 'ä¸­ãƒªã‚¹ã‚¯']
                low_risk = predictions_df[predictions_df['risk_level'] == 'ä½ãƒªã‚¹ã‚¯']
                
                # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ğŸ”´ é«˜ãƒªã‚¹ã‚¯", f"{len(high_risk)}å")
                with col2:
                    st.metric("ğŸŸ¡ ä¸­ãƒªã‚¹ã‚¯", f"{len(medium_risk)}å")
                with col3:
                    st.metric("ğŸŸ¢ ä½ãƒªã‚¹ã‚¯", f"{len(low_risk)}å")
                with col4:
                    st.metric("ğŸ“Š å¹³å‡ãƒªã‚¹ã‚¯", f"{report['summary']['average_risk']:.1%}")
                
                # é«˜ãƒªã‚¹ã‚¯ã‚¹ã‚¿ãƒƒãƒ•ã®è©³ç´°ï¼ˆæ”¹å–„ç‰ˆï¼‰
                if not high_risk.empty:
                    st.error("ğŸš¨ **é›¢è·ãƒªã‚¹ã‚¯ãŒé«˜ã„ã‚¹ã‚¿ãƒƒãƒ•**")
                    for _, staff in high_risk.head(5).iterrows():  # æœ€åˆã®5åã‚’è¡¨ç¤º
                        risk_score = staff['risk_probability'] * 100
                        accuracy = staff['prediction_accuracy'] * 100
                        staff_id = staff.get('staff_id', 'Unknown')
                        
                        # ä¿¡é ¼åŒºé–“ã®è¡¨ç¤º
                        ci_lower = staff['confidence_lower'] * 100
                        ci_upper = staff['confidence_upper'] * 100
                        
                        with st.expander(f"âš ï¸ ã‚¹ã‚¿ãƒƒãƒ•ID: {staff_id} - ãƒªã‚¹ã‚¯: {risk_score:.0f}% (ç²¾åº¦: {accuracy:.0f}%)"):
                            # ä¿¡é ¼åŒºé–“ã®è¡¨ç¤º
                            st.write(f"**95%ä¿¡é ¼åŒºé–“**: [{ci_lower:.0f}% - {ci_upper:.0f}%]")
                            st.write("")
                            
                            # ãƒªã‚¹ã‚¯è¦å› ã®è¡¨ç¤ºï¼ˆæ”¹å–„ç‰ˆï¼‰
                            st.write("**ä¸»ãªãƒªã‚¹ã‚¯è¦å› :**")
                            
                            # å¤œå‹¤ç‡
                            if 'night_ratio' in staff and staff['night_ratio'] > 0.5:
                                st.write(f"â€¢ ğŸŒ™ å¤œå‹¤ç‡: {staff['night_ratio']*100:.0f}%")
                            
                            # é€£ç¶šå‹¤å‹™
                            if 'consecutive_days' in staff and staff['consecutive_days'] > 5:
                                st.write(f"â€¢ ğŸ“… é€£ç¶šå‹¤å‹™: {staff['consecutive_days']}æ—¥")
                            
                            # ä¼‘æ—¥ä¸è¶³
                            if 'rest_ratio' in staff and staff['rest_ratio'] < 0.2:
                                st.write(f"â€¢ ğŸ–ï¸ ä¼‘æ—¥ç‡: {staff['rest_ratio']*100:.0f}%")
                            
                            st.write("")
                            
                            # 2ã¤ã®ã€Œï¼…ã€ã®èª¬æ˜
                            with st.info:
                                st.write("ğŸ“Š **2ã¤ã®æŒ‡æ¨™ã«ã¤ã„ã¦:**")
                                st.write(f"â€¢ **é›¢è·ãƒªã‚¹ã‚¯ {risk_score:.0f}%**: ã“ã®äººãŒé›¢è·ã™ã‚‹å¯èƒ½æ€§")
                                st.write(f"â€¢ **äºˆæ¸¬ç²¾åº¦ {accuracy:.0f}%**: ãã®äºˆæ¸¬ã®ç¢ºã‹ã‚‰ã—ã•")
                            
                            # ã‚¹ã‚¿ãƒƒãƒ•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°åˆ†æ
                            staff_data = st.session_state.long_df[st.session_state.long_df.get('staff_id', st.session_state.long_df.get('staff')) == staff_id]
                            
                            if not staff_data.empty:
                                # æœˆé–“å‹¤å‹™æ™‚é–“ã‚’è¨ˆç®—
                                monthly_hours = len(staff_data) * 0.5  # 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆ
                                if monthly_hours > 200:
                                    st.write(f"â€¢ éå‰°åŠ´åƒ: æœˆ{monthly_hours:.0f}æ™‚é–“ã®å‹¤å‹™")
                                
                                # é€£ç¶šå‹¤å‹™æ—¥æ•°ã‚’ç°¡æ˜“è¨ˆç®—
                                unique_dates = staff_data['ds'].dt.date.nunique()
                                if unique_dates > 25:
                                    st.write(f"â€¢ é€£ç¶šå‹¤å‹™: æœˆ{unique_dates}æ—¥å‹¤å‹™")
                                
                                # å¤œå‹¤ãƒã‚§ãƒƒã‚¯
                                if 'task' in staff_data.columns:
                                    night_shifts = staff_data['task'].str.contains('å¤œ|æ·±å¤œ|night', case=False, na=False).sum()
                                    if night_shifts > 0:
                                        night_ratio = night_shifts / len(staff_data) * 100
                                        if night_ratio > 30:
                                            st.write(f"â€¢ å¤œå‹¤éå¤š: {night_ratio:.0f}%ãŒå¤œå‹¤")
                                
                                # äºˆæ¸¬ç¢ºç‡ã‹ã‚‰æ¨å®š
                                prob = staff.get("turnover_probability", 0)
                                if prob > 0.8:
                                    st.write(f"â€¢ éå¸¸ã«é«˜ã„é›¢è·ãƒªã‚¹ã‚¯: {prob*100:.0f}%")
                            
                            st.write("**æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:**")
                            if risk_score >= 80 and accuracy >= 80:
                                st.write("â€¢ ğŸš¨ **ç·Šæ€¥å¯¾å¿œ**: å³åº§ã«å€‹åˆ¥é¢è«‡ã‚’å®Ÿæ–½")
                            elif risk_score >= 80 and accuracy < 80:
                                st.write("â€¢ âš ï¸ **è¦ç¢ºèª**: 24æ™‚é–“ä»¥å†…ã«çŠ¶æ³ç¢ºèª")
                            elif risk_score >= 50:
                                st.write("â€¢ ğŸ“… **å®šæœŸãƒ•ã‚©ãƒ­ãƒ¼**: é€±æ¬¡ã§çŠ¶æ³ç¢ºèª")
                            
                            st.write("â€¢ å‹¤å‹™è² è·ã®è»½æ¸›ã‚’æ¤œè¨")
                            st.write("â€¢ ã‚­ãƒ£ãƒªã‚¢ãƒ‘ã‚¹ã«ã¤ã„ã¦è©±ã—åˆã†")
                
                # ä¸­ãƒªã‚¹ã‚¯ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚µãƒãƒªãƒ¼
                if not medium_risk.empty:
                    st.warning(f"âš ï¸ **ä¸­ç¨‹åº¦ã®ãƒªã‚¹ã‚¯** ({len(medium_risk)}å)")
                    st.write("æ—©æœŸã®ä»‹å…¥ã«ã‚ˆã‚Šé›¢è·ã‚’é˜²ã’ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                
                # ä½ãƒªã‚¹ã‚¯ã‚¹ã‚¿ãƒƒãƒ•ã®ã‚µãƒãƒªãƒ¼
                if not low_risk.empty:
                    st.success(f"âœ… **ä½ãƒªã‚¹ã‚¯** ({len(low_risk)}å) - ç¾çŠ¶ç¶­æŒã‚’æ¨å¥¨")
                
                # æ¨å¥¨äº‹é …ã®è¡¨ç¤º
                if report['recommendations']:
                    st.divider()
                    st.subheader("ğŸ“‹ ç·åˆçš„ãªæ¨å¥¨äº‹é …")
                    for rec in report['recommendations']:
                        st.write(f"â€¢ {rec}")
                
                # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
                st.divider()
                st.subheader("ğŸ“Š çµ±è¨ˆã‚µãƒãƒªãƒ¼")
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**æœ€é«˜ãƒªã‚¹ã‚¯**: {report['summary']['max_risk']:.1%}")
                    st.write(f"**æœ€ä½ãƒªã‚¹ã‚¯**: {report['summary']['min_risk']:.1%}")
                with col2:
                    st.write(f"**å¹³å‡ãƒªã‚¹ã‚¯**: {report['summary']['average_risk']:.1%}")
                    st.write(f"**åˆ†æäººæ•°**: {report['summary']['total_staff']}å")
                
            else:
                st.info("ğŸ“Š äºˆæ¸¬ã«å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
                
        except ImportError as ie:
            st.error("é›¢è·äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ã®ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            st.info("å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸: scikit-learn, scipy, statsmodels")
            logger.debug(f"Import error: {ie}")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.debug(f"Turnover prediction error: {e}")

def run_mind_reader_analysis():
    """ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ„å›³åˆ†æã‚’å®Ÿè¡Œ"""
    if st.session_state.get("long_df") is None or st.session_state.long_df.empty:
        st.warning("âš ï¸ å…ˆã«ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        return
    
    with st.spinner("ğŸ§  ä½œæˆè€…ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ†æä¸­..."):
        try:
            from shift_suite.tasks.shift_mind_reader_lite import ShiftMindReaderLite
            
            # ä½œæˆè€…ã®æ„å›³ã‚’èª­ã¿å–ã‚‹
            reader = ShiftMindReaderLite()
            mind_results = reader.read_creator_mind(st.session_state.long_df)
            
            if mind_results:
                st.success("âœ… ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ„å›³åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸ")
                
                # åŸºæœ¬åˆ†æçµæœ
                basic_analysis = mind_results.get("basic_analysis", {})
                if basic_analysis:
                    st.subheader("ğŸ“Š **ã‚·ãƒ•ãƒˆä½œæˆã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³**")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        if "preference_patterns" in basic_analysis:
                            st.write("**ã‚¹ã‚¿ãƒƒãƒ•é…ç½®ã®å‚¾å‘:**")
                            prefs = basic_analysis["preference_patterns"]
                            for staff, pattern in list(prefs.items())[:5]:
                                st.write(f"â€¢ {staff}: {pattern}")
                    
                    with col2:
                        if "time_patterns" in basic_analysis:
                            st.write("**æ™‚é–“å¸¯ã®é…ç½®å‚¾å‘:**")
                            times = basic_analysis["time_patterns"]
                            for time_slot, count in list(times.items())[:5]:
                                st.write(f"â€¢ {time_slot}: {count}å›")
                
                # æ„æ€æ±ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³
                decision_patterns = mind_results.get("decision_patterns", {})
                if decision_patterns:
                    st.subheader("ğŸ¯ **ç™ºè¦‹ã•ã‚ŒãŸæ„æ€æ±ºå®šãƒ‘ã‚¿ãƒ¼ãƒ³**")
                    
                    # å„ªå…ˆé †ä½
                    if "priorities" in decision_patterns:
                        st.write("**é…ç½®ã®å„ªå…ˆé †ä½:**")
                        priorities = decision_patterns["priorities"]
                        for i, priority in enumerate(priorities[:5], 1):
                            st.write(f"{i}. {priority}")
                    
                    # åˆ¶ç´„è€ƒæ…®
                    if "constraints" in decision_patterns:
                        st.write("**è€ƒæ…®ã•ã‚Œã¦ã„ã‚‹åˆ¶ç´„:**")
                        constraints = decision_patterns["constraints"]
                        for constraint in constraints[:5]:
                            st.write(f"â€¢ {constraint}")
                
                # ç°¡ç•¥åŒ–ã•ã‚ŒãŸæ´å¯Ÿ
                insights = reader.get_simplified_insights()
                if insights:
                    st.info("ğŸ’¡ **ä½œæˆè€…ã®æ„å›³ï¼ˆæ¨å®šï¼‰**")
                    st.write(insights)
                
                # æ”¹å–„ææ¡ˆ
                st.success("ğŸ¯ **ã‚·ãƒ•ãƒˆä½œæˆãƒ—ãƒ­ã‚»ã‚¹ã®æ”¹å–„ææ¡ˆ**")
                st.write("â€¢ ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ˜æ–‡åŒ–ã—ã€ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦å…±æœ‰")
                st.write("â€¢ æš—é»™ã®åˆ¶ç´„ã‚’å¯è¦–åŒ–ã—ã€ãƒãƒ¼ãƒ å…¨ä½“ã§èªè­˜ã‚’çµ±ä¸€")
                st.write("â€¢ åã‚ŠãŒã‚ã‚‹é…ç½®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ç›´ã—ã€å…¬å¹³æ€§ã‚’å‘ä¸Š")
                st.write("â€¢ è‡ªå‹•åŒ–å¯èƒ½ãªåˆ¤æ–­ã‚’ç‰¹å®šã—ã€åŠ¹ç‡åŒ–ã‚’æ¨é€²")
                
            else:
                st.info("ğŸ§  åˆ†æã«å¿…è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except ImportError:
            st.error("ãƒã‚¤ãƒ³ãƒ‰ãƒªãƒ¼ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            logger.debug(f"Mind reader analysis error: {e}")

def display_constraint_discovery_results(results):
    """åˆ¶ç´„ç™ºè¦‹çµæœã®è¡¨ç¤º"""
    if not results or "discovered_constraints" not in results:
        st.info("ğŸ“‹ ã¾ã åˆ¶ç´„ç™ºè¦‹ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return
        
    constraints = results["discovered_constraints"]
    metadata = results.get("analysis_metadata", {})
    
    # æ¦‚è¦çµ±è¨ˆ
    st.subheader("ğŸ“Š åˆ¶ç´„ç™ºè¦‹çµæœæ¦‚è¦")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç™ºè¦‹åˆ¶ç´„æ•°", len(constraints))
    with col2:
        depth_counts = {}
        for c in constraints:
            depth = c.get("depth_level", "unknown")
            depth_counts[depth] = depth_counts.get(depth, 0) + 1
        st.metric("æœ€æ·±å±¤æ•°", depth_counts.get("HYPER_DEEP", 0))
    with col3:
        creator_intents = [c for c in constraints if "ã€ä½œæˆè€…æ„å›³ã€‘" in c.get("constraint", "")]
        st.metric("ä½œæˆè€…æ„å›³", len(creator_intents))
    with col4:
        confidence_scores = [c.get("confidence_score", 0) for c in constraints if c.get("confidence_score")]
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        st.metric("å¹³å‡ä¿¡é ¼åº¦", f"{avg_confidence:.1%}")
    
    # è»¸åˆ¥åˆ†æçµæœ
    st.subheader("ğŸ” è»¸åˆ¥åˆ†æçµæœ")
    
    axis_results = {}
    for constraint in constraints:
        axis = constraint.get("axis", "unknown")
        if axis not in axis_results:
            axis_results[axis] = []
        axis_results[axis].append(constraint)
    
    # è»¸åˆ¥ã‚¿ãƒ–è¡¨ç¤º
    if axis_results:
        axis_tabs = st.tabs([f"{axis} ({len(constraints)})" for axis, constraints in axis_results.items()])
        
        for (axis, axis_constraints), tab in zip(axis_results.items(), axis_tabs):
            with tab:
                st.write(f"**{axis}ã§ç™ºè¦‹ã—ãŸåˆ¶ç´„: {len(axis_constraints)}å€‹**")
                
                for i, constraint in enumerate(axis_constraints[:20]):  # ä¸Šä½20å€‹ã‚’è¡¨ç¤º
                    with st.expander(f"åˆ¶ç´„ {i+1}: {constraint.get('constraint', '')[:100]}..."):
                        st.json(constraint)
                
                if len(axis_constraints) > 20:
                    st.info(f"â„¹ï¸ ã•ã‚‰ã«{len(axis_constraints) - 20}å€‹ã®åˆ¶ç´„ãŒã‚ã‚Šã¾ã™ã€‚")
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
    st.subheader("ğŸ“¥ çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    
    # JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    json_str = json.dumps(results, ensure_ascii=False, indent=2)
    st.download_button(
        label="ğŸ“ JSONå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=json_str,
        file_name=f"constraint_discovery_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
        mime="application/json"
    )
    
    # CSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if constraints:
        import pandas as pd
        df_constraints = pd.DataFrame(constraints)
        csv_str = df_constraints.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“ˆ CSVå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_str,
            file_name=f"constraints_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )

def display_cost_tab(tab_container, data_dir):
    with tab_container:
        st.subheader("äººä»¶è²»åˆ†æ")
        display_data = st.session_state.get("display_data", {})
        df = display_data.get("daily_cost")
        if isinstance(df, pd.DataFrame):
            if not _valid_df(df):
                st.info(_("Cost simulation data not available or empty"))
                return

            try:
                # --- ã“ã“ã‹ã‚‰ãŒè¿½åŠ ãƒ»å¤‰æ›´éƒ¨åˆ† ---
                long_df = st.session_state.get("long_df")

                if (
                    long_df is not None
                    and not long_df.empty
                    and "ds" in long_df.columns
                ):
                    daily_details = (
                        long_df[long_df["parsed_slots_count"] > 0]
                        .assign(date=lambda x: x["ds"].dt.normalize())
                        .groupby("date")
                        .agg(
                            day_of_week=(
                                "ds",
                                lambda x: ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"][
                                    x.iloc[0].weekday()
                                ],
                            ),
                            total_staff=("staff", "nunique"),
                            role_breakdown=(
                                "role",
                                lambda x: ", ".join(
                                    f"{role}:{count}"
                                    for role, count in x.value_counts().items()
                                ),
                            ),
                            staff_list=(
                                "staff",
                                lambda x: ", ".join(sorted(x.unique())),
                            ),
                        )
                        .reset_index()
                    )

                    def summarize_staff(staff_str, limit=5):
                        staff_list = staff_str.split(", ")
                        if len(staff_list) > limit:
                            return (
                                ", ".join(staff_list[:limit])
                                + f", ...ä»–{len(staff_list) - limit}å"
                            )
                        return staff_str

                    daily_details["staff_list_summary"] = daily_details[
                        "staff_list"
                    ].apply(summarize_staff)

                    df["date"] = pd.to_datetime(df["date"]).dt.normalize()
                    daily_details["date"] = pd.to_datetime(
                        daily_details["date"]
                    ).dt.normalize()

                    df = pd.merge(df, daily_details, on="date", how="left")

                custom_data = [
                    "day_of_week",
                    "total_staff",
                    "role_breakdown",
                    "staff_list_summary",
                ]

                final_custom_data = [col for col in custom_data if col in df.columns]

                hovertemplate = (
                    "<b>%{x|%Y-%m-%d} (%{customdata[0]})</b><br><br>"
                    "ã‚³ã‚¹ãƒˆ: %{y:,.0f}å††<br>"
                    "æ§‹æˆäººæ•°: %{customdata[1]}äºº<br>"
                    "è·ç¨®ä¸€è¦§: %{customdata[2]}<br>"
                    "ã‚¹ã‚¿ãƒƒãƒ•: %{customdata[3]}"
                    "<extra></extra>"
                )

                st.subheader("æ—¥åˆ¥ã‚³ã‚¹ãƒˆ")
                fig_cost = px.bar(
                    df,
                    x="date",
                    y="cost",
                    title="æ—¥åˆ¥ç™ºç”Ÿäººä»¶è²»",
                    custom_data=final_custom_data if final_custom_data else None,
                )
                fig_cost.update_xaxes(tickformat="%m/%d(%a)")

                if final_custom_data:
                    fig_cost.update_traces(hovertemplate=hovertemplate)

                st.plotly_chart(
                    fig_cost, use_container_width=True, key="daily_cost_chart_enhanced"
                )

                # --- å¤‰æ›´ã“ã“ã¾ã§ ---

                st.dataframe(df, use_container_width=True, hide_index=True)

                st.divider()
                st.subheader("ç´¯è¨ˆäººä»¶è²»ã®æ¨ç§»")
                df_sorted = df.sort_values(by="date").copy()
                if "cost" in df_sorted.columns:
                    df_sorted["cumulative_cost"] = df_sorted["cost"].cumsum()
                    fig_cumulative = px.line(
                        df_sorted,
                        x="date",
                        y="cumulative_cost",
                        title="æ—¥åˆ¥ç´¯è¨ˆäººä»¶è²»",
                        labels={"date": "æ—¥ä»˜", "cumulative_cost": "ç´¯è¨ˆäººä»¶è²» (å††)"},
                        markers=True,
                    )
                    fig_cumulative.update_xaxes(tickformat="%m/%d(%a)")
                    fig_cumulative.update_layout(
                        yaxis_title="ç´¯è¨ˆäººä»¶è²» (å††)",
                        xaxis_title="æ—¥ä»˜",
                    )
                    st.plotly_chart(
                        fig_cumulative,
                        use_container_width=True,
                        key="cumulative_cost_chart",
                    )

            except Exception as e:
                log_and_display_error("daily_cost.parquet è¡¨ç¤ºã‚¨ãƒ©ãƒ¼", e)
        else:
            st.info("äººä»¶è²»åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


def display_hireplan_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("Hiring Plan (Needed FTE)"))
        display_data = st.session_state.get("display_data", {})
        df_plan = display_data.get("hire_plan")
        if isinstance(df_plan, pd.DataFrame):
            if not _valid_df(df_plan):
                st.info(_("Hiring plan data is empty"))
            else:
                display_plan_df = df_plan.rename(
                    columns={"role": _("Role"), "hire_fte": _("hire_fte")}
                )
                st.dataframe(display_plan_df, use_container_width=True, hide_index=True)
                if "role" in df_plan and "hire_fte" in df_plan:
                    fig_plan = px.bar(
                        df_plan,
                        x="role",
                        y="hire_fte",
                        labels={"role": _("Role"), "hire_fte": _("hire_fte")},
                    )
                    st.plotly_chart(
                        fig_plan, use_container_width=True, key="hireplan_chart"
                    )
        else:
            st.info(
                _("Hiring Plan") + " (hire_plan.parquet) " + _("ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            )

        # --- æœ€é©æ¡ç”¨è¨ˆç”»ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã“ã“ã«è¿½åŠ  ---
        st.divider()
        st.subheader("æœ€é©æ¡ç”¨è¨ˆç”»")
        df_optimal = display_data.get("optimal_hire_plan")
        if isinstance(df_optimal, pd.DataFrame):
            st.info("åˆ†æã®çµæœã€ä»¥ä¸‹ã®å…·ä½“çš„ãªæ¡ç”¨è¨ˆç”»ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
            st.dataframe(df_optimal, use_container_width=True, hide_index=True)
        else:
            st.info("æœ€é©æ¡ç”¨è¨ˆç”»ã®åˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")


def display_summary_report_tab(tab_container, data_dir):
    """Show auto-generated shortage summary report."""

    with tab_container:
        st.subheader(_("Summary Report"))
        report_files = sorted(Path(data_dir).glob("OverShortage_SummaryReport_*.md"))
        latest = report_files[-1] if report_files else None

        if st.button(_("Generate Summary Report")):
            try:
                from shift_suite.tasks.report_generator import generate_summary_report

                latest = generate_summary_report(Path(data_dir))
                st.success(_("Report generated"))
            except Exception as e:
                log_and_display_error("summary report generation failed", e)

        if latest and latest.exists():
            md_text = latest.read_text(encoding="utf-8")
            st.markdown(md_text)
            with open(latest, "rb") as f:
                st.download_button(
                    label=_("Download Report (Markdown)"),
                    data=f,
                    file_name=latest.name,
                    mime="text/markdown",
                    use_container_width=True,
                )


def load_leave_results_from_dir(data_dir: Path) -> dict:
    """Wrapper for :func:`dashboard.load_leave_results_from_dir`."""

    return dashboard.load_leave_results_from_dir(data_dir)


def display_leave_analysis_tab(tab_container, results_dict: dict | None = None):
    """Display leave analysis results with interactive charts.

    Parameters
    ----------
    tab_container : Any
        Streamlit tab container.
    results_dict : dict | None
        In-memory results dictionary produced by ``leave_analyzer``.  If ``None``
        the function falls back to ``st.session_state.leave_analysis_results``.
    """

    with tab_container:
        st.subheader(_("Leave Analysis"))

        if results_dict is None:
            results_dict = st.session_state.leave_analysis_results

        leave_df = results_dict.get("daily_summary")

        if not isinstance(leave_df, pd.DataFrame) or leave_df.empty:
            st.info(_("No leave analysis results available."))
            return

        st.dataframe(leave_df, use_container_width=True, hide_index=True)

        staff_balance = results_dict.get("staff_balance_daily")
        if isinstance(staff_balance, pd.DataFrame) and not staff_balance.empty:
            st.subheader("å‹¤å‹™äºˆå®šäººæ•°ã¨å…¨ä¼‘æš‡å–å¾—è€…æ•°ã®æ¨ç§»")
            fig_bal = px.line(
                staff_balance,
                x="date",
                y=["total_staff", "leave_applicants_count", "non_leave_staff"],
                markers=True,
                labels={
                    "date": _("Date"),
                    "value": _("Count"),
                    "variable": _("Metric"),
                    "total_staff": _("Total staff"),
                    "leave_applicants_count": _("Leave applicants"),
                    "non_leave_staff": _("Non-leave staff"),
                },
                title="ã‚¹ã‚¿ãƒƒãƒ•ãƒãƒ©ãƒ³ã‚¹ã®æ¨ç§»",
            )
            fig_bal.update_xaxes(tickformat="%m/%d(%a)")
            st.plotly_chart(
                fig_bal, use_container_width=True, key="staff_balance_chart"
            )
            st.dataframe(staff_balance, use_container_width=True, hide_index=True)

            daily_summary_for_chart = results_dict.get("daily_summary")
            if (
                isinstance(daily_summary_for_chart, pd.DataFrame)
                and not daily_summary_for_chart.empty
            ):
                st.subheader("æ—¥åˆ¥ ä¼‘æš‡å–å¾—è€…æ•°ï¼ˆå†…è¨³ï¼‰")
                fig_breakdown = px.bar(
                    daily_summary_for_chart,
                    x="date",
                    y="total_leave_days",
                    color="leave_type",
                    barmode="stack",
                    labels={
                        "date": _("Date"),
                        "total_leave_days": _("Leave applicants"),
                        "leave_type": _("Leave type"),
                    },
                    title="æ—¥åˆ¥ ä¼‘æš‡å–å¾—è€…æ•°ï¼ˆå†…è¨³ï¼‰",
                )
                fig_breakdown.update_xaxes(tickformat="%m/%d(%a)")
                st.plotly_chart(
                    fig_breakdown,
                    use_container_width=True,
                    key="daily_leave_breakdown_chart",
                )

        ratio_break = results_dict.get("leave_ratio_breakdown")
        if isinstance(ratio_break, pd.DataFrame) and not ratio_break.empty:
            st.subheader("æœˆåˆãƒ»æœˆä¸­ãƒ»æœˆæœ« å„æ›œæ—¥ã®ä¼‘æš‡å‰²åˆ")
            fig_ratio_break = px.bar(
                ratio_break,
                x="dayofweek",
                y="leave_ratio",
                color="leave_type",
                facet_col="month_period",
                category_orders={
                    "dayofweek": [
                        "æœˆæ›œæ—¥",
                        "ç«æ›œæ—¥",
                        "æ°´æ›œæ—¥",
                        "æœ¨æ›œæ—¥",
                        "é‡‘æ›œæ—¥",
                        "åœŸæ›œæ—¥",
                        "æ—¥æ›œæ—¥",
                    ],
                    "month_period": ["æœˆåˆ(1-10æ—¥)", "æœˆä¸­(11-20æ—¥)", "æœˆæœ«(21-æœ«æ—¥)"],
                },
                labels={
                    "dayofweek": _("Day"),
                    "leave_ratio": _("Ratio"),
                    "leave_type": _("Leave type"),
                    "month_period": _("Month period"),
                },
                title="æ›œæ—¥ãƒ»æœˆæœŸé–“åˆ¥ä¼‘æš‡å–å¾—ç‡",
            )
            st.plotly_chart(
                fig_ratio_break,
                use_container_width=True,
                key="leave_ratio_breakdown_chart",
            )
            st.dataframe(ratio_break, use_container_width=True, hide_index=True)

        daily_summary_for_chart = results_dict.get("daily_summary")
        if (
            isinstance(daily_summary_for_chart, pd.DataFrame)
            and not daily_summary_for_chart.empty
        ):
            st.subheader("ä¼‘æš‡ã‚¿ã‚¤ãƒ—åˆ¥ å–å¾—è€…æ•°ã®æ¨ç§»")
            fig_type = px.line(
                daily_summary_for_chart,
                x="date",
                y="total_leave_days",
                color="leave_type",
                markers=True,
                labels={
                    "date": _("Date"),
                    "total_leave_days": _("Leave applicants"),
                    "leave_type": _("Leave type"),
                },
                title="ä¼‘æš‡ã‚¿ã‚¤ãƒ—åˆ¥ å–å¾—è€…æ•°ã®æ¨ç§»",
            )
            fig_type.update_xaxes(tickformat="%m/%d(%a)")
            st.plotly_chart(
                fig_type, use_container_width=True, key="leave_type_trend_chart"
            )
            st.dataframe(
                daily_summary_for_chart, use_container_width=True, hide_index=True
            )

        concentration = results_dict.get("concentration_requested")
        if isinstance(concentration, pd.DataFrame) and not concentration.empty:
            conc_df = concentration.copy()

            st.subheader(_("Leave concentration graphs"))
            st.info(
                "ä¸‹ã®ã‚°ãƒ©ãƒ•ã®â—‡ãƒãƒ¼ã‚«ãƒ¼ï¼ˆé–¾å€¤è¶…éæ—¥ï¼‰ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€è©²å½“æ—¥ã«ä¼‘æš‡ã‚’ç”³è«‹ã—ãŸè·å“¡ã‚’ç¢ºèªã§ãã¾ã™ã€‚è¤‡æ•°æ—¥ã®ã‚¯ãƒªãƒƒã‚¯ã§å¯¾è±¡ã‚’è¿½åŠ ãƒ»è§£é™¤ã§ãã¾ã™ã€‚"
            )

            fig_conc = go.Figure()
            fig_conc.add_trace(
                go.Scatter(
                    x=conc_df["date"],
                    y=conc_df["leave_applicants_count"],
                    mode="lines+markers",
                    name=_("Leave applicants"),
                    line=dict(shape="spline", smoothing=0.5),
                    marker=dict(size=6),
                )
            )

            focused_mask = conc_df.get("is_concentrated")
            focused_df = (
                conc_df[focused_mask] if focused_mask is not None else pd.DataFrame()
            )
            if not focused_df.empty:
                fig_conc.add_trace(
                    go.Scatter(
                        x=focused_df["date"],
                        y=focused_df["leave_applicants_count"],
                        mode="markers",
                        marker=dict(color="red", size=12, symbol="diamond"),
                        name=_("Exceeds threshold"),
                        hoverinfo="text",
                        text=[
                            f"<b>{row['date'].strftime('%Y-%m-%d')}</b><br>ç”³è«‹è€…: {row['leave_applicants_count']}äºº<br>æ°å: {', '.join(row['staff_names'])}"
                            for _, row in focused_df.iterrows()
                        ],
                    )
                )

            fig_conc.update_layout(
                title="å¸Œæœ›ä¼‘ ç”³è«‹è€…æ•°ã®æ¨ç§»ã¨é›†ä¸­æ—¥",
                xaxis_title="æ—¥ä»˜",
                yaxis_title="ç”³è«‹è€…æ•°",
            )
            fig_conc.update_xaxes(tickformat="%m/%d(%a)")

            if plotly_events:
                selected_points = plotly_events(
                    fig_conc, click_event=True, key="leave_conc_events"
                )
            else:
                st.plotly_chart(fig_conc, use_container_width=True)
                selected_points = []

            if "selected_leave_dates" not in st.session_state:
                st.session_state.selected_leave_dates = set()

            for point in selected_points:
                try:
                    clicked_date = pd.to_datetime(point["x"]).normalize()
                    if clicked_date in st.session_state.selected_leave_dates:
                        st.session_state.selected_leave_dates.remove(clicked_date)
                    else:
                        st.session_state.selected_leave_dates.add(clicked_date)
                except Exception as e:
                    log.debug(f"ã‚¯ãƒªãƒƒã‚¯ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

            if st.button("é¸æŠã‚’ã‚¯ãƒªã‚¢"):
                st.session_state.selected_leave_dates.clear()
                st.rerun()

            selected_dates = sorted(list(st.session_state.selected_leave_dates))
            if selected_dates:
                st.markdown("---")
                st.markdown("##### é¸æŠã•ã‚ŒãŸé›†ä¸­æ—¥ã®ä¼‘æš‡ç”³è«‹è€…")

                all_names_in_selection = []
                for selected_date in selected_dates:
                    names_series = conc_df.loc[
                        conc_df["date"] == selected_date, "staff_names"
                    ]
                    if not names_series.empty and isinstance(
                        names_series.iloc[0], list
                    ):
                        names_list = names_series.iloc[0]
                        st.markdown(
                            f"**{selected_date.strftime('%Y-%m-%d')}**: {', '.join(names_list)}"
                        )
                        all_names_in_selection.extend(names_list)

                if all_names_in_selection:
                    st.markdown("##### é¸æŠç¯„å›²å†…ã§ã®ç”³è«‹å›æ•°")
                    name_counts = (
                        pd.Series(all_names_in_selection).value_counts().reset_index()
                    )
                    name_counts.columns = ["è·å“¡å", "ç”³è«‹å›æ•°"]
                    st.dataframe(name_counts, use_container_width=True, hide_index=True)


def display_gap_analysis_tab(tab_container, data_dir):
    with tab_container:
        st.subheader(_("åŸºæº–ä¹–é›¢åˆ†æ"))
        if "gap_analysis_results" in st.session_state:
            results = st.session_state.gap_analysis_results
            st.info(
                "ã€Œå®Ÿæ…‹ã®å¿…è¦äººæ•°ã€ã¨ã€ŒåŸºæº–ã®å¿…è¦äººæ•°ã€ã®å·®åˆ†ã‚’ç¤ºã—ã¾ã™ã€‚å€¤ãŒãƒ—ãƒ©ã‚¹ã®å ´åˆã€åŸºæº–ã‚ˆã‚Šã‚‚å¤šãã®äººå“¡ãŒå®Ÿæ…‹ã¨ã—ã¦å¿…è¦ã ã£ãŸã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"
            )
            st.write("#### è·ç¨®åˆ¥ æœˆé–“ç·ä¹–é›¢æ™‚é–“")
            st.dataframe(results["gap_summary"])
            st.write("#### æ™‚é–“å¸¯ãƒ»è·ç¨®åˆ¥ ä¹–é›¢ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—")
            fig = px.imshow(
                results["gap_heatmap"],
                aspect="auto",
                color_continuous_scale="RdBu_r",
            )
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("è§£æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")


def display_mind_reader_tab(tab_container, data_dir: Path) -> None:
    """Display the Mind Reader analysis tab."""
    with tab_container:
        st.subheader("ğŸ§  ã‚·ãƒ•ãƒˆä½œæˆæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹è§£èª­")

        if "mind_reader_results" not in st.session_state:
            if st.button("æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è§£èª­ã™ã‚‹"):
                with st.spinner("æ€è€ƒã‚’è§£èª­ä¸­..."):
                    engine = AdvancedBlueprintEngineV2()
                    long_df = st.session_state.get("long_df")
                    wt_df = st.session_state.get("wt_df")  # å‹¤å‹™åŒºåˆ†ãƒ‡ãƒ¼ã‚¿ã‚‚å–å¾—
                    if long_df is not None and not long_df.empty:
                        results = engine.run_full_blueprint_analysis(long_df, wt_df)
                        st.session_state.mind_reader_results = results["mind_reading"]
                        st.session_state.mece_facility_facts = results["mece_facility_facts"]
                        st.rerun()
                    else:
                        st.error("åˆ†æã®å…ƒã¨ãªã‚‹å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            results = st.session_state.mind_reader_results

        # resultså¤‰æ•°ã®å®‰å…¨æ€§ç¢ºä¿
        if 'results' not in locals() or results is None:
            results = {}

        st.markdown("#### å„ªå…ˆé †ä½ï¼ˆåˆ¤æ–­åŸºæº–ã®é‡è¦åº¦ï¼‰")
        st.info(
            "ä½œæˆè€…ãŒç„¡æ„è­˜ã«ã©ã®é …ç›®ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ã‚’æ•°å€¤åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚çµ¶å¯¾å€¤ãŒå¤§ãã„ã»ã©é‡è¦ã§ã™ã€‚"
        )
        importance_df = results.get("feature_importance") if results else None
        if importance_df is not None:
            st.dataframe(importance_df)

        st.markdown("#### æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆæ±ºå®šæœ¨ï¼‰")
        st.info(
            "ã€Œèª°ã‚’é…ç½®ã™ã‚‹ã‹ã€ã¨ã„ã†åˆ¤æ–­ã®åˆ†å²ã‚’æ¨¡å€£ã—ãŸã‚‚ã®ã§ã™ã€‚ä¸Šã«ã‚ã‚‹åˆ†å²ã»ã©ã€å„ªå…ˆçš„ã«è€ƒæ…®ã•ã‚Œã¦ã„ã¾ã™ã€‚"
        )
        tree_model = results.get("thinking_process_tree") if results else None
        if tree_model:
            fig, _ = plt.subplots(figsize=(20, 10))
            plot_tree(
                tree_model,
                filled=True,
                feature_names=getattr(tree_model, "feature_names_in_", None),
                class_names=True,
                max_depth=3,
                fontsize=10,
            )
            st.pyplot(fig)

        st.markdown("#### ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ")
        st.info(
            "æ¨ªè»¸ã¨ç¸¦è»¸ã®æŒ‡æ¨™ã®é–“ã§ã€ä½œæˆè€…ãŒã©ã®ã‚ˆã†ãªãƒãƒ©ãƒ³ã‚¹ã‚’å–ã£ã¦ããŸã‹ã‚’ç¤ºã—ã¾ã™ã€‚"
        )
        trade_off_df = results.get("trade_offs") if results else None
        if trade_off_df is not None and not trade_off_df.empty:
            fig = px.scatter(
                trade_off_df,
                x="total_cost",
                y="fairness_score",
                title="ã‚³ã‚¹ãƒˆ vs å…¬å¹³æ€§ ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•",
            )
            st.plotly_chart(fig, use_container_width=True)


def display_mece_facts_tab(tab_container, data_dir: Path) -> None:
    """MECEäº‹å®ŸæŠ½å‡ºçµæœè¡¨ç¤ºã‚¿ãƒ–"""
    with tab_container:
        st.subheader("ğŸ“‹ æ–½è¨­ãƒ«ãƒ¼ãƒ«äº‹å®ŸæŠ½å‡º (è»¸1)")
        st.info("éå»ã‚·ãƒ•ãƒˆå®Ÿç¸¾ã‹ã‚‰MECEåˆ†è§£ã«ã‚ˆã‚ŠæŠ½å‡ºã•ã‚ŒãŸæ–½è¨­å›ºæœ‰ã®é‹ç”¨ãƒ«ãƒ¼ãƒ«äº‹å®Ÿ")
        
        # MECEäº‹å®ŸæŠ½å‡ºçµæœãŒãªã„å ´åˆã®å‡¦ç†
        if "mece_facility_facts" not in st.session_state:
            st.warning("MECEäº‹å®ŸæŠ½å‡ºã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚Mind Readerã‚¿ãƒ–ã‹ã‚‰å®Ÿè¡Œã§ãã¾ã™ã€‚")
            return
        
        mece_results = st.session_state.get("mece_facility_facts", {})
        human_readable = mece_results.get("human_readable", {})
        
        if not human_readable:
            st.warning("æŠ½å‡ºã•ã‚ŒãŸäº‹å®Ÿãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        # æŠ½å‡ºã‚µãƒãƒªãƒ¼è¡¨ç¤º
        summary = human_readable.get("æŠ½å‡ºäº‹å®Ÿã‚µãƒãƒªãƒ¼", {})
        if summary:
            st.markdown("### ğŸ“Š æŠ½å‡ºã‚µãƒãƒªãƒ¼")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ç·äº‹å®Ÿæ•°", summary.get("ç·äº‹å®Ÿæ•°", 0))
            with col2:
                st.metric("ã‚«ãƒ†ã‚´ãƒªãƒ¼æ•°", len(summary) - 1)  # 'ç·äº‹å®Ÿæ•°'ã‚’é™¤ã
            with col3:
                extraction_metadata = mece_results.get("extraction_metadata", {})
                data_quality = extraction_metadata.get("data_quality", {})
                st.metric("ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§", f"{data_quality.get('completeness_ratio', 0):.1%}")
        
        # ç¢ºä¿¡åº¦åˆ¥åˆ†é¡è¡¨ç¤º
        confidence_classification = human_readable.get("ç¢ºä¿¡åº¦åˆ¥åˆ†é¡", {})
        if confidence_classification:
            st.markdown("### ğŸ¯ ç¢ºä¿¡åº¦åˆ¥äº‹å®Ÿåˆ†é¡")
            
            high_conf_tab, med_conf_tab, low_conf_tab = st.tabs(["é«˜ç¢ºä¿¡åº¦ (â‰¥80%)", "ä¸­ç¢ºä¿¡åº¦ (50-80%)", "ä½ç¢ºä¿¡åº¦ (<50%)"])
            
            with high_conf_tab:
                high_facts = confidence_classification.get("é«˜ç¢ºä¿¡åº¦", [])
                if high_facts:
                    st.success(f"é«˜ç¢ºä¿¡åº¦äº‹å®Ÿ: {len(high_facts)}ä»¶")
                    for i, fact in enumerate(high_facts[:20], 1):  # æœ€å¤§20ä»¶è¡¨ç¤º
                        with st.expander(f"äº‹å®Ÿ {i}: {fact['ã‚«ãƒ†ã‚´ãƒªãƒ¼']} > {fact['ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼']}"):
                            st.json(fact['è©³ç´°'])
                else:
                    st.info("é«˜ç¢ºä¿¡åº¦ã®äº‹å®Ÿã¯ã‚ã‚Šã¾ã›ã‚“")
            
            with med_conf_tab:
                med_facts = confidence_classification.get("ä¸­ç¢ºä¿¡åº¦", [])
                if med_facts:
                    st.warning(f"ä¸­ç¢ºä¿¡åº¦äº‹å®Ÿ: {len(med_facts)}ä»¶")
                    for i, fact in enumerate(med_facts[:20], 1):
                        with st.expander(f"äº‹å®Ÿ {i}: {fact['ã‚«ãƒ†ã‚´ãƒªãƒ¼']} > {fact['ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼']}"):
                            st.json(fact['è©³ç´°'])
                else:
                    st.info("ä¸­ç¢ºä¿¡åº¦ã®äº‹å®Ÿã¯ã‚ã‚Šã¾ã›ã‚“")
            
            with low_conf_tab:
                low_facts = confidence_classification.get("ä½ç¢ºä¿¡åº¦", [])
                if low_facts:
                    st.error(f"ä½ç¢ºä¿¡åº¦äº‹å®Ÿ: {len(low_facts)}ä»¶ (è¦ç¢ºèª)")
                    for i, fact in enumerate(low_facts[:20], 1):
                        with st.expander(f"äº‹å®Ÿ {i}: {fact['ã‚«ãƒ†ã‚´ãƒªãƒ¼']} > {fact['ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªãƒ¼']}"):
                            st.json(fact['è©³ç´°'])
                else:
                    st.info("ä½ç¢ºä¿¡åº¦ã®äº‹å®Ÿã¯ã‚ã‚Šã¾ã›ã‚“")
        
        # MECEåˆ†è§£äº‹å®Ÿè©³ç´°è¡¨ç¤º
        mece_facts = human_readable.get("MECEåˆ†è§£äº‹å®Ÿ", {})
        if mece_facts:
            st.markdown("### ğŸ” MECEåˆ†è§£äº‹å®Ÿè©³ç´°")
            
            for category, facts in mece_facts.items():
                if facts:
                    with st.expander(f"{category} ({len(facts)}ä»¶)", expanded=False):
                        facts_df = pd.DataFrame(facts)
                        st.dataframe(facts_df, use_container_width=True)
        
        # æ©Ÿæ¢°å®Ÿè¡Œç”¨åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        machine_readable = mece_results.get("machine_readable", {})
        if machine_readable:
            st.markdown("### ğŸ¤– AIå®Ÿè¡Œç”¨åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿")
            
            hard_constraints = machine_readable.get("hard_constraints", [])
            soft_constraints = machine_readable.get("soft_constraints", [])
            preferences = machine_readable.get("preferences", [])
            
            constraint_col1, constraint_col2, constraint_col3 = st.columns(3)
            with constraint_col1:
                st.metric("ãƒãƒ¼ãƒ‰åˆ¶ç´„", len(hard_constraints))
            with constraint_col2:
                st.metric("ã‚½ãƒ•ãƒˆåˆ¶ç´„", len(soft_constraints))
            with constraint_col3:
                st.metric("æ¨å¥¨è¨­å®š", len(preferences))
            
            if st.checkbox("åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿è©³ç´°ã‚’è¡¨ç¤º"):
                if hard_constraints:
                    st.markdown("#### ãƒãƒ¼ãƒ‰åˆ¶ç´„ (å¿…é ˆéµå®ˆ)")
                    hard_df = pd.DataFrame([{
                        "ID": c.get("id", ""),
                        "ã‚¿ã‚¤ãƒ—": c.get("type", ""),
                        "ã‚«ãƒ†ã‚´ãƒªãƒ¼": c.get("category", ""),
                        "ç¢ºä¿¡åº¦": c.get("confidence", 0),
                        "å„ªå…ˆåº¦": c.get("priority", "")
                    } for c in hard_constraints])
                    st.dataframe(hard_df, use_container_width=True)
                
                if soft_constraints:
                    st.markdown("#### ã‚½ãƒ•ãƒˆåˆ¶ç´„ (å¯èƒ½ãªé™ã‚Šéµå®ˆ)")
                    soft_df = pd.DataFrame([{
                        "ID": c.get("id", ""),
                        "ã‚¿ã‚¤ãƒ—": c.get("type", ""),
                        "ã‚«ãƒ†ã‚´ãƒªãƒ¼": c.get("category", ""),
                        "ç¢ºä¿¡åº¦": c.get("confidence", 0),
                        "å„ªå…ˆåº¦": c.get("priority", "")
                    } for c in soft_constraints])
                    st.dataframe(soft_df, use_container_width=True)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
        extraction_metadata = mece_results.get("extraction_metadata", {})
        if extraction_metadata and st.checkbox("æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
            st.markdown("### ğŸ“ˆ æŠ½å‡ºãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")
            st.json(extraction_metadata)
        
        # æ©Ÿæ¢°å®Ÿè¡Œç”¨åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        st.markdown("### ğŸ’¾ åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ")
        st.info("AIã‚·ãƒ•ãƒˆä½œæˆã‚·ã‚¹ãƒ†ãƒ ã§åˆ©ç”¨å¯èƒ½ãªåˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚")
        
        export_col1, export_col2 = st.columns(2)
        
        with export_col1:
            if st.button("ğŸ¤– AIå®Ÿè¡Œç”¨åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
                constraints_json = machine_readable
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
                import datetime as dt
                timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"facility_constraints_{timestamp}.json"
                
                # JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒˆå½¢å¼ã«å¤‰æ›
                import json
                json_str = json.dumps(constraints_json, ensure_ascii=False, indent=2)
                json_bytes = json_str.encode('utf-8')
                
                st.download_button(
                    label="ğŸ“¥ åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (JSON)",
                    data=json_bytes,
                    file_name=filename,
                    mime="application/json",
                    use_container_width=True
                )
                
                st.success(f"åˆ¶ç´„ãƒ‡ãƒ¼ã‚¿ãŒæº–å‚™ã•ã‚Œã¾ã—ãŸ: {filename}")
                st.info(f"ãƒãƒ¼ãƒ‰åˆ¶ç´„: {len(constraints_json.get('hard_constraints', []))}ä»¶ã€ã‚½ãƒ•ãƒˆåˆ¶ç´„: {len(constraints_json.get('soft_constraints', []))}ä»¶")
        
        with export_col2:
            if st.button("ğŸ“‹ äººé–“ç¢ºèªç”¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ", use_container_width=True):
                # äººé–“ç¢ºèªç”¨ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                report_data = {
                    "report_metadata": {
                        "generated_at": dt.datetime.now().isoformat(),
                        "report_type": "MECEæ–½è¨­ãƒ«ãƒ¼ãƒ«äº‹å®ŸæŠ½å‡ºãƒ¬ãƒãƒ¼ãƒˆ",
                        "data_period": extraction_metadata.get("data_period", {}),
                        "data_quality": extraction_metadata.get("data_quality", {})
                    },
                    "summary": summary,
                    "confidence_classification": confidence_classification,
                    "detailed_facts": mece_facts,
                    "constraints_preview": {
                        "hard_constraints_count": len(machine_readable.get("hard_constraints", [])),
                        "soft_constraints_count": len(machine_readable.get("soft_constraints", [])),
                        "preferences_count": len(machine_readable.get("preferences", []))
                    }
                }
                
                timestamp = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
                report_filename = f"facility_facts_report_{timestamp}.json"
                
                report_json_str = json.dumps(report_data, ensure_ascii=False, indent=2)
                report_json_bytes = report_json_str.encode('utf-8')
                
                st.download_button(
                    label="ğŸ“¥ ç¢ºèªç”¨ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (JSON)",
                    data=report_json_bytes,
                    file_name=report_filename,
                    mime="application/json",
                    use_container_width=True
                )
                
                st.success(f"ç¢ºèªç”¨ãƒ¬ãƒãƒ¼ãƒˆãŒæº–å‚™ã•ã‚Œã¾ã—ãŸ: {report_filename}")
                st.info("ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯äººé–“ã«ã‚ˆã‚‹ç¢ºèªãƒ»æ¤œè¨¼ç”¨ã§ã™ã€‚")


def display_ppt_tab(tab_container, data_dir_ignored, key_prefix: str = ""):
    with tab_container:
        st.subheader(_("PPT Report"))
        button_key = f"dash_generate_ppt_button_{key_prefix or 'default'}"
        if st.button(
            _("Generate PowerPoint Report (Î²)"),
            key=button_key,
            use_container_width=True,
        ):
            st.info(_("Generating PowerPoint report..."))
            try:
                from shift_suite.tasks.ppt import build_ppt

                ppt_path = build_ppt(data_dir_ignored)
                with open(ppt_path, "rb") as ppt_file_data_dash:
                    st.download_button(
                        label=_("Download Report (PPTX)"),
                        data=ppt_file_data_dash,
                        file_name=f"ShiftSuite_Report_{dt.datetime.now().strftime('%Y%m%d_%H%M')}.pptx",
                        mime="application/vnd.openxmlformats-officedocument.presentationml.presentation",
                        use_container_width=True,
                    )
                Path(ppt_path).unlink(missing_ok=True)
                st.success(_("PowerPoint report ready."))
            except ImportError as e:
                log_and_display_error("python-pptx library required for PPT", e)
            except Exception as e_ppt_dash:
                log_and_display_error(
                    _("Error generating PowerPoint report"), e_ppt_dash
                )
        else:
            st.markdown(_("Click button to generate report."))


# Multi-file results display ã®ç›´å‰ã«è¿½åŠ 
if st.session_state.get("analysis_done", False):
    with st.expander("ğŸ” ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
        col1, col2 = st.columns(2)

        with col1:
            st.write("**ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆçŠ¶æ³:**")
            st.write(f"- analysis_done: {st.session_state.get('analysis_done', False)}")
            st.write(
                f"- out_dir_path_str: {st.session_state.get('out_dir_path_str', 'None')}"
            )
            st.write(
                f"- analysis_resultsä»¶æ•°: {len(st.session_state.get('analysis_results', {}))}"
            )
            st.write(
                f"- display_dataä»¶æ•°: {len(st.session_state.get('display_data', {}))}"
            )

        with col2:
            st.write("**åˆ©ç”¨å¯èƒ½ãªdisplay_dataã‚­ãƒ¼:**")
            display_keys = list(st.session_state.get("display_data", {}).keys())
            if display_keys:
                for key in display_keys:
                    data = st.session_state.display_data[key]
                    if hasattr(data, "shape"):
                        st.write(f"- {key}: {data.shape}")
                    else:
                        st.write(f"- {key}: {type(data).__name__}")
            else:
                st.write("ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

        if st.button("ğŸ”„ display_dataã‚’å¼·åˆ¶æ›´æ–°"):
            st.session_state.display_data = {}
            st.rerun()

# Multi-file results display
if st.session_state.get("analysis_done", False) and st.session_state.analysis_results:
    st.divider()
    file_tabs = st.tabs(list(st.session_state.analysis_results.keys()))
    for tab_obj, fname in zip(file_tabs, st.session_state.analysis_results.keys()):
        with tab_obj:
            results = st.session_state.analysis_results[fname]
            log.debug("Display results for %s: type=%s", fname, type(results))
            st.subheader(_("Results for {fname}").format(fname=fname))
            out_dir_path = (
                results.get("out_dir_path_str") if isinstance(results, dict) else None
            )
            if not out_dir_path:
                if fname == "preview" and isinstance(results, pd.DataFrame):
                    st.dataframe(results, use_container_width=True)
                else:
                    st.error(
                        _("Output directory not found for {fname}").format(fname=fname)
                    )
                    log.warning(
                        "Missing out_dir_path_str for results '%s': %s", fname, results
                    )
                continue
            data_dir = Path(out_dir_path)
            tab_keys_en_dash = [
                "Mind Reader",
                "MECE Facts",
                "12è»¸åˆ¶ç´„ç™ºè¦‹",
                "Overview",
                "Heatmap",
                "Shortage",
                "Advanced Shortage",  # é«˜åº¦ä¸è¶³åˆ†æè¿½åŠ 
                "Optimization Analysis",
                "Fatigue",
                "Forecast",
                "Fairness",
                "Leave Analysis",
                "åŸºæº–ä¹–é›¢åˆ†æ",
                "Cost Analysis",
                "Hire Plan",
                "Summary Report",
                "PPT Report",
            ]
            tab_labels_dash = [_(key) for key in tab_keys_en_dash]
            inner_tabs = st.tabs(tab_labels_dash)
            tab_func_map_dash = {
                "Mind Reader": display_mind_reader_tab,
                "MECE Facts": display_mece_facts_tab,
                "12è»¸åˆ¶ç´„ç™ºè¦‹": display_constraint_discovery_tab,
                "Overview": display_overview_tab,
                "Heatmap": display_heatmap_tab,
                "Shortage": display_shortage_tab,
                "Advanced Shortage": display_advanced_shortage_tab,  # é«˜åº¦ä¸è¶³åˆ†æè¿½åŠ 
                "Optimization Analysis": display_optimization_tab,
                "Fatigue": display_fatigue_tab,
                "Forecast": display_forecast_tab,
                "Fairness": display_fairness_tab,
                "Leave Analysis": display_leave_analysis_tab,
                "åŸºæº–ä¹–é›¢åˆ†æ": display_gap_analysis_tab,
                "Cost Analysis": display_cost_tab,
                "Hire Plan": display_hireplan_tab,
                "Summary Report": display_summary_report_tab,
                "PPT Report": display_ppt_tab,
            }
            for i, key in enumerate(tab_keys_en_dash):
                if key in tab_func_map_dash:
                    if key == "PPT Report":
                        tab_func_map_dash[key](
                            inner_tabs[i], data_dir, key_prefix=fname
                        )
                    elif key == "Leave Analysis":
                        tab_func_map_dash[key](
                            inner_tabs[i], results.get("leave_analysis_results") if results else None
                        )
                    else:
                        tab_func_map_dash[key](inner_tabs[i], data_dir)


st.divider()
st.header(_("Dashboard (Upload ZIP)"))
zip_file_uploaded_dash_final_v3_display_main_dash = st.file_uploader(
    _("Upload ZIP file of 'out' folder"),
    type=["zip"],
    key="dashboard_zip_uploader_widget_final_v3_key_dash",
)

if zip_file_uploaded_dash_final_v3_display_main_dash:
    dashboard_temp_base = Path(tempfile.gettempdir()) / "ShiftSuite_Dash_Uploads"
    dashboard_temp_base.mkdir(parents=True, exist_ok=True)
    current_dash_tmp_dir = Path(
        tempfile.mkdtemp(prefix="dash_", dir=dashboard_temp_base)
    )
    log.info(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ: {current_dash_tmp_dir}")
    extracted_data_dir: Optional[Path] = None
    try:
        with zipfile.ZipFile(
            io.BytesIO(zip_file_uploaded_dash_final_v3_display_main_dash.read())
        ) as zf:
            base_resolved = current_dash_tmp_dir.resolve()
            for file_name in zf.namelist():
                dest_path = (current_dash_tmp_dir / file_name).resolve()
                if not dest_path.is_relative_to(base_resolved):
                    st.error(_("Invalid path in ZIP file."))
                    log.warning(f"ZIPå±•é–‹ä¸­ã«ä¸æ­£ãªãƒ‘ã‚¹ã‚’æ¤œå‡º: {file_name}")
                    st.stop()
                zf.extract(file_name, current_dash_tmp_dir)
            if (current_dash_tmp_dir / "out").exists() and (
                current_dash_tmp_dir / "out" / "heat_ALL.parquet"
            ).exists():
                extracted_data_dir = current_dash_tmp_dir / "out"
            elif (current_dash_tmp_dir / "heat_ALL.parquet").exists():
                extracted_data_dir = current_dash_tmp_dir
            else:
                found_heat_all = list(current_dash_tmp_dir.rglob("heat_ALL.parquet"))
                if found_heat_all:
                    extracted_data_dir = found_heat_all[0].parent
                else:
                    log_and_display_error(
                        _("heat_ALL.parquet not found in ZIP"),
                        FileNotFoundError("heat_ALL.parquet"),
                    )
                    log.error(
                        f"ZIPå±•é–‹å¾Œã€heat_ALL.parquet ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ in {current_dash_tmp_dir}"
                    )
                    st.stop()
        log.info(f"ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤ºç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {extracted_data_dir}")
    except Exception as e_zip:
        log_and_display_error(_("Error during ZIP file extraction"), e_zip)
        log.error(f"ZIPå±•é–‹ä¸­ã‚¨ãƒ©ãƒ¼: {e_zip}", exc_info=True)
        st.stop()

    import plotly.express as px
    import plotly.graph_objects as go

    tab_keys_en_dash = [
        "Mind Reader",
        "MECE Facts",
        "12è»¸åˆ¶ç´„ç™ºè¦‹",
        "Overview",
        "Heatmap",
        "Shortage",
        "Advanced Shortage",  # æ–°ã—ã„é«˜åº¦åˆ†æã‚¿ãƒ–
        "Optimization Analysis",
        "Fatigue",
        "Forecast",
        "Fairness",
        "Leave Analysis",
        "åŸºæº–ä¹–é›¢åˆ†æ",
        "Cost Analysis",
        "Hire Plan",
        "Summary Report",
        "PPT Report",
    ]
    tab_labels_dash = [_(key) for key in tab_keys_en_dash]
    tabs_obj_dash = st.tabs(tab_labels_dash)

    if extracted_data_dir:
        tab_function_map_dash = {
            "Mind Reader": display_mind_reader_tab,
            "MECE Facts": display_mece_facts_tab,
            "12è»¸åˆ¶ç´„ç™ºè¦‹": display_constraint_discovery_tab,
            "Overview": display_overview_tab,
            "Heatmap": display_heatmap_tab,
            "Shortage": display_shortage_tab,
            "Advanced Shortage": display_advanced_shortage_tab if ADVANCED_SHORTAGE_AVAILABLE else lambda tab, data: st.warning("é«˜åº¦åˆ†ææ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“"),
            "Optimization Analysis": display_optimization_tab,
            "Fatigue": display_fatigue_tab,
            "Forecast": display_forecast_tab,
            "Fairness": display_fairness_tab,
            "Leave Analysis": display_leave_analysis_tab,
            "åŸºæº–ä¹–é›¢åˆ†æ": display_gap_analysis_tab,
            "Cost Analysis": display_cost_tab,
            "Hire Plan": display_hireplan_tab,
            "Summary Report": display_summary_report_tab,
            "PPT Report": display_ppt_tab,
        }

        # å„ã‚¿ãƒ–ã«å¯¾å¿œã™ã‚‹è¡¨ç¤ºé–¢æ•°ã‚’å‘¼ã³å‡ºã™
        for i, tab_key in enumerate(tab_keys_en_dash):
            if tab_key in tab_function_map_dash:
                if tab_key == "PPT Report":
                    tab_function_map_dash[tab_key](
                        tabs_obj_dash[i], extracted_data_dir, key_prefix="zip"
                    )
                elif tab_key == "Leave Analysis":
                    leave_results_zip = load_leave_results_from_dir(extracted_data_dir)
                    tab_function_map_dash[tab_key](tabs_obj_dash[i], leave_results_zip)
                else:
                    tab_function_map_dash[tab_key](tabs_obj_dash[i], extracted_data_dir)
            else:
                with tabs_obj_dash[i]:
                    st.subheader(_(tab_key))
                    st.info(f"{_(tab_key)} ã®è¡¨ç¤ºã¯ç¾åœ¨æº–å‚™ä¸­ã§ã™ã€‚")

        st.success("âœ… åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        st.info("ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€çµæœã‚’é«˜é€Ÿãƒ“ãƒ¥ãƒ¼ã‚¢ã§å¿«é©ã«é–²è¦§ã§ãã¾ã™ã€‚")
        st.markdown(
            "### [ğŸ“ˆ åˆ†æçµæœã‚’é«˜é€Ÿãƒ“ãƒ¥ãƒ¼ã‚¢ã§è¡¨ç¤ºã™ã‚‹](http://127.0.0.1:8050)",
            unsafe_allow_html=True,
        )
        st.caption(
            "ï¼ˆæ³¨æ„: ä¸Šè¨˜ãƒªãƒ³ã‚¯ã‚’åˆ©ç”¨ã™ã‚‹ã«ã¯ã€äº‹å‰ã«åˆ¥ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ `python dash_app.py` ã‚’å®Ÿè¡Œã—ã¦ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚’èµ·å‹•ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰"
        )
        if st.button("é«˜é€Ÿãƒ“ãƒ¥ãƒ¼ã‚¢ã‚’èµ·å‹•ã™ã‚‹"):
            try:
                subprocess.Popen(["python", "dash_app.py"])
                st.toast(
                    "ãƒ“ãƒ¥ãƒ¼ã‚¢ã‚’æ–°ã—ã„ãƒ—ãƒ­ã‚»ã‚¹ã§èµ·å‹•ã—ã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://127.0.0.1:8050 ã‚’é–‹ã„ã¦ãã ã•ã„ã€‚"
                )
            except Exception as e:
                st.error(f"ãƒ“ãƒ¥ãƒ¼ã‚¢ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
    else:
        st.warning(
            "ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚"
        )

if __name__ == "__main__" and not st_runtime_exists():
    import argparse

    log.info("CLIãƒ¢ãƒ¼ãƒ‰ã§app.pyã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
    parser = argparse.ArgumentParser(
        description="Shift-Suite CLI (app.pyçµŒç”±ã®ãƒ‡ãƒãƒƒã‚°ç”¨)"
    )
    parser.add_argument("xlsx_file_cli", help="Excel ã‚·ãƒ•ãƒˆåŸæœ¬ (.xlsx)")
    parser.add_argument(
        "--sheets_cli", nargs="+", required=True, help="è§£æå¯¾è±¡ã®ã‚·ãƒ¼ãƒˆå"
    )
    parser.add_argument(
        "--header_cli", type=int, default=3, help="ãƒ˜ãƒƒãƒ€ãƒ¼é–‹å§‹è¡Œ (1-indexed)"
    )
    try:
        cli_args = parser.parse_args()
        log.info(
            f"CLI Args: file='{cli_args.xlsx_file_cli}', sheets={cli_args.sheets_cli}, header={cli_args.header_cli}"
        )
    except SystemExit:
        pass
    except Exception as e_cli:
        log.error(f"CLIãƒ¢ãƒ¼ãƒ‰ã§ã®å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼: {e_cli}", exc_info=True)
