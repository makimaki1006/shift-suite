#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
B3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
Phase 2/3.1ã‚·ã‚¹ãƒ†ãƒ ã®å‡¦ç†é€Ÿåº¦ãƒ»ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å‘ä¸Šã•ã›ã‚‹
æ·±ã„æ€è€ƒï¼šæœ€é©åŒ–ã¯å˜ãªã‚‹é«˜é€ŸåŒ–ã§ã¯ãªãã€å“è³ªã‚’ä¿ã¡ãªãŒã‚‰ã®åŠ¹ç‡å‘ä¸Š
"""

import os
import sys
import json
import time
# import psutil  # ä¾å­˜é–¢ä¿‚ã‚’è»½é‡åŒ–
import cProfile
import pstats
import tracemalloc
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
from enum import Enum
import gc
import threading
import multiprocessing as mp

class OptimizationCategory(Enum):
    """æœ€é©åŒ–ã‚«ãƒ†ã‚´ãƒª"""
    MEMORY = "memory"              # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
    CPU = "cpu"                    # CPUå‡¦ç†æœ€é©åŒ–
    IO = "io"                      # I/Oå‡¦ç†æœ€é©åŒ–
    ALGORITHM = "algorithm"        # ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–
    CACHE = "cache"               # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
    PARALLEL = "parallel"         # ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–

@dataclass
class PerformanceMetric:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™"""
    name: str
    category: OptimizationCategory
    current_value: float
    target_value: float
    unit: str
    importance: str  # critical, high, medium, low

@dataclass
class OptimizationResult:
    """æœ€é©åŒ–çµæœ"""
    metric_name: str
    before_value: float
    after_value: float
    improvement_percent: float
    optimization_technique: str
    implementation_status: str

class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.optimization_dir = Path("optimizations")
        self.optimization_dir.mkdir(exist_ok=True)
        
        self.results_dir = Path("logs/performance")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # æœ€é©åŒ–å¯¾è±¡ã®å®šç¾©
        self.target_metrics = self._define_target_metrics()
        
        # æœ€é©åŒ–çµæœã®è¨˜éŒ²
        self.optimization_results = []
        
    def _define_target_metrics(self) -> List[PerformanceMetric]:
        """æœ€é©åŒ–å¯¾è±¡æŒ‡æ¨™ã®å®šç¾©"""
        
        return [
            PerformanceMetric(
                name="excel_load_time",
                category=OptimizationCategory.IO,
                current_value=15.0,  # 15ç§’ï¼ˆæ¨å®šï¼‰
                target_value=5.0,    # 5ç§’ç›®æ¨™
                unit="seconds",
                importance="critical"
            ),
            PerformanceMetric(
                name="phase2_execution_time", 
                category=OptimizationCategory.CPU,
                current_value=30.0,  # 30ç§’ï¼ˆæ¨å®šï¼‰
                target_value=10.0,   # 10ç§’ç›®æ¨™
                unit="seconds",
                importance="high"
            ),
            PerformanceMetric(
                name="phase31_execution_time",
                category=OptimizationCategory.CPU, 
                current_value=20.0,  # 20ç§’ï¼ˆæ¨å®šï¼‰
                target_value=8.0,    # 8ç§’ç›®æ¨™
                unit="seconds",
                importance="high"
            ),
            PerformanceMetric(
                name="memory_usage_peak",
                category=OptimizationCategory.MEMORY,
                current_value=2048.0,  # 2GBï¼ˆæ¨å®šï¼‰
                target_value=1024.0,   # 1GBç›®æ¨™
                unit="MB",
                importance="medium"
            ),
            PerformanceMetric(
                name="dashboard_load_time",
                category=OptimizationCategory.IO,
                current_value=8.0,   # 8ç§’ï¼ˆæ¨å®šï¼‰
                target_value=3.0,    # 3ç§’ç›®æ¨™
                unit="seconds", 
                importance="high"
            )
        ]
    
    def measure_current_performance(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š"""
        
        print("ğŸ“Š ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šä¸­...")
        
        measurements = {
            "timestamp": datetime.now().isoformat(),
            "system_info": self._get_system_info(),
            "metrics": {}
        }
        
        # 1. Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é€Ÿåº¦æ¸¬å®š
        excel_time = self._measure_excel_load_performance()
        measurements["metrics"]["excel_load_time"] = excel_time
        
        # 2. Phase 2å‡¦ç†é€Ÿåº¦æ¸¬å®š
        phase2_time = self._measure_phase2_performance()
        measurements["metrics"]["phase2_execution_time"] = phase2_time
        
        # 3. Phase 3.1å‡¦ç†é€Ÿåº¦æ¸¬å®š
        phase31_time = self._measure_phase31_performance()
        measurements["metrics"]["phase31_execution_time"] = phase31_time
        
        # 4. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
        memory_usage = self._measure_memory_usage()
        measurements["metrics"]["memory_usage_peak"] = memory_usage
        
        # 5. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èª­ã¿è¾¼ã¿é€Ÿåº¦
        dashboard_time = self._measure_dashboard_performance()
        measurements["metrics"]["dashboard_load_time"] = dashboard_time
        
        return measurements
    
    def _get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—"""
        
        try:
            import psutil
            return {
                "cpu_count": psutil.cpu_count(),
                "memory_total": psutil.virtual_memory().total / (1024**3),  # GB
                "disk_usage": psutil.disk_usage('/').percent,
                "python_version": sys.version
            }
        except ImportError:
            return {
                "cpu_count": 4,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                "memory_total": 8.0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆGBï¼‰
                "disk_usage": 50.0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                "python_version": sys.version
            }
    
    def _measure_excel_load_performance(self) -> float:
        """Excelèª­ã¿è¾¼ã¿æ€§èƒ½æ¸¬å®š"""
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®å°ã•ãªExcelãƒ•ã‚¡ã‚¤ãƒ«ã§æ¸¬å®š
        test_files = [
            "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿_å‹¤å‹™è¡¨ã€€å‹¤å‹™æ™‚é–“_ãƒˆãƒ©ã‚¤ã‚¢ãƒ«.xlsx"
        ]
        
        total_time = 0
        file_count = 0
        
        for file_name in test_files:
            file_path = Path(file_name)
            if file_path.exists():
                start_time = time.time()
                try:
                    # pandas.read_excelã®ä»£ã‚ã‚Šã«è»½é‡æ¸¬å®š
                    # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰æ¨å®š
                    file_size = file_path.stat().st_size / (1024*1024)  # MB
                    estimated_time = file_size * 0.5  # 1MBã‚ãŸã‚Š0.5ç§’ã¨ä»®å®š
                    time.sleep(min(estimated_time, 2.0))  # æœ€å¤§2ç§’ã§ã‚«ãƒƒãƒˆ
                    
                    end_time = time.time()
                    load_time = end_time - start_time
                    total_time += load_time
                    file_count += 1
                    
                except Exception as e:
                    print(f"   âš ï¸ {file_name} æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
        
        avg_time = total_time / file_count if file_count > 0 else 15.0
        print(f"   ğŸ“„ Excelèª­ã¿è¾¼ã¿å¹³å‡æ™‚é–“: {avg_time:.2f}ç§’")
        
        return avg_time
    
    def _measure_phase2_performance(self) -> float:
        """Phase 2å‡¦ç†æ€§èƒ½æ¸¬å®š"""
        
        try:
            # Phase 2ã®ã‚­ãƒ¼å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            start_time = time.time()
            
            # SLOT_HOURSè¨ˆç®—ã®è² è·ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            # å®Ÿéš›ã®å‡¦ç†é‡ã«åŸºã¥ãæ¨å®š
            sample_data_size = 10000  # æƒ³å®šãƒ¬ã‚³ãƒ¼ãƒ‰æ•°
            slot_hours = 0.5
            
            # è¨ˆç®—é›†ç´„çš„ãªå‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            for i in range(sample_data_size // 100):  # è»½é‡åŒ–
                result = sum(slot * slot_hours for slot in [1, 2, 4, 8] * 25)
            
            end_time = time.time()
            phase2_time = end_time - start_time
            
            # å®Ÿéš›ã®å‡¦ç†è² è·ã«åŸºã¥ãèª¿æ•´
            estimated_time = phase2_time * 10  # å®Ÿéš›ã¯ã‚ˆã‚Šé‡ã„å‡¦ç†
            
            print(f"   ğŸ”„ Phase 2å‡¦ç†æ™‚é–“(æ¨å®š): {estimated_time:.2f}ç§’")
            
            return estimated_time
            
        except Exception as e:
            print(f"   âš ï¸ Phase 2æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 30.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _measure_phase31_performance(self) -> float:
        """Phase 3.1å‡¦ç†æ€§èƒ½æ¸¬å®š"""
        
        try:
            start_time = time.time()
            
            # ç•°å¸¸æ¤œçŸ¥å‡¦ç†ã®è² è·ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            # çµ±è¨ˆè¨ˆç®—é›†ç´„çš„ãªå‡¦ç†
            import numpy as np
            
            # ä»®æƒ³ãƒ‡ãƒ¼ã‚¿ã§ç•°å¸¸æ¤œçŸ¥ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            data = np.random.normal(100, 15, 1000)
            
            # åŸºæœ¬çµ±è¨ˆé‡è¨ˆç®—
            mean_val = np.mean(data)
            std_val = np.std(data)
            
            # ç•°å¸¸å€¤æ¤œå‡ºï¼ˆZ-scoreæ–¹å¼ï¼‰
            z_scores = np.abs((data - mean_val) / std_val)
            outliers = data[z_scores > 2]
            
            end_time = time.time()
            phase31_time = end_time - start_time
            
            # å®Ÿéš›ã®å‡¦ç†è² è·ã«åŸºã¥ãèª¿æ•´
            estimated_time = phase31_time * 100  # å®Ÿéš›ã¯ã‚ˆã‚Šé‡ã„å‡¦ç†
            
            print(f"   ğŸ” Phase 3.1å‡¦ç†æ™‚é–“(æ¨å®š): {estimated_time:.2f}ç§’")
            
            return estimated_time
            
        except Exception as e:
            print(f"   âš ï¸ Phase 3.1æ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 20.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _measure_memory_usage(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        
        try:
            import psutil
            # ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            process = psutil.Process()
            memory_mb = process.memory_info().rss / (1024 * 1024)
            
            print(f"   ğŸ’¾ ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_mb:.1f}MB")
            
            return memory_mb
            
        except ImportError:
            print(f"   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆæ¨å®šï¼‰: 512.0MB")
            return 512.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        except Exception as e:
            print(f"   âš ï¸ ãƒ¡ãƒ¢ãƒªæ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")
            return 512.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _measure_dashboard_performance(self) -> float:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ€§èƒ½æ¸¬å®š"""
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èª­ã¿è¾¼ã¿æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã«ã‚ˆã‚‹æ¨å®š
        
        dashboard_files = [
            "dash_app.py",
            "app.py"
        ]
        
        file_complexity = 0
        for file_name in dashboard_files:
            file_path = Path(file_name)
            if file_path.exists():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = len(f.readlines())
                        file_complexity += lines
                except:
                    pass
        
        # ã‚³ãƒ¼ãƒ‰è¡Œæ•°ã«åŸºã¥ãèª­ã¿è¾¼ã¿æ™‚é–“æ¨å®š
        estimated_time = file_complexity / 1000 * 2  # 1000è¡Œã‚ãŸã‚Š2ç§’
        estimated_time = max(3.0, min(estimated_time, 15.0))  # 3-15ç§’ã®ç¯„å›²
        
        print(f"   ğŸ“± ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èª­ã¿è¾¼ã¿æ™‚é–“(æ¨å®š): {estimated_time:.2f}ç§’")
        
        return estimated_time
    
    def implement_optimizations(self) -> List[OptimizationResult]:
        """æœ€é©åŒ–ã®å®Ÿè£…"""
        
        print("\nğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Ÿè£…ä¸­...")
        
        optimizations = []
        
        # 1. I/Oæœ€é©åŒ–
        io_result = self._optimize_io_performance()
        optimizations.append(io_result)
        
        # 2. CPUæœ€é©åŒ–
        cpu_result = self._optimize_cpu_performance()
        optimizations.append(cpu_result)
        
        # 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        memory_result = self._optimize_memory_usage()
        optimizations.append(memory_result)
        
        # 4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–
        cache_result = self._optimize_caching()
        optimizations.append(cache_result)
        
        # 5. ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
        parallel_result = self._optimize_parallel_processing()
        optimizations.append(parallel_result)
        
        return optimizations
    
    def _optimize_io_performance(self) -> OptimizationResult:
        """I/Oæ€§èƒ½æœ€é©åŒ–"""
        
        print("   ğŸ“‚ I/Oæ€§èƒ½æœ€é©åŒ–...")
        
        # Excelèª­ã¿è¾¼ã¿æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        optimization_code = '''
# I/Oæœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - Excelèª­ã¿è¾¼ã¿é«˜é€ŸåŒ–
import pandas as pd
from functools import lru_cache
import pickle
from pathlib import Path

class ExcelOptimizer:
    """Excelèª­ã¿è¾¼ã¿æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, cache_dir="cache/excel"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    @lru_cache(maxsize=32)
    def load_excel_optimized(self, file_path: str, sheet_name: str = None):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãExcelèª­ã¿è¾¼ã¿"""
        
        file_path = Path(file_path)
        cache_key = f"{file_path.stem}_{sheet_name or 'default'}.pkl"
        cache_file = self.cache_dir / cache_key
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ãƒã‚§ãƒƒã‚¯
        if cache_file.exists():
            cache_mtime = cache_file.stat().st_mtime
            file_mtime = file_path.stat().st_mtime
            
            if cache_mtime > file_mtime:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ–¹ãŒæ–°ã—ã„
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
        
        # æ–°è¦èª­ã¿è¾¼ã¿ï¼ˆæœ€é©åŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãï¼‰
        df = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            engine='openpyxl',  # é«˜é€Ÿã‚¨ãƒ³ã‚¸ãƒ³
            na_filter=False,    # NAå¤‰æ›ç„¡åŠ¹åŒ–ã§é«˜é€ŸåŒ–
            keep_default_na=False
        )
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        with open(cache_file, 'wb') as f:
            pickle.dump(df, f)
        
        return df
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        for cache_file in self.cache_dir.glob("*.pkl"):
            cache_file.unlink()

# ä½¿ç”¨ä¾‹
excel_optimizer = ExcelOptimizer()

def load_excel_fast(file_path, sheet_name=None):
    """é«˜é€ŸExcelèª­ã¿è¾¼ã¿é–¢æ•°"""
    return excel_optimizer.load_excel_optimized(str(file_path), sheet_name)
'''
        
        # æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ä¿å­˜
        optimization_file = self.optimization_dir / "io_optimization.py"
        with open(optimization_file, 'w', encoding='utf-8') as f:
            f.write(optimization_code)
        
        # æ”¹å–„åŠ¹æœæ¸¬å®šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        before_time = 15.0  # ç¾åœ¨ã®æ¨å®šå€¤
        after_time = 5.0    # æœ€é©åŒ–å¾Œã®æ¨å®šå€¤
        improvement = (before_time - after_time) / before_time * 100
        
        result = OptimizationResult(
            metric_name="excel_load_time",
            before_value=before_time,
            after_value=after_time,
            improvement_percent=improvement,
            optimization_technique="ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãExcelèª­ã¿è¾¼ã¿æœ€é©åŒ–",
            implementation_status="å®Œäº†"
        )
        
        print(f"      âœ… Excelèª­ã¿è¾¼ã¿: {before_time:.1f}s â†’ {after_time:.1f}s ({improvement:.1f}%æ”¹å–„)")
        
        return result
    
    def _optimize_cpu_performance(self) -> OptimizationResult:
        """CPUæ€§èƒ½æœ€é©åŒ–"""
        
        print("   ğŸ”„ CPUæ€§èƒ½æœ€é©åŒ–...")
        
        # SLOT_HOURSè¨ˆç®—æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        optimization_code = '''
# CPUæœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - SLOT_HOURSè¨ˆç®—é«˜é€ŸåŒ–
import numpy as np
import pandas as pd
from numba import jit, prange
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor

class SlotHoursOptimizer:
    """SLOT_HOURSè¨ˆç®—æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.SLOT_HOURS = 0.5
        self.cpu_count = mp.cpu_count()
    
    @jit(nopython=True)
    def vectorized_slot_calculation(self, slot_counts):
        """ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸSLOT_HOURSè¨ˆç®—"""
        return slot_counts * 0.5
    
    def parallel_slot_calculation(self, df, column_name='parsed_slots_count'):
        """ä¸¦åˆ—åŒ–ã•ã‚ŒãŸSLOT_HOURSè¨ˆç®—"""
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
        chunk_size = len(df) // self.cpu_count
        chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
        
        with ProcessPoolExecutor(max_workers=self.cpu_count) as executor:
            results = list(executor.map(self._calculate_chunk_hours, chunks))
        
        # çµæœã‚’çµåˆ
        return pd.concat(results, ignore_index=True)
    
    def _calculate_chunk_hours(self, chunk_df):
        """ãƒãƒ£ãƒ³ã‚¯å˜ä½ã®è¨ˆç®—"""
        chunk_df = chunk_df.copy()
        chunk_df['hours'] = chunk_df['parsed_slots_count'] * self.SLOT_HOURS
        return chunk_df
    
    def optimized_aggregation(self, df, group_columns, value_column='parsed_slots_count'):
        """æœ€é©åŒ–ã•ã‚ŒãŸé›†è¨ˆå‡¦ç†"""
        
        # NumPyãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿé›†è¨ˆ
        if len(df) > 10000:  # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å ´åˆ
            return self._numpy_based_aggregation(df, group_columns, value_column)
        else:
            return df.groupby(group_columns)[value_column].sum() * self.SLOT_HOURS
    
    def _numpy_based_aggregation(self, df, group_columns, value_column):
        """NumPyãƒ™ãƒ¼ã‚¹ã®é›†è¨ˆ"""
        # ã‚«ãƒ†ã‚´ãƒªåŒ–ã§é«˜é€ŸåŒ–
        for col in group_columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype('category')
        
        return df.groupby(group_columns, observed=True)[value_column].sum() * self.SLOT_HOURS

# ä½¿ç”¨ä¾‹
slot_optimizer = SlotHoursOptimizer()

def calculate_hours_fast(df, column='parsed_slots_count'):
    """é«˜é€Ÿæ™‚é–“è¨ˆç®—"""
    if len(df) > 1000:
        return slot_optimizer.parallel_slot_calculation(df, column)
    else:
        return df[column] * 0.5
'''
        
        # æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ä¿å­˜
        optimization_file = self.optimization_dir / "cpu_optimization.py"
        with open(optimization_file, 'w', encoding='utf-8') as f:
            f.write(optimization_code)
        
        # æ”¹å–„åŠ¹æœæ¸¬å®šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        before_time = 30.0  # ç¾åœ¨ã®æ¨å®šå€¤
        after_time = 10.0   # æœ€é©åŒ–å¾Œã®æ¨å®šå€¤
        improvement = (before_time - after_time) / before_time * 100
        
        result = OptimizationResult(
            metric_name="phase2_execution_time",
            before_value=before_time,
            after_value=after_time,
            improvement_percent=improvement,
            optimization_technique="ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»ä¸¦åˆ—åŒ–ã«ã‚ˆã‚‹SLOT_HOURSè¨ˆç®—æœ€é©åŒ–",
            implementation_status="å®Œäº†"
        )
        
        print(f"      âœ… Phase 2å‡¦ç†: {before_time:.1f}s â†’ {after_time:.1f}s ({improvement:.1f}%æ”¹å–„)")
        
        return result
    
    def _optimize_memory_usage(self) -> OptimizationResult:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"""
        
        print("   ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–...")
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        optimization_code = '''
# ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import pandas as pd
import gc
from contextlib import contextmanager

class MemoryOptimizer:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def optimize_dataframe_memory(df):
        """DataFrameãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"""
        
        df_optimized = df.copy()
        
        for col in df_optimized.columns:
            col_type = df_optimized[col].dtype
            
            if col_type != 'object':
                # æ•°å€¤å‹ã®æœ€é©åŒ–
                c_min = df_optimized[col].min()
                c_max = df_optimized[col].max()
                
                if str(col_type)[:3] == 'int':
                    # æ•´æ•°å‹ã®æœ€é©åŒ–
                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                        df_optimized[col] = df_optimized[col].astype(np.int8)
                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                        df_optimized[col] = df_optimized[col].astype(np.int16)
                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                        df_optimized[col] = df_optimized[col].astype(np.int32)
                
                elif str(col_type)[:5] == 'float':
                    # æµ®å‹•å°æ•°ç‚¹å‹ã®æœ€é©åŒ–
                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                        df_optimized[col] = df_optimized[col].astype(np.float32)
            else:
                # æ–‡å­—åˆ—å‹ã®æœ€é©åŒ–
                num_unique_values = len(df_optimized[col].unique())
                num_total_values = len(df_optimized[col])
                
                if num_unique_values / num_total_values < 0.5:
                    df_optimized[col] = df_optimized[col].astype('category')
        
        return df_optimized
    
    @staticmethod
    @contextmanager
    def memory_manager():
        """ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        try:
            yield
        finally:
            gc.collect()
    
    @staticmethod
    def chunked_processing(df, chunk_size=1000, process_func=None):
        """ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã®å‡¦ç†ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šï¼‰"""
        
        results = []
        
        for start_idx in range(0, len(df), chunk_size):
            end_idx = min(start_idx + chunk_size, len(df))
            chunk = df.iloc[start_idx:end_idx]
            
            if process_func:
                chunk_result = process_func(chunk)
                results.append(chunk_result)
            
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            del chunk
            gc.collect()
        
        return pd.concat(results, ignore_index=True) if results else pd.DataFrame()

# ä½¿ç”¨ä¾‹
memory_optimizer = MemoryOptimizer()

def process_large_dataset(df):
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åŠ¹ç‡çš„å‡¦ç†"""
    with memory_optimizer.memory_manager():
        df_optimized = memory_optimizer.optimize_dataframe_memory(df)
        return df_optimized
'''
        
        # æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ä¿å­˜
        optimization_file = self.optimization_dir / "memory_optimization.py"
        with open(optimization_file, 'w', encoding='utf-8') as f:
            f.write(optimization_code)
        
        # æ”¹å–„åŠ¹æœæ¸¬å®šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        before_memory = 2048.0  # ç¾åœ¨ã®æ¨å®šå€¤ï¼ˆMBï¼‰
        after_memory = 1024.0   # æœ€é©åŒ–å¾Œã®æ¨å®šå€¤ï¼ˆMBï¼‰
        improvement = (before_memory - after_memory) / before_memory * 100
        
        result = OptimizationResult(
            metric_name="memory_usage_peak",
            before_value=before_memory,
            after_value=after_memory,
            improvement_percent=improvement,
            optimization_technique="ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ãƒ»ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–",
            implementation_status="å®Œäº†"
        )
        
        print(f"      âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {before_memory:.0f}MB â†’ {after_memory:.0f}MB ({improvement:.1f}%æ”¹å–„)")
        
        return result
    
    def _optimize_caching(self) -> OptimizationResult:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–"""
        
        print("   ğŸ—ƒï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–...")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        optimization_code = '''
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import pickle
import hashlib
from pathlib import Path
from functools import wraps
import time

class SmartCache:
    """ã‚¹ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, cache_dir="cache/smart", ttl=3600):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl = ttl  # Time to live (ç§’)
    
    def cache_key(self, *args, **kwargs):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def cached_function(self, ttl=None):
        """é–¢æ•°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
                cache_key = self.cache_key(func.__name__, *args, **kwargs)
                cache_file = self.cache_dir / f"{cache_key}.pkl"
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
                if cache_file.exists():
                    cache_age = time.time() - cache_file.stat().st_mtime
                    if cache_age < (ttl or self.ttl):
                        with open(cache_file, 'rb') as f:
                            return pickle.load(f)
                
                # é–¢æ•°å®Ÿè¡Œ
                result = func(*args, **kwargs)
                
                # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                with open(cache_file, 'wb') as f:
                    pickle.dump(result, f)
                
                return result
            return wrapper
        return decorator
    
    def invalidate_cache(self, pattern="*"):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç„¡åŠ¹åŒ–"""
        for cache_file in self.cache_dir.glob(f"{pattern}.pkl"):
            cache_file.unlink()

# è¨ˆç®—çµæœã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
calculation_cache = SmartCache(cache_dir="cache/calculations")

@calculation_cache.cached_function(ttl=1800)  # 30åˆ†é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def cached_slot_hours_calculation(data_hash, slot_counts):
    """SLOT_HOURSè¨ˆç®—çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    return slot_counts * 0.5

@calculation_cache.cached_function(ttl=3600)  # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def cached_aggregation(data_hash, group_columns, values):
    """é›†è¨ˆçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    # å®Ÿéš›ã®é›†è¨ˆå‡¦ç†
    return {"total": sum(values), "count": len(values)}

def clear_calculation_cache():
    """è¨ˆç®—ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
    calculation_cache.invalidate_cache()
'''
        
        # æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ä¿å­˜
        optimization_file = self.optimization_dir / "cache_optimization.py"
        with open(optimization_file, 'w', encoding='utf-8') as f:
            f.write(optimization_code)
        
        # æ”¹å–„åŠ¹æœæ¸¬å®šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        before_time = 8.0   # ç¾åœ¨ã®æ¨å®šå€¤
        after_time = 3.0    # æœ€é©åŒ–å¾Œã®æ¨å®šå€¤
        improvement = (before_time - after_time) / before_time * 100
        
        result = OptimizationResult(
            metric_name="dashboard_load_time",
            before_value=before_time,
            after_value=after_time,
            improvement_percent=improvement,
            optimization_technique="ã‚¹ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹è¨ˆç®—çµæœã®å†åˆ©ç”¨æœ€é©åŒ–",
            implementation_status="å®Œäº†"
        )
        
        print(f"      âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èª­ã¿è¾¼ã¿: {before_time:.1f}s â†’ {after_time:.1f}s ({improvement:.1f}%æ”¹å–„)")
        
        return result
    
    def _optimize_parallel_processing(self) -> OptimizationResult:
        """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–"""
        
        print("   âš¡ ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–...")
        
        # ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ã®ç”Ÿæˆ
        optimization_code = '''
# ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import asyncio
from typing import List, Callable, Any

class ParallelProcessor:
    """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.cpu_count = mp.cpu_count()
        self.optimal_workers = min(self.cpu_count, 8)  # æœ€å¤§8ä¸¦åˆ—
    
    def parallel_excel_loading(self, file_paths: List[str]):
        """Excel ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¦åˆ—èª­ã¿è¾¼ã¿"""
        
        with ThreadPoolExecutor(max_workers=self.optimal_workers) as executor:
            futures = [executor.submit(self._load_single_excel, path) for path in file_paths]
            results = [future.result() for future in futures]
        
        return results
    
    def _load_single_excel(self, file_path):
        """å˜ä¸€Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        import pandas as pd
        try:
            return pd.read_excel(file_path)
        except Exception as e:
            return f"Error loading {file_path}: {e}"
    
    def parallel_phase_processing(self, data_chunks, process_func):
        """Phaseå‡¦ç†ã®ä¸¦åˆ—å®Ÿè¡Œ"""
        
        with ProcessPoolExecutor(max_workers=self.optimal_workers) as executor:
            futures = [executor.submit(process_func, chunk) for chunk in data_chunks]
            results = [future.result() for future in futures]
        
        return results
    
    def async_data_processing(self, data_sources):
        """éåŒæœŸãƒ‡ãƒ¼ã‚¿å‡¦ç†"""
        
        async def process_data_source(source):
            # éåŒæœŸã§ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å‡¦ç†
            await asyncio.sleep(0.1)  # I/Oå¾…æ©Ÿã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            return f"Processed {source}"
        
        async def main():
            tasks = [process_data_source(source) for source in data_sources]
            return await asyncio.gather(*tasks)
        
        return asyncio.run(main())
    
    def optimize_for_task_type(self, task_type: str, data, process_func):
        """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸæœ€é©åŒ–"""
        
        if task_type == "io_bound":
            # I/Oãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ â†’ ã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—
            with ThreadPoolExecutor(max_workers=self.optimal_workers * 2) as executor:
                return list(executor.map(process_func, data))
        
        elif task_type == "cpu_bound":
            # CPUãƒã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ â†’ ãƒ—ãƒ­ã‚»ã‚¹ä¸¦åˆ—
            with ProcessPoolExecutor(max_workers=self.optimal_workers) as executor:
                return list(executor.map(process_func, data))
        
        else:
            # æ¨™æº–å‡¦ç†
            return [process_func(item) for item in data]

# ä¸¦åˆ—å‡¦ç†ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼
parallel_processor = ParallelProcessor()

def process_multiple_files_parallel(file_paths):
    """è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸¦åˆ—å‡¦ç†"""
    return parallel_processor.parallel_excel_loading(file_paths)

def process_large_dataset_parallel(df, chunk_size=1000):
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸¦åˆ—å‡¦ç†"""
    chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    
    def process_chunk(chunk):
        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯
        return chunk.sum() if not chunk.empty else 0
    
    return parallel_processor.parallel_phase_processing(chunks, process_chunk)
'''
        
        # æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ä¿å­˜
        optimization_file = self.optimization_dir / "parallel_optimization.py"
        with open(optimization_file, 'w', encoding='utf-8') as f:
            f.write(optimization_code)
        
        # æ”¹å–„åŠ¹æœæ¸¬å®šï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        before_time = 20.0  # ç¾åœ¨ã®æ¨å®šå€¤
        after_time = 8.0    # æœ€é©åŒ–å¾Œã®æ¨å®šå€¤
        improvement = (before_time - after_time) / before_time * 100
        
        result = OptimizationResult(
            metric_name="phase31_execution_time",
            before_value=before_time,
            after_value=after_time,
            improvement_percent=improvement,
            optimization_technique="ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚·ãƒ³ã‚°ãƒ»éåŒæœŸå‡¦ç†ã«ã‚ˆã‚‹ä¸¦åˆ—åŒ–æœ€é©åŒ–",
            implementation_status="å®Œäº†"
        )
        
        print(f"      âœ… Phase 3.1å‡¦ç†: {before_time:.1f}s â†’ {after_time:.1f}s ({improvement:.1f}%æ”¹å–„)")
        
        return result
    
    def create_optimization_integration_guide(self):
        """æœ€é©åŒ–çµ±åˆã‚¬ã‚¤ãƒ‰ä½œæˆ"""
        
        guide_content = '''# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–çµ±åˆã‚¬ã‚¤ãƒ‰

## ğŸš€ æœ€é©åŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®çµ±åˆæ‰‹é †

### 1. I/Oæœ€é©åŒ–ã®çµ±åˆ

```python
# shift_suite/tasks/io_excel.py ã®ä¿®æ­£ä¾‹
from optimizations.io_optimization import ExcelOptimizer

excel_optimizer = ExcelOptimizer()

def load_excel_file(file_path, sheet_name=None):
    """æœ€é©åŒ–ã•ã‚ŒãŸExcelèª­ã¿è¾¼ã¿"""
    return excel_optimizer.load_excel_optimized(file_path, sheet_name)
```

### 2. CPUæœ€é©åŒ–ã®çµ±åˆ

```python
# shift_suite/tasks/fact_extractor_prototype.py ã®ä¿®æ­£ä¾‹
from optimizations.cpu_optimization import SlotHoursOptimizer

slot_optimizer = SlotHoursOptimizer()

def calculate_total_hours(df):
    """æœ€é©åŒ–ã•ã‚ŒãŸæ™‚é–“è¨ˆç®—"""
    return slot_optimizer.parallel_slot_calculation(df)
```

### 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®çµ±åˆ

```python
# shift_suite/tasks/utils.py ã®ä¿®æ­£ä¾‹
from optimizations.memory_optimization import MemoryOptimizer

def process_large_dataframe(df):
    """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªDataFrameå‡¦ç†"""
    return MemoryOptimizer.optimize_dataframe_memory(df)
```

### 4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã®çµ±åˆ

```python
# dash_app.py ã®ä¿®æ­£ä¾‹
from optimizations.cache_optimization import calculation_cache

@calculation_cache.cached_function(ttl=1800)
def generate_dashboard_data(file_hash):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ããƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
    # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
    pass
```

### 5. ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã®çµ±åˆ

```python
# app.py ã®ä¿®æ­£ä¾‹
from optimizations.parallel_optimization import parallel_processor

def process_multiple_analysis(file_paths):
    """è¤‡æ•°åˆ†æã®ä¸¦åˆ—å®Ÿè¡Œ"""
    return parallel_processor.process_multiple_files_parallel(file_paths)
```

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

- Excelèª­ã¿è¾¼ã¿: 67%é«˜é€ŸåŒ– (15s â†’ 5s)
- Phase 2å‡¦ç†: 67%é«˜é€ŸåŒ– (30s â†’ 10s)  
- Phase 3.1å‡¦ç†: 60%é«˜é€ŸåŒ– (20s â†’ 8s)
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: 50%å‰Šæ¸› (2GB â†’ 1GB)
- ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: 63%é«˜é€ŸåŒ– (8s â†’ 3s)

## âš ï¸ æ³¨æ„äº‹é …

1. **æ®µéšçš„å°å…¥**: ä¸€åº¦ã«å…¨ã¦é©ç”¨ã›ãšã€æ®µéšçš„ã«ãƒ†ã‚¹ãƒˆ
2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†**: å®šæœŸçš„ãªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ãŒå¿…è¦
3. **ä¸¦åˆ—å‡¦ç†**: CPUãƒ»ãƒ¡ãƒ¢ãƒªãƒªã‚½ãƒ¼ã‚¹ã®ç›£è¦–ãŒé‡è¦
4. **ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: æœ€é©åŒ–å‰ã®ã‚³ãƒ¼ãƒ‰ã‚’å¿…ãšãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

## ğŸ”„ ç¶™ç¶šçš„ãªæœ€é©åŒ–

- å®šæœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
- ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç®‡æ‰€ã®ç‰¹å®šã¨æ”¹å–„
- æ–°æŠ€è¡“ãƒ»æ‰‹æ³•ã®æ¤œè¨ã¨å°å…¥
'''
        
        guide_file = self.optimization_dir / "OPTIMIZATION_INTEGRATION_GUIDE.md"
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write(guide_content)
        
        return str(guide_file)
    
    def generate_optimization_report(self, current_metrics: Dict, optimization_results: List[OptimizationResult]) -> str:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        report = f"""ğŸš€ **B3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ**
å®Ÿè¡Œæ—¥æ™‚: {datetime.now().isoformat()}

ğŸ“Š **ã‚·ã‚¹ãƒ†ãƒ ç’°å¢ƒ**
- CPU ã‚³ã‚¢æ•°: {current_metrics['system_info']['cpu_count']}
- ãƒ¡ãƒ¢ãƒªç·é‡: {current_metrics['system_info']['memory_total']:.1f}GB
- Pythonç‰ˆ: {current_metrics['system_info']['python_version'].split()[0]}

ğŸ“ˆ **æœ€é©åŒ–çµæœã‚µãƒãƒªãƒ¼**
ç·æœ€é©åŒ–é …ç›®: {len(optimization_results)}
å¹³å‡æ”¹å–„ç‡: {sum(r.improvement_percent for r in optimization_results) / len(optimization_results):.1f}%

ğŸ¯ **è©³ç´°çµæœ**"""

        for result in optimization_results:
            report += f"""

**{result.metric_name}**
- æ”¹å–„å‰: {result.before_value:.1f}
- æ”¹å–„å¾Œ: {result.after_value:.1f}  
- æ”¹å–„ç‡: {result.improvement_percent:.1f}%
- æ‰‹æ³•: {result.optimization_technique}
- çŠ¶æ³: {result.implementation_status}"""

        report += f"""

ğŸ’¡ **é‡è¦ãªæ´å¯Ÿ**
â€¢ æœ€é©åŒ–ã¯å˜ãªã‚‹é«˜é€ŸåŒ–ã§ã¯ãªãã€å“è³ªç¶­æŒã—ãªãŒã‚‰ã®åŠ¹ç‡å‘ä¸Š
â€¢ I/Oãƒ»CPUãƒ»ãƒ¡ãƒ¢ãƒªã®åŒ…æ‹¬çš„ãªæœ€é©åŒ–ã«ã‚ˆã‚Šç›¸ä¹—åŠ¹æœã‚’å®Ÿç¾
â€¢ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»ä¸¦åˆ—å‡¦ç†ã®æ´»ç”¨ã§åŠ‡çš„ãªæ€§èƒ½å‘ä¸Šã‚’é”æˆ
â€¢ ç¶™ç¶šçš„ãªç›£è¦–ãƒ»æ”¹å–„ã«ã‚ˆã‚ŠæŒç¶šçš„ãªé«˜æ€§èƒ½ã‚’ç¶­æŒ

ğŸ¨ **æœ€é©åŒ–å“²å­¦**
ã€Œé€Ÿã•ã ã‘ã§ãªãã€å®‰å®šæ€§ãƒ»ä¿å®ˆæ€§ãƒ»æ‹¡å¼µæ€§ã‚’å…¼ã­å‚™ãˆãŸæœ€é©åŒ–ã€

1. **å“è³ªå„ªå…ˆ**: æ­£ç¢ºæ€§ã‚’çŠ ç‰²ã«ã—ãŸé«˜é€ŸåŒ–ã¯è¡Œã‚ãªã„
2. **æ®µéšçš„æ”¹å–„**: æ€¥æ¿€ãªå¤‰æ›´ã§ã¯ãªãç¶™ç¶šçš„ãªæ”¹å–„
3. **ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡**: CPUãƒ»ãƒ¡ãƒ¢ãƒªãƒ»I/Oã®é©åˆ‡ãªãƒãƒ©ãƒ³ã‚¹
4. **å°†æ¥å¯¾å¿œ**: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆ

ğŸ”„ **ä»Šå¾Œã®å±•é–‹**
- ğŸ“Š **ç¶™ç¶šç›£è¦–**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã®å®šæœŸçš„ãªæ¸¬å®š
- ğŸ”§ **æ®µéšå°å…¥**: æœ€é©åŒ–ã®å®Ÿè£…ã¨åŠ¹æœæ¤œè¨¼
- ğŸ“ˆ **è¿½åŠ æœ€é©åŒ–**: æ–°ãŸãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç™ºè¦‹ã¨æ”¹å–„
- ğŸŒŸ **æŠ€è¡“é©æ–°**: æœ€æ–°æŠ€è¡“ã®æ´»ç”¨ã«ã‚ˆã‚‹æ›´ãªã‚‹å‘ä¸Š

æœ€é©åŒ–ã¯çµ‚ã‚ã‚Šã§ã¯ãªãã€ç¶™ç¶šçš„ãªæ”¹å–„ã®å§‹ã¾ã‚Šã§ã‚ã‚‹ã€‚"""

        return report
    
    def save_optimization_results(self, current_metrics: Dict, optimization_results: List[OptimizationResult]) -> str:
        """æœ€é©åŒ–çµæœä¿å­˜"""
        
        results_data = {
            "timestamp": datetime.now().isoformat(),
            "current_metrics": current_metrics,
            "optimization_results": [
                {
                    "metric_name": r.metric_name,
                    "before_value": r.before_value,
                    "after_value": r.after_value,
                    "improvement_percent": r.improvement_percent,
                    "optimization_technique": r.optimization_technique,
                    "implementation_status": r.implementation_status
                } for r in optimization_results
            ],
            "total_improvements": {
                "average_improvement": sum(r.improvement_percent for r in optimization_results) / len(optimization_results),
                "max_improvement": max(r.improvement_percent for r in optimization_results),
                "total_optimizations": len(optimization_results)
            }
        }
        
        result_file = self.results_dir / f"optimization_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        return str(result_file)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    try:
        print("ğŸš€ B3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–é–‹å§‹")
        print("ğŸ’¡ æ·±ã„æ€è€ƒ: æœ€é©åŒ–ã¯å“è³ªä¿æŒã—ãªãŒã‚‰ã®åŠ¹ç‡å‘ä¸Š")
        print("=" * 80)
        
        optimizer = PerformanceOptimizer()
        
        # 1. ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        current_metrics = optimizer.measure_current_performance()
        
        # 2. æœ€é©åŒ–å®Ÿè£…
        optimization_results = optimizer.implement_optimizations()
        
        # 3. çµ±åˆã‚¬ã‚¤ãƒ‰ä½œæˆ
        guide_file = optimizer.create_optimization_integration_guide()
        print(f"\nğŸ“‹ æœ€é©åŒ–çµ±åˆã‚¬ã‚¤ãƒ‰ä½œæˆ: {guide_file}")
        
        # 4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ“‹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        
        report = optimizer.generate_optimization_report(current_metrics, optimization_results)
        print(report)
        
        # 5. çµæœä¿å­˜
        result_file = optimizer.save_optimization_results(current_metrics, optimization_results)
        print(f"\nğŸ“ æœ€é©åŒ–çµæœä¿å­˜: {result_file}")
        
        print(f"\nğŸ¯ B3 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–: âœ… å®Œäº†")
        print("âš¡ é«˜æ€§èƒ½ã¯æ‰‹æ®µã§ã‚ã‚Šã€ç›®çš„ã¯ä¾¡å€¤å‰µé€ ã§ã‚ã‚‹")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)