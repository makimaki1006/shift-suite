#!/usr/bin/env python3
"""
å®Ÿç”¨åˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³ - Streamlitéä¾å­˜ç‰ˆ
ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ»APIãƒ»è»½é‡UIå…¨ã¦ã«å¯¾å¿œå¯èƒ½ãªã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³
"""

import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import argparse
import sys

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class PracticalConstraintEngine:
    """å®Ÿç”¨åˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆStreamlitéä¾å­˜ï¼‰"""
    
    def __init__(self):
        self.engine_name = "å®Ÿç”¨åˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³"
        self.version = "1.0.0"
        self.available_files = self._scan_excel_files()
        self.analysis_cache = {}
    
    def _scan_excel_files(self) -> List[str]:
        """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒ£ãƒ³"""
        excel_extensions = ['.xlsx', '.xls']
        current_dir = Path('.')
        
        excel_files = []
        for ext in excel_extensions:
            excel_files.extend([f.name for f in current_dir.glob(f'*{ext}')])
        
        return sorted(excel_files)
    
    def analyze_single_file(self, file_path: str) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶ç´„åˆ†æ"""
        if file_path in self.analysis_cache:
            log.info(f"Using cached analysis for {file_path}")
            return self.analysis_cache[file_path]
        
        try:
            path = Path(file_path)
            if not path.exists():
                return {"error": f"ãƒ•ã‚¡ã‚¤ãƒ« {file_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            file_size = path.stat().st_size
            filename = path.name
            
            constraints = []
            
            # 1. åŸºæœ¬å¯ç”¨æ€§åˆ¶ç´„
            constraints.append({
                "id": f"basic_availability_{hash(filename) % 10000}",
                "category": "ãƒ‡ãƒ¼ã‚¿å¯ç”¨æ€§",
                "type": "ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª",
                "constraint": f"{filename}ã¯åˆ†æå¯èƒ½ãªçŠ¶æ…‹ã§ã™",
                "details": {
                    "file_size": file_size,
                    "readable": True,
                    "format": path.suffix
                },
                "confidence": 1.0,
                "priority": "é«˜",
                "actionable": True,
                "recommendations": [f"{filename}ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ•ãƒˆåˆ¶ç´„åˆ†æã‚’å®Ÿè¡Œ"]
            })
            
            # 2. ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹åˆ¶ç´„
            if file_size > 200000:  # 200KBä»¥ä¸Š
                constraints.append({
                    "id": f"large_dataset_{hash(filename) % 10000}",
                    "category": "ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒ«",
                    "type": "å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º",
                    "constraint": f"{filename}ã¯å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™",
                    "details": {
                        "size_bytes": file_size,
                        "estimated_records": file_size // 100,  # æ¦‚ç®—
                        "processing_complexity": "é«˜"
                    },
                    "confidence": 0.9,
                    "priority": "ä¸­",
                    "actionable": True,
                    "recommendations": [
                        "æ®µéšçš„åˆ†æã®å®Ÿè¡Œ",
                        "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åˆ†æã®æ¤œè¨",
                        "å‡¦ç†æ™‚é–“ã¸ã®é…æ…®"
                    ]
                })
            elif file_size < 15000:  # 15KBæœªæº€
                constraints.append({
                    "id": f"small_dataset_{hash(filename) % 10000}",
                    "category": "ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚±ãƒ¼ãƒ«",
                    "type": "å°å®¹é‡ãƒ‡ãƒ¼ã‚¿æ¤œå‡º",
                    "constraint": f"{filename}ã¯ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã™",
                    "details": {
                        "size_bytes": file_size,
                        "estimated_records": file_size // 100,
                        "processing_complexity": "ä½"
                    },
                    "confidence": 0.85,
                    "priority": "ä½",
                    "actionable": True,
                    "recommendations": [
                        "é«˜é€Ÿåˆ†æãŒå¯èƒ½",
                        "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†é©ç”¨å¯èƒ½",
                        "ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—åˆ†æã«æœ€é©"
                    ]
                })
            
            # 3. ãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„
            name_patterns = {
                'ãƒ‡ã‚¤': {'shift_type': 'æ—¥å‹¤', 'time_focus': 'æ—¥ä¸­æ™‚é–“å¸¯', 'priority': 'é«˜'},
                'ã‚·ãƒ§ãƒ¼ãƒˆ': {'shift_type': 'çŸ­æ™‚é–“', 'time_focus': 'æŸ”è»Ÿæ™‚é–“', 'priority': 'ä¸­'},
                'ãƒŠã‚¤ãƒˆ': {'shift_type': 'å¤œå‹¤', 'time_focus': 'å¤œé–“æ™‚é–“å¸¯', 'priority': 'é«˜'},
                'å¤œå‹¤': {'shift_type': 'å¤œå‹¤', 'time_focus': 'å¤œé–“æ™‚é–“å¸¯', 'priority': 'é«˜'},
                'æ—¥å‹¤': {'shift_type': 'æ—¥å‹¤', 'time_focus': 'æ—¥ä¸­æ™‚é–“å¸¯', 'priority': 'é«˜'},
                'ãƒˆãƒ©ã‚¤ã‚¢ãƒ«': {'shift_type': 'è©¦è¡Œ', 'time_focus': 'å®Ÿé¨“çš„', 'priority': 'ä¸­'},
                'ãƒ†ã‚¹ãƒˆ': {'shift_type': 'ãƒ†ã‚¹ãƒˆ', 'time_focus': 'æ¤œè¨¼ç”¨', 'priority': 'ä¸­'}
            }
            
            detected_patterns = []
            for pattern, info in name_patterns.items():
                if pattern in filename:
                    detected_patterns.append((pattern, info))
                    
                    constraints.append({
                        "id": f"pattern_{pattern.lower()}_{hash(filename) % 10000}",
                        "category": "ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„",
                        "type": f"{info['shift_type']}ã‚·ãƒ•ãƒˆç‰¹åŒ–",
                        "constraint": f"{filename}ã¯{info['shift_type']}ã‚·ãƒ•ãƒˆã«ç‰¹åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã§ã™",
                        "details": {
                            "detected_pattern": pattern,
                            "shift_type": info['shift_type'],
                            "time_focus": info['time_focus'],
                            "specialization_level": "é«˜"
                        },
                        "confidence": 0.9,
                        "priority": info['priority'],
                        "actionable": True,
                        "recommendations": [
                            f"{info['shift_type']}ã‚·ãƒ•ãƒˆã®è©³ç´°åˆ†æã‚’å„ªå…ˆ",
                            f"{info['time_focus']}ã®åˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«æ³¨ç›®",
                            "ç‰¹åŒ–åˆ†æã«ã‚ˆã‚‹é«˜ç²¾åº¦åˆ¶ç´„ç™ºè¦‹æœŸå¾…"
                        ]
                    })
            
            # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ¶ç´„æ¨æ¸¬
            quality_indicators = []
            if 'backup' not in filename.lower() and 'old' not in filename.lower():
                quality_indicators.append("æœ€æ–°æ€§")
            if file_size > 20000:
                quality_indicators.append("å……å®Ÿæ€§")
            if any(pattern in filename for pattern in ['ãƒ†ã‚¹ãƒˆ', 'ãƒˆãƒ©ã‚¤ã‚¢ãƒ«']):
                quality_indicators.append("å®Ÿé¨“æ€§")
            else:
                quality_indicators.append("æœ¬æ ¼æ€§")
            
            if quality_indicators:
                constraints.append({
                    "id": f"quality_{hash(filename) % 10000}",
                    "category": "ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ¶ç´„",
                    "type": "ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡",
                    "constraint": f"{filename}ã¯{'ãƒ»'.join(quality_indicators)}ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã§ã™",
                    "details": {
                        "quality_indicators": quality_indicators,
                        "reliability_level": "é«˜" if "æœ¬æ ¼æ€§" in quality_indicators else "ä¸­",
                        "update_status": "æœ€æ–°" if "æœ€æ–°æ€§" in quality_indicators else "ä¸æ˜"
                    },
                    "confidence": 0.8,
                    "priority": "ä¸­",
                    "actionable": True,
                    "recommendations": [
                        f"{'ãƒ»'.join(quality_indicators)}ã‚’æ´»ã‹ã—ãŸåˆ†æè¨­è¨ˆ",
                        "å“è³ªç‰¹æ€§ã«é©ã—ãŸåˆ¶ç´„ç™ºè¦‹æ‰‹æ³•ã®é¸æŠ"
                    ]
                })
            
            # 5. å®Ÿç”¨æ€§ç·åˆåˆ¶ç´„
            utility_score = self._calculate_file_utility_score(file_size, filename, detected_patterns)
            
            constraints.append({
                "id": f"utility_{hash(filename) % 10000}",
                "category": "å®Ÿç”¨æ€§ç·åˆè©•ä¾¡",
                "type": "ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢",
                "constraint": f"{filename}ã®å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢ã¯{utility_score:.1f}%ã§ã™",
                "details": {
                    "utility_score": utility_score,
                    "utility_level": "é«˜" if utility_score >= 80 else "ä¸­" if utility_score >= 60 else "ä½",
                    "recommended_usage": self._get_usage_recommendation(utility_score)
                },
                "confidence": 0.95,
                "priority": "é«˜" if utility_score >= 80 else "ä¸­",
                "actionable": True,
                "recommendations": self._get_utility_recommendations(utility_score, filename)
            })
            
            # çµæœæ§‹é€ åŒ–
            result = {
                "success": True,
                "analysis_timestamp": datetime.now().isoformat(),
                "file_info": {
                    "name": filename,
                    "path": str(path),
                    "size_bytes": file_size,
                    "format": path.suffix
                },
                "constraints": constraints,
                "summary": {
                    "total_constraints": len(constraints),
                    "categories": len(set(c["category"] for c in constraints)),
                    "high_priority": len([c for c in constraints if c["priority"] == "é«˜"]),
                    "actionable_items": len([c for c in constraints if c["actionable"]]),
                    "avg_confidence": sum(c["confidence"] for c in constraints) / len(constraints),
                    "utility_score": utility_score
                }
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.analysis_cache[file_path] = result
            
            return result
            
        except Exception as e:
            error_result = {"error": f"åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"}
            self.analysis_cache[file_path] = error_result
            return error_result
    
    def _calculate_file_utility_score(self, file_size: int, filename: str, patterns: List[tuple]) -> float:
        """ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 50.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢
        
        # ã‚µã‚¤ã‚ºã‚¹ã‚³ã‚¢
        if 20000 <= file_size <= 500000:  # é©åˆ‡ãªã‚µã‚¤ã‚º
            score += 20
        elif file_size > 500000:  # å¤§ãã™ã
            score += 10
        elif file_size < 5000:  # å°ã•ã™ã
            score += 5
        else:
            score += 15
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¹ã‚³ã‚¢
        if patterns:
            score += min(25, len(patterns) * 8)  # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«å¿œã˜ã¦åŠ ç‚¹
        
        # å“è³ªã‚¹ã‚³ã‚¢
        if 'backup' not in filename.lower() and 'old' not in filename.lower():
            score += 15  # æœ€æ–°æ€§
        
        if any(keyword in filename for keyword in ['ãƒ†ã‚¹ãƒˆ', 'ãƒˆãƒ©ã‚¤ã‚¢ãƒ«']):
            score -= 5  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯å°‘ã—æ¸›ç‚¹
        
        return min(100.0, score)
    
    def _get_usage_recommendation(self, utility_score: float) -> str:
        """å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢ã«åŸºã¥ãä½¿ç”¨æ¨å¥¨"""
        if utility_score >= 80:
            return "ãƒ¡ã‚¤ãƒ³åˆ†æãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦å„ªå…ˆä½¿ç”¨æ¨å¥¨"
        elif utility_score >= 60:
            return "ã‚µãƒ–åˆ†æãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æœ‰åŠ¹æ´»ç”¨å¯èƒ½"
        else:
            return "è£œåŠ©ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦é™å®šçš„ä½¿ç”¨æ¨å¥¨"
    
    def _get_utility_recommendations(self, utility_score: float, filename: str) -> List[str]:
        """å®Ÿç”¨æ€§ã«åŸºã¥ãæ¨å¥¨äº‹é …"""
        recommendations = []
        
        if utility_score >= 80:
            recommendations.extend([
                f"{filename}ã‚’ä¸»è¦åˆ†æå¯¾è±¡ã¨ã—ã¦é¸æŠ",
                "è©³ç´°åˆ¶ç´„åˆ†æã®å®Ÿè¡Œ",
                "åˆ¶ç´„ç™ºè¦‹çµæœã®å®Ÿç”¨åŒ–æ¤œè¨"
            ])
        elif utility_score >= 60:
            recommendations.extend([
                f"{filename}ã‚’è£œå®Œåˆ†æã¨ã—ã¦æ´»ç”¨",
                "ç‰¹å®šã‚«ãƒ†ã‚´ãƒªåˆ¶ç´„ã®æ·±æ˜ã‚Šåˆ†æ",
                "ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æå®Ÿæ–½"
            ])
        else:
            recommendations.extend([
                f"{filename}ã¯å‚è€ƒãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä½¿ç”¨",
                "åŸºæœ¬åˆ¶ç´„ç¢ºèªã®ã¿å®Ÿæ–½",
                "ã‚ˆã‚Šé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¢ç´¢æ¨å¥¨"
            ])
        
        return recommendations
    
    def batch_analyze(self, file_list: Optional[List[str]] = None) -> Dict[str, Any]:
        """ãƒãƒƒãƒåˆ¶ç´„åˆ†æ"""
        if file_list is None:
            file_list = self.available_files
        
        if not file_list:
            return {"error": "åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        log.info(f"Starting batch analysis for {len(file_list)} files")
        
        individual_results = {}
        successful_analyses = 0
        total_constraints = 0
        total_actionable = 0
        confidence_scores = []
        utility_scores = []
        
        for file_path in file_list:
            log.info(f"Analyzing {file_path}...")
            result = self.analyze_single_file(file_path)
            individual_results[file_path] = result
            
            if result.get("success"):
                successful_analyses += 1
                summary = result["summary"]
                total_constraints += summary["total_constraints"]
                total_actionable += summary["actionable_items"]
                confidence_scores.append(summary["avg_confidence"])
                utility_scores.append(summary["utility_score"])
        
        # çµ±åˆåˆ†æ
        avg_confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 0
        avg_utility = sum(utility_scores) / len(utility_scores) if utility_scores else 0
        
        # ã‚«ãƒ†ã‚´ãƒªçµ±è¨ˆ
        category_stats = {}
        priority_stats = {"é«˜": 0, "ä¸­": 0, "ä½": 0}
        
        for result in individual_results.values():
            if result.get("success"):
                for constraint in result["constraints"]:
                    category = constraint["category"]
                    priority = constraint["priority"]
                    
                    if category not in category_stats:
                        category_stats[category] = 0
                    category_stats[category] += 1
                    priority_stats[priority] += 1
        
        batch_result = {
            "success": True,
            "analysis_timestamp": datetime.now().isoformat(),
            "batch_summary": {
                "files_processed": len(file_list),
                "successful_analyses": successful_analyses,
                "failed_analyses": len(file_list) - successful_analyses,
                "total_constraints": total_constraints,
                "total_actionable": total_actionable,
                "avg_confidence": avg_confidence,
                "avg_utility_score": avg_utility,
                "category_distribution": category_stats,
                "priority_distribution": priority_stats
            },
            "individual_results": individual_results,
            "recommendations": self._generate_batch_recommendations(individual_results, avg_utility)
        }
        
        log.info(f"Batch analysis completed: {successful_analyses}/{len(file_list)} files successful")
        
        return batch_result
    
    def _generate_batch_recommendations(self, individual_results: Dict[str, Any], avg_utility: float) -> List[Dict[str, Any]]:
        """ãƒãƒƒãƒåˆ†ææ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # æœ€é©ãƒ•ã‚¡ã‚¤ãƒ«æ¨å¥¨
        successful_files = [(path, result["summary"]["utility_score"]) 
                           for path, result in individual_results.items() 
                           if result.get("success")]
        
        if successful_files:
            best_file = max(successful_files, key=lambda x: x[1])
            recommendations.append({
                "category": "æœ€é©ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ",
                "priority": "æœ€é«˜",
                "recommendation": f"{best_file[0]}ã‚’ä¸»è¦åˆ†æå¯¾è±¡ã¨ã—ã¦ä½¿ç”¨",
                "reason": f"å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢{best_file[1]:.1f}%ã§æœ€é«˜è©•ä¾¡",
                "actions": [
                    f"{best_file[0]}ã§ã®è©³ç´°åˆ¶ç´„åˆ†æå®Ÿè¡Œ",
                    "ç™ºè¦‹åˆ¶ç´„ã®å®Ÿç”¨åŒ–è¨ˆç”»ç­–å®š",
                    "ä»–ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®æ¯”è¼ƒåˆ†æå®Ÿæ–½"
                ]
            })
        
        # åˆ†ææˆ¦ç•¥æ¨å¥¨
        total_files = len([r for r in individual_results.values() if r.get("success")])
        if total_files >= 3:
            recommendations.append({
                "category": "åˆ†ææˆ¦ç•¥",
                "priority": "é«˜",
                "recommendation": "åŒ…æ‹¬çš„æ¯”è¼ƒåˆ¶ç´„åˆ†æã‚’å®Ÿæ–½",
                "reason": f"{total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§å¤šè§’çš„åˆ†æãŒå¯èƒ½",
                "actions": [
                    "ãƒ•ã‚¡ã‚¤ãƒ«é–“åˆ¶ç´„å·®ç•°ã®åˆ†æ",
                    "å…±é€šåˆ¶ç´„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º",
                    "æœ€é©åˆ¶ç´„çµ„ã¿åˆã‚ã›ã®ç™ºè¦‹"
                ]
            })
        
        # å®Ÿç”¨åŒ–æ¨å¥¨
        if avg_utility >= 70:
            recommendations.append({
                "category": "å®Ÿç”¨åŒ–è¨ˆç”»",
                "priority": "é«˜",
                "recommendation": "åˆ¶ç´„ç™ºè¦‹çµæœã®ç©æ¥µçš„å®Ÿç”¨åŒ–ã‚’æ¨é€²",
                "reason": f"å¹³å‡å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢{avg_utility:.1f}%ã§é«˜ã„å®Ÿç”¨å¯èƒ½æ€§",
                "actions": [
                    "åˆ¶ç´„ã‚’ã‚·ãƒ•ãƒˆä½œæˆãƒ«ãƒ¼ãƒ«ã«çµ„ã¿è¾¼ã¿",
                    "åˆ¶ç´„é•åã®è‡ªå‹•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰",
                    "ç¶™ç¶šçš„åˆ¶ç´„ç›£è¦–ä½“åˆ¶ã®ç¢ºç«‹"
                ]
            })
        
        return recommendations
    
    def export_analysis_results(self, results: Dict[str, Any], output_file: str = None) -> str:
        """åˆ†æçµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"constraint_analysis_{timestamp}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            log.info(f"Analysis results exported to {output_file}")
            return output_file
        except Exception as e:
            log.error(f"Export failed: {e}")
            return ""
    
    def get_system_status(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—"""
        return {
            "engine_name": self.engine_name,
            "version": self.version,
            "available_files": len(self.available_files),
            "cached_analyses": len(self.analysis_cache),
            "status": "operational",
            "capabilities": [
                "å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ¶ç´„åˆ†æ",
                "ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«åˆ¶ç´„åˆ†æ", 
                "å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢ç®—å‡º",
                "æ¨å¥¨äº‹é …è‡ªå‹•ç”Ÿæˆ",
                "çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½"
            ]
        }

def create_cli_interface():
    """CLI ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½œæˆ"""
    parser = argparse.ArgumentParser(description="å®Ÿç”¨åˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³ CLI")
    
    parser.add_argument('--mode', choices=['single', 'batch', 'status'], default='status',
                       help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰é¸æŠ')
    parser.add_argument('--file', type=str, help='å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå¯¾è±¡')
    parser.add_argument('--output', type=str, help='çµæœå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°ãƒ­ã‚°å‡ºåŠ›')
    
    return parser

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = create_cli_interface()
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    print("=" * 60)
    print("å®Ÿç”¨åˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³")
    print("=" * 60)
    
    try:
        engine = PracticalConstraintEngine()
        
        if args.mode == 'status':
            # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º
            status = engine.get_system_status()
            print(f"\nğŸ”§ {status['engine_name']} v{status['version']}")
            print(f"ğŸ“ åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«: {status['available_files']}å€‹")
            print(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿åˆ†æ: {status['cached_analyses']}å€‹")
            print(f"âš¡ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {status['status']}")
            
            print(f"\nğŸ¯ ä¸»è¦æ©Ÿèƒ½:")
            for capability in status['capabilities']:
                print(f"  âœ“ {capability}")
            
            if engine.available_files:
                print(f"\nğŸ“„ åˆ©ç”¨å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
                for i, file in enumerate(engine.available_files, 1):
                    print(f"  {i}. {file}")
            else:
                print(f"\nâš ï¸ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                print(f"   Excelå½¢å¼(.xlsx, .xls)ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„")
        
        elif args.mode == 'single':
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
            if not args.file:
                if engine.available_files:
                    args.file = engine.available_files[0]
                    print(f"ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {args.file}")
                else:
                    print("âŒ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
                    return 1
            
            print(f"\nğŸ” å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Ÿè¡Œä¸­: {args.file}")
            result = engine.analyze_single_file(args.file)
            
            if result.get("success"):
                summary = result["summary"]
                print(f"âœ… åˆ†æå®Œäº†")
                print(f"   åˆ¶ç´„æ•°: {summary['total_constraints']}")
                print(f"   ã‚«ãƒ†ã‚´ãƒªæ•°: {summary['categories']}")
                print(f"   é«˜å„ªå…ˆåº¦: {summary['high_priority']}")
                print(f"   å®Ÿè¡Œå¯èƒ½é …ç›®: {summary['actionable_items']}")
                print(f"   å¹³å‡ä¿¡é ¼åº¦: {summary['avg_confidence']:.1%}")
                print(f"   å®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢: {summary['utility_score']:.1f}%")
                
                # çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                if args.output:
                    export_file = engine.export_analysis_results(result, args.output)
                    if export_file:
                        print(f"ğŸ’¾ çµæœä¿å­˜: {export_file}")
            else:
                print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {result.get('error')}")
                return 1
        
        elif args.mode == 'batch':
            # ãƒãƒƒãƒåˆ†æ
            print(f"\nğŸš€ ãƒãƒƒãƒåˆ†æå®Ÿè¡Œä¸­...")
            result = engine.batch_analyze()
            
            if result.get("success"):
                summary = result["batch_summary"]
                print(f"âœ… ãƒãƒƒãƒåˆ†æå®Œäº†")
                print(f"   å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {summary['files_processed']}")
                print(f"   æˆåŠŸæ•°: {summary['successful_analyses']}")
                print(f"   ç·åˆ¶ç´„æ•°: {summary['total_constraints']}")
                print(f"   å®Ÿè¡Œå¯èƒ½é …ç›®: {summary['total_actionable']}")
                print(f"   å¹³å‡ä¿¡é ¼åº¦: {summary['avg_confidence']:.1%}")
                print(f"   å¹³å‡å®Ÿç”¨æ€§: {summary['avg_utility_score']:.1f}%")
                
                print(f"\nğŸ“Š ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
                for category, count in summary['category_distribution'].items():
                    print(f"   {category}: {count}å€‹")
                
                print(f"\nğŸ’¡ æ¨å¥¨äº‹é …: {len(result['recommendations'])}å€‹ç”Ÿæˆ")
                for i, rec in enumerate(result['recommendations'][:3], 1):  # ä¸Šä½3ã¤è¡¨ç¤º
                    print(f"   {i}. {rec['recommendation']}")
                
                # çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                if args.output:
                    export_file = engine.export_analysis_results(result, args.output)
                    if export_file:
                        print(f"ğŸ’¾ çµæœä¿å­˜: {export_file}")
            else:
                print(f"âŒ ãƒãƒƒãƒåˆ†æã‚¨ãƒ©ãƒ¼: {result.get('error')}")
                return 1
        
        print(f"\nâœ¨ å®Ÿè¡Œå®Œäº†")
        return 0
        
    except Exception as e:
        print(f"\nâŒ ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())