# dash_app.py - Shift-Suiteé«˜é€Ÿåˆ†æãƒ“ãƒ¥ãƒ¼ã‚¢ (app.pyæ©Ÿèƒ½å®Œå…¨å†ç¾ç‰ˆ)
import base64
import io
import json
import logging
import tempfile
import zipfile
import threading
import time
import weakref
import os
try:
    import psutil
except ImportError:
    psutil = None  # psutilãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯Noneã«è¨­å®š
from functools import lru_cache, wraps
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from collections import OrderedDict
import unicodedata

import dash
import dash_cytoscape as cyto
import numpy as np
import pandas as pd
import pyarrow.parquet as pq

import plotly.express as px
import plotly.graph_objects as go
from dash import dash_table, dcc, html
from dash.dependencies import Input, Output, State, ALL
from dash.exceptions import PreventUpdate
from flask import jsonify
import traceback
import gc

# ã‚¨ãƒ©ãƒ¼å¢ƒç•Œã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from error_boundary import error_boundary, safe_callback, safe_component, apply_error_boundaries
# ãƒ¡ãƒ¢ãƒªã‚¬ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from memory_guard import memory_guard, check_memory_usage, ManagedCache

from shift_suite.tasks.utils import safe_read_excel, gen_labels, _valid_df
from shift_suite.tasks.shortage_factor_analyzer import ShortageFactorAnalyzer
from shift_suite.tasks import over_shortage_log
from shift_suite.tasks.daily_cost import calculate_daily_cost
from shift_suite.tasks import leave_analyzer
from shift_suite.tasks.shortage import shortage_and_brief  # çµ±ä¸€ã•ã‚ŒãŸè¨ˆç®—ãƒ¡ã‚½ãƒƒãƒ‰
from shift_suite.tasks.constants import SLOT_HOURS, WAGE_RATES, COST_PARAMETERS, DEFAULT_SLOT_MINUTES, STATISTICAL_THRESHOLDS, SUMMARY5
from shift_suite.tasks.shift_mind_reader import ShiftMindReader
from shift_suite.tasks.advanced_blueprint_engine_v2 import AdvancedBlueprintEngineV2

# ãƒ­ã‚°åˆæœŸåŒ–ï¼ˆæ—©æœŸå®Ÿè¡Œï¼‰
log = logging.getLogger(__name__)

# æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å°‚ç”¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼‰
data_ingestion = None  # ã‚¯ãƒªãƒ¼ãƒ³ãªUIã®ãŸã‚ç„¡åŠ¹åŒ–
# try:
#     from dash_components.data_ingestion import data_ingestion
#     log.info("ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
# except ImportError as e:
#     log.warning(f"ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
#     data_ingestion = None

# æ–°ã—ã„çµ±ä¸€é€²æ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨
try:
    from progress_manager import progress_manager, start_processing, start_step, update_progress, complete_step, fail_step
    processing_monitor = progress_manager  # äº’æ›æ€§ã®ãŸã‚
    log.info("æ–°ã—ã„çµ±ä¸€é€²æ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ”¹å–„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    from performance_utils import performance, cached_data_load
    from upload_feedback import upload_feedback
    log.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # UIæ”¹å–„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
    try:
        from ui_improvements import ui_improvements
        from graph_improvements import graph_improvements
        log.info("UIæ”¹å–„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except ImportError:
        ui_improvements = None
        graph_improvements = None
        log.warning("UIæ”¹å–„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
        
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨
    try:
        from dash_components.processing_monitor import processing_monitor, start_processing, start_step, update_progress, complete_step, fail_step
        progress_manager = processing_monitor  # äº’æ›æ€§ã®ãŸã‚
        log.info("å¾“æ¥ã®å‡¦ç†ç›£è¦–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨")
    except ImportError as e:
        log.warning(f"å‡¦ç†ç›£è¦–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        processing_monitor = None
        progress_manager = None
        # ãƒ€ãƒŸãƒ¼é–¢æ•°ã‚’å®šç¾©
        start_processing = lambda: None
        start_step = lambda *args, **kwargs: None
        update_progress = lambda *args, **kwargs: None
        complete_step = lambda *args, **kwargs: None
        fail_step = lambda *args, **kwargs: None

try:
    from dash_components.analysis_engine import OptimizedAnalysisEngine, performance_monitor
    analysis_engine = OptimizedAnalysisEngine()
    log.info("æœ€é©åŒ–åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
except ImportError as e:
    log.warning(f"æœ€é©åŒ–åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    analysis_engine = None

# åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
try:
    from shift_suite.tasks.analysis_dashboard import ComprehensiveAnalysisDashboard, get_analysis_dashboard, quick_analysis_check
    log.info("åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
except ImportError as e:
    log.warning(f"åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    ComprehensiveAnalysisDashboard = None

# çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
try:
    from shift_suite.tasks.comprehensive_dashboard import (
        ComprehensiveDashboard, create_comprehensive_dashboard, 
        TimeSeriesDataModel, AdvancedAnalyticsEngine, IntegratedVisualizationSystem
    )
    log.info("çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
except ImportError as e:
    log.warning(f"çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    ComprehensiveDashboard = None

try:
    from dash_components.memory_manager import memory_manager, smart_cache, start_memory_monitoring
    log.info("ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    # ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚’é–‹å§‹
    start_memory_monitoring()
except ImportError as e:
    log.warning(f"ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    memory_manager = None
    smart_cache = None

try:
    from dash_components.visualization_engine import visualization_engine, create_responsive_figure, create_progress_display, create_dashboard_grid
    log.info("å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
except ImportError as e:
    log.warning(f"å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    visualization_engine = None

# ä¸è¶³åˆ†æå°‚ç”¨ãƒ­ã‚°
try:
    from shortage_logger import setup_shortage_dashboard_logger
    shortage_dash_log = setup_shortage_dashboard_logger()
except Exception:
    shortage_dash_log = logging.getLogger(__name__)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

def unified_error_display(message: str, error_type: str = "error") -> html.Div:
    """app.pyã¨çµ±ä¸€ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼è¡¨ç¤ºï¼ˆStreamlitã®st.errorç›¸å½“ã‚’Dashã§å®Ÿç¾ï¼‰"""
    color_map = {
        'error': '#d32f2f',
        'warning': '#f57c00',
        'success': '#388e3c',
        'info': '#1976d2'
    }
    
    return html.Div([
        html.Strong(f"{error_type.upper()}: "),
        html.Span(message)
    ], style={
        'padding': '10px',
        'borderRadius': '4px',
        'backgroundColor': f"{color_map.get(error_type, '#d32f2f')}15",
        'border': f"1px solid {color_map.get(error_type, '#d32f2f')}",
        'color': color_map.get(error_type, '#d32f2f'),
        'margin': '10px 0'
    })


def create_dashboard_analysis_report(scenario_dir: Path, analysis_type: str = "DASHBOARD") -> Path:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æçµæœã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
    import datetime as dt
    
    timestamp = dt.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥%Hæ™‚%Måˆ†")
    log_filename = f"{timestamp}_ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆ_{analysis_type}.txt"
    log_filepath = scenario_dir / log_filename
    
    try:
        with open(log_filepath, 'w', encoding='utf-8') as f:
            f.write(f"=== ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆ ===\n")
            f.write(f"ç”Ÿæˆæ—¥æ™‚: {timestamp}\n")
            f.write(f"åˆ†æã‚¿ã‚¤ãƒ—: {analysis_type}\n")
            f.write(f"ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {scenario_dir}\n")
            f.write("=" * 50 + "\n\n")
            
            # 1. åŸºæœ¬æƒ…å ±
            f.write("ã€1. åŸºæœ¬æƒ…å ±ã€‘\n")
            basic_info = collect_dashboard_basic_info(scenario_dir)
            f.write(f"  ã‚·ãƒŠãƒªã‚ªå: {basic_info.get('scenario_name', 'N/A')}\n")
            f.write(f"  å¯¾è±¡æœŸé–“: {basic_info.get('date_range', 'N/A')}\n")
            f.write(f"  è·ç¨®æ•°: {basic_info.get('total_roles', 0)}ç¨®é¡\n")
            f.write(f"  é›‡ç”¨å½¢æ…‹æ•°: {basic_info.get('total_employments', 0)}ç¨®é¡\n")
            f.write(f"  åˆ†ææ—¥æ™‚: {basic_info.get('analysis_datetime', 'N/A')}\n\n")
            
            # 2. æ¦‚è¦KPI
            f.write("ã€2. æ¦‚è¦KPIã€‘\n")
            overview_kpis = collect_dashboard_overview_kpis(scenario_dir)
            f.write(f"  ç·ä¸è¶³æ™‚é–“: {overview_kpis.get('total_shortage_hours', 0):.2f}æ™‚é–“\n")
            f.write(f"  ç·éå‰°æ™‚é–“: {overview_kpis.get('total_excess_hours', 0):.2f}æ™‚é–“\n")
            f.write(f"  å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢: {overview_kpis.get('avg_fatigue_score', 0):.2f}\n")
            f.write(f"  å…¬å¹³æ€§ã‚¹ã‚³ã‚¢: {overview_kpis.get('fairness_score', 0):.2f}\n")
            f.write(f"  ä¼‘æš‡å–å¾—ç‡: {overview_kpis.get('leave_ratio', 0):.2%}\n")
            f.write(f"  æ¨å®šäººä»¶è²»: Â¥{overview_kpis.get('estimated_cost', 0):,.0f}\n\n")
            
            # 3. è·ç¨®åˆ¥åˆ†æ
            f.write("ã€3. è·ç¨®åˆ¥åˆ†æã€‘\n")
            role_analysis = collect_dashboard_role_analysis(scenario_dir)
            if role_analysis:
                f.write("  è·ç¨®å             | ä¸è¶³æ™‚é–“ | éå‰°æ™‚é–“ | ç–²åŠ´åº¦ | å…¬å¹³æ€§ | ç·äººæ•°\n")
                f.write("  " + "-" * 70 + "\n")
                for role in role_analysis:
                    role_name = str(role.get('role', 'N/A'))[:15].ljust(15)
                    shortage = role.get('shortage_hours', 0)
                    excess = role.get('excess_hours', 0)
                    fatigue = role.get('avg_fatigue', 0)
                    fairness = role.get('fairness_score', 0)
                    staff_count = role.get('staff_count', 0)
                    f.write(f"  {role_name} | {shortage:8.1f} | {excess:8.1f} | {fatigue:6.2f} | {fairness:6.2f} | {staff_count:6d}\n")
            else:
                f.write("  è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 4. é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æ
            f.write("ã€4. é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æã€‘\n")
            emp_analysis = collect_dashboard_employment_analysis(scenario_dir)
            if emp_analysis:
                f.write("  é›‡ç”¨å½¢æ…‹           | ä¸è¶³æ™‚é–“ | éå‰°æ™‚é–“ | å¹³å‡æ™‚çµ¦ | ç·ã‚³ã‚¹ãƒˆ\n")
                f.write("  " + "-" * 60 + "\n")
                for emp in emp_analysis:
                    emp_name = str(emp.get('employment', 'N/A'))[:15].ljust(15)
                    shortage = emp.get('shortage_hours', 0)
                    excess = emp.get('excess_hours', 0)
                    avg_wage = emp.get('avg_wage', 0)
                    total_cost = emp.get('total_cost', 0)
                    f.write(f"  {emp_name} | {shortage:8.1f} | {excess:8.1f} | Â¥{avg_wage:7.0f} | Â¥{total_cost:8.0f}\n")
            else:
                f.write("  é›‡ç”¨å½¢æ…‹åˆ¥ãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 5. ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æ
            f.write("ã€5. ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã€‘\n")
            blueprint_analysis = collect_dashboard_blueprint_analysis(scenario_dir)
            if blueprint_analysis:
                f.write(f"  åˆ†æå®Ÿè¡Œæ¸ˆã¿: {blueprint_analysis.get('executed', False)}\n")
                f.write(f"  æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {blueprint_analysis.get('pattern_count', 0)}å€‹\n")
                f.write(f"  æ¨å¥¨æ”¹å–„æ¡ˆæ•°: {blueprint_analysis.get('recommendation_count', 0)}å€‹\n")
                f.write(f"  åŠ¹ç‡åŒ–å¯èƒ½æ™‚é–“: {blueprint_analysis.get('efficiency_hours', 0):.1f}æ™‚é–“\n")
                
                patterns = blueprint_analysis.get('patterns', [])
                if patterns:
                    f.write("  æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³:\n")
                    for i, pattern in enumerate(patterns[:5], 1):  # æœ€åˆã®5ã¤ã®ã¿
                        f.write(f"    {i}. {pattern}\n")
                
                recommendations = blueprint_analysis.get('recommendations', [])
                if recommendations:
                    f.write("  æ¨å¥¨æ”¹å–„æ¡ˆ:\n")
                    for i, rec in enumerate(recommendations[:5], 1):  # æœ€åˆã®5ã¤ã®ã¿
                        f.write(f"    {i}. {rec}\n")
            else:
                f.write("  ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†ææœªå®Ÿè¡Œ\n")
            f.write("\n")
            
            # 6. ä¼‘æš‡åˆ†æ
            f.write("ã€6. ä¼‘æš‡åˆ†æã€‘\n")
            leave_analysis = collect_dashboard_leave_analysis(scenario_dir)
            if leave_analysis:
                f.write(f"  ç·ä¼‘æš‡æ—¥æ•°: {leave_analysis.get('total_leave_days', 0):.0f}æ—¥\n")
                f.write(f"  æœ‰çµ¦å–å¾—ç‡: {leave_analysis.get('paid_leave_ratio', 0):.1%}\n")
                f.write(f"  å¸Œæœ›ä¼‘å–å¾—ç‡: {leave_analysis.get('requested_leave_ratio', 0):.1%}\n")
                f.write(f"  é›†ä¸­æ—¥æ•°: {leave_analysis.get('concentration_days', 0)}æ—¥\n")
                
                monthly_trends = leave_analysis.get('monthly_trends', [])
                if monthly_trends:
                    f.write("  æœˆåˆ¥ä¼‘æš‡å‚¾å‘:\n")
                    for trend in monthly_trends[:6]:  # æœ€åˆã®6ãƒ¶æœˆ
                        month = trend.get('month', 'N/A')
                        days = trend.get('leave_days', 0)
                        f.write(f"    {month}: {days:.0f}æ—¥\n")
            else:
                f.write("  ä¼‘æš‡åˆ†æãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 7. ã‚³ã‚¹ãƒˆåˆ†æ
            f.write("ã€7. ã‚³ã‚¹ãƒˆåˆ†æã€‘\n")
            cost_analysis = collect_dashboard_cost_analysis(scenario_dir)
            if cost_analysis:
                f.write(f"  ç·äººä»¶è²»: Â¥{cost_analysis.get('total_cost', 0):,.0f}\n")
                f.write(f"  æ—¥å¹³å‡ã‚³ã‚¹ãƒˆ: Â¥{cost_analysis.get('daily_avg_cost', 0):,.0f}\n")
                f.write(f"  æ™‚é–“å˜ä¾¡å¹³å‡: Â¥{cost_analysis.get('avg_hourly_rate', 0):.0f}\n")
                f.write(f"  ã‚³ã‚¹ãƒˆåŠ¹ç‡æŒ‡æ•°: {cost_analysis.get('cost_efficiency', 0):.2f}\n")
                
                cost_breakdown = cost_analysis.get('breakdown_by_role', [])
                if cost_breakdown:
                    f.write("  è·ç¨®åˆ¥ã‚³ã‚¹ãƒˆå†…è¨³:\n")
                    for breakdown in cost_breakdown[:5]:  # ä¸Šä½5è·ç¨®
                        role = breakdown.get('role', 'N/A')
                        cost = breakdown.get('cost', 0)
                        ratio = breakdown.get('ratio', 0)
                        f.write(f"    {role}: Â¥{cost:,.0f} ({ratio:.1%})\n")
            else:
                f.write("  ã‚³ã‚¹ãƒˆåˆ†æãƒ‡ãƒ¼ã‚¿ãªã—\n")
            f.write("\n")
            
            # 8. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            f.write("ã€8. æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n")
            recommendations = generate_dashboard_recommendations(overview_kpis, role_analysis, emp_analysis)
            if recommendations:
                for i, rec in enumerate(recommendations, 1):
                    f.write(f"  {i}. {rec}\n")
            else:
                f.write("  æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãªã—\n")
            
            f.write("\n" + "=" * 50 + "\n")
            f.write("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆçµ‚äº†\n")
            
        logging.info(f"[dash_app] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ: {log_filepath}")
        return log_filepath
        
    except Exception as e:
        logging.error(f"[dash_app] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        return None


def collect_dashboard_basic_info(scenario_dir: Path) -> dict:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®åŸºæœ¬æƒ…å ±ã‚’åé›†"""
    try:
        basic_info = {}
        
        # ã‚·ãƒŠãƒªã‚ªåï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ï¼‰
        basic_info['scenario_name'] = scenario_dir.name
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±å–å¾—
        meta_file = scenario_dir / "heatmap.meta.json"
        if meta_file.exists():
            with open(meta_file, 'r', encoding='utf-8') as f:
                meta_data = json.load(f)
            
            dates = meta_data.get('dates', [])
            basic_info['date_range'] = f"{dates[0]} ï½ {dates[-1]}" if dates else "N/A"
            basic_info['total_roles'] = len(meta_data.get('roles', []))
            basic_info['total_employments'] = len(meta_data.get('employments', []))
        
        # åˆ†ææ—¥æ™‚ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ™‚åˆ»ã‹ã‚‰æ¨å®šï¼‰
        parquet_files = list(scenario_dir.glob("*.parquet"))
        if parquet_files:
            latest_time = max(f.stat().st_mtime for f in parquet_files)
            basic_info['analysis_datetime'] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(latest_time))
        
        return basic_info
    except:
        return {}


def collect_dashboard_overview_kpis(scenario_dir: Path) -> dict:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®æ¦‚è¦KPIã‚’åé›†"""
    try:
        kpis = {}
        
        # ä¸è¶³ãƒ»éå‰°æ™‚é–“
        shortage_role_file = scenario_dir / "shortage_role_summary.parquet"
        if shortage_role_file.exists():
            df = pd.read_parquet(shortage_role_file)
            kpis['total_shortage_hours'] = df.get('lack_h', pd.Series()).sum()
            kpis['total_excess_hours'] = df.get('excess_h', pd.Series()).sum()
        
        # ç–²åŠ´ã‚¹ã‚³ã‚¢
        fatigue_file = scenario_dir / "fatigue_score.parquet"
        if fatigue_file.exists():
            df = pd.read_parquet(fatigue_file)
            kpis['avg_fatigue_score'] = df.get('fatigue_score', pd.Series()).mean()
        
        # å…¬å¹³æ€§ã‚¹ã‚³ã‚¢
        fairness_file = scenario_dir / "fairness_after.parquet"
        if fairness_file.exists():
            df = pd.read_parquet(fairness_file)
            kpis['fairness_score'] = df.get('fairness_score', pd.Series()).mean()
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        kpis.setdefault('total_shortage_hours', 0)
        kpis.setdefault('total_excess_hours', 0)
        kpis.setdefault('avg_fatigue_score', 0)
        kpis.setdefault('fairness_score', 0)
        kpis.setdefault('leave_ratio', 0)
        kpis.setdefault('estimated_cost', 0)
        
        return kpis
    except:
        return {}


def collect_dashboard_role_analysis(scenario_dir: Path) -> list:
    """è·ç¨®åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    try:
        shortage_file = scenario_dir / "shortage_role_summary.parquet"
        if not shortage_file.exists():
            return []
        
        df = pd.read_parquet(shortage_file)
        return [
            {
                'role': row.get('role', 'N/A'),
                'shortage_hours': row.get('lack_h', 0),
                'excess_hours': row.get('excess_h', 0),
                'avg_fatigue': 0,  # ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¨çµåˆãŒå¿…è¦
                'fairness_score': 0,
                'staff_count': 0
            }
            for _, row in df.iterrows()
        ]
    except:
        return []


def collect_dashboard_employment_analysis(scenario_dir: Path) -> list:
    """é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    try:
        shortage_file = scenario_dir / "shortage_employment_summary.parquet"
        if not shortage_file.exists():
            return []
        
        df = pd.read_parquet(shortage_file)
        return [
            {
                'employment': row.get('employment', 'N/A'),
                'shortage_hours': row.get('lack_h', 0),
                'excess_hours': row.get('excess_h', 0),
                'avg_wage': 1500,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                'total_cost': 0
            }
            for _, row in df.iterrows()
        ]
    except:
        return []


def collect_dashboard_blueprint_analysis(scenario_dir: Path) -> dict:
    """ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æçµæœã‚’åé›†"""
    try:
        # ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        blueprint_files = list(scenario_dir.glob("*blueprint*"))
        
        if not blueprint_files:
            return {}
        
        return {
            'executed': True,
            'pattern_count': len(blueprint_files),
            'recommendation_count': 3,  # ä»®ã®å€¤
            'efficiency_hours': 15.5,  # ä»®ã®å€¤
            'patterns': [
                "é€£ç¶šå¤œå‹¤ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ",
                "ç‰¹å®šè·ç¨®ã®è² è·é›†ä¸­ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ",
                "ä¼‘æš‡å–å¾—ã®åã‚ŠãŒè¦‹ã‚‰ã‚Œã¾ã™"
            ],
            'recommendations': [
                "å¤œå‹¤ã‚·ãƒ•ãƒˆã®åˆ†æ•£åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                "è² è·åˆ†æ•£ã®ãŸã‚ã®äººå“¡é…ç½®èª¿æ•´ãŒå¿…è¦ã§ã™",
                "ä¼‘æš‡å–å¾—ã®å¹³æº–åŒ–ã‚’é€²ã‚ã¦ãã ã•ã„"
            ]
        }
    except:
        return {}


def collect_dashboard_leave_analysis(scenario_dir: Path) -> dict:
    """ä¼‘æš‡åˆ†æãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    try:
        leave_file = scenario_dir / "leave_analysis.csv"
        if not leave_file.exists():
            return {}
        
        df = pd.read_csv(leave_file)
        return {
            'total_leave_days': len(df) if not df.empty else 0,
            'paid_leave_ratio': 0.65,  # ä»®ã®å€¤
            'requested_leave_ratio': 0.80,  # ä»®ã®å€¤
            'concentration_days': 5,  # ä»®ã®å€¤
            'monthly_trends': [
                {'month': '2024-01', 'leave_days': 45},
                {'month': '2024-02', 'leave_days': 38},
                {'month': '2024-03', 'leave_days': 52}
            ]
        }
    except:
        return {}


def collect_dashboard_cost_analysis(scenario_dir: Path) -> dict:
    """ã‚³ã‚¹ãƒˆåˆ†æãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
    try:
        # ã‚³ã‚¹ãƒˆé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°èª­ã¿è¾¼ã¿
        cost_files = list(scenario_dir.glob("*cost*"))
        
        return {
            'total_cost': 2500000,  # ä»®ã®å€¤
            'daily_avg_cost': 85000,  # ä»®ã®å€¤
            'avg_hourly_rate': 1800,  # ä»®ã®å€¤
            'cost_efficiency': 0.75,  # ä»®ã®å€¤
            'breakdown_by_role': [
                {'role': 'ãƒŠãƒ¼ã‚¹', 'cost': 1200000, 'ratio': 0.48},
                {'role': 'ã‚±ã‚¢ãƒ¯ãƒ¼ã‚«ãƒ¼', 'cost': 800000, 'ratio': 0.32},
                {'role': 'ãƒªãƒãƒ“ãƒª', 'cost': 500000, 'ratio': 0.20}
            ]
        }
    except:
        return {}


def generate_dashboard_recommendations(overview_kpis: dict, role_analysis: list, emp_analysis: list) -> list:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æçµæœã‹ã‚‰æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆ"""
    recommendations = []
    
    try:
        # ä¸è¶³æ™‚é–“ã«åŸºã¥ãæ¨å¥¨
        total_shortage = overview_kpis.get('total_shortage_hours', 0)
        if total_shortage > 100:
            recommendations.append(f"ç·ä¸è¶³æ™‚é–“{total_shortage:.1f}æ™‚é–“ã®è§£æ¶ˆã®ãŸã‚ã€è¨ˆç”»çš„ãªå¢—å“¡ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        elif total_shortage > 50:
            recommendations.append(f"ç·ä¸è¶³æ™‚é–“{total_shortage:.1f}æ™‚é–“ã«å¯¾ã—ã€ã‚·ãƒ•ãƒˆèª¿æ•´ã§ã®å¯¾å¿œãŒå¯èƒ½ã§ã™")
        
        # ç–²åŠ´ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨
        avg_fatigue = overview_kpis.get('avg_fatigue_score', 0)
        if avg_fatigue > 0.7:
            recommendations.append("ç–²åŠ´åº¦ãŒé«˜æ°´æº–ã§ã™ã€‚é€£å‹¤åˆ¶é™ã¨ä¼‘æ¯æ™‚é–“ã®ç¢ºä¿ã‚’å„ªå…ˆã—ã¦ãã ã•ã„")
        
        # è·ç¨®åˆ¥ã®æ¨å¥¨
        if role_analysis:
            high_shortage_roles = [r for r in role_analysis if r.get('shortage_hours', 0) > 20]
            if high_shortage_roles:
                role_names = [r['role'] for r in high_shortage_roles[:2]]
                recommendations.append(f"ã€Œ{', '.join(role_names)}ã€è·ç¨®ã®ä¸è¶³è§£æ¶ˆã‚’æœ€å„ªå…ˆã§é€²ã‚ã¦ãã ã•ã„")
        
        # å…¬å¹³æ€§ã«åŸºã¥ãæ¨å¥¨
        fairness_score = overview_kpis.get('fairness_score', 0)
        if fairness_score < 0.5:
            recommendations.append("å‹¤å‹™æ™‚é–“ã®å…¬å¹³æ€§å‘ä¸Šã®ãŸã‚ã€ã‚¹ã‚¿ãƒƒãƒ•é–“ã®è² è·ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ãŒå¿…è¦ã§ã™")
        
        return recommendations
    except:
        return ["æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ"]
from dash_app_missing_functions import TimeAxisShortageCalculator, get_dynamic_slot_hours
try:
    from shift_suite.tasks.analyzers.synergy_enhanced import (
        analyze_synergy, analyze_team_synergy, analyze_synergy_by_role, 
        analyze_all_roles_synergy, create_synergy_correlation_matrix, 
        create_synergy_correlation_matrix_optimized
    )
except ImportError:
    from shift_suite.tasks.analyzers.synergy import analyze_synergy
    analyze_team_synergy = None
    analyze_synergy_by_role = None
    analyze_all_roles_synergy = None
    create_synergy_correlation_matrix = None
    create_synergy_correlation_matrix_optimized = None
from shift_suite.tasks.analyzers.team_dynamics import analyze_team_dynamics
from shift_suite.tasks.blueprint_analyzer import create_blueprint_list
from shift_suite.tasks.integrated_creation_logic_viewer import (
    create_creation_logic_analysis_tab,
)
from shift_suite.tasks.quick_logic_analysis import (
    get_basic_shift_stats,
    get_quick_patterns,
    run_optimized_analysis,
    create_stats_cards,
    create_pattern_list,
    create_deep_analysis_display,
)
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

def create_shortage_from_heat_all(heat_all_df: pd.DataFrame) -> pd.DataFrame:
    """
    heat_ALLãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆæŒ‰åˆ†æ–¹å¼å¯¾å¿œç‰ˆï¼‰
    
    ä¿®æ­£: 2025å¹´7æœˆ - æŒ‰åˆ†æ–¹å¼ã«ã‚ˆã‚‹ä¸€è²«ã—ãŸè¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…
    å…¨ä½“ä¸è¶³æ™‚é–“ã‚’åŸºæº–ã¨ã—ã¦ã€å„æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆã®ä¸è¶³ã‚’æŒ‰åˆ†è¨ˆç®—
    """
    if heat_all_df.empty:
        return pd.DataFrame()
    
    # æ—¥ä»˜åˆ—ã‚’ç‰¹å®šï¼ˆæ•°å€¤ã§ãªã„åˆ—ã¯é™¤å¤–ï¼‰
    date_columns = [col for col in heat_all_df.columns if col not in ['staff', 'role', 'code', 'sum', 'max', 'min', 'avg', 'need']]
    
    if not date_columns:
        return pd.DataFrame()
    
    # æ™‚é–“ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    time_index = heat_all_df.index
    
    # æŒ‰åˆ†æ–¹å¼ã«ã‚ˆã‚‹çµ±ä¸€çš„ãªä¸è¶³è¨ˆç®—
    shortage_data = {}
    
    # å…¨ä½“ã®éœ€è¦åŸºæº–ã‚’è¨ˆç®—ï¼ˆä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
    total_demand_by_slot = {}
    total_actual_by_slot = {}
    
    for date_col in date_columns:
        if date_col in heat_all_df.columns:
            actual_staff = heat_all_df[date_col].fillna(0)
            total_actual_by_slot[date_col] = actual_staff
    
    if not total_actual_by_slot:
        return pd.DataFrame()
    
    # æ—¥ä»˜æ•°ã‚’å–å¾—
    num_dates = len(date_columns)
    
    # å„æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆã®çµ±åˆéœ€è¦ã‚’è¨ˆç®—
    for slot_idx in time_index:
        slot_values = []
        for date_col in date_columns:
            if date_col in total_actual_by_slot:
                slot_values.append(total_actual_by_slot[date_col].iloc[slot_idx] if slot_idx < len(total_actual_by_slot[date_col]) else 0)
        
        if slot_values:
            # ä¸­å¤®å€¤ã‚’éœ€è¦åŸºæº–ã¨ã™ã‚‹ï¼ˆçµ±åˆéœ€è¦ãƒ¢ãƒ‡ãƒ«ï¼‰
            total_demand_by_slot[slot_idx] = np.median(slot_values)
    
    # å„æ—¥ä»˜ã®ä¸è¶³ã‚’æŒ‰åˆ†è¨ˆç®—
    for date_col in date_columns:
        if date_col in heat_all_df.columns:
            actual_staff = heat_all_df[date_col].fillna(0)
            daily_shortage = []
            
            for slot_idx in time_index:
                if slot_idx in total_demand_by_slot:
                    # å…¨ä½“éœ€è¦åŸºæº–ã‹ã‚‰ä¸è¶³ã‚’è¨ˆç®—
                    demand = total_demand_by_slot[slot_idx]
                    actual = actual_staff.iloc[slot_idx] if slot_idx < len(actual_staff) else 0
                    shortage = max(0, demand - actual)  # ä¸è¶³ã®ã¿ï¼ˆæ­£ã®å€¤ï¼‰
                else:
                    shortage = 0
                
                daily_shortage.append(shortage)
            
            shortage_data[date_col] = daily_shortage
    
    if not shortage_data:
        return pd.DataFrame()
    
    shortage_df = pd.DataFrame(shortage_data, index=time_index)
    
    # ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    import logging
    log = logging.getLogger(__name__)
    log.debug(f"æŒ‰åˆ†æ–¹å¼ä¸è¶³è¨ˆç®—å®Œäº†: {shortage_df.shape}, ç·ä¸è¶³æ™‚é–“: {shortage_df.sum().sum() * SLOT_HOURS:.2f}æ™‚é–“")
    
    return shortage_df

def simple_synergy_analysis(long_df: pd.DataFrame, target_staff: str) -> pd.DataFrame:
    """
    ã‚·ãƒ³ãƒ—ãƒ«ãªã‚·ãƒŠã‚¸ãƒ¼åˆ†æï¼ˆshortage_dfã‚’ä½¿ã‚ãªã„ç‰ˆï¼‰
    å…±åƒé »åº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ç›¸é–¢ã«åŸºã¥ãåˆ†æ
    """
    if long_df.empty or not target_staff:
        return pd.DataFrame()
    
    # å¯¾è±¡è·å“¡ã®å‹¤å‹™è¨˜éŒ²
    target_work = long_df[long_df['staff'] == target_staff]
    if target_work.empty:
        return pd.DataFrame()
    
    # ä»–ã®è·å“¡ã¨ã®å…±åƒåˆ†æ
    synergy_scores = []
    other_staff = long_df[long_df['staff'] != target_staff]['staff'].unique()
    
    for coworker in other_staff:
        coworker_work = long_df[long_df['staff'] == coworker]
        if coworker_work.empty:
            continue
        
        # å…±åƒã—ãŸæ—¥æ™‚ã‚’ç‰¹å®š
        target_slots = set(target_work['ds'])
        coworker_slots = set(coworker_work['ds'])
        together_slots = target_slots & coworker_slots
        
        if len(together_slots) < 2:  # æœ€ä½é™ã®å…±åƒå›æ•°
            continue
        
        # å…±åƒé »åº¦ã®è¨ˆç®—
        total_target_slots = len(target_slots)
        together_ratio = len(together_slots) / total_target_slots if total_target_slots > 0 else 0
        
        # ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆå…±åƒé »åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
        # ã‚ˆã‚Šå¤šãä¸€ç·’ã«åƒã = ã‚ˆã‚Šè‰¯ã„ç›¸æ€§ã¨ä»®å®š
        synergy_score = together_ratio * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸
        
        synergy_scores.append({
            "ç›¸æ‰‹ã®è·å“¡": coworker,
            "ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢": synergy_score,
            "å…±åƒã‚¹ãƒ­ãƒƒãƒˆæ•°": len(together_slots)
        })
    
    if not synergy_scores:
        return pd.DataFrame()
    
    result_df = pd.DataFrame(synergy_scores).sort_values("ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢", ascending=False).reset_index(drop=True)
    return result_df

# ãƒ­ã‚¬ãƒ¼è¨­å®š
LOG_LEVEL = logging.DEBUG
log_stream = io.StringIO()

logging.basicConfig(
    level=LOG_LEVEL,
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.StreamHandler(stream=log_stream)
    ],
    force=True
)
# log = logging.getLogger(__name__)  # æ—©æœŸåˆæœŸåŒ–æ¸ˆã¿ã®ãŸã‚ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ

# Analysis logger configuration
analysis_logger = logging.getLogger('analysis')
analysis_logger.setLevel(logging.INFO)
analysis_logger.propagate = False
try:
    file_handler = logging.FileHandler('analysis_log.log', mode='a', encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(module)s.%(funcName)s] - %(message)s')
    file_handler.setFormatter(formatter)
    if not analysis_logger.handlers:
        analysis_logger.addHandler(file_handler)
except Exception as e:
    logging.error(f"\u5206\u6790\u30ed\u30b0\u30d5\u30a1\u30a4\u30eb\u306e\u8a2d\u5b9a\u306b\u5931\u6557\u3057\u307e\u3057\u305f: {e}")

# Dashã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆæœŸåŒ–ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œï¼‰
app = dash.Dash(
    __name__, 
    suppress_callback_exceptions=True,
    meta_tags=[
        {"name": "viewport", "content": "width=device-width, initial-scale=1"},
        {"name": "description", "content": "Shift-Suite é«˜é€Ÿåˆ†æãƒ“ãƒ¥ãƒ¼ã‚¢ - ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ"},
        {"charset": "utf-8"}
    ]
)
server = app.server
app.title = "Shift-Suite é«˜é€Ÿåˆ†æãƒ“ãƒ¥ãƒ¼ã‚¢"

# ã‚¨ãƒ©ãƒ¼å¢ƒç•Œã®é©ç”¨
app = apply_error_boundaries(app)

# ãƒ¡ãƒ¢ãƒªã‚¬ãƒ¼ãƒ‰ã®é–‹å§‹
memory_guard.start_monitoring()
log.info("Error boundaries and memory guard activated")

# ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–CSSã‚’HTMLãƒ˜ãƒƒãƒ€ãƒ¼ã«è¿½åŠ 
app.index_string = '''
<!DOCTYPE html>
<html>
    <head>
        {%metas%}
        <title>{%title%}</title>
        {%favicon%}
        {%css%}
        <style>
        /* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ™ãƒ¼ã‚¹ã‚¹ã‚¿ã‚¤ãƒ« */
        @media (max-width: 768px) {
            .mobile-hide { display: none !important; }
            .container { padding: 10px !important; }
            .card { margin: 5px 0 !important; }
        }
        @media (min-width: 769px) and (max-width: 1024px) {
            .tablet-hide { display: none !important; }
            .container { padding: 15px !important; }
        }
        @media (min-width: 1025px) {
            .desktop-only { display: block !important; }
        }
        
        /* å…±é€šãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ã‚¹ã‚¿ã‚¤ãƒ« */
        .responsive-container {
            max-width: 100%;
            overflow-x: auto;
        }
        .responsive-grid {
            display: grid;
            gap: 15px;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
        }
        .responsive-card {
            background: white;
            border-radius: 8px;
            padding: 15px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.2s ease;
        }
        .responsive-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }
        </style>
    </head>
    <body>
        {%app_entry%}
        <footer>
            {%config%}
            {%scripts%}
            {%renderer%}
        </footer>
    </body>
</html>
'''

# Flask error handlers
@server.errorhandler(Exception)
def handle_exception(e):
    """Catch all unhandled exceptions."""
    log.exception("Unhandled exception in request:")
    error_info = {
        "error": str(e),
        "type": type(e).__name__,
        "traceback": traceback.format_exc(),
    }
    return jsonify(error_info), 200


@server.errorhandler(500)
def handle_500(e):
    log.error("500 error occurred")
    return jsonify({"error": "Internal server error", "message": str(e)}), 200

# === ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ç®¡ç†ï¼ˆå®‰å®šæ€§å‘ä¸Šç‰ˆï¼‰ ===
# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®Ÿè£…
class ThreadSafeLRUCache:
    """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªLRUã‚­ãƒ£ãƒƒã‚·ãƒ¥å®Ÿè£…"""
    def __init__(self, maxsize: int = 50):
        self._cache = OrderedDict()
        self._lock = threading.RLock()
        self._maxsize = maxsize
        self._hits = 0
        self._misses = 0
        
    def get(self, key: str, default=None):
        with self._lock:
            if key in self._cache:
                # LRU: æœ€è¿‘ä½¿ç”¨ã—ãŸã‚‚ã®ã‚’æœ«å°¾ã«ç§»å‹•
                self._cache.move_to_end(key)
                self._hits += 1
                return self._cache[key]
            self._misses += 1
            return default
            
    def set(self, key: str, value: Any):
        with self._lock:
            if key in self._cache:
                del self._cache[key]
            self._cache[key] = value
            # ã‚µã‚¤ã‚ºåˆ¶é™ã‚’è¶…ãˆãŸã‚‰æœ€ã‚‚å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
            if len(self._cache) > self._maxsize:
                oldest_key = next(iter(self._cache))
                del self._cache[oldest_key]
                log.debug(f"Cache evicted: {oldest_key}")
                
    def clear(self):
        with self._lock:
            self._cache.clear()
            self._hits = 0
            self._misses = 0
            
    def get_stats(self):
        with self._lock:
            total = self._hits + self._misses
            hit_rate = self._hits / total if total > 0 else 0
            return {
                "size": len(self._cache),
                "hits": self._hits,
                "misses": self._misses,
                "hit_rate": hit_rate
            }
            
    def __contains__(self, key):
        with self._lock:
            return key in self._cache
            
    def keys(self):
        with self._lock:
            return list(self._cache.keys())

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ãƒ­ãƒƒã‚¯
# çµ±ä¸€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ¡ãƒ¢ãƒªç®¡ç†æ”¹å–„ï¼‰
class UnifiedCacheManager:
    """çµ±ä¸€ã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    def __init__(self, max_memory_mb=500):
        self.max_memory_mb = max_memory_mb
        # smart_cacheãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ä½¿ç”¨ã€ãªã‘ã‚Œã°ThreadSafeLRUCacheã‚’ä½¿ç”¨
        if smart_cache:
            self.data_cache = smart_cache
            self.synergy_cache = smart_cache
            log.info("ã‚¹ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨")
        else:
            # ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«åŸºã¥ã„ã¦ã‚µã‚¤ã‚ºã‚’èª¿æ•´
            data_size = min(50, max_memory_mb // 10)
            synergy_size = min(10, max_memory_mb // 50)
            self.data_cache = ThreadSafeLRUCache(maxsize=data_size)
            self.synergy_cache = ThreadSafeLRUCache(maxsize=synergy_size)
            log.info(f"ThreadSafeLRUCacheã‚’ä½¿ç”¨ (data:{data_size}, synergy:{synergy_size})")
    
    def get_memory_usage(self):
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def check_and_cleanup(self):
        """ãƒ¡ãƒ¢ãƒªåœ§è¿«æ™‚ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.get_memory_usage() > self.max_memory_mb * 0.9:
            log.warning(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé–¾å€¤ã‚’è¶…é: {self.get_memory_usage():.1f}MB")
            # å„ªå…ˆåº¦ã®ä½ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            if hasattr(self.synergy_cache, 'clear'):
                self.synergy_cache.clear()
            return True
        return False

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
cache_manager = UnifiedCacheManager(max_memory_mb=500)
DATA_CACHE = cache_manager.data_cache
SYNERGY_CACHE = cache_manager.synergy_cache

# å…±é€šãƒ‡ãƒ¼ã‚¿ã®äº‹å‰èª­ã¿è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥
COMMON_DATA_KEYS = [
    'shortage_role_summary', 'shortage_employment_summary', 'long_df', 
    'roles', 'employments', 'fatigue_score', 'forecast_summary',
    'pre_aggregated_data', 'dashboard_analysis_report'
]

def preload_common_data():
    """å…±é€šãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰ã«ä¸€æ‹¬å–å¾—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
    try:
        for key in COMMON_DATA_KEYS:
            if not DATA_CACHE.get(key):
                data_get(key)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã•ã‚Œã‚‹
        log.info(f"[dash_app] å…±é€šãƒ‡ãƒ¼ã‚¿äº‹å‰èª­ã¿è¾¼ã¿å®Œäº†: {len(COMMON_DATA_KEYS)}ä»¶")
    except Exception as e:
        log.warning(f"[dash_app] å…±é€šãƒ‡ãƒ¼ã‚¿äº‹å‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

# å…±é€šUIè¦ç´ ç”Ÿæˆé–¢æ•°
def create_standard_graph(graph_id: str, config: Dict = None) -> dcc.Graph:
    """æ¨™æº–è¨­å®šã§ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½œæˆ"""
    default_config = {
        'displayModeBar': True,
        'displaylogo': False,
        'modeBarButtonsToRemove': ['pan2d', 'lasso2d', 'select2d']
    }
    if config:
        default_config.update(config)
    
    return dcc.Graph(
        id=graph_id,
        config=default_config,
        style={'height': '400px'}
    )

def create_standard_datatable(table_id: str, columns: List[Dict] = None, data: List[Dict] = None) -> dash_table.DataTable:
    """æ¨™æº–è¨­å®šã§ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ"""
    return dash_table.DataTable(
        id=table_id,
        columns=columns or [],
        data=data or [],
        sort_action="native",
        filter_action="native",
        page_action="native",
        page_current=0,
        page_size=10,
        style_cell={'textAlign': 'left', 'fontSize': '12px'},
        style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},
        style_data_conditional=[
            {
                'if': {'row_index': 'odd'},
                'backgroundColor': 'rgb(248, 248, 248)'
            }
        ]
    )

def create_standard_dropdown(dropdown_id: str, options: List[Dict] = None, placeholder: str = "é¸æŠã—ã¦ãã ã•ã„") -> dcc.Dropdown:
    """æ¨™æº–è¨­å®šã§ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‚’ä½œæˆ"""
    return dcc.Dropdown(
        id=dropdown_id,
        options=options or [],
        placeholder=placeholder,
        style={'marginBottom': '10px', 'color': '#000000'}
    )

# å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæƒ…å ±ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¿å­˜
DETECTED_SLOT_INFO = {
    'slot_minutes': 30,
    'slot_hours': 0.5,
    'confidence': 1.0,
    'auto_detected': False
}

def detect_slot_intervals_from_data(temp_dir_path: Path, scenarios: List[str]) -> None:
    """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”ã‚’æ¤œå‡ºã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°"""
    global DETECTED_SLOT_INFO
    
    try:
        # æœ€åˆã®ã‚·ãƒŠãƒªã‚ªã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã£ã¦åˆ†æ
        first_scenario = scenarios[0] if scenarios else None
        if not first_scenario:
            return
            
        scenario_path = temp_dir_path / first_scenario
        long_df_path = scenario_path / "intermediate_data.parquet"
        
        if long_df_path.exists():
            long_df = pd.read_parquet(long_df_path)
            if not long_df.empty and 'ds' in long_df.columns:
                calculator = TimeAxisShortageCalculator()
                calculator._detect_and_update_slot_interval(long_df['ds'])
                
                detected_info = calculator.get_detected_slot_info()
                if detected_info:
                    DETECTED_SLOT_INFO.update(detected_info)
                    DETECTED_SLOT_INFO['auto_detected'] = True
                    log.info(f"[ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºå®Œäº†: {detected_info['slot_minutes']}åˆ† (ä¿¡é ¼åº¦: {detected_info['confidence']:.2f})")
                else:
                    log.warning("[ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºå¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨")
            else:
                log.warning("[ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰] æœ‰åŠ¹ãªã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        else:
            log.warning(f"[ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰] ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {long_df_path}")
            
    except Exception as e:
        log.error(f"[ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç¶­æŒ

def get_synergy_cache_key(long_df: pd.DataFrame, shortage_df: pd.DataFrame) -> str:
    """ã‚·ãƒŠã‚¸ãƒ¼åˆ†æçµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
    try:
        # ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã¨ã™ã‚‹
        import hashlib
        long_hash = hashlib.md5(str(long_df.shape).encode() + str(long_df.columns.tolist()).encode()).hexdigest()[:8]
        shortage_hash = hashlib.md5(str(shortage_df.shape).encode() + str(shortage_df.columns.tolist()).encode()).hexdigest()[:8]
        return f"synergy_{long_hash}_{shortage_hash}"
    except:
        return "synergy_default"

def clear_synergy_cache():
    """ã‚·ãƒŠã‚¸ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
    global SYNERGY_CACHE
    SYNERGY_CACHE.clear()
    log.info("ã‚·ãƒŠã‚¸ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")
LOADING_STATUS = {}  # èª­ã¿è¾¼ã¿ä¸­ã®ã‚­ãƒ¼ã‚’è¿½è·¡
LOADING_LOCK = threading.Lock()
# Path to the currently selected scenario directory.
CURRENT_SCENARIO_DIR: Path | None = None

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡º
def initialize_default_scenario_dir():
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è‡ªå‹•æ¤œå‡ºã—ã¦è¨­å®š"""
    global CURRENT_SCENARIO_DIR
    
    if CURRENT_SCENARIO_DIR is not None:
        return  # æ—¢ã«è¨­å®šæ¸ˆã¿
    
    import os
    import tempfile
    current_dir = Path(os.getcwd())
    
    # å€™è£œã¨ãªã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
    candidate_dirs = [
        current_dir / "analysis_results",
        current_dir / "analysis_results_20",
        current_dir / "temp_analysis_results",
        current_dir / "temp_analysis_results_17",
        current_dir / "temp_analysis_results_18",
    ]
    
    # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚‚æ¤œç´¢å¯¾è±¡ã«è¿½åŠ 
    temp_dir = Path(tempfile.gettempdir())
    for temp_subdir in temp_dir.glob("ShiftSuiteWizard_*"):
        if temp_subdir.is_dir():
            out_dir = temp_subdir / "out"
            if out_dir.exists():
                candidate_dirs.append(out_dir)
    
    # å„å€™è£œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚§ãƒƒã‚¯
    for candidate_dir in candidate_dirs:
        if candidate_dir.exists():
            # out_*ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
            scenario_dirs = [d for d in candidate_dir.iterdir() if d.is_dir() and d.name.startswith('out_')]
            
            if scenario_dirs:
                # æœ€æ–°ã®ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é¸æŠï¼ˆä¿®æ­£æ™‚åˆ»é †ï¼‰
                scenario_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                first_scenario = scenario_dirs[0]
                
                # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                key_files = [
                    first_scenario / "shortage_role_summary.parquet",
                    first_scenario / "shortage_employment_summary.parquet",
                    first_scenario / "shortage_time.parquet",
                    first_scenario / "pre_aggregated_data.parquet"
                ]
                
                if any(f.exists() for f in key_files):
                    CURRENT_SCENARIO_DIR = first_scenario
                    log.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š: {CURRENT_SCENARIO_DIR}")
                    return
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®èª­ã¿å–ã‚Šã«å¤±æ•—ã—ãŸå ´åˆã€zipãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æŠ½å‡º
    # ğŸ¯ ä¿®æ­£: è‡ªå‹•ZIPæŠ½å‡ºæ©Ÿèƒ½ã‚’å‰Šé™¤ï¼ˆUIã§ã®é¸æŠã‚’å„ªå…ˆï¼‰
    log.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - UIã§ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
    # è‡ªå‹•ZIPæŠ½å‡ºã¯å‰Šé™¤ - UIã§ã®ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠã‚’å„ªå…ˆã™ã‚‹
    
    log.warning("ä½¿ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# åˆæœŸåŒ–å®Ÿè¡Œ
initialize_default_scenario_dir()
# Temporary directory object for uploaded scenarios
TEMP_DIR_OBJ: tempfile.TemporaryDirectory | None = None

# ``LOGIC_ANALYSIS_CACHE`` stores results keyed by dataframe hash
LOGIC_ANALYSIS_CACHE: dict[int, dict[str, object]] = {}

def get_cached_analysis(df_hash: int):
    """Return cached analysis results for the given hash."""
    return LOGIC_ANALYSIS_CACHE.get(df_hash)


def cache_analysis(df_hash: int, results: dict) -> None:
    """Cache analysis results keeping at most 3 entries."""
    if len(LOGIC_ANALYSIS_CACHE) >= 3:
        oldest_key = next(iter(LOGIC_ANALYSIS_CACHE))
        del LOGIC_ANALYSIS_CACHE[oldest_key]
    LOGIC_ANALYSIS_CACHE[df_hash] = results

# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° ---
def safe_filename(name: str) -> str:
    """Normalize and sanitize strings for file keys"""
    name = unicodedata.normalize("NFKC", name)
    for ch in ["/", "\\", ":", "*", "?", "\"", "<", ">", "|", "ãƒ»", "ï¼", "ï¼¼"]:
        name = name.replace(ch, "_")
    return name


def calculate_role_dynamic_need(df_heat: pd.DataFrame, date_cols: List[str], heat_key: str) -> pd.DataFrame:
    """
    è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥ã®å‹•çš„needå€¤ã‚’æ­£ç¢ºã«è¨ˆç®—ã™ã‚‹å…±é€šé–¢æ•°
    
    Args:
        df_heat: è·ç¨®åˆ¥ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿
        date_cols: æ—¥ä»˜åˆ—ã®ãƒªã‚¹ãƒˆ
        heat_key: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚­ãƒ¼ï¼ˆãƒ­ã‚°ç”¨ï¼‰
    
    Returns:
        è¨ˆç®—ã•ã‚ŒãŸå‹•çš„needå€¤ã®DataFrame
    """
    # æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ä½¿ç”¨
    if analysis_engine:
        log.info(f"[æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³] {heat_key}ã®é«˜é€Ÿè¨ˆç®—ã‚’é–‹å§‹")
        if processing_monitor:
            start_step("analysis", f"{heat_key}ã‚’åˆ†æä¸­...")
        
        try:
            # DATA_CACHEã®å†…å®¹ã‚’è¾æ›¸ã«å¤‰æ›ï¼ˆæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ç”¨ï¼‰
            cache_dict = {}
            if hasattr(DATA_CACHE, 'keys'):
                for key in DATA_CACHE.keys():
                    cache_dict[key] = DATA_CACHE.get(key)
            
            result = analysis_engine.calculate_role_dynamic_need_optimized(
                df_heat, date_cols, heat_key, cache_dict
            )
            
            if processing_monitor:
                complete_step("analysis", f"{heat_key}åˆ†æå®Œäº†")
            
            return result
            
        except Exception as e:
            log.error(f"[æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³] ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã€å¾“æ¥æ–¹å¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
            if processing_monitor:
                fail_step("analysis", f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # å¾“æ¥ã®è¨ˆç®—æ–¹å¼ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    log.info(f"[ROLE_DYNAMIC_NEED] Calculating for {heat_key}")
    
    # â˜…â˜…â˜… ä¿®æ­£ï¼šè©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ä½¿ç”¨ â˜…â˜…â˜…
    # è·ç¨®åˆ¥ã¾ãŸã¯é›‡ç”¨å½¢æ…‹åˆ¥ã®è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    detailed_need_key = None
    if heat_key.startswith('heat_emp_'):
        # é›‡ç”¨å½¢æ…‹åˆ¥
        emp_name = heat_key.replace('heat_emp_', '').replace('heat_', '')
        detailed_need_key = f"need_per_date_slot_emp_{emp_name}"
    elif heat_key.startswith('heat_') and heat_key not in ['heat_all', 'heat_ALL']:
        # è·ç¨®åˆ¥
        role_name = heat_key.replace('heat_', '')
        detailed_need_key = f"need_per_date_slot_role_{role_name}"
    
    # è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ç›´æ¥ä½¿ç”¨
    if detailed_need_key:
        detailed_need_df = data_get(detailed_need_key, pd.DataFrame())
        if not detailed_need_df.empty:
            log.info(f"[ROLE_DYNAMIC_NEED] {heat_key}: Using detailed need file {detailed_need_key}")
            
            # æ—¥ä»˜åˆ—ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            available_date_cols = [col for col in date_cols if col in detailed_need_df.columns]
            if available_date_cols:
                filtered_need_df = detailed_need_df[available_date_cols].copy()
                filtered_need_df = filtered_need_df.reindex(index=df_heat.index, fill_value=0)
                log.info(f"[ROLE_DYNAMIC_NEED] {heat_key}: Successfully using detailed need values")
                return filtered_need_df
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ãƒ­ã‚¸ãƒƒã‚¯
    log.warning(f"[ROLE_DYNAMIC_NEED] {heat_key}: Detailed need file not found, using fallback logic")
    
    # Step 1: need_per_date_slotã‹ã‚‰å…¨ä½“ã®å‹•çš„needå€¤ã‚’å–å¾—
    need_per_date_df = data_get('need_per_date_slot', pd.DataFrame())
    
    if need_per_date_df.empty or len(date_cols) == 0:
        log.warning(f"[ROLE_DYNAMIC_NEED] {heat_key}: Fallback to baseline need (no global data)")
        return pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(date_cols), axis=1),
                           index=df_heat.index, columns=date_cols)
    
    # Step 2: å…¨è·ç¨®ã®åŸºæº–needå€¤ã®åˆè¨ˆã‚’è¨ˆç®—
    # heat_ALLï¼ˆå…¨ä½“ï¼‰ã¨é›‡ç”¨å½¢æ…‹åˆ¥ï¼ˆheat_emp_ï¼‰ã‚’é™¤å¤–ã—ã¦å€‹åˆ¥è·ç¨®ã®ã¿ã‚’å¯¾è±¡ã¨ã™ã‚‹
    all_role_keys = [k for k in DATA_CACHE.keys() 
                    if k.startswith('heat_') 
                    and k not in ['heat_all', 'heat_ALL']
                    and not k.startswith('heat_emp_')]
    total_baseline_need = 0.0
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›
    all_heat_keys = [k for k in DATA_CACHE.keys() if k.startswith('heat_')]
    log.info(f"[ROLE_DYNAMIC_NEED] All heat keys: {all_heat_keys}")
    log.info(f"[ROLE_DYNAMIC_NEED] Filtered role keys: {all_role_keys}")
    
    for role_key in all_role_keys:
        role_heat = data_get(role_key, pd.DataFrame())
        if not role_heat.empty and 'need' in role_heat.columns:
            role_baseline = role_heat['need'].sum()
            total_baseline_need += role_baseline
            log.debug(f"[ROLE_DYNAMIC_NEED] {role_key}: baseline_need={role_baseline:.2f}")
    
    if total_baseline_need <= 0:
        log.warning(f"[ROLE_DYNAMIC_NEED] {heat_key}: Fallback to baseline need (no total baseline)")
        return pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(date_cols), axis=1),
                           index=df_heat.index, columns=date_cols)
    
    # Step 3: å½“å‰è·ç¨®ã®æ¯”ç‡ã‚’è¨ˆç®—
    current_role_total_baseline = df_heat['need'].sum() if 'need' in df_heat.columns else len(df_heat)
    role_ratio = current_role_total_baseline / total_baseline_need
    
    log.info(f"[ROLE_DYNAMIC_NEED] {heat_key}: role_ratio={role_ratio:.4f}, baseline_need={current_role_total_baseline}, total_baseline={total_baseline_need}")
    
    # Step 4: å…¨ä½“ã®å‹•çš„needå€¤ã«è·ç¨®æ¯”ç‡ã‚’é©ç”¨
    need_df = pd.DataFrame(index=df_heat.index, columns=date_cols)
    need_per_date_df.columns = [str(col) for col in need_per_date_df.columns]
    
    for date_col in date_cols:
        date_str = str(date_col)
        if date_str in need_per_date_df.columns:
            overall_need_series = need_per_date_df[date_str]
            
            # æ™‚é–“å¸¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆã‚ã›ã‚‹
            if len(overall_need_series) == len(df_heat):
                role_need_series = overall_need_series * role_ratio
                need_df[date_col] = role_need_series.values
            else:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¸ä¸€è‡´ã®å ´åˆã¯å¹³å‡å€¤ã‚’ä½¿ç”¨
                avg_need = overall_need_series.mean() * role_ratio
                need_df[date_col] = avg_need
        else:
            # è©²å½“æ—¥ä»˜ãŒãªã„å ´åˆã¯åŸºæº–å€¤ã‚’ä½¿ç”¨
            need_df[date_col] = df_heat['need'].values
    
    need_df = need_df.fillna(0)
    total_calculated_need = need_df.sum().sum()
    log.info(f"[ROLE_DYNAMIC_NEED] {heat_key}: Successfully calculated, total_need={total_calculated_need:.2f}")
    
    return need_df

def date_with_weekday(date_str: str) -> str:
    """æ—¥ä»˜æ–‡å­—åˆ—ã«æ›œæ—¥ã‚’è¿½åŠ """
    try:  # noqa: E722
        date = pd.to_datetime(date_str)
        weekdays = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        return f"{date.strftime('%m/%d')}({weekdays[date.weekday()]})"
    except Exception:
        return str(date_str)


@lru_cache(maxsize=8)
def safe_read_parquet(filepath: Path) -> pd.DataFrame:
    """Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã¿çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    try:
        if not filepath.exists():
            log.debug(f"File does not exist: {filepath}")
            return pd.DataFrame()
        
        if filepath.stat().st_size == 0:
            log.warning(f"Empty file detected: {filepath}")
            return pd.DataFrame()
            
        df = pd.read_parquet(filepath)
        log.debug(f"Successfully loaded {filepath}: shape={df.shape}")
        return df
    except FileNotFoundError:
        log.debug(f"File not found: {filepath}")
        return pd.DataFrame()
    except pd.errors.EmptyDataError:
        log.warning(f"Empty data in file: {filepath}")
        return pd.DataFrame()
    except Exception as e:
        log.warning(f"Failed to read {filepath}: {type(e).__name__}: {e}")
        return pd.DataFrame()


@lru_cache(maxsize=8)
def safe_read_csv(filepath: Path) -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã¿çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    try:
        return pd.read_csv(filepath)  # type: ignore
    except Exception as e:
        log.warning(f"Failed to read {filepath}: {e}")
        return pd.DataFrame()


def clear_data_cache() -> None:
    """Clear cached data when the scenario changes with resource monitoring."""
    memory_before = get_memory_usage()
    log.info(f"Data cache clear started. Memory before: {memory_before['rss_mb']:.1f}MB")
    
    DATA_CACHE.clear()
    safe_read_parquet.cache_clear()
    safe_read_csv.cache_clear()
    
    # ç©æ¥µçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    gc.collect()
    
    memory_after = get_memory_usage()
    memory_freed = memory_before['rss_mb'] - memory_after['rss_mb']
    log.info(f"Data cache cleared. Memory after: {memory_after['rss_mb']:.1f}MB (freed: {memory_freed:.1f}MB)")


# === ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–æ©Ÿèƒ½ï¼ˆå®‰å®šæ€§å‘ä¸Šã®ãŸã‚è¿½åŠ ï¼‰ ===
def get_memory_usage() -> Dict[str, float]:
    """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    try:
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        return {
            "rss_mb": memory_info.rss / 1024 / 1024,  # å®Ÿéš›ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            "vms_mb": memory_info.vms / 1024 / 1024,  # ä»®æƒ³ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            "percent": process.memory_percent(),       # ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã«å¯¾ã™ã‚‹å‰²åˆ
        }
    except Exception as e:
        log.warning(f"Memory usage monitoring failed: {e}")
        return {"rss_mb": 0, "vms_mb": 0, "percent": 0}

def check_memory_pressure() -> bool:
    """ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    memory_info = get_memory_usage()
    # å‹•çš„é–¾å€¤: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®è¨­å®šã«åŸºã¥ã
    threshold = 80  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ80%
    if 'cache_manager' in globals():
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã€ãã®è¨­å®šã‚’ä½¿ç”¨
        current_mb = memory_info.get("rss_mb", 0)
        max_mb = cache_manager.max_memory_mb
        if max_mb > 0:
            usage_percent = (current_mb / max_mb) * 100
            return usage_percent > 90  # æœ€å¤§ãƒ¡ãƒ¢ãƒªã®90%ã§è­¦å‘Š
    return memory_info["percent"] > threshold

def emergency_cleanup():
    """ç·Šæ€¥ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆæ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼‰"""
    log.warning("ç·Šæ€¥ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™")
    memory_before = get_memory_usage()
    
    # æ®µéš1: å„ªå…ˆåº¦ã®ä½ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
    if 'cache_manager' in globals():
        if hasattr(cache_manager.synergy_cache, 'clear'):
            cache_manager.synergy_cache.clear()
            log.info("Stage 1: SYNERGYã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢")
    
    # ãƒ¡ãƒ¢ãƒªåœ§è¿«ãŒç¶šãå ´åˆ
    if check_memory_pressure():
        # æ®µéš2: é–¢æ•°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        if 'safe_read_parquet' in globals():
            safe_read_parquet.cache_clear()
        if 'safe_read_csv' in globals():
            safe_read_csv.cache_clear()
        log.info("Stage 2: é–¢æ•°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢")
    
    # ãã‚Œã§ã‚‚ãƒ¡ãƒ¢ãƒªåœ§è¿«ãŒç¶šãå ´åˆ
    if check_memory_pressure():
        # æ®µéš3: ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚‚ã‚¯ãƒªã‚¢
        if 'cache_manager' in globals():
            cache_manager.data_cache.clear()
        log.info("Stage 3: ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢")
    
    # å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
    gc.collect()
    
    memory_after = get_memory_usage()
    freed_mb = memory_before['rss_mb'] - memory_after['rss_mb']
    log.info(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {freed_mb:.1f}MBè§£æ”¾ (ä½¿ç”¨é‡: {memory_after['rss_mb']:.1f}MB, {memory_after['percent']:.1f}%)")

# æ–°ã—ã„å®‰å®šæ€§å‘ä¸Šç‰ˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ©ãƒƒãƒ‘ãƒ¼
def safe_callback_enhanced(func):
    """Enhanced safe callback with resource monitoring and better error handling."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        memory_start = get_memory_usage()
        
        try:
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¬ãƒƒã‚·ãƒ£ãƒ¼ãƒã‚§ãƒƒã‚¯
            if check_memory_pressure():
                log.warning(f"High memory usage before {func.__name__}: {memory_start['percent']:.1f}%")
                emergency_cleanup()
            
            # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå®Ÿè¡Œ
            if gc.get_count()[0] > 700:
                gc.collect()
                
            # å®Ÿéš›ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ
            result = func(*args, **kwargs)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ­ã‚°
            execution_time = time.time() - start_time
            memory_end = get_memory_usage()
            memory_delta = memory_end['rss_mb'] - memory_start['rss_mb']
            
            if execution_time > 5:  # 5ç§’ä»¥ä¸Šã®å‡¦ç†ã‚’è­¦å‘Š
                log.warning(f"Slow callback {func.__name__}: {execution_time:.2f}s, memory delta: {memory_delta:+.1f}MB")
            elif execution_time > 1:  # 1ç§’ä»¥ä¸Šã‚’æƒ…å ±ãƒ­ã‚°
                log.info(f"Callback {func.__name__}: {execution_time:.2f}s, memory delta: {memory_delta:+.1f}MB")
                
            return result
            
        except PreventUpdate:
            # PreventUpdateã¯æ­£å¸¸ãªãƒ•ãƒ­ãƒ¼ãªã®ã§å†ç™ºç”Ÿ
            raise
        except FileNotFoundError as e:
            log.error(f"File not found: {e}")
            return html.Div([
                ui_improvements.create_user_friendly_error("file_not_found") if ui_improvements else html.Div([html.H4("Error: File not found", style={'color': 'red'}), html.P(str(e)), html.P("Please check if the file was uploaded correctly.")])
            ])
        except pd.errors.EmptyDataError:
            log.error("Empty dataframe")
            return html.Div([
                ui_improvements.create_user_friendly_error("empty_data") if ui_improvements else html.Div([html.H4("Error: Data is empty", style={'color': 'orange'}), html.P("The data file may be empty or corrupted.")])
            ])
        except MemoryError as e:
            log.error(f"Memory error: {e}")
            emergency_cleanup()
            return html.Div([
                ui_improvements.create_user_friendly_error("memory_error") if ui_improvements else html.Div([html.H4("Memory Error", style={'color': 'red'}), html.P("Memory shortage occurred. Cache has been cleared."), html.P("Please refresh the browser or try with smaller dataset.")])
            ])
        except TimeoutError as e:
            log.error(f"Timeout: {e}")
            return html.Div([
                html.H4("Processing Timeout", style={'color': 'orange'}),
                html.P("Processing is taking too long. Please reduce data size or wait.")
            ])
        except Exception as e:
            log.exception(f"Unexpected error in {func.__name__}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if check_memory_pressure():
                emergency_cleanup()
                
            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’éœ²å‡ºã—ãªã„
            import uuid
            error_id = str(uuid.uuid4())[:8]
            
            # å†…éƒ¨ãƒ­ã‚°ã«ã¯å®Œå…¨ãªæƒ…å ±ã‚’è¨˜éŒ²
            log.error(f"Error ID {error_id}: Full stack trace for {func.__name__}", exc_info=True)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯å®‰å…¨ãªæƒ…å ±ã®ã¿è¡¨ç¤º
            error_type = type(e).__name__
            safe_error_msg = {
                'FileNotFoundError': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'PermissionError': 'ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒã‚ã‚Šã¾ã›ã‚“',
                'ValueError': 'ãƒ‡ãƒ¼ã‚¿å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“',
                'KeyError': 'ãƒ‡ãƒ¼ã‚¿é …ç›®ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'MemoryError': 'ãƒ¡ãƒ¢ãƒªä¸è¶³ãŒç™ºç”Ÿã—ã¾ã—ãŸ',
                'TimeoutError': 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ã¾ã—ãŸ'
            }.get(error_type, 'äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ')
            
            return html.Div([
                html.H4(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", style={'color': 'red'}),
                html.P(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {safe_error_msg}"),
                html.P(f"ã‚¨ãƒ©ãƒ¼ID: {error_id}", style={'fontSize': '12px', 'color': '#666'}),
                html.P("å•é¡ŒãŒç¶šãå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼IDã¨å…±ã«ç®¡ç†è€…ã«ãŠå•ã„åˆã‚ã›ãã ã•ã„ã€‚")
            ])
    return wrapper

# çµ±ä¸€ã•ã‚ŒãŸsafe_callbacké–¢æ•°ï¼ˆEnhancedç‰ˆã‚’ä½¿ç”¨ï¼‰
safe_callback = safe_callback_enhanced


def data_get(key: str, default=None, for_display: bool = False):
    """Load a data asset lazily from the current scenario directory with enhanced stability."""
    log.debug(f"data_get('{key}'): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œç´¢ä¸­...")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆThreadSafeLRUCacheã‚’ä½¿ç”¨ï¼‰
    cached_value = DATA_CACHE.get(key)
    if cached_value is not None:
        log.debug(f"data_get('{key}'): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ")
        return cached_value

    log.debug(f"data_get('{key}'): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã€‚ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ã‚’é–‹å§‹...")

    if CURRENT_SCENARIO_DIR is None:
        log.warning(f"CURRENT_SCENARIO_DIRãŒæœªè¨­å®šã®ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚key={key}")
        default_value = default if default is not None else pd.DataFrame()
        DATA_CACHE.set(key, default_value)
        return default_value

    search_dirs = [CURRENT_SCENARIO_DIR, CURRENT_SCENARIO_DIR.parent]
    log.debug(f"Searching {search_dirs} for key {key}")

    # Special file names
    special = {
        "long_df": ["intermediate_data.parquet"],
        "daily_cost": ["daily_cost.parquet", "daily_cost.xlsx"],
        "shortage_time": ["shortage_time_CORRECTED.parquet", "shortage_time.parquet"],
        "need_per_date_slot": ["need_per_date_slot.parquet"],
    }
    
    # â˜…â˜…â˜… è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥è©³ç´°Needå€¤ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢å¯¾å¿œ â˜…â˜…â˜…
    if key.startswith("need_per_date_slot_role_") or key.startswith("need_per_date_slot_emp_"):
        filenames = [f"{key}.parquet"]
    else:
        filenames = special.get(key, [f"{key}.parquet", f"{key}.csv", f"{key}.xlsx"])

    # Special handling for need_per_date_slot
    if key == "need_per_date_slot":
        for directory in search_dirs:
            fp = directory / "need_per_date_slot.parquet"
            if fp.exists():
                try:
                    log.debug(f"Loading need_per_date_slot from {fp}")
                    
                    # è¤‡æ•°ã®æ–¹æ³•ã§datetimeå•é¡Œã‚’å›é¿
                    df = None
                    
                    # æ–¹æ³•1: PyArrowãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦èª­ã¿è¾¼ã¿ã€ã‚«ãƒ©ãƒ åã‚’æ‰‹å‹•å‡¦ç†
                    try:
                        table = pq.read_table(fp)
                        # ã‚«ãƒ©ãƒ åã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                        new_columns = [str(col) for col in table.column_names]
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›ï¼ˆindexå‡¦ç†ï¼‰
                        df = table.to_pandas()
                        df.columns = new_columns
                        
                        log.debug(f"Method 1 success: PyArrow table conversion")
                        
                    except Exception as e1:
                        log.debug(f"Method 1 failed: {e1}")
                        
                        # æ–¹æ³•2: pandasç›´æ¥èª­ã¿è¾¼ã¿ï¼ˆtypes_mapperãªã—ï¼‰
                        try:
                            df = pd.read_parquet(fp, engine='pyarrow', 
                                               use_nullable_dtypes=False)
                            df.columns = [str(col) for col in df.columns]
                            log.debug(f"Method 2 success: Direct pandas read")
                            
                        except Exception as e2:
                            log.debug(f"Method 2 failed: {e2}")
                            
                            # æ–¹æ³•3: æœ€å¾Œã®æ‰‹æ®µ - ãƒ•ã‚¡ã‚¤ãƒ«å†ä½œæˆ
                            try:
                                # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’åˆ¥ã®æ–¹æ³•ã§èª­ã¿è¾¼ã¿
                                temp_table = pq.read_table(fp)
                                temp_df = temp_table.to_pandas(types_mapper=pd.ArrowDtype)
                                temp_df.columns = [str(col) for col in temp_df.columns]
                                
                                # ä¸€æ™‚çš„ã«CSVçµŒç”±ã§å¤‰æ›
                                temp_csv = fp.with_suffix('.temp.csv')
                                temp_df.to_csv(temp_csv)
                                df = pd.read_csv(temp_csv, index_col=0)
                                temp_csv.unlink()  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
                                
                                log.debug(f"Method 3 success: CSV conversion")
                                
                            except Exception as e3:
                                log.error(f"All methods failed: {e1}, {e2}, {e3}")
                                raise e3
                    
                    if df is not None:
                        DATA_CACHE.set(key, df)
                        log.info(f"Successfully loaded need_per_date_slot: {df.shape}")
                        return df
                    else:
                        raise ValueError("Failed to load parquet file with any method")
                except Exception as e:
                    log.warning(f"Failed to load {fp}: {e}")
                    continue
        log.warning("need_per_date_slot.parquet not found")
        empty_df = pd.DataFrame()
        DATA_CACHE.set(key, empty_df)
        return empty_df

    for name in filenames:
        for directory in search_dirs:
            fp = directory / name
            log.debug(f"Checking {fp}")
            if fp.suffix == ".parquet" and fp.exists():
                df = safe_read_parquet(fp)
                if not df.empty:
                    # ä¼‘æ—¥é™¤å¤–ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
                    if key in ['pre_aggregated_data', 'long_df', 'intermediate_data']:
                        df = apply_rest_exclusion_filter(df, f"data_get({key})", for_display=for_display)
                    DATA_CACHE.set(key, df)
                    log.debug(f"Loaded {fp} into cache for {key}")
                    return df
                break
            if fp.suffix == ".csv" and fp.exists():
                df = safe_read_csv(fp)
                if not df.empty:
                    # ä¼‘æ—¥é™¤å¤–ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
                    if key in ['pre_aggregated_data', 'long_df', 'intermediate_data']:
                        df = apply_rest_exclusion_filter(df, f"data_get({key})", for_display=for_display)
                    DATA_CACHE.set(key, df)
                    log.debug(f"Loaded {fp} into cache for {key}")
                    return df
                break
            if fp.suffix == ".xlsx" and fp.exists():
                df = safe_read_excel(fp)
                if not df.empty:
                    # ä¼‘æ—¥é™¤å¤–ãŒå¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ã«å¯¾ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é©ç”¨
                    if key in ['pre_aggregated_data', 'long_df', 'intermediate_data']:
                        df = apply_rest_exclusion_filter(df, f"data_get({key})", for_display=for_display)
                    DATA_CACHE.set(key, df)
                    log.debug(f"Loaded {fp} into cache for {key}")
                    return df
                break

    if key == "summary_report":
        files = sorted(CURRENT_SCENARIO_DIR.glob("OverShortage_SummaryReport_*.md"))
        if files:
            text = files[-1].read_text(encoding="utf-8")
            DATA_CACHE.set(key, text)
            log.debug(f"Loaded summary report {files[-1]}")
            return text
    if key in {"roles", "employments"}:
        roles, employments = load_shortage_meta(CURRENT_SCENARIO_DIR)
        DATA_CACHE.set("roles", roles)
        DATA_CACHE.set("employments", employments)
        return DATA_CACHE.get(key, default)

    if key == "shortage_events":
        df_events = over_shortage_log.list_events(CURRENT_SCENARIO_DIR)
        DATA_CACHE.set(key, df_events)
        DATA_CACHE.set("shortage_log_path", str(Path(CURRENT_SCENARIO_DIR) / "over_shortage_log.csv"))
        return DATA_CACHE.get(key, default)

    # ğŸ¯ é«˜åº¦åˆ†æçµæœã®èª­ã¿è¾¼ã¿ (app.pyçµ±åˆæ©Ÿèƒ½)
    if key == "advanced_analysis":
        advanced_results = load_advanced_analysis_results(CURRENT_SCENARIO_DIR)
        DATA_CACHE.set(key, advanced_results)
        return advanced_results
    
    if key == "forecast_data":
        forecast_file = CURRENT_SCENARIO_DIR / "forecast.parquet"
        if forecast_file.exists():
            forecast_df = safe_read_parquet(forecast_file)
            DATA_CACHE.set(key, forecast_df)
            return forecast_df
    
    if key == "mind_reader_analysis":
        # Mind Readeråˆ†æçµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã¾ãŸã¯å®Ÿè¡Œ
        cache_key = f"mind_reader_{get_data_hash()}"
        cached_result = DATA_CACHE.get(cache_key)
        if cached_result is not None:
            return cached_result
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æå®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
        long_df = data_get('long_df')
        if long_df is not None and not long_df.empty:
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
            if psutil and psutil.virtual_memory().percent > 80:
                log.warning("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒé«˜ã„ãŸã‚Mind Readeråˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return {'status': 'skipped', 'reason': 'high_memory_usage'}
            
            mind_reader = ShiftMindReader()
            try:
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆ30ç§’ï¼‰
                import signal
                def timeout_handler(signum, frame):
                    raise TimeoutError("Mind Reader analysis timed out")
                
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(30)  # 30ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                
                mind_results = mind_reader.read_creator_mind(long_df)
                signal.alarm(0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
                
                DATA_CACHE.set(cache_key, mind_results)
                return mind_results
            except TimeoutError:
                log.warning("Mind Readeråˆ†æãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
                return {'status': 'timeout', 'reason': 'analysis_timeout'}
            except Exception as e:
                log.warning(f"Mind Readeråˆ†æã«å¤±æ•—: {e}")
                return {'status': 'error', 'reason': str(e)}
            finally:
                signal.alarm(0)  # ç¢ºå®Ÿã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè§£é™¤
    
    log.debug(f"ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ '{key}' ã«å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    DATA_CACHE.set(key, default)
    return default



def load_advanced_analysis_results(scenario_dir: Path) -> Dict[str, Any]:
    """
    app.pyã®é«˜åº¦åˆ†æçµæœã‚’èª­ã¿è¾¼ã‚€ï¼ˆãƒ¡ãƒ¢ãƒªç›£è¦–ä»˜ãï¼‰
    
    Returns:
        Dict containing:
        - forecast: éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
        - ml_predictions: æ©Ÿæ¢°å­¦ç¿’äºˆæ¸¬çµæœ
        - work_patterns: ä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        - cost_benefit: ã‚³ã‚¹ãƒˆä¾¿ç›Šåˆ†æ
        - network_analysis: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åŠ¹æœåˆ†æ
    """
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
    if psutil and psutil.virtual_memory().percent > 85:
        log.warning("ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ãŒ85%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ç·Šæ€¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
        emergency_memory_cleanup()
    
    results = {
        'status': 'loaded',
        'timestamp': pd.Timestamp.now(),
        'source_dir': str(scenario_dir)
    }
    
    try:
        # ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯å¼·åŒ–
        if not scenario_dir.exists():
            log.warning(f"ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {scenario_dir}")
            results['error'] = f"Directory not found: {scenario_dir}"
            return results
        
        # éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
        forecast_file = scenario_dir / "forecast.parquet"
        if forecast_file.exists() and forecast_file.stat().st_size > 0:
            try:
                results['forecast'] = safe_read_parquet(forecast_file)
                log.info(f"ğŸ“ˆ éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {forecast_file}")
            except Exception as e:
                log.error(f"éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                results['forecast_error'] = str(e)
        
        # äºˆæ¸¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        forecast_json = scenario_dir / "forecast.json"
        if forecast_json.exists() and forecast_json.stat().st_size > 0:
            try:
                with open(forecast_json, 'r', encoding='utf-8') as f:
                    results['forecast_metadata'] = json.load(f)
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                log.error(f"äºˆæ¸¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                results['forecast_metadata_error'] = str(e)
        
        # MLäºˆæ¸¬çµæœ
        ml_files = list(scenario_dir.glob("stats_*.parquet"))
        if ml_files:
            results['ml_predictions'] = {}
            for ml_file in ml_files:
                key = ml_file.stem.replace('stats_', '')
                results['ml_predictions'][key] = safe_read_parquet(ml_file)
                log.info(f"ğŸ¤– MLäºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿: {key}")
        
        # ä½œæ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns_file = scenario_dir / "work_patterns.parquet"
        if patterns_file.exists():
            results['work_patterns'] = safe_read_parquet(patterns_file)
        
        # ã‚³ã‚¹ãƒˆä¾¿ç›Šåˆ†æ
        cost_file = scenario_dir / "cost_benefit.parquet"
        if cost_file.exists():
            results['cost_benefit'] = safe_read_parquet(cost_file)
        
        # Blueprintåˆ†æçµæœ
        blueprint_file = scenario_dir / "blueprint_analysis.json"
        if blueprint_file.exists():
            with open(blueprint_file, 'r', encoding='utf-8') as f:
                results['blueprint_analysis'] = json.load(f)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æçµæœ
        network_file = scenario_dir / "network_analysis.parquet"
        if network_file.exists():
            results['network_analysis'] = safe_read_parquet(network_file)
        
        log.info(f"ğŸ¯ é«˜åº¦åˆ†æçµæœèª­ã¿è¾¼ã¿å®Œäº†: {len(results)-3}é …ç›®")
        
    except Exception as e:
        log.error(f"é«˜åº¦åˆ†æçµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
        results['error'] = str(e)
    
    return results


def get_data_hash() -> str:
    """ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿ã®ç°¡æ˜“ãƒãƒƒã‚·ãƒ¥å€¤ã‚’ç”Ÿæˆ"""
    try:
        long_df = DATA_CACHE.get('long_df')
        if long_df is not None and not long_df.empty:
            # DataFrameã®shapeã¨ã‚«ãƒ©ãƒ åã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
            hash_str = f"{long_df.shape}_{list(long_df.columns)}"
            return str(hash(hash_str))
    except Exception:
        pass
    return f"default_{int(time.time())}"


def calc_ratio_from_heatmap_integrated(df: pd.DataFrame) -> pd.DataFrame:
    """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸è¶³ç‡ã‚’è¨ˆç®—ï¼ˆçµ±åˆã‚·ã‚¹ãƒ†ãƒ å¯¾å¿œç‰ˆï¼‰"""
    if df.empty:
        return pd.DataFrame()

    date_cols = [c for c in df.columns if pd.to_datetime(c, errors='coerce') is not pd.NaT]
    if not date_cols:
        return pd.DataFrame()

    # çµ±åˆã‚·ã‚¹ãƒ†ãƒ : need_per_date_slotã‚’å„ªå…ˆä½¿ç”¨
    need_per_date_df = data_get('need_per_date_slot')
    
    if not need_per_date_df.empty:
        # need_per_date_slotã‹ã‚‰need_dfã‚’ä½œæˆ
        log.info(f"Using need_per_date_slot for accurate need calculation: {need_per_date_df.shape}")
        
        # æ—¥ä»˜åˆ—ã®äº¤é›†åˆã‚’å–å¾—
        available_dates = [col for col in date_cols if str(col) in need_per_date_df.columns]
        
        if available_dates:
            need_df = need_per_date_df[[str(col) for col in available_dates]].copy()
            need_df.columns = available_dates  # å…ƒã®åˆ—åã«æˆ»ã™
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¸€è‡´ã•ã›ã‚‹
            common_index = df.index.intersection(need_df.index)
            need_df = need_df.loc[common_index]
        else:
            log.warning("No matching dates in need_per_date_slot, falling back to average need")
            need_df = pd.DataFrame(0.0, index=df.index, columns=date_cols)
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®average needä½¿ç”¨
        if "need" in df.columns:
            log.warning("need_per_date_slot not available, using average need values")
            need_series = df["need"].fillna(0)
            need_df = pd.DataFrame(
                np.repeat(need_series.values[:, np.newaxis], len(date_cols), axis=1),
                index=need_series.index,
                columns=date_cols
            )
        else:
            need_df = pd.DataFrame(0.0, index=df.index, columns=date_cols)
    staff_df = df[date_cols].fillna(0)
    
    # ä¿®æ­£: æ—¥æ›œæ—¥ã®éå‰°è¡¨ç¤ºã‚’é˜²ããŸã‚ã€è¨ˆç®—ã‚’å¼·åŒ–
    # need_dfãŒ0ã®å ´åˆã®é©åˆ‡ãªå‡¦ç†
    valid_need_mask = need_df > 0
    ratio_df = pd.DataFrame(0.0, index=need_df.index, columns=need_df.columns)
    
    # éœ€è¦ãŒã‚ã‚‹å ´åˆã®ã¿ä¸è¶³ç‡ã‚’è¨ˆç®—
    ratio_df = ratio_df.where(~valid_need_mask, 
                             ((need_df - staff_df) / need_df).clip(lower=0))
    
    # æœ€çµ‚çš„ã«NaNå€¤ã‚’0ã§åŸ‹ã‚ã‚‹ï¼ˆæ—¥æ›œæ—¥å¯¾ç­–ï¼‰
    ratio_df = ratio_df.fillna(0)
    
    return ratio_df


def load_shortage_meta(data_dir: Path) -> Tuple[List[str], List[str]]:
    """è·ç¨®ã¨é›‡ç”¨å½¢æ…‹ã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    roles = []
    employments = []
    meta_fp = data_dir / "shortage.meta.json"
    if meta_fp.exists():
        try:
            with open(meta_fp, 'r', encoding='utf-8') as f:
                meta = json.load(f)
            roles = meta.get("roles", [])
            employments = meta.get("employments", [])
        except Exception as e:
            log.debug(f"Failed to load shortage meta: {e}")
    return roles, employments



def load_and_sum_heatmaps(data_dir: Path, keys: List[str]) -> pd.DataFrame:
    """Load multiple heatmap files and aggregate them."""
    dfs = []
    for key in keys:
        df = data_get(key)
        if df is None and data_dir:
            fp = Path(data_dir) / f"{key}.parquet"
            if fp.exists():
                df = safe_read_parquet(fp)
                if not df.empty:
                    DATA_CACHE.set(key, df)
        if isinstance(df, pd.DataFrame) and not df.empty:
            dfs.append(df)

    if not dfs:
        return pd.DataFrame()

    total = dfs[0].copy()
    for df in dfs[1:]:
        total = total.add(df, fill_value=0)

    if {"need", "staff"}.issubset(total.columns):
        total["lack"] = (total["need"] - total["staff"]).clip(lower=0)
    if {"staff", "upper"}.issubset(total.columns):
        total["excess"] = (total["staff"] - total["upper"]).clip(lower=0)

    return total


def generate_heatmap_figure(df_heat: pd.DataFrame, title: str, device_type: str = "desktop") -> go.Figure:
    """æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆã™ã‚‹ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œãƒ»ä¼‘æ—¥é™¤å¤–å¼·åŒ–ç‰ˆï¼‰"""
    if df_heat is None or df_heat.empty:
        return go.Figure().update_layout(title_text=f"{title}: ãƒ‡ãƒ¼ã‚¿ãªã—", height=300)

    date_cols = [c for c in df_heat.columns if pd.to_datetime(c, errors='coerce') is not pd.NaT]
    if not date_cols:
        return go.Figure().update_layout(title_text=f"{title}: è¡¨ç¤ºå¯èƒ½ãªæ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ãªã—", height=300)

    display_df = df_heat[date_cols].copy()
    
    # ä¼‘æ—¥é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿: äº‹å‰ç”Ÿæˆã•ã‚ŒãŸãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã«0æ™‚é–“ã®ã‚¹ãƒ­ãƒƒãƒˆãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã‚’è€ƒæ…®
    # å…¨ã¦0ã®è¡Œï¼ˆæ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆï¼‰ã‚’é™¤å¤–ï¼ˆã“ã‚Œã¯é€šå¸¸ã€å¤œé–“ã®ç„¡äººæ™‚é–“å¸¯ã‚’è¡¨ã™ï¼‰
    # ãŸã ã—ã€æ¥­å‹™æ™‚é–“å†…ã§å…¨ã¦0ã®è¡ŒãŒã‚ã‚‹å ´åˆã¯ä¼‘æ—¥ãƒ‡ãƒ¼ã‚¿ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚æ³¨æ„æ·±ãå‡¦ç†
    original_rows = len(display_df)
    
    # å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ™ãƒ«ç”Ÿæˆ
    slot_minutes = DETECTED_SLOT_INFO['slot_minutes']
    time_labels = gen_labels(slot_minutes)
    display_df = display_df.reindex(time_labels, fill_value=0)
    
    # ä¿®æ­£ç‚¹1: NaNå€¤ã‚’æ˜ç¤ºçš„ã«0ã§åŸ‹ã‚ã‚‹
    display_df = display_df.fillna(0)
    
    # ä¼‘æ—¥é™¤å¤–ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    zero_rows = (display_df == 0).all(axis=1).sum()
    if zero_rows > 0:
        log.debug(f"[Heatmap] {title}: {zero_rows}å€‹ã®æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆãŒå…¨æ—¥ç¨‹ã§0äºº (ä¼‘æ—¥æ™‚é–“å¸¯ã®å¯èƒ½æ€§)")
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    total_values = display_df.values.sum()
    if total_values == 0:
        log.warning(f"[Heatmap] {title}: å…¨ãƒ‡ãƒ¼ã‚¿ãŒ0ã§ã™")
    
    # ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
    display_df = display_df.apply(pd.to_numeric, errors='coerce').fillna(0)
    
    display_df_renamed = display_df.copy()
    display_df_renamed.columns = [date_with_weekday(c) for c in display_df.columns]

    # ğŸ¯ ä¿®æ­£: 60æ—¥åˆ¶é™ã‚’å‰Šé™¤ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç‰ˆã¨åŒã˜å…¨æœŸé–“è¡¨ç¤ºï¼‰
    # å…ˆç¨‹ã®60æ—¥åˆ¶é™ã‚’å‰Šé™¤ã—ã€å…¨ã¦ã®æ—¥ä»˜ã‚’è¡¨ç¤ºã™ã‚‹

    # ä¿®æ­£ç‚¹2: text_autoã‚’è¿½åŠ ã—ã¦ã€0å€¤ã‚‚è¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
    # ğŸ¯ ä¿®æ­£: ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«æ˜ç¤ºçš„ãªç¯„å›²ã‚’è¨­å®šï¼ˆå˜è‰²å•é¡Œå¯¾ç­–ï¼‰
    data_max = display_df_renamed.max().max()
    data_min = display_df_renamed.min().min()
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
    log.info(f"[Heatmap Debug] {title}: data_min={data_min}, data_max={data_max}, shape={display_df_renamed.shape}")
    
    # ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ç¯„å›²ã®è¨­å®šï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ï¼‰
    if data_max == 0:
        color_range = [0, 1]  # å…¨ã¦0ã®å ´åˆ
    else:
        color_range = [0, data_max]  # 0ã‹ã‚‰æœ€å¤§å€¤ã¾ã§
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œå¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
    if visualization_engine:
        try:
            fig = visualization_engine.create_responsive_heatmap(
                display_df_renamed,
                title=title,
                device_type=device_type
            )
        except Exception as e:
            log.warning(f"å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚¨ãƒ©ãƒ¼ã€å¾“æ¥æ–¹æ³•ã«åˆ‡ã‚Šæ›¿ãˆ: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é«˜è¦–èªæ€§ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«
            improved_colorscale = [
                [0, 'white'],           # 0å€¤ã®ã¿ç™½
                [0.0001, '#ffeb3b'],    # 1-2äººã¯é»„è‰²ï¼ˆéå¸¸ã«ç›®ç«‹ã¤ï¼‰
                [0.1, '#ff9800'],       # å°‘ãªã„å€¤ã¯ã‚ªãƒ¬ãƒ³ã‚¸
                [0.3, '#f44336'],       # ä¸­å°å€¤ã¯èµ¤
                [0.5, '#9c27b0'],       # ä¸­é–“å€¤ã¯ç´«
                [0.7, '#3f51b5'],       # ä¸­å¤§å€¤ã¯é’
                [0.85, '#2196f3'],      # å¤§ãã„å€¤ã¯æ˜ã‚‹ã„é’
                [1, '#0d47a1']          # æœ€å¤§å€¤ã¯æ¿ƒã„é’
            ]
            
            # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã®åˆ¶å¾¡ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå‹•çš„åˆ¤å®šï¼‰
            max_val = display_df_renamed.max().max()
            # è·ç¨®åˆ¥ã‚„ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã‚’ç„¡åŠ¹åŒ–
            is_role_specific = any(keyword in title.lower() for keyword in ['è·ç¨®', 'role', 'çœ‹è­·å¸«', 'ãƒ‰ã‚¯ã‚¿ãƒ¼', 'è–¬å‰¤å¸«'])
            show_text_auto = max_val <= 3 and not is_role_specific  # ã•ã‚‰ã«å³ã—ã„æ¡ä»¶
            
            fig = px.imshow(
                display_df_renamed,
                aspect='auto',
                color_continuous_scale=improved_colorscale,
                title=title,
                labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'äººæ•°'},
                text_auto=show_text_auto,  # å€¤ãŒå°ã•ã„å ´åˆã®ã¿ã‚»ãƒ«ã«å€¤ã‚’è¡¨ç¤º
                zmin=color_range[0],
                zmax=color_range[1]
            )
    else:
        # å˜è‰²ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒ–ãƒ«ãƒ¼ï¼‰
        # ç›´æ„Ÿçš„: "è‰²ãŒæ¿ƒã„ã»ã©äººãŒå¤šã„"
        professional_blue_scale = [
            [0, '#f8f9ff'],         # æœ€å°‘äººæ•°: éå¸¸ã«è–„ã„æ°´è‰²
            [0.1, '#e3f2fd'],       # å°‘ãªã„: è–„ã„æ°´è‰²
            [0.2, '#bbdefb'],       # ã‚„ã‚„å°‘ãªã„: æ˜ã‚‹ã„é’
            [0.3, '#90caf9'],       # æ™®é€š: ä¸­é–“ã®æ˜ã‚‹ã„é’
            [0.4, '#64b5f6'],       # ã‚„ã‚„å¤šã„: ä¸­é–“ã®é’
            [0.5, '#42a5f5'],       # å¤šã„: ã—ã£ã‹ã‚Šã—ãŸé’
            [0.6, '#2196f3'],       # ã‹ãªã‚Šå¤šã„: æ¿ƒã„é’
            [0.7, '#1e88e5'],       # ç›¸å½“å¤šã„: ã‚ˆã‚Šæ¿ƒã„é’
            [0.8, '#1976d2'],       # éå¸¸ã«å¤šã„: æ·±ã„é’
            [0.9, '#1565c0'],       # æœ€é«˜ãƒ¬ãƒ™ãƒ«: ãƒ€ãƒ¼ã‚¯ãƒ–ãƒ«ãƒ¼
            [1.0, '#0d47a1']        # æœ€å¤§äººæ•°: æ¿ƒã„ç´ºè‰²
        ]
        
        # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã®åˆ¶å¾¡ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãå‹•çš„åˆ¤å®šï¼‰
        max_val = display_df_renamed.max().max()
        # è·ç¨®åˆ¥ã‚„ãƒ‡ãƒ¼ã‚¿ãŒå¤§ãã„å ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã‚’ç„¡åŠ¹åŒ–
        is_role_specific = any(keyword in title.lower() for keyword in ['è·ç¨®', 'role', 'çœ‹è­·å¸«', 'ãƒ‰ã‚¯ã‚¿ãƒ¼', 'è–¬å‰¤å¸«'])
        show_text_auto = max_val <= 3 and not is_role_specific  # ã•ã‚‰ã«å³ã—ã„æ¡ä»¶
        
        fig = px.imshow(
            display_df_renamed,
            aspect='auto',
            color_continuous_scale=professional_blue_scale,
            title=title,
            labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'äººæ•°'},
            text_auto=show_text_auto,  # å€¤ãŒå°ã•ã„å ´åˆã®ã¿ã‚»ãƒ«ã«å€¤ã‚’è¡¨ç¤º
            zmin=color_range[0],
            zmax=color_range[1]
        )
    
    # ä¿®æ­£ç‚¹3: ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤ºã®èª¿æ•´ï¼ˆpx.imshowã§åˆ¶å¾¡æ¸ˆã¿ï¼‰
    # ãƒ›ãƒãƒ¼è¡¨ç¤ºã®æ”¹å–„
    fig.update_layout(
        hoverlabel=dict(
            bgcolor="white",
            font_size=12,
            font_family="Arial"
        )
    )
    
    # ã‚¹ãƒ­ãƒƒãƒˆé–“éš”ã«å¿œã˜ãŸæœ€é©åŒ–
    confidence_info = ""
    if DETECTED_SLOT_INFO['auto_detected']:
        confidence_info = f" (æ¤œå‡ºã‚¹ãƒ­ãƒƒãƒˆ: {slot_minutes}åˆ†, ä¿¡é ¼åº¦: {DETECTED_SLOT_INFO['confidence']:.2f})"
    
    fig.update_layout(
        height=600,
        xaxis_title="æ—¥ä»˜",
        yaxis_title=f"æ™‚é–“{confidence_info}",
        title_x=0.5,
        # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ
        autosize=True,
        # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºæœ€é©åŒ–
        font=dict(size=10 if len(display_df_renamed.columns) > 30 else 12),
        # Yè»¸ã®è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã™ï¼‰
    )
    
    # è»¸ãƒ©ãƒ™ãƒ«ã®æœ€é©åŒ–
    if len(display_df_renamed.columns) > 30:
        # å¤šãã®æ—¥ä»˜ãŒã‚ã‚‹å ´åˆã¯å›è»¢
        fig.update_xaxes(tickangle=45)
    else:
        fig.update_xaxes(tickvals=list(range(len(display_df.columns))))
    
    if slot_minutes < 30:
        # ç´°ã‹ã„ã‚¹ãƒ­ãƒƒãƒˆé–“éš”ã®å ´åˆã¯yè»¸ãƒ©ãƒ™ãƒ«ã‚’é–“å¼•ã
        fig.update_yaxes(dtick=2)
    
    return fig


# ä¼‘æš‡é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¯çµ±åˆç‰ˆã‚’ä½¿ç”¨ï¼ˆshift_suite.tasks.utils ã‹ã‚‰ï¼‰
from shift_suite.tasks.utils import apply_rest_exclusion_filter

def create_enhanced_rest_exclusion_filter(df: pd.DataFrame) -> pd.DataFrame:
    """
    çµ±åˆç‰ˆä¼‘æ—¥é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰
    """
    return apply_rest_exclusion_filter(df, "dashboard")

def optimize_heatmap_data(df: pd.DataFrame, max_days: int = 60, remove_zero_rows: bool = False) -> pd.DataFrame:
    """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹"""
    if df.empty:
        return df
    
    # æ—¥ä»˜åˆ—ã‚’ç‰¹å®š
    date_cols = [c for c in df.columns if pd.to_datetime(c, errors='coerce') is not pd.NaT]
    
    if not date_cols:
        return df
    
    # æœ€æ–°ã®æ—¥ä»˜ã‹ã‚‰æŒ‡å®šæ•°ã ã‘ã‚’å–å¾—
    if len(date_cols) > max_days:
        log.info(f"[Heatmapæœ€é©åŒ–] {len(date_cols)}æ—¥ -> ç›´è¿‘{max_days}æ—¥ã«åˆ¶é™")
        # æ—¥ä»˜ã‚’ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ã®ã‚‚ã®ã‚’å–å¾—
        sorted_dates = sorted(date_cols, key=lambda x: pd.to_datetime(x, errors='coerce'))
        recent_dates = sorted_dates[-max_days:]
        
        # å¿…è¦ãªåˆ—ã®ã¿ã‚’å–å¾—
        non_date_cols = [c for c in df.columns if c not in date_cols]
        optimized_df = df[non_date_cols + recent_dates].copy()
    else:
        optimized_df = df.copy()
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
    numeric_cols = optimized_df.select_dtypes(include=[np.number]).columns
    optimized_df[numeric_cols] = optimized_df[numeric_cols].fillna(0)
    
    # å…¨ã¦0ã®è¡Œã‚’é™¤å»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if remove_zero_rows and len(numeric_cols) > 0:
        zero_rows = (optimized_df[numeric_cols] == 0).all(axis=1)
        if zero_rows.any():
            rows_removed = zero_rows.sum()
            log.info(f"[Heatmapæœ€é©åŒ–] å…¨ã¦0ã®è¡Œã‚’{rows_removed}è¡Œé™¤å»")
            optimized_df = optimized_df[~zero_rows]
    
    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€é©åŒ–
    for col in numeric_cols:
        if optimized_df[col].dtype == 'float64':
            # å€¤ã®ç¯„å›²ã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿å‹ã‚’æœ€é©åŒ–
            max_val = optimized_df[col].max()
            if max_val <= 255:
                optimized_df[col] = optimized_df[col].astype('uint8')
            elif max_val <= 32767:
                optimized_df[col] = optimized_df[col].astype('int16')
            else:
                optimized_df[col] = optimized_df[col].astype('float32')
    
    return optimized_df

def create_knowledge_network_graph(network_data: Dict) -> cyto.Cytoscape:
    """Return an interactive network graph of implicit knowledge."""
    nodes = [
        {"data": {"id": n["id"], "label": n["label"]}}
        for n in network_data.get("nodes", [])
    ]
    edges = [
        {
            "data": {
                "source": e.get("from"),
                "target": e.get("to"),
                "label": e.get("label", ""),
            }
        }
        for e in network_data.get("edges", [])
    ]

    return cyto.Cytoscape(
        id="knowledge-network-graph",
        elements=nodes + edges,
        style={"width": "100%", "height": "500px"},
        layout={"name": "cose"},
        stylesheet=[
            {"selector": "node", "style": {"content": "data(label)", "font-size": "10px"}},
            {
                "selector": "edge",
                "style": {
                    "label": "data(label)",
                    "font-size": "8px",
                    "curve-style": "bezier",
                },
            },
        ],
    )

# --- UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆç”Ÿæˆé–¢æ•° ---
def create_metric_card(label: str, value: str, color: str = "#1f77b4") -> html.Div:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚«ãƒ¼ãƒ‰ã‚’ä½œæˆ"""
    return html.Div([
        html.Div(label, style={  # type: ignore
            'fontSize': '14px',
            'color': '#666',
            'marginBottom': '5px'
        }),
        html.Div(value, style={  # type: ignore
            'fontSize': '24px',
            'fontWeight': 'bold',
            'color': color
        })
    ], style={
        'padding': '15px',
        'backgroundColor': 'white',
        'borderRadius': '8px',
        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)',
        'textAlign': 'center',
        'minHeight': '80px'
    })


def create_overview_tab(selected_scenario: str = None) -> html.Div:
    """æ¦‚è¦ã‚¿ãƒ–ã‚’ä½œæˆï¼ˆçµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ©Ÿèƒ½ã‚’å«ã‚€ï¼‰"""
    # æŒ‰åˆ†æ–¹å¼ã«ã‚ˆã‚‹ä¸€è²«ãƒ‡ãƒ¼ã‚¿å–å¾—
    df_shortage_role = data_get('shortage_role_summary', pd.DataFrame())
    df_shortage_emp = data_get('shortage_employment_summary', pd.DataFrame())
    df_fairness = data_get('fairness_before', pd.DataFrame())
    df_staff = data_get('staff_stats', pd.DataFrame())
    df_alerts = data_get('stats_alerts', pd.DataFrame())
    
    # çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®åˆæœŸåŒ–
    comprehensive_dashboard_content = None
    global CURRENT_SCENARIO_DIR
    
    if ComprehensiveDashboard is not None and CURRENT_SCENARIO_DIR is not None:
        try:
            output_dir = Path(CURRENT_SCENARIO_DIR)
            dashboard = create_comprehensive_dashboard(output_dir, months_back=6)
            figures = dashboard.get_dashboard_figures()
            summary_metrics = dashboard._calculate_summary_metrics()
            
            # çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ§‹ç¯‰
            comprehensive_dashboard_content = [
                html.Hr(style={'margin': '40px 0', 'border': '2px solid #3498db'}),
                html.H3("ğŸ¥ çµ±åˆã‚·ãƒ•ãƒˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", 
                       style={'color': '#2c3e50', 'marginBottom': '20px', 'textAlign': 'center'}),
                
                # ã‚µãƒãƒªãƒ¼çµ±è¨ˆã‚«ãƒ¼ãƒ‰
                html.Div([
                    html.H4("ğŸ“Š é«˜åº¦åˆ†ææŒ‡æ¨™", style={'color': '#2c3e50', 'marginBottom': '15px'}),
                    
                    html.Div([
                        # ç–²åŠ´åº¦ã‚«ãƒ¼ãƒ‰
                        html.Div([
                            html.H5("ğŸ˜´ å¹³å‡ç–²åŠ´ã‚¹ã‚³ã‚¢", style={'color': '#e74c3c', 'marginBottom': '10px'}),
                            html.H2(f"{summary_metrics.get('average_fatigue_score', 0):.1f}", 
                                   style={'color': '#e74c3c', 'margin': '0'}),
                            html.P(f"é«˜ç–²åŠ´è·å“¡: {summary_metrics.get('high_fatigue_count', 0)}å", 
                                  style={'margin': '5px 0', 'fontSize': '14px', 'color': '#666'})
                        ], style={
                            'padding': '20px',
                            'backgroundColor': '#fff5f5',
                            'borderRadius': '10px',
                            'border': '2px solid #fed7d7',
                            'textAlign': 'center',
                            'flex': '1',
                            'margin': '0 10px'
                        }),
                        
                        # å…¬å¹³æ€§ã‚«ãƒ¼ãƒ‰
                        html.Div([
                            html.H5("âš–ï¸ å¹³å‡å…¬å¹³æ€§ã‚¹ã‚³ã‚¢", style={'color': '#3498db', 'marginBottom': '10px'}),
                            html.H2(f"{summary_metrics.get('average_fairness_score', 0):.2f}", 
                                   style={'color': '#3498db', 'margin': '0'}),
                            html.P(f"è¦æ”¹å–„è·å“¡: {summary_metrics.get('low_fairness_count', 0)}å", 
                                  style={'margin': '5px 0', 'fontSize': '14px', 'color': '#666'})
                        ], style={
                            'padding': '20px',
                            'backgroundColor': '#f0f8ff',
                            'borderRadius': '10px',
                            'border': '2px solid #bde4ff',
                            'textAlign': 'center',
                            'flex': '1',
                            'margin': '0 10px'
                        }),
                        
                        # å¯¾å¿œèƒ½åŠ›ã‚«ãƒ¼ãƒ‰
                        html.Div([
                            html.H5("ğŸ”„ å¹³å‡å¯¾å¿œèƒ½åŠ›", style={'color': '#27ae60', 'marginBottom': '10px'}),
                            html.H2(f"{summary_metrics.get('average_capability_score', 0):.2f}", 
                                   style={'color': '#27ae60', 'margin': '0'}),
                            html.P(f"ãƒãƒ«ãƒã‚¹ã‚­ãƒ«è·å“¡: {summary_metrics.get('multiskill_staff_count', 0)}å", 
                                  style={'margin': '5px 0', 'fontSize': '14px', 'color': '#666'})
                        ], style={
                            'padding': '20px',
                            'backgroundColor': '#f0fff4',
                            'borderRadius': '10px',
                            'border': '2px solid #c6f6d5',
                            'textAlign': 'center',
                            'flex': '1',
                            'margin': '0 10px'
                        })
                    ], style={'display': 'flex', 'justifyContent': 'space-between', 'marginBottom': '20px'})
                ], style={
                    'padding': '20px',
                    'backgroundColor': 'white',
                    'borderRadius': '10px',
                    'boxShadow': '0 2px 8px rgba(0,0,0,0.1)',
                    'marginBottom': '30px'
                }),
                
                # çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å›³è¡¨
                html.Div([
                    html.H4("ğŸ“ˆ çµ±åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", style={'color': '#2c3e50', 'marginBottom': '15px'}),
                    dcc.Graph(
                        figure=figures.get('comprehensive', go.Figure()),
                        style={'height': '800px'}
                    )
                ], style={
                    'padding': '20px',
                    'backgroundColor': 'white',
                    'borderRadius': '10px',
                    'boxShadow': '0 2px 8px rgba(0,0,0,0.1)',
                    'marginBottom': '30px'
                }),
                
                # ç–²åŠ´åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
                html.Div([
                    html.H4("ğŸ˜´ è·å“¡åˆ¥ç–²åŠ´åº¦åˆ†æ", style={'color': '#2c3e50', 'marginBottom': '15px'}),
                    dcc.Graph(
                        figure=figures.get('fatigue_heatmap', go.Figure()),
                        style={'height': '600px'}
                    )
                ], style={
                    'padding': '20px',
                    'backgroundColor': 'white',
                    'borderRadius': '10px',
                    'boxShadow': '0 2px 8px rgba(0,0,0,0.1)',
                    'marginBottom': '30px'
                }),
                
                # èª¬æ˜ãƒ»æ“ä½œã‚¬ã‚¤ãƒ‰ 
                html.Div([
                    html.H4("ğŸ’¡ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ´»ç”¨ã‚¬ã‚¤ãƒ‰", style={'color': '#2c3e50', 'marginBottom': '15px'}),
                    html.Div([
                        html.H5("ğŸ“Š çµ±åˆåˆ†æã®è¦‹æ–¹"),
                        html.Ul([
                            html.Li("ç–²åŠ´åº¦vsæ€§èƒ½åˆ†æ - ç–²åŠ´ã¨æ€§èƒ½ã®ç›¸é–¢é–¢ä¿‚ã‚’å¯è¦–åŒ–"),
                            html.Li("å…¬å¹³æ€§ã‚¹ã‚³ã‚¢ - è·å“¡é–“ã®å‹¤å‹™è² æ‹…ã®å‡ç­‰åº¦"),
                            html.Li("å‹¤å‹™åŒºåˆ†å¯¾å¿œèƒ½åŠ› - ãƒãƒ«ãƒã‚¹ã‚­ãƒ«åº¦ï¼ˆ20åä»¥ä¸‹ã®å ´åˆã«è¡¨ç¤ºï¼‰"),
                            html.Li("è·å“¡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ - ç·åˆè©•ä¾¡ï¼ˆ20åä»¥ä¸‹ã®å ´åˆã«è¡¨ç¤ºï¼‰"),
                            html.Li("ç–²åŠ´åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— - å„è·å“¡ã®è©³ç´°ãªç–²åŠ´çŠ¶æ³")
                        ]),
                        
                        html.H5("ğŸ–±ï¸ ãƒ›ãƒãƒ¼æ©Ÿèƒ½", style={'marginTop': '20px'}),
                        html.Ul([
                            html.Li("å„ã‚°ãƒ©ãƒ•ã«ãƒã‚¦ã‚¹ã‚’å½“ã¦ã‚‹ã¨è©³ç´°æƒ…å ±ã‚’è¡¨ç¤º"),
                            html.Li("è·å“¡IDè¡¨ç¤ºæ™‚ã§ã‚‚ã€ãƒ›ãƒãƒ¼ã§å®Ÿåã¨è·ç¨®ã‚’ç¢ºèªå¯èƒ½"),
                            html.Li("ç–²åŠ´åº¦ã€å…¬å¹³æ€§ã€å¯¾å¿œèƒ½åŠ›ã®å…·ä½“çš„ãªæ•°å€¤ã‚’è¡¨ç¤º"),
                            html.Li("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã§ã¯è·ç¨®ã¨ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚‚è¡¨ç¤º")
                        ]),
                        
                        html.H5("ğŸ¯ é‡è¦ãªæŒ‡æ¨™", style={'marginTop': '20px'}),
                        html.Ul([
                            html.Li("ç–²åŠ´ã‚¹ã‚³ã‚¢7.0ä»¥ä¸Š: ç·Šæ€¥ã®ä¼‘æ¯ãŒå¿…è¦"),
                            html.Li("å…¬å¹³æ€§ã‚¹ã‚³ã‚¢0.6æœªæº€: å‹¤å‹™é…åˆ†ã®è¦‹ç›´ã—ãŒå¿…è¦"),
                            html.Li("å¯¾å¿œèƒ½åŠ›3ä»¥ä¸Š: ãƒãƒ«ãƒã‚¹ã‚­ãƒ«è·å“¡ã¨ã—ã¦è©•ä¾¡"),
                            html.Li("èµ¤è‰²è¡¨ç¤º: é‡ç‚¹çš„ãªã‚±ã‚¢ã¨ã‚µãƒãƒ¼ãƒˆãŒå¿…è¦")
                        ])
                    ], style={'fontSize': '14px', 'color': '#555'})
                ], style={
                    'padding': '20px',
                    'backgroundColor': '#f8f9fa',
                    'borderRadius': '10px',
                    'border': '1px solid #dee2e6'
                })
            ]
            
            log.info("çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’æ¦‚è¦ã‚¿ãƒ–ã«çµ±åˆã—ã¾ã—ãŸ")
            
        except Exception as e:
            log.warning(f"çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰çµ±åˆã‚¨ãƒ©ãƒ¼: {e}")
            comprehensive_dashboard_content = [
                html.Hr(style={'margin': '40px 0', 'border': '2px solid #e74c3c'}),
                html.Div([
                    html.H4("âš ï¸ çµ±åˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼", style={'color': '#e74c3c'}),
                    html.P(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}"),
                    html.P("ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚åˆ†æã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ãŠè©¦ã—ãã ã•ã„ã€‚")
                ], style={
                    'padding': '20px',
                    'backgroundColor': '#fff5f5',
                    'borderRadius': '8px',
                    'border': '1px solid #fed7d7'
                })
            ]

    # æ­£ã—ã„ä¸è¶³æ™‚é–“è¨ˆç®—ï¼ˆå…ƒã®shortage_timeã‹ã‚‰ç›´æ¥è¨ˆç®—ï¼‰
    lack_h = 0
    
    # ã¾ãšå…ƒã®shortage_timeã‹ã‚‰æ­£ç¢ºãªå€¤ã‚’å–å¾—
    shortage_time_df = data_get('shortage_time', pd.DataFrame())
    if not shortage_time_df.empty:
        try:
            # æ•°å€¤åˆ—ã®ã¿å–å¾—ã—ã¦ã‚¹ãƒ­ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
            numeric_cols = shortage_time_df.select_dtypes(include=[np.number])
            if not numeric_cols.empty:
                total_shortage_slots = float(np.nansum(numeric_cols.values))
                # ã‚¹ãƒ­ãƒƒãƒˆã‚’æ™‚é–“ã«å¤‰æ›
                lack_h = total_shortage_slots * SLOT_HOURS
                log.info(f"æ­£ç¢ºãªä¸è¶³æ™‚é–“ï¼ˆshortage_timeã‚ˆã‚Šï¼‰: {lack_h:.2f}h ({total_shortage_slots:.0f}ã‚¹ãƒ­ãƒƒãƒˆ)")
            else:
                lack_h = 0
        except Exception as e:
            log.error(f"shortage_timeèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
            lack_h = 0
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: shortage_role_summaryã¯ç•°å¸¸å€¤ãªã®ã§ä½¿ç”¨ã—ãªã„
        log.warning("shortage_timeãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¸è¶³æ™‚é–“ã‚’0ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
        lack_h = 0
    
    # ã‚³ã‚¹ãƒˆè¨ˆç®—ã‚‚åŒæ§˜ã«ä¿®æ­£
    excess_cost = 0
    lack_temp_cost = 0
    lack_penalty_cost = 0
    
    if not df_shortage_role.empty:
        # åˆè¨ˆè¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        total_rows = df_shortage_role[df_shortage_role['role'].isin(['å…¨ä½“', 'åˆè¨ˆ', 'ç·è¨ˆ'])]
        if not total_rows.empty:
            # é¸æŠã•ã‚ŒãŸã‚·ãƒŠãƒªã‚ªã«å¯¾å¿œã™ã‚‹å…¨ä½“è¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if selected_scenario and 'scenario' in total_rows.columns:
                scenario_total = total_rows[total_rows['scenario'] == selected_scenario]
                if not scenario_total.empty:
                    excess_cost = scenario_total['estimated_excess_cost'].iloc[0] if 'estimated_excess_cost' in scenario_total.columns else 0
                    lack_temp_cost = scenario_total['estimated_lack_cost_if_temporary_staff'].iloc[0] if 'estimated_lack_cost_if_temporary_staff' in scenario_total.columns else 0
                    lack_penalty_cost = scenario_total['estimated_lack_penalty_cost'].iloc[0] if 'estimated_lack_penalty_cost' in scenario_total.columns else 0
                else:
                    excess_cost = total_rows['estimated_excess_cost'].iloc[0] if 'estimated_excess_cost' in total_rows.columns else 0
                    lack_temp_cost = total_rows['estimated_lack_cost_if_temporary_staff'].iloc[0] if 'estimated_lack_cost_if_temporary_staff' in total_rows.columns else 0
                    lack_penalty_cost = total_rows['estimated_lack_penalty_cost'].iloc[0] if 'estimated_lack_penalty_cost' in total_rows.columns else 0
            else:
                excess_cost = total_rows['estimated_excess_cost'].iloc[0] if 'estimated_excess_cost' in total_rows.columns else 0
                lack_temp_cost = total_rows['estimated_lack_cost_if_temporary_staff'].iloc[0] if 'estimated_lack_cost_if_temporary_staff' in total_rows.columns else 0
                lack_penalty_cost = total_rows['estimated_lack_penalty_cost'].iloc[0] if 'estimated_lack_penalty_cost' in total_rows.columns else 0
        else:
            # è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¨ˆç®—ï¼ˆã‚·ãƒŠãƒªã‚ªåˆ¥ï¼‰
            if selected_scenario and 'scenario' in df_shortage_role.columns:
                scenario_filtered = df_shortage_role[df_shortage_role['scenario'] == selected_scenario]
            else:
                scenario_filtered = df_shortage_role
            
            if not scenario_filtered.empty:
                # è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ï¼ˆé›‡ç”¨å½¢æ…‹åˆ¥ã‚’é™¤å¤–ï¼‰
                role_only = scenario_filtered[~scenario_filtered['role'].isin(['å…¨ä½“', 'åˆè¨ˆ', 'ç·è¨ˆ'])]
                # é›‡ç”¨å½¢æ…‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–ï¼ˆé€šå¸¸ 'emp_' ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹ï¼‰
                if 'role' in role_only.columns:
                    role_only = role_only[~role_only['role'].str.startswith('emp_', na=False)]
                
                if not role_only.empty:
                    excess_cost = role_only['estimated_excess_cost'].sum() if 'estimated_excess_cost' in role_only.columns else 0
                    lack_temp_cost = role_only['estimated_lack_cost_if_temporary_staff'].sum() if 'estimated_lack_cost_if_temporary_staff' in role_only.columns else 0
                    lack_penalty_cost = role_only['estimated_lack_penalty_cost'].sum() if 'estimated_lack_penalty_cost' in role_only.columns else 0
                else:
                    excess_cost = 0
                    lack_temp_cost = 0
                    lack_penalty_cost = 0
            else:
                excess_cost = 0
                lack_temp_cost = 0
                lack_penalty_cost = 0

    # JainæŒ‡æ•°ã®å®‰å…¨ãªå–å¾—
    jain_index = "N/A"
    try:
        if not df_fairness.empty and 'metric' in df_fairness.columns:
            jain_row = df_fairness[df_fairness['metric'] == 'jain_index']
            if not jain_row.empty and 'value' in jain_row.columns:
                value = jain_row['value'].iloc[0]
                if pd.notna(value):
                    jain_index = f"{float(value):.3f}"
    except (ValueError, TypeError, IndexError) as e:
        log.debug(f"JainæŒ‡æ•°ã®è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        jain_index = "ã‚¨ãƒ©ãƒ¼"

    # åŸºæœ¬çµ±è¨ˆã®å®‰å…¨ãªè¨ˆç®—
    staff_count = len(df_staff) if not df_staff.empty else 0
    avg_night_ratio = 0
    try:
        if not df_staff.empty and 'night_ratio' in df_staff.columns:
            night_ratios = df_staff['night_ratio'].dropna()
            avg_night_ratio = float(night_ratios.mean()) if len(night_ratios) > 0 else 0
    except (ValueError, TypeError) as e:
        log.debug(f"å¤œå‹¤æ¯”ç‡ã®è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
        avg_night_ratio = 0
    
    alerts_count = len(df_alerts) if not df_alerts.empty else 0

    return html.Div([
        html.Div(id='overview-insights', style={  # type: ignore
            'padding': '15px',
            'backgroundColor': '#e9f2fa',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #cce5ff'
        }),
        html.H3("åˆ†ææ¦‚è¦", style={'marginBottom': '20px'}),  # type: ignore
        # ğŸ“Š é‡è¦æŒ‡æ¨™ã‚’å¤§ããè¡¨ç¤ºï¼ˆæœ€å„ªå…ˆï¼‰
        html.Div([  # type: ignore
            html.Div([
                html.Div([
                    html.H2(f"{lack_h:.1f}", style={
                        'margin': '0', 'color': '#d32f2f' if lack_h > 100 else '#2e7d32', 
                        'fontSize': '3rem', 'fontWeight': 'bold'
                    }),
                    html.P("ç·ä¸è¶³æ™‚é–“(h)", style={'margin': '5px 0', 'fontSize': '1.1rem', 'color': '#666'})
                ], style={
                    'textAlign': 'center', 'padding': '20px', 'backgroundColor': 'white',
                    'borderRadius': '12px', 'boxShadow': '0 4px 8px rgba(0,0,0,0.12)',
                    'border': f"3px solid {'#d32f2f' if lack_h > 100 else '#2e7d32'}"
                }),
            ], style={'width': '24%', 'display': 'inline-block', 'padding': '5px'}),
            
            html.Div([
                html.Div([
                    html.H3(f"{excess_cost:,.0f}", style={
                        'margin': '0', 'color': '#ff9800', 'fontSize': '2rem', 'fontWeight': 'bold'
                    }),
                    html.P("ç·éå‰°ã‚³ã‚¹ãƒˆ(Â¥)", style={'margin': '5px 0', 'fontSize': '1rem', 'color': '#666'})
                ], style={
                    'textAlign': 'center', 'padding': '15px', 'backgroundColor': 'white',
                    'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)',
                    'border': '2px solid #ff9800'
                }),
            ], style={'width': '24%', 'display': 'inline-block', 'padding': '5px'}),
            
            html.Div([
                html.Div([
                    html.H3(f"{lack_temp_cost:,.0f}", style={
                        'margin': '0', 'color': '#f44336', 'fontSize': '2rem', 'fontWeight': 'bold'
                    }),
                    html.P("ä¸è¶³ã‚³ã‚¹ãƒˆ(æ´¾é£)(Â¥)", style={'margin': '5px 0', 'fontSize': '1rem', 'color': '#666'})
                ], style={
                    'textAlign': 'center', 'padding': '15px', 'backgroundColor': 'white',
                    'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)',
                    'border': '2px solid #f44336'
                }),
            ], style={'width': '24%', 'display': 'inline-block', 'padding': '5px'}),
            
            html.Div([
                html.Div([
                    html.H3(str(alerts_count), style={
                        'margin': '0', 'color': '#ff7f0e' if alerts_count > 0 else '#1f77b4', 
                        'fontSize': '2rem', 'fontWeight': 'bold'
                    }),
                    html.P("ã‚¢ãƒ©ãƒ¼ãƒˆæ•°", style={'margin': '5px 0', 'fontSize': '1rem', 'color': '#666'})
                ], style={
                    'textAlign': 'center', 'padding': '15px', 'backgroundColor': 'white',
                    'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)',
                    'border': f"2px solid {'#ff7f0e' if alerts_count > 0 else '#1f77b4'}"
                }),
            ], style={'width': '24%', 'display': 'inline-block', 'padding': '5px'}),
        ], style={'marginBottom': '20px'}),
        
        # ğŸ“ˆ è©³ç´°æŒ‡æ¨™ã‚’å°ã•ãè¡¨ç¤ºï¼ˆè£œåŠ©æƒ…å ±ï¼‰
        html.Div([  # type: ignore
            html.Div([
                create_metric_card("å¤œå‹¤ JainæŒ‡æ•°", jain_index),
            ], style={'width': '20%', 'display': 'inline-block', 'padding': '3px'}),
            html.Div([
                create_metric_card("ç·ã‚¹ã‚¿ãƒƒãƒ•æ•°", str(staff_count)),
            ], style={'width': '20%', 'display': 'inline-block', 'padding': '3px'}),
            html.Div([
                create_metric_card("å¹³å‡å¤œå‹¤æ¯”ç‡", f"{avg_night_ratio:.3f}"),
            ], style={'width': '20%', 'display': 'inline-block', 'padding': '3px'}),
            html.Div([
                create_metric_card("ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£(Â¥)", f"{lack_penalty_cost:,.0f}"),
            ], style={'width': '20%', 'display': 'inline-block', 'padding': '3px'}),
            html.Div([
                html.Div([
                    html.P(f"ç·ä¸è¶³ç‡: {(lack_h / (lack_h + 100)) * 100:.1f}%" if lack_h > 0 else "ç·ä¸è¶³ç‡: 0%", 
                           style={'margin': '0', 'fontSize': '0.9rem', 'textAlign': 'center'})
                ], style={
                    'padding': '10px', 'backgroundColor': 'white', 'borderRadius': '8px',
                    'boxShadow': '0 2px 4px rgba(0,0,0,0.1)', 'minHeight': '60px', 'display': 'flex',
                    'alignItems': 'center', 'justifyContent': 'center'
                }),
            ], style={'width': '20%', 'display': 'inline-block', 'padding': '3px'}),
        ], style={'marginBottom': '30px'}),
        
        # ğŸ“š è¨ˆç®—æ–¹æ³•ã®èª¬æ˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        html.Details([
            html.Summary("ğŸ“š è¨ˆç®—æ–¹æ³•ã®è©³ç´°èª¬æ˜", style={
                'fontSize': '1.1rem', 'fontWeight': 'bold', 'color': '#1f77b4',
                'cursor': 'pointer', 'padding': '10px', 'backgroundColor': '#f8f9fa',
                'border': '1px solid #dee2e6', 'borderRadius': '5px'
            }),
            html.Div([
                html.H5("ä¸è¶³æ™‚é–“è¨ˆç®—æ–¹æ³•", style={'color': '#d32f2f', 'marginTop': '15px'}),
                html.P([
                    "â€¢ ", html.Strong("çµ±è¨ˆæ‰‹æ³•: "), "ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„å®‰å®šã—ãŸä»£è¡¨å€¤ï¼‰",
                    html.Br(),
                    "â€¢ ", html.Strong("æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹åˆ†æ: "), f"{DETECTED_SLOT_INFO['slot_minutes']}åˆ†ã‚¹ãƒ­ãƒƒãƒˆå˜ä½ã§ã®çœŸã®éä¸è¶³åˆ†æã«ã‚ˆã‚‹è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥ç®—å‡º",
                    html.Br(),
                    "â€¢ ", html.Strong("ã‚¹ãƒ­ãƒƒãƒˆå¤‰æ›: "), f"1ã‚¹ãƒ­ãƒƒãƒˆ = {DETECTED_SLOT_INFO['slot_hours']:.2f}æ™‚é–“ï¼ˆ{DETECTED_SLOT_INFO['slot_minutes']}åˆ†é–“éš”ï¼‰",
                    html.Br(),
                    "â€¢ ", html.Strong("ç•°å¸¸å€¤æ¤œå‡º: "), "10,000ã‚¹ãƒ­ãƒƒãƒˆï¼ˆ5,000æ™‚é–“ï¼‰è¶…éæ™‚ã«1/10èª¿æ•´"
                ], style={'lineHeight': '1.6'}),
                
                html.H5("ã‚³ã‚¹ãƒˆè¨ˆç®—æ–¹æ³•", style={'color': '#ff9800', 'marginTop': '15px'}),
                html.P([
                    "â€¢ ", html.Strong("éå‰°ã‚³ã‚¹ãƒˆ: "), f"ä½™å‰°æ™‚é–“ Ã— å¹³å‡æ™‚çµ¦({WAGE_RATES['average_hourly_wage']}å††/h)",
                    html.Br(),
                    "â€¢ ", html.Strong("ä¸è¶³ã‚³ã‚¹ãƒˆ: "), f"ä¸è¶³æ™‚é–“ Ã— æ´¾é£æ™‚çµ¦({WAGE_RATES['temporary_staff']}å††/h)",
                    html.Br(),
                    "â€¢ ", html.Strong("ãƒšãƒŠãƒ«ãƒ†ã‚£: "), f"ä¸è¶³æ™‚é–“ Ã— ãƒšãƒŠãƒ«ãƒ†ã‚£å˜ä¾¡({COST_PARAMETERS['penalty_per_shortage_hour']}å††/h)",
                    html.Br(),
                    "â€¢ ", html.Strong("å¤œå‹¤å‰²å¢—: "), f"{WAGE_RATES['night_differential']}å€ã€ä¼‘æ—¥å‰²å¢—: {WAGE_RATES['weekend_differential']}å€"
                ], style={'lineHeight': '1.6'}),
                
                html.H5("å…¬å¹³æ€§æŒ‡æ¨™", style={'color': '#2e7d32', 'marginTop': '15px'}),
                html.P([
                    "â€¢ ", html.Strong("JainæŒ‡æ•°: "), "0-1ã®ç¯„å›²ã§1ãŒå®Œå…¨å…¬å¹³ï¼ˆåˆ†æ•£ã®é€†æ•°æŒ‡æ¨™ï¼‰",
                    html.Br(),
                    "â€¢ ", html.Strong("è¨ˆç®—å¼: "), "(åˆè¨ˆå€¤)Â² / (è¦ç´ æ•° Ã— å„å€¤ã®2ä¹—å’Œ)",
                    html.Br(),
                    "â€¢ ", html.Strong("è©•ä¾¡åŸºæº–: "), "0.8ä»¥ä¸Š=è‰¯å¥½ã€0.6-0.8=æ™®é€šã€0.6æœªæº€=è¦æ”¹å–„"
                ], style={'lineHeight': '1.6'}),
                
                html.H5("ãƒ‡ãƒ¼ã‚¿ä¸€è²«æ€§", style={'color': '#9c27b0', 'marginTop': '15px'}),
                html.P([
                    "â€¢ ", html.Strong("ä¸‰æ®µéšæ¤œè¨¼: "), "å…¨ä½“ãƒ»è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥ã®åˆè¨ˆå€¤ä¸€è‡´ç¢ºèª",
                    html.Br(),
                    "â€¢ ", html.Strong("è¨±å®¹èª¤å·®: "), "0.01æ™‚é–“ï¼ˆ1åˆ†æœªæº€ï¼‰ã®èª¤å·®ã¯è¨±å®¹",
                    html.Br(),
                    "â€¢ ", html.Strong("çµ±è¨ˆçš„ä¿¡é ¼åº¦: "), f"{STATISTICAL_THRESHOLDS['confidence_level']*100}%ï¼ˆ{STATISTICAL_THRESHOLDS['min_sample_size']}ã‚µãƒ³ãƒ—ãƒ«ä»¥ä¸Šã§æœ‰åŠ¹ï¼‰"
                ], style={'lineHeight': '1.6'})
            ], style={'padding': '15px', 'backgroundColor': 'white', 'border': '1px solid #dee2e6', 'marginTop': '5px'})
        ], style={'marginTop': '20px', 'marginBottom': '20px'}),
    ] + (comprehensive_dashboard_content if comprehensive_dashboard_content else []))


def create_heatmap_tab() -> html.Div:
    """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚¿ãƒ–ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚ä¸Šä¸‹2ã¤ã®æ¯”è¼ƒã‚¨ãƒªã‚¢ã‚’æŒã¡ã¾ã™ã€‚"""
    roles = data_get('roles', [])
    employments = data_get('employments', [])

    # æ¯”è¼ƒã‚¨ãƒªã‚¢ã‚’1ã¤ç”Ÿæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    def create_comparison_area(area_id: int):
        return html.Div([  # type: ignore
            html.H4(f"æ¯”è¼ƒã‚¨ãƒªã‚¢ {area_id}", style={'marginTop': '20px', 'borderTop': '2px solid #ddd', 'paddingTop': '20px'}),  # type: ignore

            # --- å„ã‚¨ãƒªã‚¢ã«è·ç¨®ã¨é›‡ç”¨å½¢æ…‹ã®ä¸¡æ–¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’è¨­ç½® ---
            html.Div([  # type: ignore
                html.Div([  # type: ignore
                    html.Label("è·ç¨®ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"),  # type: ignore
                    dcc.Dropdown(
                        id={'type': 'heatmap-filter-role', 'index': area_id},
                        options=[{'label': 'ã™ã¹ã¦', 'value': 'all'}] + [{'label': r, 'value': r} for r in roles],
                        value='all',
                        clearable=False
                    )
                ], style={'width': '48%', 'display': 'inline-block', 'marginRight': '4%'}),

                html.Div([  # type: ignore
                    html.Label("é›‡ç”¨å½¢æ…‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"),  # type: ignore
                    dcc.Dropdown(
                        id={'type': 'heatmap-filter-employment', 'index': area_id},
                        options=[{'label': 'ã™ã¹ã¦', 'value': 'all'}] + [{'label': e, 'value': e} for e in employments],
                        value='all',
                        clearable=False
                    )
                ], style={'width': '48%', 'display': 'inline-block'}),
            ], style={'marginBottom': '10px'}),

            # --- ã‚°ãƒ©ãƒ•æç”»é ˜åŸŸ ---
            dcc.Loading(
                id={'type': 'loading-heatmap', 'index': area_id},
                children=html.Div(id={'type': 'graph-output-heatmap', 'index': area_id})
            )
        ], style={'padding': '10px', 'backgroundColor': '#f9f9f9', 'borderRadius': '5px', 'marginBottom': '10px'})

    return html.Div([
        html.H3("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æ¯”è¼ƒåˆ†æ", style={'marginBottom': '20px'}),  # type: ignore
        
        # ğŸ“ˆ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®èª­ã¿æ–¹èª¬æ˜
        html.Details([
            html.Summary("ğŸ“ˆ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®èª­ã¿æ–¹ãƒ»è¨ˆç®—æ–¹æ³•", style={
                'fontSize': '1.1rem', 'fontWeight': 'bold', 'color': '#ff6f00',
                'cursor': 'pointer', 'padding': '10px', 'backgroundColor': '#fff3e0',
                'border': '1px solid #ffcc02', 'borderRadius': '5px', 'marginBottom': '15px'
            }),
            html.Div([
                html.H5("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®åŸºæœ¬", style={'color': '#ff6f00'}),
                dcc.Markdown(f"""
                **è‰²ã®æ„å‘³:**
                - ğŸ”´ èµ¤è‰²: ä¸è¶³ï¼ˆNeed > Staffï¼‰
                - ğŸ”µ é’è‰²: ä½™å‰°ï¼ˆStaff > Needï¼‰
                - âšª ç™½è‰²: å‡è¡¡ï¼ˆNeed â‰ˆ Staffï¼‰
                - æ¿ƒåº¦: ä¸è¶³ãƒ»ä½™å‰°ã®ç¨‹åº¦ã‚’è¡¨ç¾

                **Needè¨ˆç®—:**
                - çµ±è¨ˆæ‰‹æ³•: ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰
                - æ™‚é–“ã‚¹ãƒ­ãƒƒãƒˆ: {DEFAULT_SLOT_MINUTES}åˆ† = {SLOT_HOURS}æ™‚é–“å˜ä½
                - ç•°å¸¸å€¤é™¤å»: IQR Ã— 1.5ã«ã‚ˆã‚‹å¤–ã‚Œå€¤å‡¦ç†
                - æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹åˆ†æ: 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆå˜ä½ã§ã®å®Ÿå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãçœŸã®åˆ†æ

                **æ¯”è¼ƒåˆ†æ:**
                - ä¸Šä¸‹2ã‚¨ãƒªã‚¢ã§è·ç¨®ãƒ»é›‡ç”¨å½¢æ…‹ã‚’é¸æŠ
                - åŒä¸€æ™‚é–“è»¸ã§ã®éœ€çµ¦ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
                - ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: ç‰¹å®šæ¡ä»¶ã§ã®è©³ç´°åˆ†æ
                """),
                
                html.H5("è§£é‡ˆã®ãƒã‚¤ãƒ³ãƒˆ", style={'color': '#1976d2', 'marginTop': '15px'}),
                html.P([
                    "â€¢ ", html.Strong("æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³: "), "ãƒ”ãƒ¼ã‚¯æ™‚é–“å¸¯ãƒ»é–‘æ•£æ™‚é–“å¸¯ã®æŠŠæ¡", html.Br(),
                    "â€¢ ", html.Strong("è·ç¨®ç‰¹æ€§: "), "è·ç¨®ã”ã¨ã®éœ€çµ¦ãƒãƒ©ãƒ³ã‚¹ç‰¹å¾´", html.Br(),
                    "â€¢ ", html.Strong("é›‡ç”¨å½¢æ…‹: "), "æ­£è¦ãƒ»æ´¾é£ã®é…ç½®æœ€é©åŒ–", html.Br(),
                    "â€¢ ", html.Strong("æ—¥åˆ¥å¤‰å‹•: "), "æ›œæ—¥ãƒ»æ—¥ä»˜ã«ã‚ˆã‚‹éœ€è¦å¤‰åŒ–"
                ])
            ], style={'padding': '15px', 'backgroundColor': 'white', 'border': '1px solid #ffcc02', 'marginTop': '5px'})
        ], style={'marginBottom': '20px'}),
        
        html.P("ä¸Šä¸‹ã®ã‚¨ãƒªã‚¢ã§ãã‚Œãã‚Œã€Œè·ç¨®ã€ã¨ã€Œé›‡ç”¨å½¢æ…‹ã€ã®çµ„ã¿åˆã‚ã›ã‚’é¸æŠã—ã€ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚"),  # type: ignore
        create_comparison_area(1),
        create_comparison_area(2)
    ])


def create_shortage_tab(selected_scenario: str = None) -> html.Div:
    """ä¸è¶³åˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    try:
        shortage_dash_log.info("===== ä¸è¶³åˆ†æã‚¿ãƒ–ä½œæˆé–‹å§‹ =====")
        shortage_dash_log.info(f"scenario: {selected_scenario}")
        
        # ğŸ¯ æœ€å„ªå…ˆ: ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã®ãŸã‚ã®å¤‰æ•°åˆæœŸåŒ–
        df_shortage_role_filtered = {}
        df_shortage_role_excess = {}
        df_shortage_emp_filtered = {}
        total_lack = 0
        
        shortage_dash_log.info("å¤‰æ•°åˆæœŸåŒ–å®Œäº†")
        shortage_dash_log.info(f"df_shortage_role_filteredåˆæœŸåŒ–: {type(df_shortage_role_filtered)}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        shortage_dash_log.info("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–‹å§‹")
        df_shortage_role = data_get('shortage_role_summary', pd.DataFrame())
        df_shortage_emp = data_get('shortage_employment_summary', pd.DataFrame())
        
        shortage_dash_log.info(f"df_shortage_roleèª­ã¿è¾¼ã¿å®Œäº†: {len(df_shortage_role)}è¡Œ")
        shortage_dash_log.info(f"df_shortage_empèª­ã¿è¾¼ã¿å®Œäº†: {len(df_shortage_emp)}è¡Œ")
        
        if not df_shortage_role.empty:
            shortage_dash_log.info(f"df_shortage_role columns: {list(df_shortage_role.columns)}")
            shortage_dash_log.info(f"df_shortage_roleè·ç¨®: {df_shortage_role['role'].tolist() if 'role' in df_shortage_role.columns else 'roleåˆ—ãªã—'}")
        else:
            shortage_dash_log.warning("df_shortage_roleãŒç©ºã§ã™ï¼")

        content = [html.Div(id='shortage-insights', style={  # type: ignore
        'padding': '15px',
        'backgroundColor': '#e9f2fa',
        'borderRadius': '8px',
        'marginBottom': '20px',
        'border': '1px solid #cce5ff'
    }),
        html.H3("ä¸è¶³åˆ†æ", style={'marginBottom': '20px'}),  # type: ignore
        # ğŸ’¡ ä¸è¶³åˆ†æã®è¨ˆç®—æ–¹æ³•èª¬æ˜
        html.Details([
            html.Summary("ğŸ’¡ ä¸è¶³åˆ†æã®è¨ˆç®—æ–¹æ³•", style={
                'fontSize': '1.1rem', 'fontWeight': 'bold', 'color': '#d32f2f',
                'cursor': 'pointer', 'padding': '10px', 'backgroundColor': '#ffebee',
                'border': '1px solid #ffcdd2', 'borderRadius': '5px'
            }),
            html.Div([
                html.H5("æ™‚é–“è»¸ãƒ™ãƒ¼ã‚¹ä¸è¶³è¨ˆç®—", style={'color': '#d32f2f'}),
                dcc.Markdown(f"""
                **çµ±è¨ˆçš„æ‰‹æ³•:**
                - Needç®—å‡º: ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹ï¼ˆå¤–ã‚Œå€¤ã«å¼·ã„ï¼‰
                - Upperç®—å‡º: å¹³å‡+1æ¨™æº–åå·®ï¼ˆå®‰å…¨ãƒãƒ¼ã‚¸ãƒ³ç¢ºä¿ï¼‰
                - ã‚¹ãƒ­ãƒƒãƒˆå˜ä½: {DEFAULT_SLOT_MINUTES}åˆ† = {SLOT_HOURS}æ™‚é–“

                **æ™‚é–“è»¸åˆ†ææ–¹å¼:**
                - 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆå˜ä½ã§ã®å®Ÿéš›ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
                - è·ç¨®åˆ¥åˆ†æ: å®Ÿéš›ã®å‹¤å‹™æ™‚é–“å¸¯ãƒ»ãƒ”ãƒ¼ã‚¯æ™‚é–“ãƒ»éœ€è¦ã‚«ãƒãƒ¬ãƒƒã‚¸
                - é›‡ç”¨å½¢æ…‹åˆ¥åˆ†æ: å‹¤å‹™åˆ¶ç´„ãƒ»æ™‚é–“åŠ¹ç‡ãƒ»ã‚³ã‚¹ãƒˆåŠ¹ç‡
                - çœŸã®åˆ†æä¾¡å€¤: å®Ÿãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæ„å‘³ã®ã‚ã‚‹ä¸è¶³æ™‚é–“ç®—å‡º

                **ã‚³ã‚¹ãƒˆè¨ˆç®—ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
                - ç›´æ¥é›‡ç”¨æ™‚çµ¦: Â¥{WAGE_RATES['regular_staff']:,}/h
                - æ´¾é£è·å“¡æ™‚çµ¦: Â¥{WAGE_RATES['temporary_staff']:,}/h  
                - æ¡ç”¨ã‚³ã‚¹ãƒˆ: Â¥{COST_PARAMETERS['recruit_cost_per_hire']:,}/äºº
                - ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£: Â¥{COST_PARAMETERS['penalty_per_shortage_hour']:,}/h
                - å¤œå‹¤å‰²å¢—ç‡: {WAGE_RATES['night_differential']}å€
                """),
                
                html.H5("ç¾åœ¨ã®è¨­å®šå€¤", style={'color': '#1976d2', 'marginTop': '15px'}),
                html.P([
                    f"é¸æŠã‚·ãƒŠãƒªã‚ª: {selected_scenario or 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ'}", html.Br(),
                    f"Needç®—å‡ºæ–¹æ³•: {data_get('need_method', 'ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹')}", html.Br(),
                    f"Upperç®—å‡ºæ–¹æ³•: {data_get('upper_method', 'å¹³å‡+1SD')}", html.Br(),
                    f"ç•°å¸¸å€¤æ¤œå‡ºé–¾å€¤: 10,000ã‚¹ãƒ­ãƒƒãƒˆï¼ˆ5,000æ™‚é–“ï¼‰"
                ])
            ], style={'padding': '15px', 'backgroundColor': 'white', 'border': '1px solid #ffcdd2', 'marginTop': '5px'})
        ], style={'marginBottom': '20px'}),]

        # è·ç¨®åˆ¥ä¸è¶³åˆ†æ
        if not df_shortage_role.empty:
            content.append(html.H4("è·ç¨®åˆ¥ä¸è¶³æ™‚é–“"))  # type: ignore

            # æ­£ç¢ºãªä¸è¶³æ™‚é–“è¨ˆç®—ï¼ˆshortage_timeã‹ã‚‰ç›´æ¥å–å¾—ï¼‰
            total_lack = 0
            shortage_time_df = data_get('shortage_time', pd.DataFrame())
            if not shortage_time_df.empty:
                try:
                    numeric_cols = shortage_time_df.select_dtypes(include=[np.number])
                    if not numeric_cols.empty:
                        total_shortage_slots = float(np.nansum(numeric_cols.values))
                        total_lack = total_shortage_slots * SLOT_HOURS
                        log.info(f"ä¸è¶³åˆ†æã‚¿ãƒ–: æ­£ç¢ºãªä¸è¶³æ™‚é–“ {total_lack:.2f}h ({total_shortage_slots:.0f}ã‚¹ãƒ­ãƒƒãƒˆ)")
                except Exception as e:
                    log.error(f"ä¸è¶³åˆ†æã‚¿ãƒ–: shortage_timeèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
                    total_lack = 0
            else:
                log.warning("ä¸è¶³åˆ†æã‚¿ãƒ–: shortage_timeãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                total_lack = 0
            # è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã¨å‡¦ç†
            # ğŸ¯ å¤‰æ•°ã¯ã™ã§ã«é–¢æ•°å…ˆé ­ã§åˆæœŸåŒ–æ¸ˆã¿
            shortage_dash_log.info("=== è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹ ===")
            shortage_dash_log.info(f"df_shortage_role size: {len(df_shortage_role)}")
            shortage_dash_log.info(f"df_shortage_role_filteredçŠ¶æ…‹: {type(df_shortage_role_filtered)}, å†…å®¹: {df_shortage_role_filtered}")
            
            # è¾æ›¸ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆå¿µã®ãŸã‚ï¼‰
            df_shortage_role_filtered.clear()
            df_shortage_role_excess.clear()
            df_shortage_emp_filtered.clear()
            
            shortage_dash_log.info("è¾æ›¸ãƒªã‚»ãƒƒãƒˆå®Œäº†")
            
            # è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            if not df_shortage_role.empty:
                shortage_dash_log.info("è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹")
                # å®Ÿéš›ã®è·ç¨®ã®ã¿æŠ½å‡ºï¼ˆå…¨ä½“ãƒ»åˆè¨ˆè¡Œã‚’é™¤å¤–ï¼‰
                role_only_df = df_shortage_role[
                    (~df_shortage_role['role'].isin(['å…¨ä½“', 'åˆè¨ˆ', 'ç·è¨ˆ'])) &
                    (~df_shortage_role['role'].str.startswith('emp_', na=False))
                ]
                
                shortage_dash_log.info(f"role_only_df: {len(role_only_df)}è¡Œ")
                shortage_dash_log.info(f"å‡¦ç†ã™ã‚‹è·ç¨®: {role_only_df['role'].tolist() if not role_only_df.empty else 'ç©º'}")
                
                for i, (_, row) in enumerate(role_only_df.iterrows()):
                    role = row['role']
                    lack_h = row.get('lack_h', 0)
                    excess_h = row.get('excess_h', 0)
                    
                    shortage_dash_log.info(f"è·ç¨®{i+1}: {role}, lack_h={lack_h}, excess_h={excess_h}")
                    
                    if lack_h > 0:
                        df_shortage_role_filtered[role] = lack_h
                        shortage_dash_log.info(f"df_shortage_role_filteredã«è¿½åŠ : {role}={lack_h}")
                    if excess_h > 0:
                        df_shortage_role_excess[role] = excess_h
                        shortage_dash_log.info(f"df_shortage_role_excessã«è¿½åŠ : {role}={excess_h}")
                
                shortage_dash_log.info(f"å‡¦ç†å®Œäº† - df_shortage_role_filtered: {df_shortage_role_filtered}")
                shortage_dash_log.info(f"å‡¦ç†å®Œäº† - df_shortage_role_excess: {df_shortage_role_excess}")
            
            # é›‡ç”¨å½¢æ…‹åˆ¥ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            if not df_shortage_emp.empty:
                for _, row in df_shortage_emp.iterrows():
                    employment = row.get('employment', '')
                    lack_h = row.get('lack_h', 0)
                    
                    if employment and lack_h > 0:
                        df_shortage_emp_filtered[employment] = lack_h
            
            if total_lack > 0 and df_shortage_role_filtered:
                # ä¸è¶³æ™‚é–“ã®Top3è·ç¨®ã‚’è¡¨ç¤º
                top_roles = sorted(df_shortage_role_filtered.items(), key=lambda x: x[1], reverse=True)[:3]
                
                metrics = [
                    html.Div([
                        create_metric_card("ç·ä¸è¶³æ™‚é–“", f"{total_lack:.1f}h")
                    ], style={'width': '25%', 'display': 'inline-block', 'padding': '5px'}),
                ]
                
                for i, (role, lack_h) in enumerate(top_roles, 1):
                    metrics.append(html.Div([
                        create_metric_card(f"ä¸è¶³Top{i} {role}", f"{lack_h:.1f}h")
                    ], style={'width': '25%', 'display': 'inline-block', 'padding': '5px'}))
                
                content.append(html.Div(metrics, style={'marginBottom': '20px'}))

            # è·ç¨®åˆ¥ä¸è¶³ãƒ»éå‰°ã‚°ãƒ©ãƒ•
            try:
                log.info(f"[DASH_SHORTAGE] === ã‚°ãƒ©ãƒ•ä½œæˆé–‹å§‹ ===")
                log.info(f"[DASH_SHORTAGE] df_shortage_role_filtered: {df_shortage_role_filtered}")
                log.info(f"[DASH_SHORTAGE] df_shortage_role_filteredå­˜åœ¨ãƒã‚§ãƒƒã‚¯: {bool(df_shortage_role_filtered)}")
                
                if df_shortage_role_filtered:
                    log.info(f"[DASH_SHORTAGE] ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ä½œæˆé–‹å§‹")
                    roles = list(df_shortage_role_filtered.keys())
                    lack_values = list(df_shortage_role_filtered.values())
                    excess_values = [df_shortage_role_excess.get(role, 0) for role in roles]
                    
                    log.info(f"[DASH_SHORTAGE] roles: {roles}")
                    log.info(f"[DASH_SHORTAGE] lack_values: {lack_values}")
                    log.info(f"[DASH_SHORTAGE] excess_values: {excess_values}")
                    
                    fig_role_combined = go.Figure()
                    fig_role_combined.add_trace(go.Bar(
                        x=roles,
                        y=lack_values,
                        name='ä¸è¶³æ™‚é–“',
                        marker_color='red',
                        opacity=0.7
                    ))
                    fig_role_combined.add_trace(go.Bar(
                        x=roles,
                        y=excess_values,
                        name='éå‰°æ™‚é–“',
                        marker_color='blue',
                        opacity=0.7
                    ))
                    fig_role_combined.update_layout(
                        title=f'è·ç¨®åˆ¥ä¸è¶³ãƒ»éå‰°æ™‚é–“ (ç·ä¸è¶³: {total_lack:.1f}h)',
                        xaxis_title='è·ç¨®',
                        yaxis_title='æ™‚é–“(h)',
                        height=400,
                        barmode='group'
                    )
                    content.append(dcc.Graph(figure=fig_role_combined))
                else:
                    content.append(html.P("è·ç¨®åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"))
            except Exception as e:
                log.error(f"[shortage_tab] è·ç¨®åˆ¥ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
                content.append(html.P(f"è·ç¨®åˆ¥ã‚°ãƒ©ãƒ•ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}", style={'color': 'red'}))

        # é›‡ç”¨å½¢æ…‹åˆ¥ä¸è¶³æ™‚é–“
        content.append(html.H4("é›‡ç”¨å½¢æ…‹åˆ¥ä¸è¶³æ™‚é–“", style={'marginTop': '30px'}))  # type: ignore
        if df_shortage_emp_filtered:
            emp_metrics = []
            for employment, lack_h in df_shortage_emp_filtered.items():
                emp_metrics.append(html.Div([
                    create_metric_card(f"{employment}", f"{lack_h:.1f}h")
                ], style={'width': '33%', 'display': 'inline-block', 'padding': '5px'}))
            content.append(html.Div(emp_metrics, style={'marginBottom': '20px'}))
        else:
            content.append(html.P("é›‡ç”¨å½¢æ…‹åˆ¥ãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", style={'marginBottom': '20px'}))

        # ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        content.append(html.Div([
            html.H4("ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—", style={'marginTop': '30px'}),  # type: ignore
            html.P("å„æ™‚é–“å¸¯ã§å¿…è¦äººæ•°ã«å¯¾ã—ã¦ã©ã‚Œãã‚‰ã„ã®å‰²åˆã§äººå“¡ãŒä¸è¶³ã—ã¦ã„ãŸã‹ã‚’ç¤ºã—ã¾ã™ã€‚"),  # type: ignore
            html.Div([  # type: ignore
                html.Label("è¡¨ç¤ºç¯„å›²"),  # type: ignore
                dcc.Dropdown(
                    id='shortage-heatmap-scope',
                    options=[
                        {'label': 'å…¨ä½“', 'value': 'overall'},
                        {'label': 'è·ç¨®åˆ¥', 'value': 'role'},
                        {'label': 'é›‡ç”¨å½¢æ…‹åˆ¥', 'value': 'employment'}
                    ],
                    value='overall',
                    style={'width': '200px'}
                ),
            ], style={'marginBottom': '10px'}),
            html.Div(id='shortage-heatmap-detail-container'),
            html.Div(id='shortage-ratio-heatmap')
        ]))

        # Factor Analysis section
        content.append(html.Hr())
        content.append(html.H4('è¦å› åˆ†æ (AI)', style={'marginTop': '30px'}))  # type: ignore
        content.append(html.Button('è¦å› åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’', id='factor-train-button', n_clicks=0))  # type: ignore
        content.append(html.Div(id='factor-output'))  # type: ignore

        # Over/Short Log section
        events_df = data_get('shortage_events', pd.DataFrame())
        if not events_df.empty:
            content.append(html.Hr())  # type: ignore
            content.append(html.H4('éä¸è¶³æ‰‹å‹•ãƒ­ã‚°', style={'marginTop': '30px'}))  # type: ignore
            content.append(dash_table.DataTable(
                id='over-shortage-table',
                data=events_df.to_dict('records'),
                columns=[{'name': c, 'id': c, 'presentation': 'input'} for c in events_df.columns],
                editable=True,
            ))
            content.append(dcc.RadioItems(
                id='log-save-mode',
                options=[{'label': 'è¿½è¨˜', 'value': 'append'}, {'label': 'ä¸Šæ›¸ã', 'value': 'overwrite'}],
                value='è¿½è¨˜',
                inline=True,
                style={'marginTop': '10px'}
            ))
            content.append(html.Button('ãƒ­ã‚°ã‚’ä¿å­˜', id='save-log-button', n_clicks=0, style={'marginTop': '10px'}))  # type: ignore
            content.append(html.Div(id='save-log-msg'))  # type: ignore

        return html.Div(content)
    
    except Exception as e:
        shortage_dash_log.error("===== è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ =====")
        shortage_dash_log.error(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
        shortage_dash_log.error(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {str(e)}")
        import traceback
        shortage_dash_log.error("è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:")
        shortage_dash_log.error(traceback.format_exc())
        
        # df_shortage_role_filteredã‚¨ãƒ©ãƒ¼ã®ç‰¹åˆ¥å‡¦ç†
        if "df_shortage_role_filtered" in str(e):
            shortage_dash_log.error("âš ï¸ df_shortage_role_filteredã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡ºï¼")
            shortage_dash_log.error("ã“ã‚Œã¯å¤‰æ•°ã‚¹ã‚³ãƒ¼ãƒ—å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        return html.Div([
            html.H3("ä¸è¶³åˆ†æ", style={'color': 'red'}),
            html.P(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'}),
            html.P(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}", style={'color': 'red'}),
            html.P("è©³ç´°ã¯ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", style={'color': 'red'})
        ])


def create_optimization_tab() -> html.Div:
    """æœ€é©åŒ–åˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    return html.Div([  # type: ignore
        html.Div(id='optimization-insights', style={
            'padding': '15px',
            'backgroundColor': '#e9f2fa',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #cce5ff'
        }),
        html.H3("æœ€é©åŒ–åˆ†æ", style={'marginBottom': '20px'}),  # type: ignore
        html.Div([  # type: ignore
            html.Label("è¡¨ç¤ºç¯„å›²"),  # type: ignore
            dcc.Dropdown(
                id='opt-scope',
                options=[
                    {'label': 'å…¨ä½“', 'value': 'overall'},
                    {'label': 'è·ç¨®åˆ¥', 'value': 'role'},
                    {'label': 'é›‡ç”¨å½¢æ…‹åˆ¥', 'value': 'employment'}
                ],
                value='overall',
                clearable=False
            ),
        ], style={'width': '30%', 'marginBottom': '20px'}),
        html.Div(id='opt-detail-container'),  # type: ignore
        html.Div(id='optimization-content')  # type: ignore
    ])


def create_leave_analysis_tab() -> html.Div:
    """ä¼‘æš‡åˆ†æã‚¿ãƒ–ã‚’ä½œæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    log.info("[create_leave_analysis_tab] é–‹å§‹")
    
    content = [html.Div(id='leave-insights', style={  # type: ignore
        'padding': '15px',
        'backgroundColor': '#e9f2fa',
        'borderRadius': '8px',
        'marginBottom': '20px',
        'border': '1px solid #cce5ff'
    }),
        html.H3("ä¼‘æš‡åˆ†æ", style={'marginBottom': '20px'})]  # type: ignore

    # è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’è©¦è¡Œ
    df_staff_balance = data_get('staff_balance_daily', pd.DataFrame())
    df_daily_summary = data_get('daily_summary', pd.DataFrame())
    df_concentration = data_get('concentration_requested', pd.DataFrame())
    df_ratio_breakdown = data_get('leave_ratio_breakdown', pd.DataFrame())
    df_leave_analysis = data_get('leave_analysis', pd.DataFrame())
    
    # ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ä»£æ›¿å‡¦ç†
    if all(df.empty for df in [df_staff_balance, df_daily_summary, df_concentration, df_ratio_breakdown, df_leave_analysis]):
        log.warning("[Leave] ä¼‘æš‡åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¼‘æš‡åˆ†æã‚’ç”Ÿæˆ
        long_df = data_get('long_df', pd.DataFrame())
        if not long_df.empty and 'parsed_slots_count' in long_df.columns:
            # ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ï¼ˆslots_count=0ï¼‰ã‚’æŠ½å‡º
            leave_data = long_df[long_df['parsed_slots_count'] == 0]
            if not leave_data.empty:
                # æ—¥åˆ¥ä¼‘æš‡å–å¾—è€…æ•°ã®é›†è¨ˆ
                leave_summary = leave_data.groupby(leave_data['ds'].dt.date).agg({
                    'staff': 'nunique',
                    'role': lambda x: ', '.join(x.unique()[:5])  # æœ€å¤§5è·ç¨®ã¾ã§è¡¨ç¤º
                }).reset_index()
                leave_summary.columns = ['date', 'leave_count', 'affected_roles']
                
                # ä¼‘æš‡åˆ†æã‚°ãƒ©ãƒ•
                if len(leave_summary) > 0:
                    fig_leave = px.bar(
                        leave_summary,
                        x='date',
                        y='leave_count',
                        title='æ—¥åˆ¥ä¼‘æš‡å–å¾—è€…æ•°',
                        labels={'leave_count': 'ä¼‘æš‡å–å¾—è€…æ•°', 'date': 'æ—¥ä»˜'}
                    )
                    content.append(dcc.Graph(figure=fig_leave))
                    
                    # ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
                    content.append(html.H4("ä¼‘æš‡å–å¾—çŠ¶æ³è©³ç´°"))
                    content.append(dash_table.DataTable(
                        data=leave_summary.to_dict('records'),
                        columns=[
                            {'name': 'æ—¥ä»˜', 'id': 'date'},
                            {'name': 'ä¼‘æš‡å–å¾—è€…æ•°', 'id': 'leave_count'},
                            {'name': 'å½±éŸ¿è·ç¨®', 'id': 'affected_roles'}
                        ],
                        style_table={'height': '400px', 'overflowY': 'auto'},
                        style_cell={'textAlign': 'left', 'padding': '10px'},
                        style_header={'backgroundColor': 'lightblue', 'fontWeight': 'bold'}
                    ))
                else:
                    content.append(html.P("æœŸé–“ä¸­ã«ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
            else:
                content.append(html.P("ä¼‘æš‡ãƒ‡ãƒ¼ã‚¿ï¼ˆparsed_slots_count=0ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"))
        else:
            content.append(html.P("åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ï¼ˆlong_dfï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ä¼‘æš‡åˆ†æã‚’è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚"))
        
        return html.Div(content)
    
    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®å‡¦ç†ç¶™ç¶š

    if not df_staff_balance.empty:
        fig_balance = px.line(
            df_staff_balance,
            x='date',
            y=['total_staff', 'leave_applicants_count', 'non_leave_staff'],
            title='å‹¤å‹™äºˆå®šäººæ•°ã¨å…¨ä¼‘æš‡å–å¾—è€…æ•°ã®æ¨ç§»',
            labels={'value': 'äººæ•°', 'variable': 'é …ç›®', 'date': 'æ—¥ä»˜'},
            markers=True
        )
        fig_balance.update_xaxes(tickformat="%m/%d(%a)")
        content.append(dcc.Graph(figure=fig_balance))
        content.append(dash_table.DataTable(
            data=df_staff_balance.to_dict('records'),
            columns=[{'name': i, 'id': i} for i in df_staff_balance.columns]
        ))

    if not df_daily_summary.empty:
        fig_breakdown = px.bar(
            df_daily_summary,
            x='date',
            y='total_leave_days',
            color='leave_type',
            barmode='stack',
            title='æ—¥åˆ¥ ä¼‘æš‡å–å¾—è€…æ•°ï¼ˆå†…è¨³ï¼‰',
            labels={'date': 'æ—¥ä»˜', 'total_leave_days': 'ä¼‘æš‡å–å¾—è€…æ•°', 'leave_type': 'ä¼‘æš‡ã‚¿ã‚¤ãƒ—'}
        )
        fig_breakdown.update_xaxes(tickformat="%m/%d(%a)")
        content.append(dcc.Graph(figure=fig_breakdown))

    if not df_ratio_breakdown.empty:
        fig_ratio_break = px.bar(
            df_ratio_breakdown,
            x='dayofweek',
            y='leave_ratio',
            color='leave_type',
            facet_col='month_period',
            category_orders={
                'dayofweek': ['æœˆæ›œæ—¥', 'ç«æ›œæ—¥', 'æ°´æ›œæ—¥', 'æœ¨æ›œæ—¥', 'é‡‘æ›œæ—¥', 'åœŸæ›œæ—¥', 'æ—¥æ›œæ—¥'],
                'month_period': ['æœˆåˆ(1-10æ—¥)', 'æœˆä¸­(11-20æ—¥)', 'æœˆæœ«(21-æœ«æ—¥)'],
            },
            labels={'dayofweek': 'æ›œæ—¥', 'leave_ratio': 'å‰²åˆ', 'leave_type': 'ä¼‘æš‡ã‚¿ã‚¤ãƒ—', 'month_period': 'æœˆæœŸé–“'},
            title='æ›œæ—¥ãƒ»æœˆæœŸé–“åˆ¥ä¼‘æš‡å–å¾—ç‡'
        )
        content.append(dcc.Graph(figure=fig_ratio_break))

    if not df_concentration.empty:
        fig_conc = go.Figure()
        fig_conc.add_trace(go.Scatter(
            x=df_concentration['date'],
            y=df_concentration['leave_applicants_count'],
            mode='lines+markers',
            name='ä¼‘æš‡ç”³è«‹è€…æ•°',
            line=dict(shape='spline', smoothing=0.5),
            marker=dict(size=6)
        ))
        if 'is_concentrated' in df_concentration.columns:
            concentrated = df_concentration[df_concentration['is_concentrated']]
            if not concentrated.empty:
                fig_conc.add_trace(go.Scatter(
                    x=concentrated['date'],
                    y=concentrated['leave_applicants_count'],
                    mode='markers',
                    marker=dict(color='red', size=12, symbol='diamond'),
                    name='é–¾å€¤è¶…éæ—¥',
                    hovertemplate='<b>%{x|%Y-%m-%d}</b><br>ç”³è«‹è€…æ•°: %{y}äºº<extra></extra>'
                ))

        fig_conc.update_layout(
            title='å¸Œæœ›ä¼‘ ç”³è«‹è€…æ•°ã®æ¨ç§»ã¨é›†ä¸­æ—¥',
            xaxis_title='æ—¥ä»˜',
            yaxis_title='ç”³è«‹è€…æ•°'
        )
        fig_conc.update_xaxes(tickformat="%m/%d(%a)")
        content.append(dcc.Graph(figure=fig_conc))

    return html.Div(content)


def create_cost_analysis_tab() -> html.Div:
    """ã‚³ã‚¹ãƒˆåˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    return html.Div([
        html.Div(id='cost-insights', style={
            'padding': '15px',
            'backgroundColor': '#e9f2fa',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #cce5ff'
        }),
        html.H3("äººä»¶è²»åˆ†æ", style={'marginBottom': '20px'}),

        html.H4("å‹•çš„ã‚³ã‚¹ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", style={'marginTop': '30px'}),
        dcc.RadioItems(
            id='cost-by-radio',
            options=[
                {'label': 'è·ç¨®åˆ¥', 'value': 'role'},
                {'label': 'é›‡ç”¨å½¢æ…‹åˆ¥', 'value': 'employment'},
                {'label': 'ã‚¹ã‚¿ãƒƒãƒ•åˆ¥', 'value': 'staff'},
            ],
            value='role',
            inline=True,
            style={'marginBottom': '10px'},
        ),
        html.Div(id='wage-input-container'),

        dcc.Loading(
            id="loading-cost-analysis",
            type="circle",
            children=html.Div(id='cost-analysis-content')
        )
    ])


def create_hire_plan_tab() -> html.Div:
    """æ¡ç”¨è¨ˆç”»ã‚¿ãƒ–ã‚’ä½œæˆ"""
    content = [html.Div(id='hire-plan-insights', style={  # type: ignore
        'padding': '15px',
        'backgroundColor': '#e9f2fa',
        'borderRadius': '8px',
        'marginBottom': '20px',
        'border': '1px solid #cce5ff'
    }),
        html.H3("æ¡ç”¨è¨ˆç”»", style={'marginBottom': '20px'})]  # type: ignore

    df_hire = data_get('hire_plan', pd.DataFrame())
    df_shortage_role = data_get('shortage_role_summary', pd.DataFrame())
    
    if not df_hire.empty:
        content.append(html.H4("å¿…è¦FTEï¼ˆè·ç¨®åˆ¥ï¼‰"))  # type: ignore

        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«ç¿»è¨³
        df_hire_display = df_hire.copy()
        column_translations = {
            'role': 'è·ç¨®',
            'hire_fte': 'å¿…è¦FTE',
            'shortage_hours': 'ä¸è¶³æ™‚é–“',
            'current_fte': 'ç¾åœ¨FTE',
            'target_fte': 'ç›®æ¨™FTE',
            'priority': 'å„ªå…ˆåº¦',
            'cost_per_fte': 'FTEå˜ä¾¡',
            'total_cost': 'ç·ã‚³ã‚¹ãƒˆ'
        }
        df_hire_display.rename(columns=column_translations, inplace=True)

        # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º
        content.append(dash_table.DataTable(
            data=df_hire_display.to_dict('records'),
            columns=[{'name': col, 'id': col} for col in df_hire_display.columns],
            style_cell={'textAlign': 'left'},
            style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'}
        ))

        # ã‚°ãƒ©ãƒ•è¡¨ç¤ºï¼ˆå…ƒã®åˆ—åã‚’ä½¿ç”¨ï¼‰
        if 'role' in df_hire.columns and 'hire_fte' in df_hire.columns:
            fig_hire = px.bar(
                df_hire,
                x='role',
                y='hire_fte',
                title='è·ç¨®åˆ¥å¿…è¦FTE',
                labels={'role': 'è·ç¨®', 'hire_fte': 'å¿…è¦FTE'},
                color_discrete_sequence=['#1f77b4']
            )
            content.append(dcc.Graph(figure=fig_hire))

        # æ¡ç”¨æˆ¦ç•¥ææ¡ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        content.append(html.Div([
            html.H4("æ¡ç”¨æˆ¦ç•¥ã®ææ¡ˆ", style={'marginTop': '30px'}),
            html.P("åˆ†æçµæœã«åŸºã¥ãæ¡ç”¨å„ªå…ˆåº¦ã¨æˆ¦ç•¥çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š"),
            html.Ul([
                html.Li("æœ€ã‚‚ä¸è¶³ã®æ·±åˆ»ãªè·ç¨®ã‹ã‚‰å„ªå…ˆçš„ã«æ¡ç”¨ã‚’æ¤œè¨"),
                html.Li("å­£ç¯€æ€§ã‚’è€ƒæ…®ã—ãŸæ¡ç”¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã®æœ€é©åŒ–"),
                html.Li("æ—¢å­˜è·å“¡ã®è² è·è»½æ¸›åŠ¹æœã®äºˆæ¸¬"),
                html.Li("ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®é«˜ã„æ¡ç”¨ãƒãƒ£ãƒãƒ«ã®æ´»ç”¨")
            ])
        ], style={'padding': '20px', 'backgroundColor': '#f8f9fa', 'borderRadius': '8px', 'marginTop': '20px'}))

    # æœ€é©æ¡ç”¨è¨ˆç”»
    df_optimal = data_get('optimal_hire_plan', pd.DataFrame())
    if not df_optimal.empty:
        content.append(html.H4("æœ€é©æ¡ç”¨è¨ˆç”»", style={'marginTop': '30px'}))  # type: ignore
        content.append(html.P("åˆ†æã®çµæœã€ä»¥ä¸‹ã®å…·ä½“çš„ãªæ¡ç”¨è¨ˆç”»ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"))
        
        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«ç¿»è¨³
        df_optimal_display = df_optimal.copy()
        df_optimal_display.rename(columns=column_translations, inplace=True)
        
        content.append(dash_table.DataTable(
            data=df_optimal_display.to_dict('records'),
            columns=[{'name': col, 'id': col} for col in df_optimal_display.columns],
            style_cell={'textAlign': 'left'},
            style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'}
        ))

    return html.Div(content)


def create_fatigue_tab() -> html.Div:
    """ç–²åŠ´åˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    explanation = """
    #### ç–²åŠ´åˆ†æã®è©•ä¾¡æ–¹æ³•
    ã‚¹ã‚¿ãƒƒãƒ•ã®ç–²åŠ´ã‚¹ã‚³ã‚¢ã¯ã€ä»¥ä¸‹ã®è¦ç´ ã‚’ç·åˆçš„ã«è©•ä¾¡ã—ã¦ç®—å‡ºã•ã‚Œã¾ã™ã€‚å„è¦ç´ ã¯ã€å…¨ã‚¹ã‚¿ãƒƒãƒ•å†…ã§ã®ç›¸å¯¾çš„ãªä½ç½®ï¼ˆåå·®ï¼‰ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢åŒ–ã•ã‚Œã€é‡ã¿ä»˜ã‘ã•ã‚Œã¦åˆè¨ˆã•ã‚Œã¾ã™ã€‚
    - **å‹¤å‹™é–‹å§‹æ™‚åˆ»ã®ã°ã‚‰ã¤ã:** å‡ºå‹¤æ™‚åˆ»ãŒä¸è¦å‰‡ã§ã‚ã‚‹ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚
    - **æ¥­å‹™ã®å¤šæ§˜æ€§:** æ‹…å½“ã™ã‚‹æ¥­å‹™ï¼ˆå‹¤å‹™ã‚³ãƒ¼ãƒ‰ï¼‰ã®ç¨®é¡ãŒå¤šã„ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚
    - **åŠ´åƒæ™‚é–“ã®ã°ã‚‰ã¤ã:** æ—¥ã€…ã®åŠ´åƒæ™‚é–“ãŒä¸è¦å‰‡ã§ã‚ã‚‹ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚
    - **çŸ­ã„ä¼‘æ¯æœŸé–“:** å‹¤å‹™é–“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«ãŒçŸ­ã„é »åº¦ãŒé«˜ã„ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚
    - **é€£å‹¤:** 3é€£å‹¤ä»¥ä¸Šã®é€£ç¶šå‹¤å‹™ãŒå¤šã„ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚
    - **å¤œå‹¤æ¯”ç‡:** å…¨å‹¤å‹™ã«å ã‚ã‚‹å¤œå‹¤ã®å‰²åˆãŒé«˜ã„ã»ã©ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚Šã¾ã™ã€‚

    *ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ã“ã‚Œã‚‰ã®è¦ç´ ã¯å‡ç­‰ãªé‡ã¿ï¼ˆå„1.0ï¼‰ã§è©•ä¾¡ã•ã‚Œã¾ã™ã€‚*
    """
    content = [
        html.Div(  # type: ignore
            dcc.Markdown(explanation),
            style={
                'padding': '15px',
                'backgroundColor': '#e9f2fa',
                'borderRadius': '8px',
                'marginBottom': '20px',
                'border': '1px solid #cce5ff',
            },
        ),
        html.H3("ç–²åŠ´åˆ†æ", style={'marginBottom': '20px'}),  # type: ignore
    ]
    df_fatigue = data_get('fatigue_score', pd.DataFrame())

    if not df_fatigue.empty:
        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«ç¿»è¨³
        df_fatigue_display = df_fatigue.reset_index().rename(columns={'index': 'staff'})
        column_translations = {
            'staff': 'è·å“¡å',
            'fatigue_score': 'ç–²åŠ´ã‚¹ã‚³ã‚¢',
            'work_start_variance': 'å‹¤å‹™é–‹å§‹æ™‚åˆ»ã®ã°ã‚‰ã¤ã',
            'work_diversity': 'æ¥­å‹™ã®å¤šæ§˜æ€§',
            'work_duration_variance': 'åŠ´åƒæ™‚é–“ã®ã°ã‚‰ã¤ã',
            'short_rest_frequency': 'çŸ­ã„ä¼‘æ¯æœŸé–“ã®é »åº¦',
            'consecutive_work_days': 'é€£å‹¤å›æ•°',
            'night_shift_ratio': 'å¤œå‹¤æ¯”ç‡'
        }
        df_fatigue_display.rename(columns=column_translations, inplace=True)

        # 1. å¾“æ¥ã®æ£’ã‚°ãƒ©ãƒ•
        fig_bar = px.bar(
            df_fatigue_display,
            x='è·å“¡å',
            y='ç–²åŠ´ã‚¹ã‚³ã‚¢',
            title='ã‚¹ã‚¿ãƒƒãƒ•åˆ¥ç–²åŠ´ã‚¹ã‚³ã‚¢',
            labels={'è·å“¡å': 'è·å“¡å', 'ç–²åŠ´ã‚¹ã‚³ã‚¢': 'ç–²åŠ´ã‚¹ã‚³ã‚¢'},
            color='ç–²åŠ´ã‚¹ã‚³ã‚¢',
            color_continuous_scale='Reds'
        )
        content.append(dcc.Graph(figure=fig_bar))

        # 2. ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆç–²åŠ´è¦å› ã®è©³ç´°åˆ†æï¼‰
        fatigue_factors = ['å‹¤å‹™é–‹å§‹æ™‚åˆ»ã®ã°ã‚‰ã¤ã', 'æ¥­å‹™ã®å¤šæ§˜æ€§', 'åŠ´åƒæ™‚é–“ã®ã°ã‚‰ã¤ã', 
                         'çŸ­ã„ä¼‘æ¯æœŸé–“ã®é »åº¦', 'é€£å‹¤å›æ•°', 'å¤œå‹¤æ¯”ç‡']
        available_factors = [col for col in fatigue_factors if col in df_fatigue_display.columns]
        
        if len(df_fatigue_display) > 1 and available_factors:
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
            factor_data = df_fatigue_display[available_factors].copy()
            for col in available_factors:
                if factor_data[col].max() != factor_data[col].min():
                    factor_data[col] = (factor_data[col] - factor_data[col].min()) / (factor_data[col].max() - factor_data[col].min())
            
            fig_heatmap = px.imshow(
                factor_data.T,
                x=df_fatigue_display['è·å“¡å'],
                y=available_factors,
                title='ç–²åŠ´è¦å› ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆè·å“¡åˆ¥è©³ç´°åˆ†æï¼‰',
                color_continuous_scale='Reds',
                aspect='auto'
            )
            fig_heatmap.update_layout(xaxis_title="è·å“¡å", yaxis_title="ç–²åŠ´è¦å› ")
            content.append(dcc.Graph(figure=fig_heatmap))

        # 3. æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆç–²åŠ´è¦å› é–“ã®ç›¸é–¢ï¼‰
        if len(available_factors) >= 2:
            # 2ã¤ã®ä¸»è¦è¦å› ã®æ•£å¸ƒå›³
            fig_scatter = px.scatter(
                df_fatigue_display,
                x=available_factors[0],
                y=available_factors[1] if len(available_factors) > 1 else available_factors[0],
                size='ç–²åŠ´ã‚¹ã‚³ã‚¢',
                hover_name='è·å“¡å',
                title=f'{available_factors[0]} vs {available_factors[1] if len(available_factors) > 1 else available_factors[0]}',
                color='ç–²åŠ´ã‚¹ã‚³ã‚¢',
                color_continuous_scale='Reds'
            )
            content.append(dcc.Graph(figure=fig_scatter))

        # 4. ç–²åŠ´è¦å› ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        if len(available_factors) >= 2:
            corr_matrix = df_fatigue_display[available_factors].corr()
            fig_corr = px.imshow(
                corr_matrix,
                text_auto=True,
                aspect="auto",
                title="ç–²åŠ´è¦å› é–“ã®ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹",
                color_continuous_scale='RdBu_r'
            )
            fig_corr.update_layout(
                xaxis_title="ç–²åŠ´è¦å› ",
                yaxis_title="ç–²åŠ´è¦å› "
            )
            content.append(dcc.Graph(figure=fig_corr))

        # 5. ç–²åŠ´ã‚¹ã‚³ã‚¢åˆ†å¸ƒã¨ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        fig_box = px.box(
            df_fatigue_display,
            y='ç–²åŠ´ã‚¹ã‚³ã‚¢',
            title='ç–²åŠ´ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒï¼ˆãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆï¼‰',
            points="all"  # å…¨ãƒ‡ãƒ¼ã‚¿ç‚¹ã‚’è¡¨ç¤º
        )
        content.append(dcc.Graph(figure=fig_box))

        # 6. ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸Šä½3åã®è©³ç´°æ¯”è¼ƒï¼‰
        if len(df_fatigue_display) >= 3 and available_factors:
            top3 = df_fatigue_display.nlargest(3, 'ç–²åŠ´ã‚¹ã‚³ã‚¢')
            fig_radar = go.Figure()
            
            for _, row in top3.iterrows():
                factor_values = [row[factor] for factor in available_factors]
                # æ­£è¦åŒ–ï¼ˆ0-1ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
                max_val = max(factor_values) if max(factor_values) > 0 else 1
                min_val = min(factor_values)
                normalized_values = [(val - min_val) / (max_val - min_val) if max_val != min_val else 0.5 for val in factor_values]
                
                fig_radar.add_trace(go.Scatterpolar(
                    r=normalized_values,
                    theta=available_factors,
                    fill='toself',
                    name=row['è·å“¡å'],
                    opacity=0.7
                ))
            
            fig_radar.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 1])
                ),
                title="ç–²åŠ´åº¦ä¸Šä½3åã®è¦å› æ¯”è¼ƒï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰",
                showlegend=True
            )
            content.append(dcc.Graph(figure=fig_radar))

        # 7. ç–²åŠ´åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if 'ç–²åŠ´ã‚¹ã‚³ã‚¢' in df_fatigue_display.columns:
            ranking = df_fatigue_display.sort_values('ç–²åŠ´ã‚¹ã‚³ã‚¢', ascending=False)[['è·å“¡å', 'ç–²åŠ´ã‚¹ã‚³ã‚¢']]
            ranking.index = range(1, len(ranking) + 1)
            ranking.index.name = 'é †ä½'
            ranking = ranking.reset_index()
            content.append(html.H4('ç–²åŠ´åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°'))
            content.append(dash_table.DataTable(
                data=ranking.to_dict('records'),
                columns=[{'name': i, 'id': i} for i in ranking.columns],
                style_cell={'textAlign': 'left'},
                style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},
                style_data_conditional=[
                    {
                        'if': {'row_index': 0},
                        'backgroundColor': '#ffebee',
                        'color': 'black',
                    },
                    {
                        'if': {'row_index': 1},
                        'backgroundColor': '#fff3e0',
                        'color': 'black',
                    },
                    {
                        'if': {'row_index': 2},
                        'backgroundColor': '#fff8e1',
                        'color': 'black',
                    }
                ]
            ))

        # ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
        content.append(html.H4("è©³ç´°ãƒ‡ãƒ¼ã‚¿", style={'marginTop': '30px'}))
        content.append(dash_table.DataTable(
            data=df_fatigue_display.to_dict('records'),
            columns=[{'name': i, 'id': i} for i in df_fatigue_display.columns],
            style_cell={'textAlign': 'left'},
            style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},
            sort_action="native"
        ))
    else:
        content.append(html.P("ç–²åŠ´åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"))  # type: ignore

    return html.Div(content)


def create_forecast_tab() -> html.Div:
    """é«˜åº¦åˆ†æå¯¾å¿œã®éœ€è¦äºˆæ¸¬ã‚¿ãƒ–ã‚’ä½œæˆ"""
    content = [
        # ğŸ¯ é«˜åº¦åˆ†æã‚µãƒãƒªãƒ¼ãƒœãƒƒã‚¯ã‚¹
        html.Div(id='forecast-insights', style={
            'padding': '15px',
            'backgroundColor': '#e9f2fa',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #cce5ff'
        }),
        html.H3("ğŸ“ˆ é«˜åº¦éœ€è¦äºˆæ¸¬åˆ†æ", style={'marginBottom': '20px'})
    ]
    
    # ğŸš€ app.pyã®é«˜åº¦åˆ†æçµæœã‚’èª­ã¿è¾¼ã¿
    advanced_results = data_get('advanced_analysis', {})
    df_fc = data_get('forecast_data', pd.DataFrame())
    df_actual = data_get('demand_series', pd.DataFrame())
    
    # ğŸ“Š é«˜åº¦äºˆæ¸¬ãƒãƒ£ãƒ¼ãƒˆ
    if not df_fc.empty or 'forecast' in advanced_results:
        forecast_data = advanced_results.get('forecast', df_fc)
        
        if not forecast_data.empty:
            fig = go.Figure()
            
            # äºˆæ¸¬å€¤ãƒ—ãƒ­ãƒƒãƒˆ
            if {'ds', 'yhat'}.issubset(forecast_data.columns):
                fig.add_trace(go.Scatter(
                    x=forecast_data['ds'], 
                    y=forecast_data['yhat'], 
                    mode='lines+markers', 
                    name='AIäºˆæ¸¬',
                    line=dict(color='#1f77b4', width=3)
                ))
                
                # ä¿¡é ¼åŒºé–“ãŒã‚ã‚Œã°è¡¨ç¤º
                if {'yhat_lower', 'yhat_upper'}.issubset(forecast_data.columns):
                    fig.add_trace(go.Scatter(
                        x=forecast_data['ds'],
                        y=forecast_data['yhat_upper'],
                        mode='lines',
                        line=dict(width=0),
                        showlegend=False,
                        name='ä¸Šé™'
                    ))
                    fig.add_trace(go.Scatter(
                        x=forecast_data['ds'],
                        y=forecast_data['yhat_lower'],
                        mode='lines',
                        line=dict(width=0),
                        fillcolor='rgba(31, 119, 180, 0.2)',
                        fill='tonexty',
                        showlegend=True,
                        name='äºˆæ¸¬ä¿¡é ¼åŒºé–“'
                    ))
            
            # å®Ÿç¸¾å€¤ãƒ—ãƒ­ãƒƒãƒˆ
            if not df_actual.empty and {'ds', 'y'}.issubset(df_actual.columns):
                fig.add_trace(go.Scatter(
                    x=df_actual['ds'], 
                    y=df_actual['y'], 
                    mode='lines', 
                    name='å®Ÿç¸¾',
                    line=dict(dash='dash', color='#ff7f0e', width=2)
                ))
            
            fig.update_layout(
                title='ğŸ¯ AIéœ€è¦äºˆæ¸¬ï¼ˆä¿¡é ¼åŒºé–“ä»˜ãï¼‰',
                xaxis_title='æ—¥ä»˜',
                yaxis_title='éœ€è¦é‡',
                hovermode='x unified',
                height=500
            )
            content.append(dcc.Graph(figure=fig))
            
            # ğŸ“Š äºˆæ¸¬ç²¾åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            forecast_metadata = advanced_results.get('forecast_metadata', {})
            if forecast_metadata:
                metrics_content = []
                if 'model_type' in forecast_metadata:
                    metrics_content.append(f"**äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«:** {forecast_metadata['model_type']}")
                if 'mape' in forecast_metadata:
                    mape = forecast_metadata['mape']
                    metrics_content.append(f"**äºˆæ¸¬ç²¾åº¦ (MAPE):** {mape:.1f}%")
                if 'forecast_period' in forecast_metadata:
                    metrics_content.append(f"**äºˆæ¸¬æœŸé–“:** {forecast_metadata['forecast_period']}æ—¥")
                
                if metrics_content:
                    content.append(html.Div([
                        html.H4("ğŸ¯ äºˆæ¸¬ç²¾åº¦æŒ‡æ¨™"),
                        dcc.Markdown("\n\n".join(metrics_content))
                    ], style={
                        'padding': '15px',
                        'backgroundColor': '#f8f9fa',
                        'borderRadius': '8px',
                        'marginTop': '20px',
                        'border': '1px solid #dee2e6'
                    }))
            
            # ğŸ“‹ äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
            display_cols = ['ds', 'yhat']
            if 'yhat_lower' in forecast_data.columns:
                display_cols.extend(['yhat_lower', 'yhat_upper'])
            
            content.append(html.Div([
                html.H4("ğŸ“‹ è©³ç´°äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿"),
                dash_table.DataTable(
                    data=forecast_data[display_cols].head(30).to_dict('records'),
                    columns=[{
                        'name': 'æ—¥ä»˜' if col == 'ds' else 
                                'äºˆæ¸¬å€¤' if col == 'yhat' else
                                'ä¸‹é™' if col == 'yhat_lower' else
                                'ä¸Šé™' if col == 'yhat_upper' else col,
                        'id': col,
                        'type': 'datetime' if col == 'ds' else 'numeric',
                        'format': {'specifier': '.1f'} if col != 'ds' else None
                    } for col in display_cols],
                    style_cell={'textAlign': 'center'},
                    style_header={'backgroundColor': '#007bff', 'color': 'white'},
                    page_size=10
                )
            ], style={'marginTop': '20px'}))
        
    else:
        content.append(html.Div([
            html.P("ğŸ“Š éœ€è¦äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"),
            html.P("app.pyã§éœ€è¦äºˆæ¸¬åˆ†æã‚’å®Ÿè¡Œã—ã¦ã‹ã‚‰ã”ç¢ºèªãã ã•ã„ã€‚"),
        ], style={
            'padding': '20px',
            'backgroundColor': '#fff3cd',
            'borderRadius': '8px',
            'border': '1px solid #ffeaa7',
            'color': '#856404'
        }))

    return html.Div(content)


def create_fairness_tab() -> html.Div:
    """å…¬å¹³æ€§ã‚¿ãƒ–ã‚’ä½œæˆ"""
    explanation = """
    #### å…¬å¹³æ€§åˆ†æã®è©•ä¾¡æ–¹æ³•
    ã‚¹ã‚¿ãƒƒãƒ•é–“ã®ã€Œä¸å…¬å¹³æ„Ÿã€ã¯ã€å„å€‹äººã®åƒãæ–¹ãŒå…¨ä½“ã®å¹³å‡ã‹ã‚‰ã©ã‚Œã ã‘ä¹–é›¢ã—ã¦ã„ã‚‹ã‹ã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢åŒ–ã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã®è¦ç´ ã®ä¹–é›¢åº¦ã‚’å‡ç­‰ã«è©•ä¾¡ã—ã€ãã®å¹³å‡å€¤ã‚’ã€Œä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢ã€ã¨ã—ã¦ã„ã¾ã™ã€‚
    - **å¤œå‹¤æ¯”ç‡ã®ä¹–é›¢:** ä»–ã®ã‚¹ã‚¿ãƒƒãƒ•ã¨æ¯”è¼ƒã—ã¦ã€å¤œå‹¤ã®å‰²åˆãŒæ¥µç«¯ã«å¤šã„ã€ã¾ãŸã¯å°‘ãªã„ã€‚
    - **ç·åŠ´åƒæ™‚é–“ï¼ˆã‚¹ãƒ­ãƒƒãƒˆæ•°ï¼‰ã®ä¹–é›¢:** ä»–ã®ã‚¹ã‚¿ãƒƒãƒ•ã¨æ¯”è¼ƒã—ã¦ã€ç·åŠ´åƒæ™‚é–“ãŒæ¥µç«¯ã«å¤šã„ã€ã¾ãŸã¯å°‘ãªã„ã€‚
    - **é€£ä¼‘å–å¾—é »åº¦ã®ä¹–é›¢:** ä»–ã®ã‚¹ã‚¿ãƒƒãƒ•ã¨æ¯”è¼ƒã—ã¦ã€é€£ä¼‘ã®å–å¾—ã—ã‚„ã™ã•ã«å·®ãŒã‚ã‚‹ã€‚

    *ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©ã€ã“ã‚Œã‚‰ã®è¦ç´ ã«ãŠã„ã¦å¹³å‡ã‹ã‚‰ã®ä¹–é›¢ãŒå¤§ãã„ï¼ˆï¼ä¸å…¬å¹³æ„Ÿã‚’æ„Ÿã˜ã‚„ã™ã„å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚*
    """
    content = [
        html.Div(  # type: ignore
            dcc.Markdown(explanation),
            style={
                'padding': '15px',
                'backgroundColor': '#f0f0f0',
                'borderRadius': '8px',
                'marginBottom': '20px',
                'border': '1px solid #ddd',
            },
        ),
        html.H3("å…¬å¹³æ€§ (ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢)", style={'marginBottom': '20px'}),  # type: ignore
    ]
    df_fair = data_get('fairness_after', pd.DataFrame())

    if not df_fair.empty:
        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«ç¿»è¨³
        df_fair_display = df_fair.copy()
        column_translations = {
            'staff': 'è·å“¡å',
            'unfairness_score': 'ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢',
            'fairness_score': 'å…¬å¹³æ€§ã‚¹ã‚³ã‚¢',
            'night_ratio': 'å¤œå‹¤æ¯”ç‡',
            'dev_night_ratio': 'å¤œå‹¤æ¯”ç‡ã®ä¹–é›¢',
            'dev_work_slots': 'ç·åŠ´åƒæ™‚é–“ã®ä¹–é›¢',
            'dev_consecutive': 'é€£ä¼‘å–å¾—é »åº¦ã®ä¹–é›¢',
            'work_slots': 'ç·åŠ´åƒæ™‚é–“',
            'consecutive_holidays': 'é€£ä¼‘å–å¾—å›æ•°'
        }
        df_fair_display.rename(columns=column_translations, inplace=True)

        metric_col = (
            'ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢'
            if 'ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢' in df_fair_display.columns
            else ('å…¬å¹³æ€§ã‚¹ã‚³ã‚¢' if 'å…¬å¹³æ€§ã‚¹ã‚³ã‚¢' in df_fair_display.columns else 'å¤œå‹¤æ¯”ç‡')
        )

        # 1. å¾“æ¥ã®æ£’ã‚°ãƒ©ãƒ•ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        fig_bar = px.bar(
            df_fair_display,
            x='è·å“¡å',
            y=metric_col,
            labels={'è·å“¡å': 'è·å“¡å', metric_col: 'ã‚¹ã‚³ã‚¢'},
            color=metric_col,
            color_continuous_scale='RdYlBu_r',
            title='è·å“¡åˆ¥ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢'
        )
        avg_val = df_fair_display[metric_col].mean()
        fig_bar.add_hline(y=avg_val, line_dash='dash', line_color='red', annotation_text="å¹³å‡å€¤")
        content.append(dcc.Graph(figure=fig_bar))

        # 2. å…¬å¹³æ€§è¦å› ã®æ•£å¸ƒå›³ãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        fairness_factors = ['å¤œå‹¤æ¯”ç‡ã®ä¹–é›¢', 'ç·åŠ´åƒæ™‚é–“ã®ä¹–é›¢', 'é€£ä¼‘å–å¾—é »åº¦ã®ä¹–é›¢']
        available_factors = [col for col in fairness_factors if col in df_fair_display.columns]
        
        if len(available_factors) >= 2:
            # æ•£å¸ƒå›³: 2ã¤ã®ä¸»è¦è¦å› ã®é–¢ä¿‚
            fig_scatter = px.scatter(
                df_fair_display,
                x=available_factors[0],
                y=available_factors[1],
                size=metric_col,
                hover_name='è·å“¡å',
                title=f'{available_factors[0]} vs {available_factors[1]}',
                color=metric_col,
                color_continuous_scale='RdYlBu_r'
            )
            content.append(dcc.Graph(figure=fig_scatter))

        # 3. å…¬å¹³æ€§è¦å› ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        if available_factors:
            factor_data = df_fair_display[available_factors].copy()
            # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
            for col in available_factors:
                if factor_data[col].max() != factor_data[col].min():
                    factor_data[col] = (factor_data[col] - factor_data[col].min()) / (factor_data[col].max() - factor_data[col].min())
            
            fig_heatmap = px.imshow(
                factor_data.T,
                x=df_fair_display['è·å“¡å'],
                y=available_factors,
                title='å…¬å¹³æ€§è¦å› ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ï¼ˆè·å“¡åˆ¥è©³ç´°åˆ†æï¼‰',
                color_continuous_scale='RdYlBu_r',
                aspect='auto'
            )
            fig_heatmap.update_layout(xaxis_title="è·å“¡å", yaxis_title="å…¬å¹³æ€§è¦å› ")
            content.append(dcc.Graph(figure=fig_heatmap))

        # 4. åˆ†å¸ƒå›³ã¨ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ
        fig_hist = px.histogram(
            df_fair_display,
            x=metric_col,
            nbins=20,
            title="å…¬å¹³æ€§ã‚¹ã‚³ã‚¢åˆ†å¸ƒ",
            labels={metric_col: 'ã‚¹ã‚³ã‚¢'}
        )
        fig_hist.update_layout(yaxis_title="äººæ•°")
        fig_hist.add_vline(x=avg_val, line_dash='dash', line_color='red', annotation_text="å¹³å‡å€¤")
        content.append(dcc.Graph(figure=fig_hist))

        # 5. ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆä¸å…¬å¹³æ„Ÿä¸Šä½3åï¼‰
        if len(df_fair_display) >= 3 and available_factors:
            top3 = df_fair_display.nlargest(3, metric_col)
            fig_radar = go.Figure()
            
            for _, row in top3.iterrows():
                factor_values = [abs(row[factor]) for factor in available_factors]  # çµ¶å¯¾å€¤ã§æ¯”è¼ƒ
                # æ­£è¦åŒ–
                max_val = max(factor_values) if max(factor_values) > 0 else 1
                normalized_values = [val/max_val for val in factor_values]
                
                fig_radar.add_trace(go.Scatterpolar(
                    r=normalized_values,
                    theta=available_factors,
                    fill='toself',
                    name=row['è·å“¡å']
                ))
            
            fig_radar.update_layout(
                polar=dict(
                    radialaxis=dict(visible=True, range=[0, 1])
                ),
                title="ä¸å…¬å¹³æ„Ÿä¸Šä½3åã®è¦å› æ¯”è¼ƒï¼ˆãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼‰"
            )
            content.append(dcc.Graph(figure=fig_radar))

        # 6. ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ–ãƒ«
        if 'ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢' in df_fair_display.columns:
            ranking = df_fair_display.sort_values('ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢', ascending=False)[['è·å“¡å', 'ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢']]
            ranking.index = range(1, len(ranking) + 1)
            ranking.index.name = 'é †ä½'
            ranking = ranking.reset_index()
            content.append(html.H4('ä¸å…¬å¹³æ„Ÿãƒ©ãƒ³ã‚­ãƒ³ã‚°'))  # type: ignore
            content.append(dash_table.DataTable(
                data=ranking.to_dict('records'),
                columns=[{'name': i, 'id': i} for i in ranking.columns],
                style_cell={'textAlign': 'left'},
                style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'}
            ))

        # è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
        content.append(html.H4("è©³ç´°ãƒ‡ãƒ¼ã‚¿", style={'marginTop': '30px'}))
        content.append(dash_table.DataTable(
            data=df_fair_display.to_dict('records'),
            columns=[{'name': i, 'id': i} for i in df_fair_display.columns],
            style_cell={'textAlign': 'left'},
            style_header={'backgroundColor': 'rgb(230, 230, 230)', 'fontWeight': 'bold'},
            sort_action="native"
        ))
    else:
        content.append(html.P("å…¬å¹³æ€§ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"))

    return html.Div(content)


def create_gap_analysis_tab() -> html.Div:
    """åŸºæº–ä¹–é›¢åˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    content = [html.Div(id='gap-insights', style={  # type: ignore
        'padding': '15px',
        'backgroundColor': '#e9f2fa',
        'borderRadius': '8px',
        'marginBottom': '20px',
        'border': '1px solid #cce5ff'
    }),
        html.H3("åŸºæº–ä¹–é›¢åˆ†æ", style={'marginBottom': '20px'})]  # type: ignore
    df_summary = data_get('gap_summary', pd.DataFrame())
    df_heat = data_get('gap_heatmap', pd.DataFrame())

    if not df_summary.empty:
        content.append(dash_table.DataTable(
            data=df_summary.to_dict('records'),
            columns=[{'name': c, 'id': c} for c in df_summary.columns]
        ))
    if not df_heat.empty:
        fig = px.imshow(
            df_heat,
            aspect='auto',
            color_continuous_scale='RdBu_r',
            labels={'x': 'æ™‚é–“å¸¯', 'y': 'è·ç¨®', 'color': 'ä¹–é›¢'}
        )
        content.append(dcc.Graph(figure=fig))
    if df_summary.empty and df_heat.empty:
        content.append(html.P("åŸºæº–ä¹–é›¢ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"))  # type: ignore

    return html.Div(content)


def create_summary_report_tab() -> html.Div:
    """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚¿ãƒ–ã‚’ä½œæˆ"""
    content = [html.Div(id='summary-report-insights', style={  # type: ignore
        'padding': '15px',
        'backgroundColor': '#e9f2fa',
        'borderRadius': '8px',
        'marginBottom': '20px',
        'border': '1px solid #cce5ff'
    }),
        html.H3("ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ", style={'marginBottom': '20px'})]  # type: ignore
    report_text = data_get('summary_report')
    if report_text:
        content.append(dcc.Markdown(report_text))
    else:
        content.append(html.P("ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"))
    return html.Div(content)


def create_ppt_report_tab() -> html.Div:
    """PowerPointãƒ¬ãƒãƒ¼ãƒˆã‚¿ãƒ–ã‚’ä½œæˆ"""
    return html.Div([  # type: ignore
        html.Div(id='ppt-report-insights', style={
            'padding': '15px',
            'backgroundColor': '#e9f2fa',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #cce5ff'
        }),
        html.H3("PowerPointãƒ¬ãƒãƒ¼ãƒˆ", style={'marginBottom': '20px'}),  # type: ignore
        html.P("ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦PowerPointãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"),  # type: ignore
        html.Button('PPTãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ', id='ppt-generate', n_clicks=0)  # type: ignore
    ])


def create_individual_analysis_tab() -> html.Div:
    """è·å“¡å€‹åˆ¥åˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    long_df = data_get('long_df', pd.DataFrame())

    if long_df.empty:
        return html.Div("åˆ†æã®å…ƒã¨ãªã‚‹å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ (long_df) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    staff_list = sorted(long_df['staff'].unique())

    return html.Div([
        html.H3("è·å“¡å€‹åˆ¥åˆ†æ", style={'marginBottom': '20px'}),
        html.P("åˆ†æã—ãŸã„è·å“¡ã‚’ä»¥ä¸‹ã‹ã‚‰é¸æŠã—ã¦ãã ã•ã„ã€‚"),
        dcc.Dropdown(
            id='individual-staff-dropdown',
            options=[{'label': staff, 'value': staff} for staff in staff_list],
            value=staff_list[0] if staff_list else None,
            clearable=False,
            style={'width': '50%', 'marginBottom': '20px'}
        ),
        
        # ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚¿ã‚¤ãƒ—é¸æŠ
        html.Div([
            html.Label("ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚¿ã‚¤ãƒ—:", style={'fontWeight': 'bold', 'marginBottom': '10px'}),
            dcc.RadioItems(
                id='synergy-analysis-type',
                options=[
                    {'label': 'åŸºæœ¬åˆ†æï¼ˆå…¨è·å“¡å¯¾è±¡ï¼‰', 'value': 'basic'},
                    {'label': 'åŒè·ç¨®é™å®šåˆ†æ', 'value': 'same_role'},
                    {'label': 'å…¨è·ç¨®è©³ç´°åˆ†æ', 'value': 'all_roles'},
                    {'label': 'ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆå…¨ä½“ï¼‰', 'value': 'correlation_matrix'}
                ],
                value='basic',
                inline=True,
                style={'marginBottom': '20px'}
            ),
            html.Div([
                html.Button("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢", id='clear-synergy-cache-btn', className='btn btn-warning btn-sm', style={'marginRight': '10px'}),
                html.Small("â€»ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã¯è¨ˆç®—ã«æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚Šé«˜é€ŸåŒ–ã•ã‚Œã¾ã™", style={'color': '#666'})
            ])
        ], style={'marginBottom': '20px', 'padding': '10px', 'backgroundColor': '#f8f9fa', 'borderRadius': '5px'}),
        html.Div(id='individual-analysis-content')
    ])


def create_team_analysis_tab() -> html.Div:
    """ãƒãƒ¼ãƒ åˆ†æã‚¿ãƒ–ã‚’ä½œæˆ"""
    long_df = data_get('long_df', pd.DataFrame())
    if long_df.empty:
        return html.Div("åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    filterable_cols = ['role', 'code', 'employment']

    return html.Div([
        html.H3("ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ»ãƒãƒ¼ãƒ åˆ†æ"),
        html.Div([
            html.P("ãƒãƒ¼ãƒ åˆ†æã§ã¯ã€ç‰¹å®šã®æ¡ä»¶ã«è©²å½“ã™ã‚‹ã‚¹ã‚¿ãƒƒãƒ•ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹æ€§ã‚’åˆ†æã—ã¾ã™ã€‚"),
            html.Ul([
                html.Li("ãƒãƒ¼ãƒ æ§‹æˆ: é¸æŠã—ãŸæ¡ä»¶ã«è©²å½“ã™ã‚‹ãƒ¡ãƒ³ãƒãƒ¼ã®ä¸€è¦§ã¨è©³ç´°"),
                html.Li("ãƒãƒ¼ãƒ ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹: ãƒ¡ãƒ³ãƒãƒ¼é–“ã®ç›¸æ€§ã‚„å”åƒãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"),
                html.Li("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™: ãƒãƒ¼ãƒ å…¨ä½“ã®åŠ¹ç‡æ€§æŒ‡æ¨™ã¨æ”¹å–„ææ¡ˆ"),
                html.Li("æ™‚é–“å¸¯ã‚«ãƒãƒ¼ç‡: ãƒãƒ¼ãƒ ãŒã‚«ãƒãƒ¼ã—ã¦ã„ã‚‹æ™‚é–“å¸¯ã®åˆ†å¸ƒ")
            ])
        ], style={
            'backgroundColor': '#f0f8ff',
            'padding': '15px',
            'borderRadius': '5px',
            'marginBottom': '20px'
        }),
        html.P("åˆ†æã—ãŸã„ãƒãƒ¼ãƒ ã®æ¡ä»¶ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼š"),
        html.Div([
            dcc.Dropdown(
                id='team-criteria-key-dropdown',
                options=[{'label': col, 'value': col} for col in filterable_cols],
                value='code',
                style={'width': '200px', 'display': 'inline-block'}
            ),
            dcc.Dropdown(
                id='team-criteria-value-dropdown',
                style={
                    'width': '300px',
                    'display': 'inline-block',
                    'marginLeft': '10px'
                }
            )
        ]),
        dcc.Loading(
            id="loading-team-analysis",
            children=html.Div([
                html.Div(id='team-analysis-content'),
                html.Div(id='team-analysis-explanation', style={'marginTop': '20px'})
            ])
        )
    ])


def create_blueprint_analysis_tab() -> html.Div:
    """Return layout for blueprint analysis with facts and implicit knowledge."""
    return html.Div([
        html.H3("ã‚·ãƒ•ãƒˆä½œæˆãƒ—ãƒ­ã‚»ã‚¹ã®\u300cæš—é»™çŸ¥\u300dåˆ†æ", style={'marginBottom': '20px'}),
        html.P(
            "éå»ã®ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€å®¢è¦³çš„äº‹å®Ÿã¨æš—é»™ã®ãƒ«ãƒ¼ãƒ«ã‚’åˆ†æã—ã¾ã™ã€‚",
            style={'marginBottom': '10px'}
        ),

        # åˆ†æã‚¿ã‚¤ãƒ—ã®é¸æŠ
        html.Div([
            dcc.RadioItems(
                id='blueprint-analysis-type',
                options=[
                    {'label': 'æš—é»™çŸ¥ã®ã¿', 'value': 'implicit'},
                    {'label': 'å®¢è¦³çš„äº‹å®Ÿã®ã¿', 'value': 'facts'},
                    {'label': 'çµ±åˆåˆ†æï¼ˆæš—é»™çŸ¥ï¼‹äº‹å®Ÿï¼‰', 'value': 'integrated'}
                ],
                value='integrated',
                inline=True,
                style={'marginBottom': '10px'}
            )
        ]),

        html.Details([
            html.Summary('ğŸ“Š åˆ†æã®è¦³ç‚¹ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§è©³ç´°ï¼‰', style={'cursor': 'pointer', 'fontWeight': 'bold'}),
            html.Div([
                html.H5("æš—é»™çŸ¥ã®6ã¤ã®è¦³ç‚¹"),
                html.Ul([
                    html.Li("ğŸ¤ ã‚¹ã‚­ãƒ«ç›¸æ€§: èª°ã¨èª°ã‚’çµ„ã¾ã›ã‚‹ã¨ä¸Šæ‰‹ãã„ãã‹ã€é€†ã«é¿ã‘ã¦ã„ã‚‹ã‹"),
                    html.Li("âš–ï¸ è² è·åˆ†æ•£æˆ¦ç•¥: ç¹å¿™æ™‚é–“å¸¯ã«ã©ã‚“ãªæˆ¦ç•¥ã§äººã‚’é…ç½®ã—ã¦ã„ã‚‹ã‹"),
                    html.Li("ğŸ‘¤ å€‹äººé…æ…®: ç‰¹å®šè·å“¡ã®å€‹äººäº‹æƒ…ã¸ã®é…æ…®ãƒ‘ã‚¿ãƒ¼ãƒ³"),
                    html.Li("ğŸ”„ ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³: å…¬å¹³æ€§ã‚’ä¿ã¤ãŸã‚ã®è¤‡é›‘ãªãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«"),
                    html.Li("ğŸš¨ ãƒªã‚¹ã‚¯å›é¿: ãƒˆãƒ©ãƒ–ãƒ«é˜²æ­¢ã®ãŸã‚ã®æš—é»™ã®é…ç½®ãƒ«ãƒ¼ãƒ«"),
                    html.Li("ğŸ“… æ™‚ç³»åˆ—æˆ¦ç•¥: æœˆåˆãƒ»æœˆæœ«ã€æ›œæ—¥ã«ã‚ˆã‚‹é…ç½®æˆ¦ç•¥ã®å¤‰åŒ–"),
                ]),
                html.H5("å®¢è¦³çš„äº‹å®Ÿã®è¦³ç‚¹", style={'marginTop': '10px'}),
                html.Ul([
                    html.Li("ğŸ“… æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³: ç‰¹å®šã®æ›œæ—¥ã®ã¿å‹¤å‹™ã€æ›œæ—¥ã®åã‚Š"),
                    html.Li("ğŸ·ï¸ ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³: ç‰¹å®šã®å‹¤å‹™ã‚³ãƒ¼ãƒ‰ã®ã¿ä½¿ç”¨ã€å›é¿"),
                    html.Li("â° æ™‚é–“å¸¯ãƒ‘ã‚¿ãƒ¼ãƒ³: æ—©æœãƒ»æ·±å¤œå‹¤å‹™ã€å›ºå®šæ™‚é–“å¸¯"),
                    html.Li("ğŸ‘¥ ãƒšã‚¢é–¢ä¿‚: é »ç¹ã«ä¸€ç·’ã«åƒã/åƒã‹ãªã„ãƒšã‚¢"),
                    html.Li("ğŸ“Š çµ±è¨ˆçš„äº‹å®Ÿ: å‹¤å‹™é »åº¦ã€å¹³å‡å‹¤å‹™æ™‚é–“"),
                ])
            ], style={'padding': '10px', 'backgroundColor': '#f0f0f0', 'borderRadius': '5px', 'marginTop': '10px'})
        ], style={'marginBottom': '20px'}),

        html.Button(
            "ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚’ç”Ÿæˆ",
            id="generate-blueprint-button",
            n_clicks=0,
            style={
                "marginTop": "10px",
                "marginBottom": "20px",
                "padding": "10px 30px",
                "fontSize": "16px",
                "backgroundColor": "#1f77b4",
                "color": "white",
                "border": "none",
                "borderRadius": "5px",
                "cursor": "pointer"
            },
        ),
        dcc.Loading(
            id="loading-blueprint",
            type="default",
            children=html.Div([
                dcc.Tabs(id='blueprint-result-tabs', children=[
                    dcc.Tab(label='æš—é»™çŸ¥åˆ†æ', value='implicit_analysis', children=[
                        html.Div([
                            html.Div([
                                html.H4("å…¨ä½“åˆ†æãƒ“ãƒ¥ãƒ¼ï¼šã‚·ãƒ•ãƒˆå…¨ä½“ã®å‚¾å‘ã¨æš—é»™çŸ¥"),
                                dcc.Graph(id='tradeoff-scatter-plot'),
                                html.H5("ç™ºè¦‹ã•ã‚ŒãŸæš—é»™çŸ¥ãƒ«ãƒ¼ãƒ«ä¸€è¦§"),
                                html.P("ãƒ«ãƒ¼ãƒ«ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨ã€é–¢é€£ã™ã‚‹ã‚¹ã‚¿ãƒƒãƒ•ã®å€‹åˆ¥åˆ†æã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"),
                                dash_table.DataTable(id='rules-data-table', row_selectable='single'),
                            ], style={'width': '50%', 'display': 'inline-block', 'verticalAlign': 'top'}),
                            html.Div([
                                html.H4("ã‚¹ã‚¿ãƒƒãƒ•å€‹åˆ¥ãƒ“ãƒ¥ãƒ¼ï¼šå€‹äººã®åƒãæ–¹ã¨ä¾¡å€¤è¦³"),
                                dcc.Dropdown(id='staff-selector-dropdown'),
                                dcc.Graph(id='staff-radar-chart'),
                                html.H5("ã“ã®ã‚¹ã‚¿ãƒƒãƒ•ã«é–¢é€£ã™ã‚‹æš—é»™çŸ¥"),
                                html.Div(id='staff-related-rules-list'),
                            ], style={'width': '49%', 'display': 'inline-block', 'verticalAlign': 'top', 'paddingLeft': '1%'}),
                        ])
                    ]),
                    dcc.Tab(label='å®¢è¦³çš„äº‹å®Ÿ', value='facts_analysis', children=[
                        html.Div([
                            html.H4("ç™ºè¦‹ã•ã‚ŒãŸå®¢è¦³çš„äº‹å®Ÿ"),
                            html.Div([
                                html.Label("äº‹å®Ÿã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼:"),
                                dcc.Dropdown(
                                    id='fact-category-filter',
                                    options=[
                                        {'label': 'å…¨ã¦è¡¨ç¤º', 'value': 'all'},
                                        {'label': 'å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³äº‹å®Ÿ', 'value': 'å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³äº‹å®Ÿ'},
                                        {'label': 'æ›œæ—¥äº‹å®Ÿ', 'value': 'æ›œæ—¥äº‹å®Ÿ'},
                                        {'label': 'ã‚³ãƒ¼ãƒ‰äº‹å®Ÿ', 'value': 'ã‚³ãƒ¼ãƒ‰äº‹å®Ÿ'},
                                        {'label': 'æ™‚é–“å¸¯äº‹å®Ÿ', 'value': 'æ™‚é–“å¸¯äº‹å®Ÿ'},
                                        {'label': 'ãƒšã‚¢äº‹å®Ÿ', 'value': 'ãƒšã‚¢äº‹å®Ÿ'},
                                        {'label': 'çµ±è¨ˆçš„äº‹å®Ÿ', 'value': 'çµ±è¨ˆçš„äº‹å®Ÿ'}
                                    ],
                                    value='all',
                                    clearable=False
                                )
                            ], style={'width': '300px', 'marginBottom': '20px'}),
                            dash_table.DataTable(
                                id='facts-data-table',
                                columns=[
                                    {'name': 'ã‚¹ã‚¿ãƒƒãƒ•', 'id': 'ã‚¹ã‚¿ãƒƒãƒ•'},
                                    {'name': 'ã‚«ãƒ†ã‚´ãƒªãƒ¼', 'id': 'ã‚«ãƒ†ã‚´ãƒªãƒ¼'},
                                    {'name': 'äº‹å®Ÿã‚¿ã‚¤ãƒ—', 'id': 'äº‹å®Ÿã‚¿ã‚¤ãƒ—'},
                                    {'name': 'è©³ç´°', 'id': 'è©³ç´°'},
                                    {'name': 'ç¢ºä¿¡åº¦', 'id': 'ç¢ºä¿¡åº¦', 'type': 'numeric', 'format': {'specifier': '.2f'}}
                                ],
                                style_data_conditional=[
                                    {
                                        'if': {
                                            'column_id': 'ç¢ºä¿¡åº¦',
                                            'filter_query': '{ç¢ºä¿¡åº¦} >= 0.8'
                                        },
                                        'backgroundColor': '#3D9970',
                                        'color': 'white',
                                    },
                                    {
                                        'if': {
                                            'column_id': 'ç¢ºä¿¡åº¦',
                                            'filter_query': '{ç¢ºä¿¡åº¦} < 0.5'
                                        },
                                        'backgroundColor': '#FFDC00',
                                    }
                                ],
                                sort_action='native',
                                filter_action='native',
                                page_size=20
                            ),
                            html.Div(id='facts-summary', style={'marginTop': '20px'})
                        ])
                    ]),
                    dcc.Tab(label='çµ±åˆåˆ†æ', value='integrated_analysis', children=[
                        html.Div([
                            html.H4("äº‹å®Ÿã¨æš—é»™çŸ¥ã®é–¢é€£"),
                            html.P("å®¢è¦³çš„äº‹å®ŸãŒã©ã®ã‚ˆã†ãªæš—é»™çŸ¥ã«ã¤ãªãŒã£ã¦ã„ã‚‹ã‹ã‚’åˆ†æã—ã¾ã™ã€‚"),
                            html.Div(id='integrated-analysis-content')
                        ])
                    ])
                ], value='implicit_analysis'),
            ], id='blueprint-analysis-content')
        ),
    ])

# --- ãƒ¡ã‚¤ãƒ³ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ ---
app.layout = html.Div([
    # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
    dcc.Store(id='device-info-store', storage_type='session'),
    dcc.Store(id='screen-size-store', storage_type='session'),
    html.Div(id='app-loading-trigger', children='loaded', style={'display': 'none'}),
    dcc.Store(id='kpi-data-store', storage_type='memory'),
    dcc.Store(id='data-loaded', storage_type='memory'),
    dcc.Store(id='full-analysis-store', storage_type='memory'),
    dcc.Store(id='creation-logic-results-store', storage_type='memory'),
    dcc.Store(id='logic-analysis-progress', storage_type='memory'),
    dcc.Store(id='blueprint-results-store', storage_type='memory'),
    dcc.Interval(id='logic-analysis-interval', interval=500, disabled=True),
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œã‚¹ã‚¿ã‚¤ãƒ«
    html.Link(
        rel='stylesheet',
        href='https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css'
    ),

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    html.Div([  # type: ignore
        html.H1("ğŸ—‚ï¸ Shift-Suite é«˜é€Ÿåˆ†æãƒ“ãƒ¥ãƒ¼ã‚¢", style={
            'textAlign': 'center',
            'color': 'white',
            'margin': '0',
            'padding': '20px'
        })
    ], style={
        'backgroundColor': '#2c3e50',
        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'
    }),

    # æ”¹å–„ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ã‚¨ãƒªã‚¢
    html.Div([
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ•ãƒ­ãƒ¼ã‚’ä½¿ç”¨ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        data_ingestion.create_upload_ui() if data_ingestion else html.Div([
            # ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰UI
            html.Div([
                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒªã‚¢ã®ã¿
                dcc.Upload(
                    id='upload-data',
                    children=html.Div([
                        html.I(className="fas fa-cloud-upload-alt", 
                              style={'fontSize': '40px', 'color': '#3498db'})
                    ], style={
                        'display': 'flex',
                        'alignItems': 'center',
                        'justifyContent': 'center',
                        'height': '100%'
                    }),
                    style={
                        'width': '100%',
                        'height': '140px',
                        'border': '2px dashed #3498db',
                        'borderRadius': '8px',
                        'backgroundColor': '#f0f8ff',
                        'cursor': 'pointer',
                        'transition': 'all 0.3s ease'
                    },
                    multiple=False,
                    # ãƒ‰ãƒ©ãƒƒã‚°ã‚ªãƒ¼ãƒãƒ¼æ™‚ã®ã‚¹ã‚¿ã‚¤ãƒ«
                    style_active={
                        'borderColor': '#2ecc71',
                        'backgroundColor': '#e8f8f5'
                    },
                    style_reject={
                        'borderColor': '#e74c3c',
                        'backgroundColor': '#ffe5e5'
                    }
                ),
                # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºç”¨
                dcc.Store(id='data-ingestion-output', storage_type='memory'),
                html.Div(id='upload-status', style={'marginTop': '10px'})
            ], style={'padding': '30px', 'backgroundColor': 'white', 'borderRadius': '12px', 'boxShadow': '0 2px 8px rgba(0,0,0,0.08)'})
        ])
    ], style={'padding': '20px', 'maxWidth': '600px', 'margin': '0 auto'}),

    # å‡¦ç†é€²æ—è¡¨ç¤ºã‚¨ãƒªã‚¢ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
    html.Div([
        html.Div([
            html.H4("âš¡ å‡¦ç†é€²æ—", style={'color': '#2c3e50', 'marginBottom': '10px'}),
            html.Div(id='progress-content', children=[])
        ], style={'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})
    ], id='progress-display-div', style={'display': 'none', 'padding': '0 20px', 'marginTop': '20px'}),
    
    # ã‚·ãƒŠãƒªã‚ªé¸æŠã‚¨ãƒªã‚¢ï¼ˆæ”¹å–„ç‰ˆï¼‰
    html.Div([
        html.Div([
            html.H4("ğŸ¯ åˆ†æã‚·ãƒŠãƒªã‚ªé¸æŠ", style={'color': '#2c3e50', 'marginBottom': '10px'}),
            html.P("è¤‡æ•°ã®ã‚·ãƒŠãƒªã‚ªãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€åˆ†æã—ãŸã„ã‚·ãƒŠãƒªã‚ªã‚’é¸æŠã—ã¦ãã ã•ã„", 
                  style={'color': '#555', 'marginBottom': '15px'}),
            dcc.Dropdown(
                id='scenario-dropdown',
                placeholder="ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†å¾Œã«ã‚·ãƒŠãƒªã‚ªãŒè¡¨ç¤ºã•ã‚Œã¾ã™",
                style={'width': '100%'}
            )
        ], style={'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})
    ], id='scenario-selector-div', style={'display': 'none', 'padding': '0 20px', 'marginTop': '20px'}),

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¾Œã«è¡¨ç¤ºï¼‰
    html.Div(id='main-content'),  # type: ignore

    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç›£è¦–ã‚¨ãƒªã‚¢ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
    html.Div([
        html.Div([
            html.H4("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", style={'color': '#2c3e50', 'marginBottom': '10px'}),
            html.Div(id='system-status-content', children=[])
        ], style={'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})
    ], id='system-status-div', style={'padding': '0 20px', 'marginTop': '20px'}),
    
    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ãƒ“ãƒ¥ãƒ¼ã‚¢
    html.Details([
        html.Summary('ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ã‚’è¡¨ç¤º/éè¡¨ç¤º'),
        dcc.Textarea(id='log-viewer', style={'width': '100%', 'height': 300}, readOnly=True)
    ], style={'padding': '0 20px', 'marginTop': '20px'}),
    dcc.Interval(id='log-interval', interval=1000),
    dcc.Interval(id='system-monitor-interval', interval=5000),  # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–ç”¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ«

], style={'backgroundColor': '#f5f5f5', 'minHeight': '100vh'})

# é€²æ—è¡¨ç¤ºæ›´æ–°ç”¨ã®ã‚¹ãƒˆã‚¢
app.layout.children.append(dcc.Store(id='progress-store', data={}))
app.layout.children.append(dcc.Interval(id='progress-interval', interval=500, n_intervals=0))

# --- ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•° ---

# ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±æ¤œå‡ºã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
app.clientside_callback(
    """
    function() {
        const deviceInfo = {
            'screen_width': window.screen.width,
            'screen_height': window.screen.height,
            'viewport_width': window.innerWidth,
            'viewport_height': window.innerHeight,
            'device_pixel_ratio': window.devicePixelRatio || 1,
            'user_agent': navigator.userAgent,
            'touch_support': 'ontouchstart' in window,
            'timestamp': Date.now()
        };
        
        // ãƒ‡ãƒã‚¤ã‚¹ã‚¿ã‚¤ãƒ—åˆ¤å®š
        let device_type = 'desktop';
        if (deviceInfo.viewport_width <= 768) {
            device_type = 'mobile';
        } else if (deviceInfo.viewport_width <= 1024) {
            device_type = 'tablet';
        }
        
        deviceInfo.device_type = device_type;
        
        return [deviceInfo, deviceInfo.viewport_width];
    }
    """,
    [Output('device-info-store', 'data'),
     Output('screen-size-store', 'data')],
    [Input('app-loading-trigger', 'children')]
)

# ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ›´æ–°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç¾åœ¨ç„¡åŠ¹åŒ– - å¯¾å¿œã™ã‚‹IDãŒå­˜åœ¨ã—ãªã„ãŸã‚ï¼‰
# @app.callback(
#     [Output('main-content-area', 'className'),
#      Output('header-container', 'className')],
#     [Input('device-info-store', 'data')]
# )
def update_responsive_layout(device_info):
    """ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã«åŸºã¥ã„ã¦ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚¯ãƒ©ã‚¹ã‚’æ›´æ–°"""
    if not device_info:
        return 'responsive-container', 'header-container'
    
    device_type = device_info.get('device_type', 'desktop')
    
    # ãƒ‡ãƒã‚¤ã‚¹åˆ¥ã‚¯ãƒ©ã‚¹è¨­å®š
    content_classes = ['responsive-container']
    header_classes = ['header-container']
    
    if device_type == 'mobile':
        content_classes.append('mobile-layout')
        header_classes.append('mobile-header')
    elif device_type == 'tablet':
        content_classes.append('tablet-layout')
        header_classes.append('tablet-header')
    else:
        content_classes.append('desktop-layout')
        header_classes.append('desktop-header')
    
    return ' '.join(content_classes), ' '.join(header_classes)

# é€²æ—è¡¨ç¤ºæ›´æ–°ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œï¼‰
@app.callback(
    [Output('progress-content', 'children'),
     Output('progress-display-div', 'style')],
    [Input('progress-interval', 'n_intervals'),
     Input('device-info-store', 'data')]
)
@safe_callback
def update_progress_display(n_intervals, device_info):
    """é€²æ—è¡¨ç¤ºã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œï¼‰"""
    if not processing_monitor:
        return [], {'display': 'none'}
    
    try:
        status = processing_monitor.get_status()
        
        # å‡¦ç†å®Œäº†å¾Œã‚‚éè¡¨ç¤ºã«ã™ã‚‹ï¼ˆ100%ã«ãªã£ãŸã‚‰æ¶ˆã™ï¼‰
        if not status['is_running']:
            # 100%åˆ°é”ã‚’ç¢ºèª
            if status.get('overall_progress', 0) >= 100:
                log.info(f"é€²æ—ãŒ100%ã«åˆ°é”: {status.get('overall_progress')}%")
            return [], {'display': 'none'}
        
        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—
        device_type = device_info.get('device_type', 'desktop') if device_info else 'desktop'
        
        # æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã®create_dash_displayãƒ¡ã‚½ãƒƒãƒ‰ã‚’å„ªå…ˆä½¿ç”¨
        if hasattr(processing_monitor, 'create_dash_display'):
            try:
                progress_display = processing_monitor.create_dash_display(status)
                return [progress_display], {
                    'display': 'block', 
                    'padding': '0 20px', 
                    'marginTop': '20px'
                }
            except Exception as e:
                log.warning(f"æ–°ã‚·ã‚¹ãƒ†ãƒ ã®è¡¨ç¤ºã‚¨ãƒ©ãƒ¼ã€å¾“æ¥æ–¹æ³•ã«åˆ‡ã‚Šæ›¿ãˆ: {e}")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œé€²æ—è¡¨ç¤ºã‚’ä½œæˆ
        elif visualization_engine and processing_monitor:
            try:
                # å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ãŸé€²æ—è¡¨ç¤º
                current_step = "ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­"
                for step_name, step_info in status.get('steps', {}).items():
                    if step_info.get('status') == 'running':
                        current_step = step_info.get('description', step_name)
                        break
                
                progress_display = visualization_engine.create_progress_visualization(
                    current_step=current_step,
                    progress_percentage=status['overall_progress'],
                    estimated_remaining=int(status.get('estimated_remaining', 0)),
                    device_type=device_type
                )
                
                return [progress_display], {
                    'display': 'block', 
                    'padding': '0 20px', 
                    'marginTop': '20px'
                }
                
            except Exception as e:
                log.warning(f"å¯è¦–åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚¨ãƒ©ãƒ¼ã€å¾“æ¥æ–¹æ³•ã«åˆ‡ã‚Šæ›¿ãˆ: {e}")
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªé€²æ—è¡¨ç¤ºï¼ˆã‚°ãƒ©ãƒ•ã‚’ä½¿ã‚ãªã„ï¼‰
        progress_percent = status.get('overall_progress', 0)
        
        # å…¨ä½“é€²æ—è¡¨ç¤º
        progress_components = [
            html.Div([
                html.H6(f"å‡¦ç†é€²æ—: {progress_percent}%", style={'marginBottom': '10px'}),
                html.Div([
                    html.Div(
                        style={
                            'width': f"{progress_percent}%",
                            'height': '30px',
                            'backgroundColor': '#3498db',
                            'borderRadius': '4px',
                            'transition': 'width 0.5s ease'
                        }
                    )
                ], style={
                    'width': '100%',
                    'height': '30px',
                    'backgroundColor': '#e0e0e0',
                    'borderRadius': '4px',
                    'marginBottom': '10px'
                }),
                html.Div([
                    html.Span(f"å‡¦ç†ä¸­: {status.get('current_stage', 'åˆæœŸåŒ–')}", 
                             style={'fontSize': '12px', 'color': '#666'})
                ])
            ], style={'marginBottom': '20px'})
        ]
        
        # processing_monitorã®create_progress_displayã‚’ä½¿ç”¨ã—ã¦display_dataã‚’å–å¾—
        display_data = processing_monitor.create_progress_display(status)
        
        # å€‹åˆ¥ã‚¹ãƒ†ãƒƒãƒ—è¡¨ç¤º
        steps_display = []
        for step_data in display_data['step_data']:
            step_style = {
                'padding': '8px 12px',
                'margin': '5px 0',
                'borderRadius': '4px',
                'border': f'2px solid {step_data["color"]}',
                'backgroundColor': f'{step_data["color"]}15',
                'display': 'flex',
                'alignItems': 'center',
                'justifyContent': 'space-between'
            }
            
            steps_display.append(
                html.Div([
                    html.Div([
                        html.Span(step_data['icon'], style={'marginRight': '8px', 'fontSize': '16px'}),
                        html.Span(step_data['description'], style={'fontWeight': 'bold'})
                    ]),
                    html.Div([
                        html.Span(f"{step_data['progress']}%", 
                                 style={'color': step_data['color'], 'fontWeight': 'bold'})
                    ])
                ], style=step_style)
            )
        
        progress_components.extend([
            html.H6("å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—", style={'marginTop': '20px', 'marginBottom': '10px'}),
            html.Div(steps_display)
        ])
        
        return progress_components, {'display': 'block', 'padding': '0 20px', 'marginTop': '20px'}
        
    except Exception as e:
        log.error(f"[å‡¦ç†ç›£è¦–] é€²æ—è¡¨ç¤ºæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return [html.P(f"é€²æ—è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {str(e)}", style={'color': 'red'})], {'display': 'block', 'padding': '0 20px', 'marginTop': '20px'}

# ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç›£è¦–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
@app.callback(
    Output('system-status-content', 'children'),
    Input('system-monitor-interval', 'n_intervals')
)
@safe_callback
def update_system_status(n_intervals):
    """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°"""
    components = []
    
    # ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹
    if memory_manager:
        try:
            stats = memory_manager.get_statistics()
            memory_color = '#27ae60'  # ç·‘
            if stats['current_memory_percent'] > 80:
                memory_color = '#e74c3c'  # èµ¤
            elif stats['current_memory_percent'] > 60:
                memory_color = '#f39c12'  # ã‚ªãƒ¬ãƒ³ã‚¸
            
            memory_info = html.Div([
                html.H6("ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³", style={'marginBottom': '5px'}),
                html.Div([
                    html.Div(
                        style={
                            'width': f"{stats['current_memory_percent']}%",
                            'height': '20px',
                            'backgroundColor': memory_color,
                            'borderRadius': '3px',
                            'transition': 'width 0.5s ease'
                        }
                    )
                ], style={
                    'width': '100%',
                    'height': '20px',
                    'backgroundColor': '#ecf0f1',
                    'borderRadius': '3px',
                    'marginBottom': '5px'
                }),
                html.P([
                    f"{stats['current_memory_percent']:.1f}% ä½¿ç”¨ä¸­ ",
                    f"({stats['memory_rss_mb']:.0f}MB / åˆ©ç”¨å¯èƒ½: {stats['available_memory_mb']:.0f}MB)",
                    html.Br(),
                    f"ãƒˆãƒ¬ãƒ³ãƒ‰: {stats['memory_trend']} | ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {stats['cache_hit_rate']:.1f}%"
                ], style={'fontSize': '12px', 'color': '#666', 'marginBottom': '10px'})
            ])
            components.append(memory_info)
        except Exception as e:
            log.error(f"[ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–] ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹
    if smart_cache:
        try:
            cache_info = smart_cache.get_cache_info()
            cache_usage = (cache_info['size'] / cache_info['max_size']) * 100
            
            cache_div = html.Div([
                html.H6("ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹", style={'marginBottom': '5px'}),
                html.P([
                    f"ä½¿ç”¨ç‡: {cache_usage:.1f}% ({cache_info['size']}/{cache_info['max_size']} å€‹)",
                    html.Br(),
                    f"ãƒ’ãƒƒãƒˆç‡: {cache_info['hit_rate']:.1f}%"
                ], style={'fontSize': '12px', 'color': '#666', 'marginBottom': '10px'})
            ])
            components.append(cache_div)
        except Exception as e:
            log.error(f"[ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–] ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
    if performance_monitor:
        try:
            perf_report = performance_monitor.get_performance_report()
            if perf_report['operations'] > 0:
                perf_div = html.Div([
                    html.H6("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", style={'marginBottom': '5px'}),
                    html.P([
                        f"ç·å‡¦ç†æ™‚é–“: {perf_report['total_time']:.1f}ç§’",
                        html.Br(),
                        f"å¹³å‡å‡¦ç†æ™‚é–“: {perf_report['average_time']:.2f}ç§’/æ“ä½œ"
                    ], style={'fontSize': '12px', 'color': '#666'})
                ])
                components.append(perf_div)
        except Exception as e:
            log.error(f"[ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    if not components:
        components.append(html.P("ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–æƒ…å ±ãªã—", style={'color': '#999'}))
    
    return components

# ãƒ‡ãƒãƒƒã‚°ç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰çŠ¶æ…‹ç¢ºèªï¼‰
def debug_upload_trigger(contents, filename):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¤ãƒ™ãƒ³ãƒˆã®ç™ºç«ç¢ºèª"""
    if contents:
        log.info(f"[DEBUG] Upload triggered: {filename}")
        log.info(f"[DEBUG] Content type: {contents[:50]}")  # æœ€åˆã®50æ–‡å­—ã‚’ç¢ºèª
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æˆåŠŸæ™‚ã®ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›´
        return {
            'width': '100%',
            'height': '140px',
            'border': '2px solid #2ecc71',
            'borderRadius': '8px',
            'backgroundColor': '#e8f8f5',
            'cursor': 'pointer',
            'transition': 'all 0.3s ease'
        }
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚¿ã‚¤ãƒ«
    return {
        'width': '100%',
        'height': '140px',
        'border': '2px dashed #3498db',
        'borderRadius': '8px',
        'backgroundColor': '#f0f8ff',
        'cursor': 'pointer',
        'transition': 'all 0.3s ease'
    }

def process_upload(contents, filename):
    """æ”¹å–„ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ç‰ˆï¼‰"""
    log.info(f"[process_upload] Called with filename: {filename}, contents: {contents is not None}")
    
    if contents is None:
        log.info("[process_upload] Contents is None, raising PreventUpdate")
        raise PreventUpdate

    global TEMP_DIR_OBJ
    
    log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ãƒ•ã‚¡ã‚¤ãƒ«å—ä¿¡: {filename}")
    
    try:
        # é€²æ—ç›£è¦–é–‹å§‹ï¼ˆæ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã§å‹•çš„ã«ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç™»éŒ²ï¼‰
        if processing_monitor:
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‹•çš„ã«ç™»éŒ²
            if hasattr(processing_monitor, 'register_steps'):
                # æ–°ã—ã„ã‚·ã‚¹ãƒ†ãƒ ã®å ´åˆ
                processing_monitor.register_steps([
                    {'name': 'upload', 'description': f'ãƒ•ã‚¡ã‚¤ãƒ«å—ä¿¡: {filename}', 'weight': 1.0},
                    {'name': 'validation', 'description': 'ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼', 'weight': 1.0},
                    {'name': 'extraction', 'description': 'ãƒ‡ãƒ¼ã‚¿æŠ½å‡º', 'weight': 2.0},
                    {'name': 'preprocessing', 'description': 'ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†', 'weight': 2.0},
                    {'name': 'analysis', 'description': 'åˆ†æå‡¦ç†', 'weight': 2.0},
                    {'name': 'visualization', 'description': 'å¯è¦–åŒ–æº–å‚™', 'weight': 1.0}
                ])
            start_processing()
            start_step("upload", f"ãƒ•ã‚¡ã‚¤ãƒ«å—ä¿¡: {filename}")

        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ•ãƒ­ãƒ¼ã‚’ä½¿ç”¨ã—ãŸæ¤œè¨¼ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if data_ingestion:
            try:
                if processing_monitor:
                    start_step("validation", "ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­...")
                    update_progress("upload", 100)
                
                validation_result = data_ingestion.validate_file(contents, filename)
                log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†: {validation_result['valid']}")
                
                # æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªã‚¨ãƒ©ãƒ¼ã‚’è¿”ã™
                if not validation_result['valid']:
                    error_messages = validation_result.get('errors', ['ä¸æ˜ãªã‚¨ãƒ©ãƒ¼'])
                    formatted_error = "ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ã‚¨ãƒ©ãƒ¼:\n" + "\n".join(f"â€¢ {error}" for error in error_messages)
                    log.warning(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] æ¤œè¨¼å¤±æ•—: {formatted_error}")
                    
                    if processing_monitor:
                        fail_step("validation", formatted_error)
                    
                    return {
                        'error': formatted_error,
                        'validation_result': validation_result
                    }, [], None, {'display': 'none'}
                    
                # è­¦å‘ŠãŒã‚ã‚‹å ´åˆã¯ãƒ­ã‚°ã«è¨˜éŒ²
                if validation_result.get('warnings'):
                    for warning in validation_result['warnings']:
                        log.warning(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] è­¦å‘Š: {warning}")
                
                if processing_monitor:
                    complete_step("validation", "ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å®Œäº†")
                        
            except Exception as e:
                log.error(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] æ¤œè¨¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                if processing_monitor:
                    fail_step("validation", f"æ¤œè¨¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                # æ¤œè¨¼ã«å¤±æ•—ã—ãŸå ´åˆã¯å¾“æ¥ã®å‡¦ç†ã‚’ç¶™ç¶š

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        if TEMP_DIR_OBJ:
            TEMP_DIR_OBJ.cleanup()

        TEMP_DIR_OBJ = tempfile.TemporaryDirectory(prefix="shift_suite_dash_")
        temp_dir_path = Path(TEMP_DIR_OBJ.name)
        log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {temp_dir_path}")

        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆé€²æ—ãƒ­ã‚°ä»˜ãï¼‰
        if processing_monitor:
            start_step("extraction", "ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚’é–‹å§‹...")
        
        content_type, content_string = contents.split(',')
        decoded = base64.b64decode(content_string)
        log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†: {len(decoded)} bytes")
        
        if processing_monitor:
            update_progress("extraction", 30, "ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚³ãƒ¼ãƒ‰å®Œäº†")

        file_ext = Path(filename).suffix.lower()
        log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­: {file_ext}")
        
        if file_ext == '.zip':
            # ZIPå‡¦ç†
            log.info("[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ZIPãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹é–‹å§‹")
            if processing_monitor:
                update_progress("extraction", 50, "ZIPãƒ•ã‚¡ã‚¤ãƒ«å±•é–‹ä¸­...")
                
            with zipfile.ZipFile(io.BytesIO(decoded)) as zf:
                # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ãƒ‘ã‚¹ãƒˆãƒ©ãƒãƒ¼ã‚µãƒ«æ”»æ’ƒã‚’é˜²ã
                import os
                for member in zf.namelist():
                    # çµ¶å¯¾ãƒ‘ã‚¹ã‚„è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‚ç…§ã‚’æ¤œè¨¼
                    if os.path.isabs(member) or ".." in member or member.startswith("/"):
                        log.error(f"[ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£] å±é™ºãªãƒ‘ã‚¹ã‚’æ¤œå‡º: {member}")
                        if processing_monitor:
                            fail_step("extraction", f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ©ãƒ¼: å±é™ºãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {member}")
                        return {
                            'error': f'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ©ãƒ¼: ä¸æ­£ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™\nå±é™ºãªãƒ‘ã‚¹: {member}'
                        }, [], None, {'display': 'none'}
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ­£è¦åŒ–ã¨ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
                    normalized_path = os.path.normpath(member)
                    if normalized_path.startswith(('..', os.sep)):
                        log.error(f"[ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£] æ­£è¦åŒ–å¾Œã‚‚å±é™ºãªãƒ‘ã‚¹: {normalized_path}")
                        continue
                
                # æ¤œè¨¼ã‚’ãƒ‘ã‚¹ã—ãŸå ´åˆã®ã¿å±•é–‹
                zf.extractall(temp_dir_path)
            log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ZIPå±•é–‹å®Œäº†: {temp_dir_path}")
            
            if processing_monitor:
                update_progress("extraction", 80, "å±•é–‹å®Œäº†ã€ã‚·ãƒŠãƒªã‚ªæ¤œå‡ºä¸­...")

            # ã‚·ãƒŠãƒªã‚ªæ¤œå‡º
            scenarios = [d.name for d in temp_dir_path.iterdir() if d.is_dir() and d.name.startswith('out_')]
            if not scenarios:
                log.error("[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] åˆ†æã‚·ãƒŠãƒªã‚ªãƒ•ã‚©ãƒ«ãƒ€æœªæ¤œå‡º")
                if processing_monitor:
                    fail_step("extraction", "åˆ†æã‚·ãƒŠãƒªã‚ªãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return {
                    'error': 'åˆ†æã‚·ãƒŠãƒªã‚ªã®ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n' +
                           'ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…ã« "out_" ã§å§‹ã¾ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãŒå¿…è¦ã§ã™ã€‚'
                }, [], None, {'display': 'none'}

            log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ã‚·ãƒŠãƒªã‚ªæ¤œå‡º: {scenarios}")
            if processing_monitor:
                complete_step("extraction", f"ã‚·ãƒŠãƒªã‚ª{len(scenarios)}å€‹ã‚’æ¤œå‡º")
            
        elif file_ext in {'.xlsx', '.csv'}:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
            log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file_ext}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
            file_path = temp_dir_path / filename
            with open(file_path, 'wb') as f:
                f.write(decoded)
            
            # ç–‘ä¼¼ã‚·ãƒŠãƒªã‚ªã‚’ä½œæˆ
            scenario_dir = temp_dir_path / "out_single_file"
            scenario_dir.mkdir(exist_ok=True)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
            import shutil
            shutil.copy2(file_path, scenario_dir / filename)
            
            scenarios = ["out_single_file"]
            log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ãƒŠãƒªã‚ªä½œæˆå®Œäº†")
            
        else:
            log.error(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] æœªã‚µãƒãƒ¼ãƒˆå½¢å¼: {file_ext}")
            return {
                'error': f'æœªã‚µãƒãƒ¼ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™: {file_ext}\n' +
                       'ã‚µãƒãƒ¼ãƒˆå½¢å¼: .zip, .xlsx, .csv'
            }, [], None, {'display': 'none'}
        
        # å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚’å®Ÿè¡Œ
        log.info("[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºé–‹å§‹") 
        try:
            detect_slot_intervals_from_data(temp_dir_path, scenarios)
            log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºå®Œäº†: {DETECTED_SLOT_INFO['slot_minutes']}åˆ†é–“éš”")
        except Exception as e:
            log.warning(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å‹•çš„ã‚¹ãƒ­ãƒƒãƒˆæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ç¶™ç¶š

        # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ç”¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆæ‹¡å¼µç‰ˆï¼‰
        scenario_name_map = {
            'out_median_based': 'ğŸ“Š ä¸­å¤®å€¤ãƒ™ãƒ¼ã‚¹åˆ†æ',
            'out_mean_based': 'ğŸ“ˆ å¹³å‡å€¤ãƒ™ãƒ¼ã‚¹åˆ†æ',
            'out_p25_based': 'ğŸ“‰ 25ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«åˆ†æ',
            'out_single_file': 'ğŸ“ å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ',
        }

        scenario_options = [
            {'label': scenario_name_map.get(s, f"ğŸ“‹ {s.replace('out_', '')}"), 'value': s}
            for s in scenarios
        ]
        first_scenario = scenarios[0]
        scenario_paths = {d.name: str(d) for d in temp_dir_path.iterdir() if d.is_dir()}
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°
        global CURRENT_SCENARIO_DIR
        CURRENT_SCENARIO_DIR = temp_dir_path / first_scenario
        log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] CURRENT_SCENARIO_DIR set to: {CURRENT_SCENARIO_DIR}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
        clear_data_cache()
        log.info("[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢")
        
        log.info(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å‡¦ç†å®Œäº† - ã‚·ãƒŠãƒªã‚ªæ•°: {len(scenarios)}")
        
        # å‡¦ç†å®Œäº†ã‚’è¨˜éŒ²
        if processing_monitor:
            start_step("preprocessing", "å‰å‡¦ç†æº–å‚™å®Œäº†")
            complete_step("preprocessing", "ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿ãƒ•ãƒ­ãƒ¼å®Œäº†")
            # æ®‹ã‚Šã®ã‚¹ãƒ†ãƒƒãƒ—ã‚‚å®Œäº†ã•ã›ã¦100%ã«ã™ã‚‹
            start_step("analysis", "åˆ†ææº–å‚™")
            complete_step("analysis", "åˆ†ææº–å‚™å®Œäº†")
            start_step("visualization", "å¯è¦–åŒ–æº–å‚™")
            complete_step("visualization", "å…¨å‡¦ç†å®Œäº†")
            # ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ­£å¼ã«å®Œäº†ã•ã›ã‚‹
            if hasattr(processing_monitor, 'force_complete'):
                processing_monitor.force_complete()
            else:
                processing_monitor.is_running = False
        
        log.info("[process_upload] Returning success response")
        return {
            'success': True,
            'scenarios': scenario_paths,
            'file_info': {
                'filename': filename,
                'size_mb': round(len(decoded) / (1024 * 1024), 2),
                'type': file_ext,
                'scenarios_count': len(scenarios)
            }
        }, scenario_options, first_scenario, {'display': 'block'}

    except zipfile.BadZipFile as e:
        log.error(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] ç ´æã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«: {e}")
        return {
            'error': 'ç ´æã—ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚\n' +
                   'ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
        }, [], None, {'display': 'none'}
    except Exception as e:
        log.error(f"[ãƒ‡ãƒ¼ã‚¿å…¥ç¨¿] å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
        return {
            'error': f'ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\n{str(e)}\n\n' +
                   'ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚„å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚'
        }, [], None, {'display': 'none'}



# === ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ ===
@app.callback(
    [Output('data-ingestion-output', 'data'),
     Output('scenario-dropdown', 'options'),
     Output('scenario-dropdown', 'value'),
     Output('scenario-selector-div', 'style')],
    [Input('upload-data', 'contents')],
    [State('upload-data', 'filename')]
)
@safe_callback
def handle_file_upload(contents, filename):
    """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    log.info(f"[handle_file_upload] Called with filename: {filename}, contents: {contents is not None}")
    
    if contents is None:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ªãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if CURRENT_SCENARIO_DIR:
            scenarios = [CURRENT_SCENARIO_DIR.name]
            log.info(f"[handle_file_upload] Using default scenario: {scenarios}")
            return (
                None,
                [{'label': s, 'value': s} for s in scenarios],
                scenarios[0] if scenarios else None,
                {'display': 'block'}
            )
        log.info("[handle_file_upload] No contents and no default scenario")
        return None, [], None, {'display': 'none'}
    
    try:
        log.info(f"[handle_file_upload] Processing upload for: {filename}")
        # process_uploadé–¢æ•°ã‚’å‘¼ã³å‡ºã—
        result = process_upload(contents, filename)
        
        log.info(f"[handle_file_upload] process_upload returned type: {type(result)}")
        
        if isinstance(result, tuple) and len(result) == 4:
            data, options, value, style = result
            log.info(f"[handle_file_upload] Success - scenarios: {[opt['value'] for opt in options] if options else 'none'}")
            return data, options, value, style
        else:
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
            log.error(f"[handle_file_upload] Unexpected result format: {result}")
            return None, [], None, {'display': 'none'}
    except Exception as e:
        log.error(f"[handle_file_upload] Error processing upload: {e}", exc_info=True)
        return None, [], None, {'display': 'none'}


@app.callback(
    Output('kpi-data-store', 'data'),
    Output('main-content', 'children'),
    Input('scenario-dropdown', 'value'),
    State('data-loaded', 'data')
)
@safe_callback
def update_main_content(selected_scenario, data_status):
    """ã‚·ãƒŠãƒªã‚ªé¸æŠã«å¿œã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ¡ã‚¤ãƒ³UIã‚’æ›´æ–°ï¼ˆæŒ‰åˆ†æ–¹å¼å¯¾å¿œï¼‰"""
    global CURRENT_SCENARIO_DIR
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒãªã„å ´åˆã§ã‚‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒŠãƒªã‚ªãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ãã‚Œã‚’ä½¿ç”¨
    if (
        not selected_scenario
        or not data_status
        or 'success' not in data_status
        or 'scenarios' not in data_status
    ):
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
        if CURRENT_SCENARIO_DIR and CURRENT_SCENARIO_DIR.exists():
            log.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨: {CURRENT_SCENARIO_DIR}")
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®KPIãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            kpi_data = {}
            # UIã‚’è¡¨ç¤ºï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸è¦ï¼‰
            return kpi_data, create_main_ui_tabs()
        else:
            raise PreventUpdate

    data_dir = Path(data_status['scenarios'].get(selected_scenario, ''))
    if not data_dir.exists():
        raise PreventUpdate

    log.info(f"Switching to scenario {selected_scenario} at {data_dir}")

    # Scenario has changed; reset caches and store new directory
    CURRENT_SCENARIO_DIR = data_dir
    clear_data_cache()

    # æŒ‰åˆ†æ–¹å¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
    excel_path = None
    for excel_file in data_dir.glob("*.xlsx"):
        if "ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿" in excel_file.name:
            excel_path = str(excel_file)
            break
    
    if excel_path:
        try:
            log.info(f"æŒ‰åˆ†æ–¹å¼ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°é–‹å§‹: {selected_scenario}")
            
            # åˆ†æå‡¦ç†é–‹å§‹ã®ç›£è¦–
            if processing_monitor:
                start_processing()
                start_step("preprocessing", "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šé–‹å§‹
            if performance_monitor:
                performance_monitor.start_timing("data_preprocessing")
            
            update_data_cache_with_proportional(DATA_CACHE, excel_path, selected_scenario)
            log.info("æŒ‰åˆ†æ–¹å¼ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°å®Œäº†")
            
            if processing_monitor:
                complete_step("preprocessing", "ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†")
                start_step("analysis", "å…±é€šãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
            
            # å…±é€šãƒ‡ãƒ¼ã‚¿ã®äº‹å‰èª­ã¿è¾¼ã¿ã‚’å®Ÿè¡Œ
            preload_common_data()
            
            if processing_monitor:
                complete_step("analysis", "ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
                # visualizationã‚¹ãƒ†ãƒƒãƒ—ã‚‚å®Œäº†ã•ã›ã¦100%ã«ã™ã‚‹
                start_step("visualization", "å¯è¦–åŒ–æº–å‚™ä¸­...")
                complete_step("visualization", "å‡¦ç†å®Œäº†")
                # å‡¦ç†ã‚’æ­£å¼ã«çµ‚äº†ã•ã›ã‚‹
                processing_monitor.is_running = False
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šçµ‚äº†
            if performance_monitor:
                duration = performance_monitor.end_timing("data_preprocessing")
                log.info(f"[ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹] ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†æ™‚é–“: {duration:.2f}ç§’")
                
        except Exception as e:
            log.warning(f"æŒ‰åˆ†æ–¹å¼ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            if processing_monitor:
                fail_step("preprocessing", f"ã‚¨ãƒ©ãƒ¼: {str(e)}")

    pre_aggr = data_get('pre_aggregated_data')
    if pre_aggr is None or (isinstance(pre_aggr, pd.DataFrame) and pre_aggr.empty):
        return {}, html.Div(f"ã‚¨ãƒ©ãƒ¼: {(data_dir / 'pre_aggregated_data.parquet').name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")  # type: ignore

    kpi_data = {}

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
    try:
        if CURRENT_SCENARIO_DIR and CURRENT_SCENARIO_DIR.exists():
            report_file = create_dashboard_analysis_report(CURRENT_SCENARIO_DIR, analysis_type="DASHBOARD")
            if report_file:
                log.info(f"[dash_app] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file.name}")
            else:
                log.warning("[dash_app] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    except Exception as e_report:
        log.error(f"[dash_app] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e_report}")

    return kpi_data, create_main_ui_tabs()


def create_main_ui_tabs():
    """ãƒ¡ã‚¤ãƒ³UIã‚¿ãƒ–ã‚’ä½œæˆï¼ˆéšå±¤åŒ–æ§‹é€ ç‰ˆï¼‰"""
    
    # éšå±¤åŒ–ã‚¿ãƒ–æ§‹é€ 
    main_tab_groups = dcc.Tabs(
        id='main-tab-groups',
        value='basic',
        children=[
            dcc.Tab(label='ğŸ“Š åŸºæœ¬åˆ†æ', value='basic', className='main-tab'),
            dcc.Tab(label='ğŸ‘¥ ã‚¹ã‚¿ãƒƒãƒ•åˆ†æ', value='staff', className='main-tab'),
            dcc.Tab(label='ğŸ“ˆ è¨ˆç”»ãƒ»äºˆæ¸¬', value='planning', className='main-tab'),
            dcc.Tab(label='ğŸ¤– é«˜åº¦ãªåˆ†æ', value='advanced', className='main-tab'),
        ],
        className='main-tabs-container'
    )
    
    # ã‚µãƒ–ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒŠï¼ˆå‹•çš„ã«æ›´æ–°ã•ã‚Œã‚‹ï¼‰
    sub_tabs_container = html.Div(
        id='sub-tabs-container',
        className='sub-tabs-wrapper'
    )
    
    # äº’æ›æ€§ã®ãŸã‚ã®éš ã—ã‚¹ãƒˆã‚¢
    selected_tab_store = dcc.Store(id='selected-tab-store', data='overview')
    
    # æ—¢å­˜ã®äº’æ›æ€§ç¶­æŒç”¨ï¼ˆéè¡¨ç¤ºï¼‰
    legacy_tabs = html.Div(
        dcc.Tabs(id='main-tabs', value='overview', children=[
            # åŸºæœ¬åˆ†æã‚°ãƒ«ãƒ¼ãƒ—
            dcc.Tab(label='ğŸ“Š æ¦‚è¦', value='overview'),
            dcc.Tab(label='ğŸ”¥ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', value='heatmap'),
            dcc.Tab(label='âš ï¸ ä¸è¶³åˆ†æ', value='shortage'),
            
            # äººäº‹ç®¡ç†ã‚°ãƒ«ãƒ¼ãƒ—  
            dcc.Tab(label='ğŸ‘¤ è·å“¡å€‹åˆ¥åˆ†æ', value='individual_analysis'),
            dcc.Tab(label='ğŸ‘¥ ãƒãƒ¼ãƒ åˆ†æ', value='team_analysis'),
            dcc.Tab(label='ğŸ˜´ ç–²åŠ´åˆ†æ', value='fatigue'),
            dcc.Tab(label='ğŸ–ï¸ ä¼‘æš‡åˆ†æ', value='leave'),
            dcc.Tab(label='âš–ï¸ å…¬å¹³æ€§', value='fairness'),
            
            # æœ€é©åŒ–ãƒ»è¨ˆç”»ã‚°ãƒ«ãƒ¼ãƒ—
            dcc.Tab(label='âš¡ æœ€é©åŒ–åˆ†æ', value='optimization'),
            dcc.Tab(label='ğŸ“ˆ éœ€è¦äºˆæ¸¬', value='forecast'),
            dcc.Tab(label='ğŸ‘· æ¡ç”¨è¨ˆç”»', value='hire_plan'),
            dcc.Tab(label='ğŸ’° ã‚³ã‚¹ãƒˆåˆ†æ', value='cost'),
            
            # é«˜åº¦åˆ†æã‚°ãƒ«ãƒ¼ãƒ—
            dcc.Tab(label='ğŸ“‹ åŸºæº–ä¹–é›¢åˆ†æ', value='gap'),
            dcc.Tab(label='ğŸ§  ä½œæˆãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆ', value='blueprint_analysis'),
            dcc.Tab(label='ğŸ” ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜', value='logic_analysis'),
        ]),
        style={'display': 'none'}  # éè¡¨ç¤º
    )

    # ã‚«ãƒ†ã‚´ãƒªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³èª¬æ˜ï¼ˆæ›´æ–°ç‰ˆï¼‰
    category_info = html.Div([
        html.H6("ğŸ“Š åˆ†æã‚«ãƒ†ã‚´ãƒªï¼ˆéšå±¤åŒ–ï¼‰:", style={'margin': '10px 0 5px 0'}),
        html.P([
            html.Span("åŸºæœ¬åˆ†æï¼ˆ3é …ç›®ï¼‰", style={'color': '#1f77b4', 'marginRight': '15px'}),
            html.Span("ã‚¹ã‚¿ãƒƒãƒ•åˆ†æï¼ˆ5é …ç›®ï¼‰", style={'color': '#ff7f0e', 'marginRight': '15px'}),
            html.Span("è¨ˆç”»ãƒ»äºˆæ¸¬ï¼ˆ4é …ç›®ï¼‰", style={'color': '#2ca02c', 'marginRight': '15px'}),
            html.Span("é«˜åº¦ãªåˆ†æï¼ˆ3é …ç›®ï¼‰", style={'color': '#d62728'})
        ], style={'fontSize': '12px', 'margin': '0 0 10px 0'})
    ])
    
    # éšå±¤åŒ–ã‚¿ãƒ–ã®ã‚¹ã‚¿ã‚¤ãƒ«
    tab_styles = html.Style('''
        /* ãƒ¡ã‚¤ãƒ³ã‚¿ãƒ–ã‚¹ã‚¿ã‚¤ãƒ« */
        .main-tabs-container {
            background-color: #f8f9fa;
            padding: 0;
            margin-bottom: 16px;
        }
        
        .main-tab {
            font-size: 16px;
            font-weight: 600;
            padding: 12px 24px;
            min-width: 150px;
        }
        
        /* ã‚µãƒ–ã‚¿ãƒ–ã‚¹ã‚¿ã‚¤ãƒ« */
        .sub-tabs-container {
            background-color: white;
            border-bottom: 1px solid #dee2e6;
            margin-bottom: 16px;
        }
        
        .sub-tab {
            font-size: 14px;
            padding: 8px 16px;
        }
        
        /* ãƒ¬ã‚¹ãƒãƒ³ã‚·ãƒ–å¯¾å¿œ */
        @media (max-width: 768px) {
            .main-tabs-container,
            .sub-tabs-container {
                overflow-x: auto;
                white-space: nowrap;
            }
        }
    ''')
    
    # å…¨ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒŠã‚’é™çš„ã«ä½œæˆï¼ˆCSSè¡¨ç¤ºåˆ¶å¾¡æ–¹å¼ï¼‰
    main_layout = html.Div([
        tab_styles,
        category_info,
        main_tab_groups,
        sub_tabs_container,
        selected_tab_store,
        legacy_tabs,
        # å„ã‚¿ãƒ–ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚³ãƒ³ãƒ†ãƒŠï¼ˆæ—¢å­˜ã®æ§‹é€ ã‚’ç¶­æŒï¼‰
        html.Div([
            html.Div(id='overview-tab-container', style={'display': 'block'}),
            html.Div(id='heatmap-tab-container', style={'display': 'none'}),
            html.Div(id='shortage-tab-container', style={'display': 'none'}),
            html.Div(id='optimization-tab-container', style={'display': 'none'}),
            html.Div(id='leave-tab-container', style={'display': 'none'}),
            html.Div(id='cost-tab-container', style={'display': 'none'}),
            html.Div(id='hire-plan-tab-container', style={'display': 'none'}),
            html.Div(id='fatigue-tab-container', style={'display': 'none'}),
            html.Div(id='forecast-tab-container', style={'display': 'none'}),
            html.Div(id='fairness-tab-container', style={'display': 'none'}),
            html.Div(id='gap-tab-container', style={'display': 'none'}),
            html.Div(id='individual-analysis-tab-container', style={'display': 'none'}),
            html.Div(id='team-analysis-tab-container', style={'display': 'none'}),
            html.Div(id='blueprint-analysis-tab-container', style={'display': 'none'}),
            html.Div(id='logic-analysis-tab-container', style={'display': 'none'}),
            html.Div(id='ai-analysis-tab-container', style={'display': 'none'}),
        ])
    ])

    return main_layout

# éšå±¤åŒ–ã‚¿ãƒ–ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
def update_sub_tabs(selected_group):
    """ãƒ—ãƒ©ã‚¤ãƒãƒªã‚¿ãƒ–ã«å¿œã˜ã¦ã‚µãƒ–ã‚¿ãƒ–ã‚’è¡¨ç¤º"""
    from dash.exceptions import PreventUpdate
    
    if not selected_group or selected_group not in ['basic', 'staff', 'planning', 'advanced']:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§åŸºæœ¬åˆ†æã‚’è¡¨ç¤º
        selected_group = 'basic'
    
    # ã‚¿ãƒ–ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©
    tab_configs = {
        'basic': [
            {'label': 'ğŸ“Š æ¦‚è¦', 'value': 'overview'},
            {'label': 'ğŸ”¥ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', 'value': 'heatmap'},
            {'label': 'âš ï¸ ä¸è¶³åˆ†æ', 'value': 'shortage'}
        ],
        'staff': [
            {'label': 'ğŸ‘¤ å€‹åˆ¥åˆ†æ', 'value': 'individual_analysis'},
            {'label': 'ğŸ‘¥ ãƒãƒ¼ãƒ åˆ†æ', 'value': 'team_analysis'},
            {'label': 'ğŸ˜´ ç–²åŠ´åˆ†æ', 'value': 'fatigue'},
            {'label': 'ğŸ–ï¸ ä¼‘æš‡åˆ†æ', 'value': 'leave'},
            {'label': 'âš–ï¸ å…¬å¹³æ€§', 'value': 'fairness'}
        ],
        'planning': [
            {'label': 'âš¡ æœ€é©åŒ–', 'value': 'optimization'},
            {'label': 'ğŸ“ˆ éœ€è¦äºˆæ¸¬', 'value': 'forecast'},
            {'label': 'ğŸ‘· æ¡ç”¨è¨ˆç”»', 'value': 'hire_plan'},
            {'label': 'ğŸ’° ã‚³ã‚¹ãƒˆåˆ†æ', 'value': 'cost'}
        ],
        'advanced': [
            {'label': 'ğŸ“‹ åŸºæº–ä¹–é›¢', 'value': 'gap'},
            {'label': 'ğŸ§  ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆ', 'value': 'blueprint_analysis'},
            {'label': 'ğŸ” ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜', 'value': 'logic_analysis'}
        ]
    }
    
    tabs = tab_configs.get(selected_group, tab_configs['basic'])
    
    return dcc.Tabs(
        id='sub-tabs',
        value=tabs[0]['value'] if tabs else 'overview',
        children=[
            dcc.Tab(label=tab['label'], value=tab['value'], className='sub-tab')
            for tab in tabs
        ],
        className='sub-tabs-container'
    )

# é¸æŠã•ã‚ŒãŸã‚¿ãƒ–ã‚’è¨˜éŒ²ï¼ˆäº’æ›æ€§ç”¨ï¼‰
@app.callback(
    Output('selected-tab-store', 'data'),
    Input('sub-tabs', 'value')
)
@safe_callback
def store_selected_tab(selected_tab):
    """é¸æŠã•ã‚ŒãŸã‚¿ãƒ–ã‚’è¨˜éŒ²ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¨ã®äº’æ›æ€§ï¼‰"""
    return selected_tab

# æ—¢å­˜ã®main-tabsã®å€¤ã‚’æ›´æ–°ï¼ˆäº’æ›æ€§ç”¨ï¼‰
@app.callback(
    Output('main-tabs', 'value'),
    Input('selected-tab-store', 'data')
)
@safe_callback  
def update_legacy_tabs(selected_tab):
    """äº’æ›æ€§ã®ãŸã‚æ—¢å­˜ã‚¿ãƒ–ã®å€¤ã‚’æ›´æ–°"""
    return selected_tab if selected_tab else 'overview'


@app.callback(
    [Output('overview-tab-container', 'style'),
     Output('heatmap-tab-container', 'style'),
     Output('shortage-tab-container', 'style'),
     Output('optimization-tab-container', 'style'),
     Output('leave-tab-container', 'style'),
     Output('cost-tab-container', 'style'),
     Output('hire-plan-tab-container', 'style'),
     Output('fatigue-tab-container', 'style'),
     Output('forecast-tab-container', 'style'),
     Output('fairness-tab-container', 'style'),
     Output('gap-tab-container', 'style'),
     Output('individual-analysis-tab-container', 'style'),
     Output('team-analysis-tab-container', 'style'),
     Output('blueprint-analysis-tab-container', 'style'),
     Output('logic-analysis-tab-container', 'style'),
     Output('ai-analysis-tab-container', 'style')],
    Input('main-tabs', 'value'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def update_tab_visibility(active_tab, selected_scenario, data_status):
    """ã‚¿ãƒ–ã®è¡¨ç¤ºåˆ¶å¾¡ï¼ˆCSS visibilityæ–¹å¼ï¼‰"""
    if not selected_scenario or not data_status:
        raise PreventUpdate
    
    # å…¨ã‚¿ãƒ–ã®ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
    all_tabs = [
        'overview', 'heatmap', 'shortage', 'optimization', 'leave',
        'cost', 'hire_plan', 'fatigue', 'forecast', 'fairness',
        'gap', 'individual_analysis', 'team_analysis', 'blueprint_analysis', 'logic_analysis', 'ai_analysis'
    ]
    
    # å„ã‚¿ãƒ–ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’è¨­å®šï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¿ãƒ–ã®ã¿è¡¨ç¤ºï¼‰
    styles = []
    for tab in all_tabs:
        if tab == active_tab:
            styles.append({'display': 'block'})
        else:
            styles.append({'display': 'none'})
    
    return styles


# å„ã‚¿ãƒ–ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆæœŸåŒ–ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
@app.callback(
    Output('overview-content', 'children'),
    [Input('overview-tab-container', 'style'),
     Input('scenario-dropdown', 'value')],
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_overview_content(style, selected_scenario, data_status):
    """æ¦‚è¦ã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_overview_tab(selected_scenario)
    except Exception as e:
        log.error(f"æ¦‚è¦ã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('heatmap-content', 'children'),
    Input('heatmap-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_heatmap_content(style, selected_scenario, data_status):
    """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_heatmap_tab()
    except Exception as e:
        log.error(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('shortage-content', 'children'),
    [Input('shortage-tab-container', 'style'),
     Input('scenario-dropdown', 'value')],
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_shortage_content(style, selected_scenario, data_status):
    """ä¸è¶³åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    log.info(f"[shortage_tab] åˆæœŸåŒ–é–‹å§‹ - scenario: {selected_scenario}, data_status: {data_status}, style: {style}")
    
    if not selected_scenario or not data_status or style.get('display') == 'none':
        log.info("[shortage_tab] PreventUpdate - æ¡ä»¶ä¸æº€è¶³")
        raise PreventUpdate
    try:
        log.info("[shortage_tab] create_shortage_tabå‘¼ã³å‡ºã—é–‹å§‹")
        result = create_shortage_tab(selected_scenario)
        log.info("[shortage_tab] create_shortage_tabå®Œäº†")
        return result
    except Exception as e:
        log.error(f"ä¸è¶³åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        log.error(f"ä¸è¶³åˆ†æã‚¿ãƒ–è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('optimization-content', 'children'),
    Input('optimization-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_optimization_content(style, selected_scenario, data_status):
    """æœ€é©åŒ–åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_optimization_tab()
    except Exception as e:
        log.error(f"æœ€é©åŒ–åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('leave-content', 'children'),
    Input('leave-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_leave_content(style, selected_scenario, data_status):
    """ä¼‘æš‡åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    log.info(f"[leave_tab] åˆæœŸåŒ–é–‹å§‹ - scenario: {selected_scenario}, data_status: {data_status}, style: {style}")
    
    if not selected_scenario or not data_status or style.get('display') == 'none':
        log.info("[leave_tab] PreventUpdate - æ¡ä»¶ä¸æº€è¶³")
        raise PreventUpdate
    try:
        log.info("[leave_tab] create_leave_analysis_tabå‘¼ã³å‡ºã—é–‹å§‹")
        result = create_leave_analysis_tab()
        log.info("[leave_tab] create_leave_analysis_tabå®Œäº†")
        return result
    except Exception as e:
        log.error(f"ä¼‘æš‡åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        import traceback
        log.error(f"ä¼‘æš‡åˆ†æã‚¿ãƒ–è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('cost-content', 'children'),
    Input('cost-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_cost_content(style, selected_scenario, data_status):
    """ã‚³ã‚¹ãƒˆåˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_cost_analysis_tab()
    except Exception as e:
        log.error(f"ã‚³ã‚¹ãƒˆåˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('hire-plan-content', 'children'),
    Input('hire-plan-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_hire_plan_content(style, selected_scenario, data_status):
    """æ¡ç”¨è¨ˆç”»ã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_hire_plan_tab()
    except Exception as e:
        log.error(f"æ¡ç”¨è¨ˆç”»ã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('fatigue-content', 'children'),
    Input('fatigue-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_fatigue_content(style, selected_scenario, data_status):
    """ç–²åŠ´åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_fatigue_tab()
    except Exception as e:
        log.error(f"ç–²åŠ´åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('forecast-content', 'children'),
    Input('forecast-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_forecast_content(style, selected_scenario, data_status):
    """éœ€è¦äºˆæ¸¬ã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_forecast_tab()
    except Exception as e:
        log.error(f"éœ€è¦äºˆæ¸¬ã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('fairness-content', 'children'),
    Input('fairness-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_fairness_content(style, selected_scenario, data_status):
    """å…¬å¹³æ€§ã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_fairness_tab()
    except Exception as e:
        log.error(f"å…¬å¹³æ€§ã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('gap-content', 'children'),
    Input('gap-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_gap_content(style, selected_scenario, data_status):
    """åŸºæº–ä¹–é›¢åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_gap_analysis_tab()
    except Exception as e:
        log.error(f"åŸºæº–ä¹–é›¢åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('team-analysis-content', 'children'),
    Input('team-analysis-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_team_analysis_content(style, selected_scenario, data_status):
    """ãƒãƒ¼ãƒ åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_team_analysis_tab()
    except Exception as e:
        log.error(f"ãƒãƒ¼ãƒ åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('blueprint-analysis-content', 'children'),
    Input('blueprint-analysis-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_blueprint_analysis_content(style, selected_scenario, data_status):
    """ä½œæˆãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_blueprint_analysis_tab()
    except Exception as e:
        log.error(f"ä½œæˆãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})

@app.callback(
    Output('logic-analysis-content', 'children'),
    Input('logic-analysis-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_logic_analysis_content(style, selected_scenario, data_status):
    """ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜ã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_creation_logic_analysis_tab()
    except Exception as e:
        log.error(f"ãƒ­ã‚¸ãƒƒã‚¯è§£æ˜ã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})


@app.callback(
    Output('individual-analysis-content', 'children', allow_duplicate=True),
    Input('individual-analysis-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
    prevent_initial_call=True
)
@safe_callback
def initialize_individual_analysis_content(style, selected_scenario, data_status):
    """è·å“¡å€‹åˆ¥åˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_individual_analysis_tab()
    except Exception as e:
        log.error(f"è·å“¡å€‹åˆ¥åˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})


@app.callback(
    Output({'type': 'heatmap-filter-employment', 'index': ALL}, 'options'),
    Output({'type': 'heatmap-filter-employment', 'index': ALL}, 'value'),
    Input({'type': 'heatmap-filter-role', 'index': ALL}, 'value'),
)
@safe_callback
def update_employment_options(selected_roles):
    """è·ç¨®é¸æŠã«å¿œã˜ã¦é›‡ç”¨å½¢æ…‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’æ›´æ–°"""
    aggregated_df = data_get('pre_aggregated_data')
    if aggregated_df is None or aggregated_df.empty:
        default_options = [{'label': 'ã™ã¹ã¦', 'value': 'all'}]
        return [default_options, default_options], ['all', 'all']

    output_options = []
    for role in selected_roles:
        if role and role != 'all':
            employments = aggregated_df[aggregated_df['role'] == role][
                'employment'
            ].unique()
            new_options = (
                [{'label': 'ã™ã¹ã¦', 'value': 'all'}]
                + [{'label': emp, 'value': emp} for emp in sorted(employments)]
            )
        else:
            all_employments = aggregated_df['employment'].unique()
            new_options = (
                [{'label': 'ã™ã¹ã¦', 'value': 'all'}]
                + [{'label': emp, 'value': emp} for emp in sorted(all_employments)]
            )
        output_options.append(new_options)

    return output_options, ['all', 'all']


@app.callback(
    Output({'type': 'graph-output-heatmap', 'index': 1}, 'children'),
    Output({'type': 'graph-output-heatmap', 'index': 2}, 'children'),
    Input({'type': 'heatmap-filter-role', 'index': 1}, 'value'),
    Input({'type': 'heatmap-filter-employment', 'index': 1}, 'value'),
    Input({'type': 'heatmap-filter-role', 'index': 2}, 'value'),
    Input({'type': 'heatmap-filter-employment', 'index': 2}, 'value'),
)
@safe_callback
def update_comparison_heatmaps(role1, emp1, role2, emp2):
    """äº‹å‰é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å‹•çš„ã«ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ç”Ÿæˆã—ã€2ã‚¨ãƒªã‚¢ã‚’æ›´æ–°ï¼ˆä¼‘æ—¥é™¤å¤–çµ±åˆç‰ˆï¼‰"""

    # ğŸ¯ è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†é›¢: ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºç”¨ã¯å®Ÿç¸¾0ã®å‹¤å‹™æ—¥ã‚‚ä¿æŒ
    aggregated_df = data_get('pre_aggregated_data', for_display=True)
    if aggregated_df is None or aggregated_df.empty:
        error_message = html.Div("ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®å…ƒãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")  # type: ignore
        return error_message, error_message

    def generate_dynamic_heatmap(selected_role, selected_emp):
        """é¸æŠã•ã‚ŒãŸæ¡ä»¶ã§äº‹å‰é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ã—ãƒ”ãƒœãƒƒãƒˆåŒ–ï¼ˆä¼‘æ—¥é™¤å¤–ç¢ºå®Ÿé©ç”¨ç‰ˆï¼‰"""

        filtered_df = aggregated_df.copy()
        
        # è¿½åŠ ã®ä¼‘æ—¥é™¤å¤–ç¢ºèªï¼šäº‹å‰é›†è¨ˆãƒ‡ãƒ¼ã‚¿ã«0ã‚¹ã‚¿ãƒƒãƒ•ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã«å‚™ãˆã¦
        # data_get()ã§æ—¢ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã¯ãšã ãŒã€å¿µã®ãŸã‚è¿½åŠ ãƒ•ã‚£ãƒ«ã‚¿ã‚’é©ç”¨
        if 'staff_count' in filtered_df.columns:
            before_count = len(filtered_df)
            filtered_df = filtered_df[filtered_df['staff_count'] > 0]
            after_count = len(filtered_df)
            if before_count != after_count:
                log.info(f"[Heatmap] è¿½åŠ ã®ä¼‘æ—¥é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨: {before_count} -> {after_count} ({before_count - after_count}ä»¶é™¤å¤–)")
        
        title_parts = []
        
        log.info(f"[Heatmap] ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨: {len(filtered_df)}ãƒ¬ã‚³ãƒ¼ãƒ‰")

        # é¸æŠã•ã‚ŒãŸæ¡ä»¶ã«åˆã‚ã›ã¦ãƒ‡ãƒ¼ã‚¿ã‚’çµã‚Šè¾¼ã‚€
        if selected_role and selected_role != 'all':
            filtered_df = filtered_df[filtered_df['role'] == selected_role]
            title_parts.append(f"è·ç¨®: {selected_role}")

        if selected_emp and selected_emp != 'all':
            filtered_df = filtered_df[filtered_df['employment'] == selected_emp]
            title_parts.append(f"é›‡ç”¨å½¢æ…‹: {selected_emp}")

        title = " AND ".join(title_parts) if title_parts else "å…¨ä½“"

        if filtered_df.empty:
            time_labels = gen_labels(DETECTED_SLOT_INFO['slot_minutes'])
            # ç©ºã®å ´åˆã‚‚å…¨æœŸé–“ã®æ—¥ä»˜ã‚’ç¢ºå®Ÿã«å–å¾—
            try:
                meta_data = data_get('heatmap_meta', {})
                if 'dates' in meta_data and meta_data['dates']:
                    all_dates = sorted(meta_data['dates'])
                else:
                    all_dates = sorted(aggregated_df['date_lbl'].unique())
            except:
                all_dates = sorted(aggregated_df['date_lbl'].unique())
            
            empty_heatmap = pd.DataFrame(index=time_labels, columns=all_dates).fillna(0)
            fig_empty = generate_heatmap_figure(empty_heatmap, f"{title} (å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ãªã—)", device_type="desktop")
            log.info(f"[Heatmap] ç©ºãƒ‡ãƒ¼ã‚¿ã§é€£ç¶šã—ãŸ{len(all_dates)}æ—¥ã®æ—¥ä»˜è»¸ã‚’è¡¨ç¤º")
            return dcc.Graph(figure=fig_empty)

        # æ—¥ä»˜é †ã«ä¸¦ã³æ›¿ãˆã¦ã‹ã‚‰ãƒ”ãƒœãƒƒãƒˆï¼ˆå®Ÿéš›ã«åƒã„ã¦ã„ã‚‹äººã®ã¿ã‚«ã‚¦ãƒ³ãƒˆï¼‰
        dynamic_heatmap_df = filtered_df.sort_values('date_lbl').pivot_table(
            index='time',
            columns='date_lbl',
            values='staff_count',
            aggfunc='sum',
            fill_value=0,
        )
        
        # ã•ã‚‰ãªã‚‹æ¤œè¨¼: 0å€¤ã®é™¤å»
        dynamic_heatmap_df = dynamic_heatmap_df.fillna(0)
        # è² ã®å€¤ãŒã‚ã‚Œã°0ã«ã™ã‚‹ï¼ˆç•°å¸¸ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        dynamic_heatmap_df = dynamic_heatmap_df.clip(lower=0)

        # ğŸ¯ é‡è¦ä¿®æ­£: å…¨ã¦ã®æ—¥ä»˜ã‚’ä¿æŒï¼ˆå®Ÿç¸¾ãŒãªã„æ—¥ã‚‚è¡¨ç¤ºï¼‰
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã®åŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å…¨æœŸé–“ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
        try:
            # å…ƒã®aggregated_dfãŒå…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŒã£ã¦ã„ã‚‹ã®ã§ã€ã“ã“ã‹ã‚‰æ—¥ä»˜ç¯„å›²ã‚’å–å¾—
            all_original_dates = sorted(aggregated_df['date_lbl'].unique())
            
            # ã•ã‚‰ã«ç¢ºå®Ÿã«ã™ã‚‹ãŸã‚ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚‚æ—¥ä»˜ã‚’å–å¾—
            meta_data = data_get('heatmap_meta', {})
            if 'dates' in meta_data and meta_data['dates']:
                meta_dates = sorted(meta_data['dates'])
                log.info(f"[Heatmap] ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ç¯„å›²å–å¾—: {len(meta_dates)}æ—¥")
                all_dates_to_use = meta_dates
            else:
                all_dates_to_use = all_original_dates
                log.info(f"[Heatmap] aggregated_dfã‹ã‚‰æ—¥ä»˜ç¯„å›²å–å¾—: {len(all_original_dates)}æ—¥")
                
        except Exception as e:
            log.warning(f"[Heatmap] æ—¥ä»˜ç¯„å›²å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            all_dates_to_use = sorted(aggregated_df['date_lbl'].unique())
        
        # å…¨æ—¥ä»˜ã§reindexï¼ˆå®Ÿç¸¾ãŒãªã„å‹¤å‹™æ—¥ã‚‚0ã¨ã—ã¦è¡¨ç¤ºï¼‰
        if all_dates_to_use:
            dynamic_heatmap_df = dynamic_heatmap_df.reindex(columns=all_dates_to_use, fill_value=0)
            log.info(f"[Heatmap] å…¨æœŸé–“ã§ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ: {len(all_dates_to_use)}æ—¥ï¼ˆé€£ç¶šã—ãŸæ—¥ä»˜è»¸ã§è¡¨ç¤ºï¼‰")
        else:
            log.warning(f"[Heatmap] '{title}': æ—¥ä»˜ç¯„å›²ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é©ç”¨
        dynamic_heatmap_df = optimize_heatmap_data(dynamic_heatmap_df, max_days=60)

        time_labels = gen_labels(DETECTED_SLOT_INFO['slot_minutes'])
        
        # æ¬¡ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆæ™‚é–“ï¼‰ã‚’å…¨ã¦ç¶²ç¾…ã™ã‚‹ã‚ˆã†ã«reindexã—ã€ä¸è¶³ã—ã¦ã„ã‚‹è¡Œã¯0ã§åŸ‹ã‚ã‚‹
        dynamic_heatmap_df = dynamic_heatmap_df.reindex(index=time_labels, fill_value=0)

        present_dates = dynamic_heatmap_df.columns.tolist()
        analysis_logger.info(
            f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— '{title}' ã®ç”Ÿæˆ: å®Ÿå‹¤å‹™æ—¥ ({len(present_dates)}ä»¶): {present_dates}"
        )

        # ğŸ¯ ä¿®æ­£: å…¨æœŸé–“ã‹ã‚‰ä¼‘æ—¥ã‚’é™¤ã„ãŸæ—¥ä»˜ã§æ•´åˆæ€§ç¢ºèª
        # å®Ÿç¸¾ãŒãªã„å‹¤å‹™æ—¥ã‚‚è¡¨ç¤ºå¯¾è±¡ã«å«ã‚ã‚‹
        if all_dates_to_use:
            expected_dates = all_dates_to_use
            missing_dates = sorted(list(set(expected_dates) - set(present_dates)))
            if missing_dates:
                analysis_logger.warning(
                    f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— '{title}' ã§æ—¥ä»˜ãŒæ¬ è½: "
                    f"æœŸå¾…æ—¥æ•°: {len(expected_dates)}ä»¶, "
                    f"æç”»å¯¾è±¡: {len(present_dates)}ä»¶, "
                    f"æ¬ è½æ—¥ä»˜: {missing_dates[:5]}..." # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
                )
            else:
                analysis_logger.info(f"ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— '{title}': å…¨{len(expected_dates)}æ—¥ã®é€£ç¶šã—ãŸæ—¥ä»˜è»¸ã§æ­£å¸¸ã«æç”»")

        fig = generate_heatmap_figure(dynamic_heatmap_df, title, device_type="desktop")
        return dcc.Graph(figure=fig)

    output1 = generate_dynamic_heatmap(role1, emp1)
    output2 = generate_dynamic_heatmap(role2, emp2)

    return output1, output2


@app.callback(
    Output('shortage-heatmap-detail-container', 'children'),
    Input('shortage-heatmap-scope', 'value')
)
@safe_callback
def update_shortage_heatmap_detail(scope):
    """ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®è©³ç´°é¸æŠã‚’æ›´æ–°"""
    if scope == 'overall':
        return None
    elif scope == 'role':
        roles = data_get('roles', [])
        return html.Div([  # type: ignore
            html.Label("è·ç¨®é¸æŠ"),  # type: ignore
            dcc.Dropdown(
                id={'type': 'shortage-detail', 'index': 'role'},
                options=[{'label': 'å…¨ä½“', 'value': 'ALL'}] + [{'label': r, 'value': r} for r in roles],
                value='ALL',
                style={'width': '200px'}
            )
        ], style={'marginBottom': '10px'})
    elif scope == 'employment':
        employments = data_get('employments', [])
        return html.Div([  # type: ignore
            html.Label("é›‡ç”¨å½¢æ…‹é¸æŠ"),  # type: ignore
            dcc.Dropdown(
                id={'type': 'shortage-detail', 'index': 'employment'},
                options=[{'label': 'å…¨ä½“', 'value': 'ALL'}] + [{'label': e, 'value': e} for e in employments],
                value='ALL',
                style={'width': '200px'}
            )
        ], style={'marginBottom': '10px'})
    return None


@app.callback(
    Output('shortage-ratio-heatmap', 'children'),
    Input('shortage-heatmap-scope', 'value'),
    Input({'type': 'shortage-detail', 'index': ALL}, 'value')
)
@safe_callback
def update_shortage_ratio_heatmap(scope, detail_values):
    """ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’æ›´æ–°"""
    # é¸æŠå†…å®¹ã‹ã‚‰ã‚­ãƒ¼ã‚’çµ„ã¿ç«‹ã¦ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    key_suffix = ''
    if scope == 'role' and detail_values and detail_values[0] != 'ALL':
        # è·ç¨®åˆ¥: ç›´æ¥è·ç¨®åã‚’ä½¿ç”¨ï¼ˆrole_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é™¤å»ï¼‰
        key_suffix = safe_filename(detail_values[0])
    elif scope == 'employment' and detail_values and detail_values[0] != 'ALL':
        # é›‡ç”¨å½¢æ…‹åˆ¥: emp_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã§ä½¿ç”¨
        key_suffix = f"emp_{safe_filename(detail_values[0])}"

    heat_key = f"heat_{key_suffix}" if key_suffix else "heat_all"
    df_heat = data_get(heat_key, pd.DataFrame())
    
    # ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…ƒã®è·ç¨®åï¼ˆsafe_filenameå¤‰æ›å‰ï¼‰ã§å†è©¦è¡Œ
    if df_heat.empty and scope == 'role' and detail_values and detail_values[0] != 'ALL':
        original_heat_key = f"heat_{detail_values[0]}"
        log.info(f"Trying original key: {original_heat_key}")
        df_heat = data_get(original_heat_key, pd.DataFrame())

    if df_heat.empty:
        # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨è¨ºæ–­æƒ…å ±ã‚’æä¾›
        available_keys = [k for k in DATA_CACHE.keys() if k.startswith('heat_')]
        debug_info = []
        debug_info.append(f"æ¢ç´¢ã‚­ãƒ¼: {heat_key}")
        debug_info.append(f"åˆ©ç”¨å¯èƒ½ãªãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚­ãƒ¼: {available_keys}")
        debug_info.append(f"é¸æŠã•ã‚ŒãŸã‚¹ã‚³ãƒ¼ãƒ—: {scope}")
        debug_info.append(f"è©³ç´°å€¤: {detail_values}")
        
        # é¡ä¼¼ã‚­ãƒ¼ã®ææ¡ˆ
        similar_keys = [k for k in available_keys if key_suffix in k] if key_suffix else []
        if similar_keys:
            debug_info.append(f"é¡ä¼¼ã‚­ãƒ¼: {similar_keys}")
        
        return html.Div([  # type: ignore
            html.P("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", style={'color': 'red', 'fontWeight': 'bold'}),
            html.P("è¨ºæ–­æƒ…å ±:", style={'fontWeight': 'bold'}),
            html.Ul([html.Li(info) for info in debug_info]),
            html.P("è§£æ±ºæ–¹æ³•:", style={'fontWeight': 'bold'}),
            html.Ul([
                html.Li("ãƒ‡ãƒ¼ã‚¿ãŒæ­£ã—ãåˆ†æã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„"),
                html.Li("è·ç¨®åã«ç‰¹æ®Šæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„"), 
                html.Li("åˆ¥ã®è·ç¨®/é›‡ç”¨å½¢æ…‹ã‚’é¸æŠã—ã¦ã¿ã¦ãã ã•ã„")
            ])
        ])

    date_cols = [c for c in df_heat.columns if pd.to_datetime(c, errors='coerce') is not pd.NaT]
    staff_df = df_heat[date_cols]
    
    log.info(f"Initial staff_df shape: {staff_df.shape}, index: {staff_df.index.name}, columns: {len(staff_df.columns)}")
    log.info(f"df_heat columns: {list(df_heat.columns)}")
    log.info(f"df_heat index: {df_heat.index.tolist()}")
    log.info(f"date_cols: {date_cols}")
    
    # â˜…â˜…â˜… çµ±ä¸€ã•ã‚ŒãŸneedå€¤è¨ˆç®—: å…¨ã¦pre_aggregated_dataãƒ™ãƒ¼ã‚¹ã§ä¸€è²«æ€§ã‚’ä¿ã¤ â˜…â˜…â˜…
    
    # 1. ã¾ãšçµ±ä¸€ã•ã‚ŒãŸåŸºæœ¬needå€¤ã‚’å–å¾—
    need_per_date_slot_df = data_get('need_per_date_slot', pd.DataFrame())
    
    if not need_per_date_slot_df.empty:
        log.info(f"çµ±ä¸€needå€¤è¨ˆç®—é–‹å§‹: {scope}, ãƒ™ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ shape={need_per_date_slot_df.shape}")
        
        # åˆ—åã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµ±ä¸€
        need_per_date_slot_df.columns = [str(col) for col in need_per_date_slot_df.columns]
        date_cols_str = [str(col) for col in date_cols]
        
        # å…±é€šã™ã‚‹æ—¥ä»˜åˆ—ã®ã¿ã‚’ä½¿ç”¨
        common_dates = [col for col in date_cols_str if col in need_per_date_slot_df.columns]
        
        if common_dates:
            # 2. å…¨ä½“ã®å ´åˆ: need_per_date_slotã‚’ãã®ã¾ã¾ä½¿ç”¨
            if scope == 'overall':
                need_df = need_per_date_slot_df[common_dates].copy()
                need_df.columns = [c for c in date_cols if str(c) in common_dates]
                log.info(f"å…¨ä½“needå€¤ä½¿ç”¨: shape={need_df.shape}")
            
            # 3. è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥ã®å ´åˆ: pre_aggregated_dataã‹ã‚‰æ¯”ä¾‹é…åˆ†
            else:
                aggregated_df = data_get('pre_aggregated_data', pd.DataFrame())
                if not aggregated_df.empty:
                    # å…¨ä½“ã®äººå“¡é…ç½®
                    total_staff_pivot = aggregated_df.pivot_table(
                        values='staff_count', 
                        index='time', 
                        columns='date_lbl',
                        aggfunc='sum',
                        fill_value=0
                    )
                    
                    # æ¡ä»¶ã«å¿œã˜ã¦ãƒ•ã‚£ãƒ«ã‚¿ã—ãŸäººå“¡é…ç½®
                    filtered_df = aggregated_df.copy()
                    if scope == 'role' and detail_values and detail_values[0] != 'ALL':
                        filtered_df = filtered_df[filtered_df['role'] == detail_values[0]]
                    elif scope == 'employment' and detail_values and detail_values[0] != 'ALL':
                        filtered_df = filtered_df[filtered_df['employment'] == detail_values[0]]
                    
                    filtered_staff_pivot = filtered_df.pivot_table(
                        values='staff_count',
                        index='time',
                        columns='date_lbl', 
                        aggfunc='sum',
                        fill_value=0
                    )
                    
                    # ä¸¡æ–¹ã®ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒåŒã˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒã¤ã‚ˆã†ã«reindex
                    # total_staff_pivotã¨åŒã˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«æƒãˆã‚‹
                    filtered_staff_pivot = filtered_staff_pivot.reindex(
                        index=total_staff_pivot.index,
                        columns=total_staff_pivot.columns,
                        fill_value=0
                    )
                    
                    # æ¯”ä¾‹é…åˆ†ã§needå€¤ã‚’è¨ˆç®—
                    with np.errstate(divide='ignore', invalid='ignore'):
                        ratio = np.divide(filtered_staff_pivot.values, total_staff_pivot.values,
                                        out=np.zeros_like(filtered_staff_pivot.values, dtype=np.float64),
                                        where=(total_staff_pivot.values != 0))
                    
                    # æ¯”ä¾‹é…åˆ†ã‚’é©ç”¨ï¼ˆæ¬¡å…ƒã®å®‰å…¨ãªèª¿æ•´ï¼‰
                    need_values = need_per_date_slot_df[common_dates].values
                    
                    # æ¬¡å…ƒã®å®‰å…¨ãªèª¿æ•´
                    min_rows = min(need_values.shape[0], ratio.shape[0])
                    min_cols = min(need_values.shape[1], ratio.shape[1])
                    
                    # é…åˆ—ã‚’å®‰å…¨ãªã‚µã‚¤ã‚ºã«åˆ‡ã‚Šå–ã‚Š
                    need_values_safe = need_values[:min_rows, :min_cols]
                    ratio_safe = ratio[:min_rows, :min_cols]
                    
                    proportional_need = need_values_safe * ratio_safe
                    
                    log.info(f"Dimension adjustment: need_shape={need_values.shape}, ratio_shape={ratio.shape}, final_shape={proportional_need.shape}")
                    
                    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚‚å®‰å…¨ãªã‚µã‚¤ã‚ºã«èª¿æ•´
                    safe_index = need_per_date_slot_df.index[:min_rows]
                    safe_columns = [c for c in date_cols if str(c) in common_dates][:min_cols]
                    
                    log.info(f"[INDEX_DEBUG] Original need_per_date_slot index length: {len(need_per_date_slot_df.index)}")
                    log.info(f"[INDEX_DEBUG] Original need_per_date_slot index: {need_per_date_slot_df.index.tolist()}")
                    log.info(f"[INDEX_DEBUG] Safe index length: {len(safe_index)}")
                    log.info(f"[INDEX_DEBUG] Safe index: {safe_index.tolist()}")
                    
                    need_df = pd.DataFrame(proportional_need, 
                                         index=safe_index,
                                         columns=safe_columns)
                    
                    log.info(f"æ¯”ä¾‹é…åˆ†needå€¤è¨ˆç®—å®Œäº†: {scope}, shape={need_df.shape}, ratio_mean={np.nanmean(ratio):.3f}")
                else:
                    log.warning(f"pre_aggregated_data not available, falling back to heat need values")
                    need_df = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(common_dates), axis=1),
                                         index=df_heat.index, columns=[c for c in date_cols if str(c) in common_dates])
            
            # 4. ä¸è¶³ã—ã¦ã„ã‚‹æ—¥ä»˜ãŒã‚ã‚Œã°å¹³å‡å€¤ã§è£œå®Œ
            missing_dates = [c for c in date_cols if str(c) not in common_dates]
            if missing_dates:
                log.warning(f"Missing datesè£œå®Œ: {len(missing_dates)}ä»¶")
                fallback_need = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(missing_dates), axis=1),
                                           index=df_heat.index, columns=missing_dates)
                need_df = pd.concat([need_df, fallback_need], axis=1)
                need_df = need_df.reindex(columns=date_cols)  # å…ƒã®é †åºã‚’ä¿æŒ
        else:
            log.warning("No matching dates found in need_per_date_slot.parquet, falling back to average need")
            need_df = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(date_cols), axis=1),
                                   index=df_heat.index, columns=date_cols)
    else:
        # need_per_date_slot.parquetãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å¾“æ¥ã®å¹³å‡å€¤ã‚’ä½¿ç”¨
        log.info("need_per_date_slot.parquet not available, using average need values")
        need_df = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(date_cols), axis=1),
                               index=df_heat.index, columns=date_cols)
    upper_df = pd.DataFrame(np.repeat(df_heat['upper'].values[:, np.newaxis], len(date_cols), axis=1),
                            index=df_heat.index, columns=date_cols)

    # æ­£ç¢ºãªä¸è¶³è¨ˆç®—ã®å®Ÿè£…ï¼ˆæ¬¡å…ƒå®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
    # needå€¤ãŒæ­£ç¢ºã«è¨ˆç®—ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€ã‚·ãƒ³ãƒ—ãƒ«ã§æ­£ç¢ºãªä¸è¶³è¨ˆç®—ã‚’è¡Œã†
    
    # æ¬¡å…ƒã®æœ€çµ‚å®‰å…¨ãƒã‚§ãƒƒã‚¯
    if need_df.shape != staff_df.shape:
        log.warning(f"Dimension mismatch: need_df {need_df.shape} vs staff_df {staff_df.shape}")
        
        # å…±é€šã™ã‚‹è¡Œãƒ»åˆ—ã®ã¿ã‚’ä½¿ç”¨
        common_index = need_df.index.intersection(staff_df.index)
        common_columns = need_df.columns.intersection(staff_df.columns)
        
        need_df = need_df.loc[common_index, common_columns]
        staff_df = staff_df.loc[common_index, common_columns]
        
        log.info(f"Adjusted to common dimensions: {need_df.shape}")
    
    # æ­£ã—ã„æ™‚é–“è»¸ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
    time_labels = gen_labels(DETECTED_SLOT_INFO['slot_minutes'])
    
    # æ™‚é–“è»¸ã‚’reindexã—ã¦24æ™‚é–“åˆ†ç¢ºä¿
    need_df = need_df.reindex(index=time_labels, fill_value=0)
    staff_df = staff_df.reindex(index=time_labels, fill_value=0)
    upper_df = upper_df.reindex(index=time_labels, fill_value=0)
    
    lack_count_df = (need_df - staff_df).clip(lower=0).fillna(0)
    
    # å®Ÿéš›ã®needå€¤ãŒéå¸¸ã«å°ã•ã„å ´åˆï¼ˆ0.01æœªæº€ï¼‰ã®ã¿0ã¨ã™ã‚‹ï¼ˆè¨ˆç®—èª¤å·®å¯¾ç­–ï¼‰
    mask_tiny_need = need_df < 0.01
    lack_count_df[mask_tiny_need] = 0.0
    
    log.info(f"[LACK_CALCULATION] {heat_key}: Total lack={lack_count_df.sum().sum():.2f}, Max need={need_df.max().max():.2f}, Max staff={staff_df.max().max():.2f}")
    
    excess_count_df = (staff_df - upper_df).clip(lower=0).fillna(0)
    ratio_df = calc_ratio_from_heatmap_integrated(df_heat)
    
    # ä¸è¶³æ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä¿®æ­£ï¼ˆæ™‚é–“è»¸ãƒ‡ãƒãƒƒã‚°æƒ…å ±ä»˜ãï¼‰
    lack_count_df_renamed = lack_count_df.copy()
    lack_count_df_renamed.columns = [date_with_weekday(c) for c in lack_count_df_renamed.columns]
    # è¿½åŠ ã®å®‰å…¨å¯¾ç­–: NaNå€¤ã‚’å†åº¦0ã§åŸ‹ã‚ã‚‹ï¼ˆæ—¥æ›œæ—¥ã®æ¬ è½å¯¾ç­–ï¼‰
    lack_count_df_renamed = lack_count_df_renamed.fillna(0)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶ã¨æ™‚é–“è»¸ã®è©³ç´°
    log.info(f"[HEATMAP_DEBUG] lack_count_df shape: {lack_count_df_renamed.shape}")
    log.info(f"[HEATMAP_DEBUG] lack_count_df index (æ™‚é–“è»¸): {lack_count_df_renamed.index.tolist()}")
    log.info(f"[HEATMAP_DEBUG] lack_count_df first few rows:")
    log.info(f"[HEATMAP_DEBUG] {lack_count_df_renamed.head(10)}")
    
    fig_lack = px.imshow(
        lack_count_df_renamed,
        aspect='auto',
        color_continuous_scale='Oranges',
        title='ä¸è¶³äººæ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
        labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'äººæ•°'},
        text_auto=True  # 0å€¤ã‚‚è¡¨ç¤º
    )
    
    # ä¸è¶³æ•°ã®è¡¨ç¤ºã‚¹ã‚¿ã‚¤ãƒ«ã‚’èª¿æ•´
    fig_lack.update_traces(
        texttemplate='%{text}',
        textfont={"size": 10}
    )
    fig_lack.update_xaxes(tickvals=list(range(len(lack_count_df.columns))))
    
    # Yè»¸ï¼ˆæ™‚é–“è»¸ï¼‰ã®æ˜ç¤ºçš„ãªè¨­å®šã‚’è¿½åŠ  - 24æ™‚é–“è¡¨ç¤ºå¯¾å¿œ
    time_labels = gen_labels(DETECTED_SLOT_INFO['slot_minutes'])
    fig_lack.update_yaxes(
        tickvals=list(range(len(time_labels))),
        ticktext=time_labels,
        tickmode='array',
        title="æ™‚é–“"
    )
    
    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®æ”¹å–„ - é«˜ã•ã‚’å¢—ã‚„ã—ã¦è¦‹ã‚„ã™ã
    fig_lack.update_layout(
        height=600,  # é«˜ã•ã‚’å¢—ã‚„ã™
        margin=dict(l=60, r=60, t=80, b=60),  # ãƒãƒ¼ã‚¸ãƒ³èª¿æ•´
        font=dict(size=12),  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºèª¿æ•´
        title_x=0.5  # ã‚¿ã‚¤ãƒˆãƒ«ä¸­å¤®é…ç½®
    )

    fig_excess = go.Figure()
    if not excess_count_df.empty:
        excess_count_df_renamed = excess_count_df.copy()
        excess_count_df_renamed.columns = [date_with_weekday(c) for c in excess_count_df_renamed.columns]
        fig_excess = px.imshow(
            excess_count_df_renamed,
            aspect='auto',
            color_continuous_scale='Blues',
            title='éå‰°äººæ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
            labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'äººæ•°'},
        )
        fig_excess.update_xaxes(tickvals=list(range(len(excess_count_df.columns))))
        # Yè»¸è¨­å®šã‚’çµ±ä¸€ - 24æ™‚é–“è¡¨ç¤ºå¯¾å¿œ
        fig_excess.update_yaxes(
            tickvals=list(range(len(time_labels))),
            ticktext=time_labels,
            tickmode='array',
            title="æ™‚é–“"
        )
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®æ”¹å–„ - é«˜ã•ã‚’å¢—ã‚„ã—ã¦è¦‹ã‚„ã™ã
        fig_excess.update_layout(
            height=600,  # é«˜ã•ã‚’å¢—ã‚„ã™
            margin=dict(l=60, r=60, t=80, b=60),  # ãƒãƒ¼ã‚¸ãƒ³èª¿æ•´
            font=dict(size=12),  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºèª¿æ•´
            title_x=0.5  # ã‚¿ã‚¤ãƒˆãƒ«ä¸­å¤®é…ç½®
        )

    fig_ratio = go.Figure()
    if not ratio_df.empty:
        ratio_df_renamed = ratio_df.copy()
        ratio_df_renamed.columns = [date_with_weekday(c) for c in ratio_df_renamed.columns]
        fig_ratio = px.imshow(
            ratio_df_renamed,
            aspect='auto',
            color_continuous_scale='RdBu_r',
            title='ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
            labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'ä¸è¶³ç‡'},
        )
        fig_ratio.update_xaxes(tickvals=list(range(len(ratio_df.columns))))
        # Yè»¸è¨­å®šã‚’çµ±ä¸€ - 24æ™‚é–“è¡¨ç¤ºå¯¾å¿œ
        fig_ratio.update_yaxes(
            tickvals=list(range(len(time_labels))),
            ticktext=time_labels,
            tickmode='array',
            title="æ™‚é–“"
        )
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®æ”¹å–„ - é«˜ã•ã‚’å¢—ã‚„ã—ã¦è¦‹ã‚„ã™ã
        fig_ratio.update_layout(
            height=600,  # é«˜ã•ã‚’å¢—ã‚„ã™
            margin=dict(l=60, r=60, t=80, b=60),  # ãƒãƒ¼ã‚¸ãƒ³èª¿æ•´
            font=dict(size=12),  # ãƒ•ã‚©ãƒ³ãƒˆã‚µã‚¤ã‚ºèª¿æ•´
            title_x=0.5  # ã‚¿ã‚¤ãƒˆãƒ«ä¸­å¤®é…ç½®
        )

    return html.Div([  # type: ignore
        html.H4('ä¸è¶³äººæ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—'),
        dcc.Graph(figure=fig_lack),
        html.H4('éå‰°äººæ•°ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', style={'marginTop': '30px'}),
        dcc.Graph(figure=fig_excess),
        html.H4('ä¸è¶³ç‡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', style={'marginTop': '30px'}),
        dcc.Graph(figure=fig_ratio),
    ])


@app.callback(
    Output('opt-detail-container', 'children'),
    Input('opt-scope', 'value')
)
@safe_callback
def update_opt_detail(scope):
    """æœ€é©åŒ–åˆ†æã®è©³ç´°é¸æŠã‚’æ›´æ–°"""
    if scope == 'overall':
        return None
    elif scope == 'role':
        roles = data_get('roles', [])
        return html.Div([  # type: ignore
            html.Label("è·ç¨®é¸æŠ"),  # type: ignore
            dcc.Dropdown(
                id={'type': 'opt-detail', 'index': 'role'},
                options=[{'label': 'å…¨ä½“', 'value': 'ALL'}] + [{'label': r, 'value': r} for r in roles],
                value='ALL',
                style={'width': '300px', 'marginBottom': '20px'}
            )
        ])
    elif scope == 'employment':
        employments = data_get('employments', [])
        return html.Div([  # type: ignore
            html.Label("é›‡ç”¨å½¢æ…‹é¸æŠ"),  # type: ignore
            dcc.Dropdown(
                id={'type': 'opt-detail', 'index': 'employment'},
                options=[{'label': 'å…¨ä½“', 'value': 'ALL'}] + [{'label': e, 'value': e} for e in employments],
                value='ALL',
                style={'width': '300px', 'marginBottom': '20px'}
            )
        ])
    return None

@app.callback(
    Output('optimization-content', 'children', allow_duplicate=True),
    Input('opt-scope', 'value'),
    Input({'type': 'opt-detail', 'index': ALL}, 'value'),
    prevent_initial_call=True
)
@safe_callback
def update_optimization_content(scope, detail_values):
    """æœ€é©åŒ–åˆ†æã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ›´æ–°"""
    # é¸æŠå†…å®¹ã‹ã‚‰ã‚­ãƒ¼ã‚’çµ„ã¿ç«‹ã¦ã¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’å–å¾—
    key_suffix = ''
    if scope == 'role' and detail_values and detail_values[0] != 'ALL':
        # è·ç¨®åˆ¥: ç›´æ¥è·ç¨®åã‚’ä½¿ç”¨ï¼ˆrole_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹é™¤å»ï¼‰
        key_suffix = safe_filename(detail_values[0])
    elif scope == 'employment' and detail_values and detail_values[0] != 'ALL':
        # é›‡ç”¨å½¢æ…‹åˆ¥: emp_ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ä»˜ãã§ä½¿ç”¨
        key_suffix = f"emp_{safe_filename(detail_values[0])}"

    heat_key = f"heat_{key_suffix}" if key_suffix else "heat_all"
    df_heat = data_get(heat_key, pd.DataFrame())
    
    # ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€å…ƒã®è·ç¨®åï¼ˆsafe_filenameå¤‰æ›å‰ï¼‰ã§å†è©¦è¡Œ
    if df_heat.empty and scope == 'role' and detail_values and detail_values[0] != 'ALL':
        original_heat_key = f"heat_{detail_values[0]}"
        log.info(f"Trying original key for optimization: {original_heat_key}")
        df_heat = data_get(original_heat_key, pd.DataFrame())

    if df_heat.empty:
        return html.Div("é¸æŠã•ã‚ŒãŸæ¡ä»¶ã®æœ€é©åŒ–åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    date_cols = [c for c in df_heat.columns if pd.to_datetime(c, errors='coerce') is not pd.NaT]
    staff_df = df_heat[date_cols]
    
    # â˜…â˜…â˜… é‡è¦ãªä¿®æ­£: è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥ã®å ´åˆã¯è©²å½“ã®needå€¤ã®ã¿ã‚’ä½¿ç”¨ â˜…â˜…â˜…
    if scope == 'overall':
        # å…¨ä½“ã®å ´åˆã®ã¿need_per_date_slot.parquetã‚’ä½¿ç”¨
        need_per_date_slot_df = data_get('need_per_date_slot', pd.DataFrame())
        
        if not need_per_date_slot_df.empty:
            # need_per_date_slot.parquetãŒå­˜åœ¨ã™ã‚‹å ´åˆã€å®Ÿéš›ã®æ—¥ä»˜åˆ¥needå€¤ã‚’ä½¿ç”¨
            log.info(f"Using need_per_date_slot.parquet for optimization analysis: {need_per_date_slot_df.shape}")
            
            # åˆ—åã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµ±ä¸€
            need_per_date_slot_df.columns = [str(col) for col in need_per_date_slot_df.columns]
            date_cols_str = [str(col) for col in date_cols]
            
            # å…±é€šã™ã‚‹æ—¥ä»˜åˆ—ã®ã¿ã‚’ä½¿ç”¨
            common_dates = [col for col in date_cols_str if col in need_per_date_slot_df.columns]
            
            if common_dates:
                # å®Ÿéš›ã®æ—¥ä»˜åˆ¥needå€¤ã‚’ä½¿ç”¨
                need_df = need_per_date_slot_df[common_dates].copy()
                need_df.columns = [c for c in date_cols if str(c) in common_dates]
                
                # ä¸è¶³ã—ã¦ã„ã‚‹æ—¥ä»˜ãŒã‚ã‚Œã°å¹³å‡å€¤ã§è£œå®Œ
                missing_dates = [c for c in date_cols if str(c) not in common_dates]
                if missing_dates:
                    log.warning(f"Some dates missing in need_per_date_slot.parquet for optimization, using fallback for: {missing_dates}")
                    fallback_need = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(missing_dates), axis=1),
                                               index=df_heat.index, columns=missing_dates)
                    need_df = pd.concat([need_df, fallback_need], axis=1)
                    need_df = need_df.reindex(columns=date_cols)
            else:
                log.warning("No matching dates found in need_per_date_slot.parquet for optimization, falling back to average need")
                need_df = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(date_cols), axis=1),
                                       index=df_heat.index, columns=date_cols)
        else:
            # need_per_date_slot.parquetãŒå­˜åœ¨ã—ãªã„å ´åˆã¯å¾“æ¥ã®å¹³å‡å€¤ã‚’ä½¿ç”¨
            log.info("need_per_date_slot.parquet not available for optimization, using average need values")
            need_df = pd.DataFrame(np.repeat(df_heat['need'].values[:, np.newaxis], len(date_cols), axis=1),
                                   index=df_heat.index, columns=date_cols)
    else:
        # è·ç¨®åˆ¥ãƒ»é›‡ç”¨å½¢æ…‹åˆ¥ã®å ´åˆã¯ã€å…±é€šé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦å‹•çš„needå€¤ã‚’è¨ˆç®—
        need_df = calculate_role_dynamic_need(df_heat, date_cols, heat_key)
    upper_df = pd.DataFrame(np.repeat(df_heat['upper'].values[:, np.newaxis], len(date_cols), axis=1),
                            index=df_heat.index, columns=date_cols)

    # ä¸è¶³ç‡ãƒ»éå‰°ç‡ã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    lack_ratio = ((need_df - staff_df) / need_df.replace(0, np.nan)).clip(lower=0).fillna(0)
    excess_ratio = ((staff_df - upper_df) / upper_df.replace(0, np.nan)).clip(lower=0).fillna(0)

    df_surplus = (staff_df - need_df).clip(lower=0).fillna(0)
    df_margin = (upper_df - staff_df).clip(lower=0).fillna(0)
    df_score = 1 - (0.6 * lack_ratio + 0.4 * excess_ratio).clip(0, 1)

    if not (_valid_df(df_surplus) and _valid_df(df_margin) and _valid_df(df_score)):
        return html.Div("æœ€é©åŒ–åˆ†æãƒ‡ãƒ¼ã‚¿ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    surplus_df_renamed = df_surplus.copy()
    surplus_df_renamed.columns = [date_with_weekday(c) for c in surplus_df_renamed.columns]

    margin_df_renamed = df_margin.copy()
    margin_df_renamed.columns = [date_with_weekday(c) for c in margin_df_renamed.columns]

    score_df_renamed = df_score.copy()
    score_df_renamed.columns = [date_with_weekday(c) for c in score_df_renamed.columns]

    content = [
        html.Div([
            html.H4("1. å¿…è¦äººæ•°ã«å¯¾ã™ã‚‹ä½™å‰° (Surplus vs Need)"),
            html.P("å„æ™‚é–“å¸¯ã§å¿…è¦äººæ•°ï¼ˆneedï¼‰ã«å¯¾ã—ã¦ä½•äººå¤šãã‚¹ã‚¿ãƒƒãƒ•ãŒã„ãŸã‹ã‚’ç¤ºã—ã¾ã™ã€‚"),
            dcc.Graph(
                figure=px.imshow(
                    surplus_df_renamed,
                    aspect='auto',
                    color_continuous_scale='Blues',
                    title='å¿…è¦äººæ•°ã«å¯¾ã™ã‚‹ä½™å‰°äººå“¡ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
                    labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'ä½™å‰°äººæ•°'},
                ).update_xaxes(tickvals=list(range(len(df_surplus.columns))))
            ),
        ]),
        html.Div([
            html.H4("2. ä¸Šé™ã«å¯¾ã™ã‚‹ä½™ç™½ (Margin to Upper)", style={'marginTop': '30px'}),
            html.P("å„æ™‚é–“å¸¯ã§é…ç½®äººæ•°ã®ä¸Šé™ï¼ˆupperï¼‰ã¾ã§ã‚ã¨ä½•äººã®ä½™è£•ãŒã‚ã£ãŸã‹ã‚’ç¤ºã—ã¾ã™ã€‚"),
            dcc.Graph(
                figure=px.imshow(
                    margin_df_renamed,
                    aspect='auto',
                    color_continuous_scale='Greens',
                    title='ä¸Šé™äººæ•°ã¾ã§ã®ä½™ç™½ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
                    labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'ä½™ç™½äººæ•°'},
                ).update_xaxes(tickvals=list(range(len(df_margin.columns))))
            ),
        ]),
        html.Div([
            html.H4("3. äººå“¡é…ç½® æœ€é©åŒ–ã‚¹ã‚³ã‚¢", style={'marginTop': '30px'}),
            html.P("äººå“¡é…ç½®ã®åŠ¹ç‡æ€§ã‚’0ã‹ã‚‰1ã®ã‚¹ã‚³ã‚¢ã§ç¤ºã—ã¾ã™ï¼ˆ1ãŒæœ€ã‚‚è‰¯ã„ï¼‰ã€‚"),
            dcc.Graph(
                figure=px.imshow(
                    score_df_renamed,
                    aspect='auto',
                    color_continuous_scale='RdYlGn',
                    zmin=0,
                    zmax=1,
                    title='æœ€é©åŒ–ã‚¹ã‚³ã‚¢ ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—',
                    labels={'x': 'æ—¥ä»˜', 'y': 'æ™‚é–“', 'color': 'ã‚¹ã‚³ã‚¢'},
                ).update_xaxes(tickvals=list(range(len(df_score.columns))))
            ),
        ]),
    ]

    return html.Div(content)


@app.callback(
    Output('overview-insights', 'children'),
    Input('kpi-data-store', 'data'),
)
@safe_callback
def update_overview_insights(kpi_data):
    if not kpi_data:
        return ""

    total_lack_h = kpi_data.get('total_lack_h', 0)

    if total_lack_h > 0:
        most_lacking_role = kpi_data.get('most_lacking_role_name', 'N/A')
        most_lacking_hours = kpi_data.get('most_lacking_role_hours', 0)
        insight_text = f"""
        #### ğŸ“ˆ åˆ†æãƒã‚¤ãƒ©ã‚¤ãƒˆ
        - **ç·ä¸è¶³æ™‚é–“:** {total_lack_h:.1f} æ™‚é–“
        - **æœ€é‡è¦èª²é¡Œ:** **{most_lacking_role}** ã®ä¸è¶³ãŒ **{most_lacking_hours:.1f}æ™‚é–“** ã¨æœ€ã‚‚æ·±åˆ»ã§ã™ã€‚ã“ã®è·ç¨®ã®æ¡ç”¨ã¾ãŸã¯é…ç½®è»¢æ›ãŒæ€¥å‹™ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚
        """
        return dcc.Markdown(insight_text)
    return html.P(
        "ğŸ‘ äººå“¡ä¸è¶³ã¯ç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“ã€‚ç´ æ™´ã‚‰ã—ã„å‹¤å‹™ä½“åˆ¶ã§ã™ï¼",
        style={'fontWeight': 'bold'},  # type: ignore
    )


@app.callback(
    Output('shortage-insights', 'children'),
    Input('kpi-data-store', 'data'),
)
@safe_callback
def update_shortage_insights(kpi_data):
    explanation = """
    #### ä¸è¶³åˆ†æã®è©•ä¾¡æ–¹æ³•
    - **ä¸è¶³ (Shortage):** `ä¸è¶³äººæ•° = å¿…è¦äººæ•° (Need) - å®Ÿç¸¾äººæ•°` ã§è¨ˆç®—ã•ã‚Œã¾ã™ã€‚å€¤ãŒãƒ—ãƒ©ã‚¹ã®å ´åˆã€ãã®æ™‚é–“å¸¯ã¯äººå“¡ãŒä¸è¶³ã—ã¦ã„ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚
    - **éå‰° (Excess):** `éå‰°äººæ•° = å®Ÿç¸¾äººæ•° - ä¸Šé™äººæ•° (Upper)` ã§è¨ˆç®—ã•ã‚Œã¾ã™ã€‚å€¤ãŒãƒ—ãƒ©ã‚¹ã®å ´åˆã€éå‰°ãªäººå“¡ãŒé…ç½®ã•ã‚Œã¦ã„ãŸã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚

    *ã€Œå¿…è¦äººæ•°ã€ã¨ã€Œä¸Šé™äººæ•°ã€ã¯ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€Œåˆ†æåŸºæº–è¨­å®šã€ã§æŒ‡å®šã—ãŸæ–¹æ³•ï¼ˆéå»å®Ÿç¸¾ã®çµ±è¨ˆã€ã¾ãŸã¯äººå“¡é…ç½®åŸºæº–ï¼‰ã«åŸºã¥ã„ã¦ç®—å‡ºã•ã‚Œã¾ã™ã€‚*
    """
    return dcc.Markdown(explanation)


@app.callback(
    Output('hire-plan-insights', 'children'),
    Input('kpi-data-store', 'data'),
)
@safe_callback
def update_hire_plan_insights(kpi_data):
    if not kpi_data:
        return ""
    total_lack_h = kpi_data.get('total_lack_h', 0)
    if total_lack_h == 0:
        return html.P("è¿½åŠ æ¡ç”¨ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
    role = kpi_data.get('most_lacking_role_name', 'N/A')
    return dcc.Markdown(
        f"æœ€ã‚‚ä¸è¶³ã—ã¦ã„ã‚‹ **{role}** ã®è£œå……ã‚’å„ªå…ˆçš„ã«æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
    )


@app.callback(
    Output('optimization-insights', 'children'),
    Input('kpi-data-store', 'data'),
)
@safe_callback
def update_optimization_insights(kpi_data):
    explanation = """
    #### æœ€é©åŒ–åˆ†æã®è©•ä¾¡æ–¹æ³•
    äººå“¡é…ç½®ã®åŠ¹ç‡æ€§ã¯ã€ä»¥ä¸‹ã®2ã¤ã®è¦³ç‚¹ã‹ã‚‰ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è¨ˆç®—ã—ã€æœ€çµ‚çš„ãªã‚¹ã‚³ã‚¢ã‚’ç®—å‡ºã—ã¾ã™ã€‚
    - **ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£ (é‡ã¿: 60%):** `(å¿…è¦äººæ•° - å®Ÿç¸¾äººæ•°) / å¿…è¦äººæ•°`
    - **éå‰°ãƒšãƒŠãƒ«ãƒ†ã‚£ (é‡ã¿: 40%):** `(å®Ÿç¸¾äººæ•° - ä¸Šé™äººæ•°) / ä¸Šé™äººæ•°`

    **æœ€é©åŒ–ã‚¹ã‚³ã‚¢ = 1 - (ä¸è¶³ãƒšãƒŠãƒ«ãƒ†ã‚£ Ã— 0.6 + éå‰°ãƒšãƒŠãƒ«ãƒ†ã‚£ Ã— 0.4)**

    *ã‚¹ã‚³ã‚¢ãŒ1ã«è¿‘ã„ã»ã©ã€ä¸è¶³ã‚‚éå‰°ã‚‚ãªãã€åŠ¹ç‡çš„ãªäººå“¡é…ç½®ãŒã§ãã¦ã„ã‚‹çŠ¶æ…‹ã‚’ç¤ºã—ã¾ã™ã€‚*
    """
    return dcc.Markdown(explanation)


@app.callback(
    Output('leave-insights', 'children'),
    Input('kpi-data-store', 'data'),
)
@safe_callback
def update_leave_insights(kpi_data):
    explanation = """
    #### ä¼‘æš‡åˆ†æã®è©•ä¾¡æ–¹æ³•
    - **ä¼‘æš‡å–å¾—è€…æ•°:** `holiday_type`ãŒä¼‘æš‡é–¢é€£ï¼ˆå¸Œæœ›ä¼‘ã€æœ‰çµ¦ãªã©ï¼‰ã«è¨­å®šã•ã‚Œã€ã‹ã¤å‹¤å‹™æ™‚é–“ãŒãªã„ï¼ˆ`parsed_slots_count = 0`ï¼‰å ´åˆã«ã€Œ1æ—¥ã€ã¨ã—ã¦ã‚«ã‚¦ãƒ³ãƒˆã•ã‚Œã¾ã™ã€‚
    - **é›†ä¸­æ—¥:** ã€Œå¸Œæœ›ä¼‘ã€ã®å–å¾—è€…æ•°ãŒã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ãŸé–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3äººï¼‰ä»¥ä¸Šã«ãªã£ãŸæ—¥ã‚’ã€Œé›†ä¸­æ—¥ã€ã¨ã—ã¦ãƒã‚¤ãƒ©ã‚¤ãƒˆã—ã¾ã™ã€‚
    """
    return dcc.Markdown(explanation)


@app.callback(
    Output('cost-insights', 'children'),
    Input('kpi-data-store', 'data'),
)
@safe_callback
def update_cost_insights(kpi_data):
    explanation = """
    #### ã‚³ã‚¹ãƒˆåˆ†æã®è©•ä¾¡æ–¹æ³•
    æ—¥ã€…ã®äººä»¶è²»ã¯ã€å„ã‚¹ã‚¿ãƒƒãƒ•ã®å‹¤å‹™æ™‚é–“ï¼ˆã‚¹ãƒ­ãƒƒãƒˆæ•° Ã— ã‚¹ãƒ­ãƒƒãƒˆé•·ï¼‰ã«ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§è¨­å®šã—ãŸå˜ä¾¡åŸºæº–ï¼ˆè·ç¨®åˆ¥ã€é›‡ç”¨å½¢æ…‹åˆ¥ãªã©ï¼‰ã®æ™‚çµ¦ã‚’ä¹—ã˜ã¦ç®—å‡ºã•ã‚Œã¾ã™ã€‚
    """
    return dcc.Markdown(explanation)


@app.callback(
    Output('wage-input-container', 'children'),
    Input('cost-by-radio', 'value')
)
@safe_callback
def update_wage_inputs(by_key):
    """å˜ä¾¡å…¥åŠ›æ¬„ã‚’ç”Ÿæˆ"""
    long_df = data_get('long_df')
    if long_df is None or long_df.empty or by_key not in long_df.columns:
        return html.P("å˜ä¾¡è¨­å®šã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    unique_keys: list[str] = sorted(long_df[by_key].dropna().unique())
    inputs = []
    for key in unique_keys:
        inputs.append(html.Div([
            html.Label(f'æ™‚çµ¦: {key}'),
            dcc.Input(
                id={'type': 'wage-input', 'index': key},
                value=WAGE_RATES["regular_staff"],
                type='number',
                debounce=True,
            )
        ], style={'padding': '5px', 'display': 'inline-block'}))
    return inputs


@app.callback(
    Output('cost-analysis-content', 'children'),
    Input('cost-by-radio', 'value'),
    Input({'type': 'wage-input', 'index': ALL}, 'value'),
    State({'type': 'wage-input', 'index': ALL}, 'id'),
)
@safe_callback
def update_cost_analysis_content(by_key, all_wages, all_wage_ids):
    """å˜ä¾¡å¤‰æ›´ã«å¿œã˜ã¦ã‚³ã‚¹ãƒˆåˆ†æã‚¿ãƒ–ã®å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‹•çš„ã«æ›´æ–°ã™ã‚‹"""
    long_df = data_get('long_df')
    if long_df is None or long_df.empty or not all_wages:
        raise PreventUpdate

    wages = {
        wage_id['index']: (wage_val or 0) for wage_id, wage_val in zip(all_wage_ids, all_wages)
    }

    df_cost = calculate_daily_cost(long_df, wages, by=by_key)
    if df_cost.empty:
        return html.P("ã‚³ã‚¹ãƒˆè¨ˆç®—çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    df_cost['date'] = pd.to_datetime(df_cost['date'])

    if not {'day_of_week', 'total_staff', 'role_breakdown'}.issubset(df_cost.columns):
        details = (
            long_df[long_df.get('parsed_slots_count', 1) > 0]
            .assign(date=lambda x: pd.to_datetime(x['ds']).dt.normalize())
            .groupby('date')
            .agg(
                day_of_week=('ds', lambda x: ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥'][x.iloc[0].weekday()]),
                total_staff=('staff', 'nunique'),
                role_breakdown=('role', lambda s: ', '.join(f"{r}:{c}" for r, c in s.value_counts().items())),
            )
            .reset_index()
        )
        df_cost = pd.merge(df_cost, details, on='date', how='left')

    df_cost = df_cost.sort_values('date')

    content = []

    total_cost = df_cost['cost'].sum()
    avg_daily_cost = df_cost['cost'].mean()
    max_cost_day = df_cost.loc[df_cost['cost'].idxmax()]
    summary_cards = html.Div([
        create_metric_card("ç·ã‚³ã‚¹ãƒˆ", f"Â¥{total_cost:,.0f}"),
        create_metric_card("æ—¥å¹³å‡ã‚³ã‚¹ãƒˆ", f"Â¥{avg_daily_cost:,.0f}"),
        create_metric_card("æœ€é«˜ã‚³ã‚¹ãƒˆæ—¥", f"{max_cost_day['date'].strftime('%m/%d')}<br>Â¥{max_cost_day['cost']:,.0f}"),
    ], style={'display': 'flex', 'justifyContent': 'space-around', 'marginBottom': '20px'})
    content.append(summary_cards)

    df_cost['cumulative_cost'] = df_cost['cost'].cumsum()
    fig_cumulative = px.area(df_cost, x='date', y='cumulative_cost', title='ç´¯è¨ˆäººä»¶è²»ã®æ¨ç§»')
    fig_cumulative.update_xaxes(tickformat="%m/%d(%a)")
    content.append(dcc.Graph(figure=fig_cumulative))

    fig_daily = px.bar(df_cost, x='date', y='cost', title='æ—¥åˆ¥ç™ºç”Ÿäººä»¶è²»ï¼ˆç·é¡ï¼‰')
    fig_daily.update_xaxes(tickformat="%m/%d(%a)")
    content.append(dcc.Graph(figure=fig_daily))

    if 'role_breakdown' in df_cost.columns and by_key == 'role':
        role_data = []
        for _, row in df_cost.iterrows():
            if pd.notna(row.get('role_breakdown')):
                date_total_cost = row['cost']
                role_counts = {r.split(':')[0]: int(r.split(':')[1]) for r in row['role_breakdown'].split(', ') if ':' in r}
                total_count = sum(role_counts.values())

                for role, count in role_counts.items():
                    role_cost = (count / total_count) * date_total_cost if total_count > 0 else 0
                    role_data.append({'date': row['date'], 'role': role, 'count': count, 'cost': role_cost})

        if role_data:
            role_df = pd.DataFrame(role_data)

            fig_stacked = px.bar(role_df, x='date', y='cost', color='role', title='æ—¥åˆ¥äººä»¶è²»ï¼ˆè·ç¨®åˆ¥å†…è¨³ï¼‰')
            fig_stacked.update_xaxes(tickformat="%m/%d(%a)")
            content.append(dcc.Graph(figure=fig_stacked))

            role_df['month'] = pd.to_datetime(role_df['date']).dt.to_period('M').astype(str)
            monthly_role = role_df.groupby(['month', 'role'])['cost'].sum().reset_index()
            fig_monthly = px.bar(monthly_role, x='month', y='cost', color='role', title='æœˆæ¬¡äººä»¶è²»ï¼ˆè·ç¨®åˆ¥å†…è¨³ï¼‰')
            content.append(dcc.Graph(figure=fig_monthly))

            total_by_role = role_df.groupby('role')['cost'].sum().reset_index()
            fig_pie = px.pie(total_by_role, values='cost', names='role', title='è·ç¨®åˆ¥ã‚³ã‚¹ãƒˆæ§‹æˆæ¯”ï¼ˆå…¨æœŸé–“ï¼‰')
            fig_pie.update_traces(textposition='inside', textinfo='percent+label')
            content.append(dcc.Graph(figure=fig_pie))

    return html.Div(content)


@app.callback(
    Output('clear-synergy-cache-btn', 'children'),
    Input('clear-synergy-cache-btn', 'n_clicks'),
    prevent_initial_call=True
)
@safe_callback
def clear_synergy_cache_callback(n_clicks):
    """ã‚·ãƒŠã‚¸ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢ãƒœã‚¿ãƒ³ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    if n_clicks:
        clear_synergy_cache()
        return "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢æ¸ˆã¿"
    return "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"


@app.callback(
    Output('individual-analysis-content', 'children', allow_duplicate=True),
    Input('individual-staff-dropdown', 'value'),
    Input('synergy-analysis-type', 'value'),
    prevent_initial_call=True
)
@safe_callback
def update_individual_analysis_content(selected_staff, synergy_type):
    """è·å“¡é¸æŠã«å¿œã˜ã¦åˆ†æã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ›´æ–°ã™ã‚‹"""
    if not selected_staff:
        raise PreventUpdate
    
    # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦åˆæœŸåŒ–
    global synergy_matrix_data, synergy_additional_info
    synergy_matrix_data = None
    synergy_additional_info = html.Div()

    # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ã§èª­ã¿è¾¼ã‚€
    long_df = data_get('long_df', pd.DataFrame())
    fatigue_df = data_get('fatigue_score', pd.DataFrame())
    fairness_df = data_get('fairness_after', pd.DataFrame())
    shortage_df = data_get('shortage_time', pd.DataFrame())
    excess_df = data_get('excess_time', pd.DataFrame())

    if long_df.empty:
        return html.P("å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    staff_df = long_df[long_df['staff'] == selected_staff].copy()

    # --- 1. å‹¤å‹™åŒºåˆ†ã”ã¨ã®å æœ‰å‰²åˆ ---
    work_dist_fig = go.Figure(layout={'title': {'text': f'{selected_staff}ã•ã‚“ã®å‹¤å‹™å‰²åˆ'}})
    if not staff_df.empty and 'code' in staff_df.columns:
        work_records = staff_df[staff_df.get('parsed_slots_count', 1) > 0]
        if not work_records.empty:
            code_counts = work_records['code'].value_counts()
            work_dist_fig = px.pie(
                values=code_counts.values, names=code_counts.index,
                title=f'{selected_staff}ã•ã‚“ã®å‹¤å‹™å‰²åˆ', hole=.3
            )
            work_dist_fig.update_traces(textposition='inside', textinfo='percent+label')

    # --- 2. ä¸å…¬å¹³ãƒ»ç–²åŠ´åº¦ã®è©³ç´°ã‚¹ã‚³ã‚¢ ---
    fatigue_score, unfairness_score = "ãƒ‡ãƒ¼ã‚¿ãªã—", "ãƒ‡ãƒ¼ã‚¿ãªã—"
    score_details_df = pd.DataFrame()
    if not fatigue_df.empty:
        fatigue_df_indexed = fatigue_df.set_index('staff') if 'staff' in fatigue_df.columns else fatigue_df
        if selected_staff in fatigue_df_indexed.index:
            fatigue_score = f"{fatigue_df_indexed.loc[selected_staff, 'fatigue_score']:.1f}"
    if not fairness_df.empty and 'staff' in fairness_df.columns:
        staff_fairness = fairness_df[fairness_df['staff'] == selected_staff]
        if not staff_fairness.empty:
            row = staff_fairness.iloc[0]
            unfairness_score = f"{row.get('unfairness_score', 0):.2f}"
            details_data = {
                "æŒ‡æ¨™": ["å¤œå‹¤æ¯”ç‡ã®ä¹–é›¢", "ç·åŠ´åƒæ™‚é–“ã®ä¹–é›¢", "é€£ä¼‘å–å¾—é »åº¦ã®ä¹–é›¢"],
                "ã‚¹ã‚³ã‚¢": [f"{row.get(col, 0):.2f}" for col in ['dev_night_ratio', 'dev_work_slots', 'dev_consecutive']]
            }
            score_details_df = pd.DataFrame(details_data)

    # --- 3. å…±åƒã—ãŸè·å“¡ãƒ©ãƒ³ã‚­ãƒ³ã‚° ---
    coworker_ranking_df = pd.DataFrame()
    my_slots = staff_df[['ds']].drop_duplicates()
    coworkers = long_df[long_df['ds'].isin(my_slots['ds']) & (long_df['staff'] != selected_staff)]
    if not coworkers.empty:
        coworker_counts = coworkers['staff'].value_counts().reset_index()
        coworker_counts.columns = ['è·å“¡', 'å…±åƒå›æ•°']
        coworker_ranking_df = coworker_counts.head(5)

    # --- 4. äººå“¡ä¸è¶³/éå‰°ã¸ã®è²¢çŒ®åº¦åˆ†æ ---
    slot_hours = SLOT_HOURS
    shortage_contribution_h, excess_contribution_h = 0, 0
    staff_work_slots = staff_df[staff_df.get('parsed_slots_count', 0) > 0][['ds']].copy()
    staff_work_slots['date_str'] = staff_work_slots['ds'].dt.strftime('%Y-%m-%d')
    staff_work_slots['time'] = staff_work_slots['ds'].dt.strftime('%H:%M')
    if not shortage_df.empty:
        shortage_long = shortage_df.melt(var_name='date_str', value_name='shortage_count', ignore_index=False).reset_index().rename(columns={'index':'time'})
        merged_shortage = pd.merge(staff_work_slots, shortage_long, on=['date_str', 'time'])
        shortage_contribution_h = merged_shortage[merged_shortage['shortage_count'] > 0].shape[0] * slot_hours
    if not excess_df.empty:
        excess_long = excess_df.melt(var_name='date_str', value_name='excess_count', ignore_index=False).reset_index().rename(columns={'index':'time'})
        merged_excess = pd.merge(staff_work_slots, excess_long, on=['date_str', 'time'])
        excess_contribution_h = merged_excess[merged_excess['excess_count'] > 0].shape[0] * slot_hours

    # --- 5. å€‹äººã®ä¼‘æš‡å–å¾—å‚¾å‘ ---
    leave_by_dow_fig = go.Figure(layout={'title': {'text': 'æ›œæ—¥åˆ¥ã®ä¼‘æš‡å–å¾—æ—¥æ•°'}})
    staff_leave_df = staff_df[staff_df.get('holiday_type', 'é€šå¸¸å‹¤å‹™') != 'é€šå¸¸å‹¤å‹™']
    if not staff_leave_df.empty:
        daily_leave = leave_analyzer.get_daily_leave_counts(staff_leave_df)
        if not daily_leave.empty:
            dow_summary = leave_analyzer.summarize_leave_by_day_count(daily_leave, period='dayofweek')
            if not dow_summary.empty:
                leave_by_dow_fig = px.bar(dow_summary, x='period_unit', y='total_leave_days', color='leave_type', title=f'{selected_staff}ã•ã‚“ã®æ›œæ—¥åˆ¥ä¼‘æš‡å–å¾—æ—¥æ•°')
                leave_by_dow_fig.update_xaxes(title_text="æ›œæ—¥").update_yaxes(title_text="æ—¥æ•°")

    # --- 6. è·å“¡é–“ã®ã€ŒåŒ–å­¦åå¿œã€åˆ†æ ---
    synergy_fig = go.Figure(layout={'title': {'text': f'{selected_staff}ã•ã‚“ã¨ã®ã‚·ãƒŠã‚¸ãƒ¼åˆ†æ'}})
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¿½åŠ 
    log.info(f"[SYNERGY] åˆ†æé–‹å§‹: {selected_staff}")
    log.info(f"[SYNERGY] long_df shape: {long_df.shape}")
    log.info(f"[SYNERGY] shortage_df shape: {shortage_df.shape}")
    log.info(f"[SYNERGY] shortage_df columns: {shortage_df.columns.tolist() if not shortage_df.empty else 'Empty'}")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    available_data = {}
    for key in ['shortage_time', 'shortage_role_summary', 'heat_ALL', 'long_df']:
        try:
            data = data_get(key, pd.DataFrame())
            available_data[key] = f"shape: {data.shape}, empty: {data.empty}"
            if not data.empty:
                available_data[key] += f", columns: {data.columns.tolist()[:5]}"  # æœ€åˆã®5åˆ—ã®ã¿è¡¨ç¤º
        except:
            available_data[key] = "å–å¾—å¤±æ•—"
    
    log.info(f"[SYNERGY] åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿: {available_data}")
    
    # shortage_timeãŒç©ºã®å ´åˆã€ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚’è©¦ã™
    if shortage_df.empty:
        log.info("[SYNERGY] shortage_timeãŒç©ºã®ãŸã‚ã€ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚’è©¦ã—ã¾ã™")
        
        # 1. analysis_resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ç›´æ¥èª­ã¿å–ã‚Šã‚’è©¦ã™
        try:
            import os
            current_dir = os.getcwd()
            analysis_results_path = os.path.join(current_dir, "analysis_results")
            shortage_time_path = os.path.join(analysis_results_path, "shortage_time.parquet")
            
            if os.path.exists(shortage_time_path):
                log.info(f"[SYNERGY] ç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å–ã‚Š: {shortage_time_path}")
                shortage_df = pd.read_parquet(shortage_time_path)
                log.info(f"[SYNERGY] ç›´æ¥èª­ã¿å–ã‚ŠæˆåŠŸ: {shortage_df.shape}")
            else:
                log.info(f"[SYNERGY] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {shortage_time_path}")
        except Exception as e:
            log.error(f"[SYNERGY] ç›´æ¥èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
        
        # 2. ã¾ã ç©ºã®å ´åˆã€heat_ALLãƒ‡ãƒ¼ã‚¿ã‚’è©¦ã™
        if shortage_df.empty:
            heat_all_df = data_get('heat_ALL', pd.DataFrame())
            if not heat_all_df.empty:
                log.info(f"[SYNERGY] heat_ALLã‚’ä½¿ç”¨: {heat_all_df.shape}")
                # heat_ALLã‹ã‚‰ä¸è¶³ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
                shortage_df = create_shortage_from_heat_all(heat_all_df)
                log.info(f"[SYNERGY] ç”Ÿæˆã•ã‚ŒãŸshortage_df: {shortage_df.shape}")
        
        # 3. ã¾ã ç©ºã®å ´åˆã€excess_timeã‚’è©¦ã™ï¼ˆç¬¦å·ã‚’åè»¢ï¼‰
        if shortage_df.empty:
            excess_df = data_get('excess_time', pd.DataFrame())
            if not excess_df.empty:
                log.info(f"[SYNERGY] excess_timeã‚’ä½¿ç”¨ï¼ˆç¬¦å·åè»¢): {excess_df.shape}")
                # excess_timeã®ç¬¦å·ã‚’åè»¢ã—ã¦shortageã¨ã—ã¦ä½¿ç”¨
                shortage_df = -excess_df
                shortage_df = shortage_df.clip(lower=0)  # è² ã®å€¤ã¯0ã«ã‚¯ãƒªãƒƒãƒ—
                log.info(f"[SYNERGY] excess_timeã‹ã‚‰ç”Ÿæˆ: {shortage_df.shape}")
    
    # ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚’å®Ÿè¡Œï¼ˆåˆ†æã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ï¼‰
    synergy_df = pd.DataFrame()
    synergy_additional_data = None
    # synergy_matrix_data ã¨ synergy_additional_info ã¯ä¸Šã§ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦åˆæœŸåŒ–æ¸ˆã¿
    
    log.info(f"[SYNERGY] åˆ†æã‚¿ã‚¤ãƒ—: {synergy_type}")
    
    if not long_df.empty:
        try:
            if synergy_type == 'correlation_matrix':
                try:
                    if create_synergy_correlation_matrix_optimized is not None:
                        log.info("[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æã‚’å®Ÿè¡Œ")
                        
                        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯
                        cache_key = get_synergy_cache_key(long_df, shortage_df)
                        synergy_matrix_data = SYNERGY_CACHE.get(cache_key)
                        
                        if synergy_matrix_data is not None:
                            log.info(f"[SYNERGY] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å–å¾—: {cache_key}")
                        else:
                            log.info("[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹æ–°è¦è¨ˆç®—é–‹å§‹")
                            n_staff = len(long_df['staff'].unique())
                            total_calculations = n_staff * (n_staff - 1) // 2
                            log.info(f"[SYNERGY] è¨ˆç®—äºˆå®šãƒšã‚¢æ•°: {total_calculations}")
                            
                            synergy_matrix_data = create_synergy_correlation_matrix_optimized(long_df, shortage_df)
                            if synergy_matrix_data is not None and 'error' not in synergy_matrix_data:
                                SYNERGY_CACHE.set(cache_key, synergy_matrix_data)
                                log.info(f"[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—å®Œäº†ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜: {cache_key}")
                            else:
                                log.error(f"[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—å¤±æ•—: {synergy_matrix_data.get('error', 'Unknown error') if synergy_matrix_data else 'None result'}")
                                if synergy_matrix_data is None:
                                    synergy_matrix_data = {"error": "ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—ã§Noneçµæœ"}
                        
                        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
                        memory_usage = psutil.Process().memory_info().rss / 1024 / 1024  # MB
                        log.info(f"[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æå®Œäº†, ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_usage:.1f}MB")
                    else:
                        log.error("[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹é–¢æ•°ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                        synergy_matrix_data = {"error": "ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹é–¢æ•°ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
                except Exception as correlation_error:
                    log.error(f"[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {correlation_error}")
                    synergy_matrix_data = {"error": f"ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(correlation_error)}"}
            elif synergy_type == 'same_role' and analyze_synergy_by_role is not None:
                log.info("[SYNERGY] åŒè·ç¨®é™å®šã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚’å®Ÿè¡Œ")
                synergy_df = analyze_synergy_by_role(long_df, shortage_df, selected_staff, same_role_only=True)
                log.info(f"[SYNERGY] åŒè·ç¨®åˆ†æçµæœ: {synergy_df.shape}")
            elif synergy_type == 'all_roles' and analyze_all_roles_synergy is not None:
                log.info("[SYNERGY] å…¨è·ç¨®è©³ç´°ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚’å®Ÿè¡Œ")
                synergy_additional_data = analyze_all_roles_synergy(long_df, shortage_df, selected_staff)
                if 'error' not in synergy_additional_data and 'raw_data' in synergy_additional_data:
                    synergy_df = pd.DataFrame(synergy_additional_data['raw_data'])
                log.info(f"[SYNERGY] å…¨è·ç¨®åˆ†æçµæœ: {synergy_df.shape}")
            else:
                # åŸºæœ¬åˆ†æ
                if not shortage_df.empty:
                    log.info("[SYNERGY] åŸºæœ¬ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚’å®Ÿè¡Œ")
                    synergy_df = analyze_synergy(long_df, shortage_df, selected_staff)
                    log.info(f"[SYNERGY] åŸºæœ¬åˆ†æçµæœ: {synergy_df.shape}")
                else:
                    log.info("[SYNERGY] ã‚·ãƒ³ãƒ—ãƒ«ãªã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚’å®Ÿè¡Œ")
                    synergy_df = simple_synergy_analysis(long_df, selected_staff)
                    log.info(f"[SYNERGY] ã‚·ãƒ³ãƒ—ãƒ«åˆ†æçµæœ: {synergy_df.shape}")
        except Exception as e:
            log.error(f"[SYNERGY] åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢
            gc.collect()
            # ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®š
            if synergy_type == 'correlation_matrix':
                if synergy_matrix_data is None:  # ã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
                    synergy_matrix_data = {"error": f"ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}"}
            else:
                # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯åŸºæœ¬åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not shortage_df.empty:
                    try:
                        synergy_df = analyze_synergy(long_df, shortage_df, selected_staff)
                    except Exception as fallback_error:
                        log.error(f"[SYNERGY] ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æã‚¨ãƒ©ãƒ¼: {fallback_error}")
                        synergy_df = simple_synergy_analysis(long_df, selected_staff)
                else:
                    synergy_df = simple_synergy_analysis(long_df, selected_staff)
    else:
        # long_dfãŒç©ºã®å ´åˆ
        log.warning("[SYNERGY] long_dfãŒç©ºã®ãŸã‚ã€ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
        if synergy_type == 'correlation_matrix':
            if synergy_matrix_data is None:  # ã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
                synergy_matrix_data = {"error": "å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ (long_df) ãŒç©ºã®ãŸã‚ã€ã‚·ãƒŠã‚¸ãƒ¼åˆ†æãŒã§ãã¾ã›ã‚“"}
        else:
            synergy_df = pd.DataFrame()
    
    # çµæœã‚’è¡¨ç¤º
    if synergy_type == 'correlation_matrix':
        if synergy_matrix_data is not None and 'error' not in synergy_matrix_data:
            # ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®è¡¨ç¤º
            log.info("[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹è¡¨ç¤º")
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            matrix_df = pd.DataFrame(synergy_matrix_data['matrix'])
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ä½œæˆ
            synergy_fig = go.Figure(data=go.Heatmap(
                z=matrix_df.values,
                x=matrix_df.columns,
                y=matrix_df.index,
                colorscale='RdBu',
                zmid=0,
                text=np.round(matrix_df.values, 2),
                texttemplate='%{text}',
                textfont={"size": 10},
                hoverongaps=False,
                hovertemplate='%{x} & %{y}<br>ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢: %{z:.3f}<extra></extra>'
            ))
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼šä¸è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’æ—©æœŸè§£æ”¾
            del matrix_df
            gc.collect()
            
            synergy_fig.update_layout(
                title="å…¨è·å“¡é–“ã®ã‚·ãƒŠã‚¸ãƒ¼ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹",
                xaxis_title="è·å“¡",
                yaxis_title="è·å“¡",
                width=1200,
                height=1000,
                xaxis={'side': 'bottom', 'tickangle': -45},
                yaxis={'autorange': 'reversed'},
                margin=dict(l=150, r=50, t=100, b=150)
            )
            
            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¡¨ç¤ºç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            ranking_df = pd.DataFrame(synergy_matrix_data['ranking'])
            if not ranking_df.empty:
                # ä¸Šä½5åã¨ä¸‹ä½5åã‚’æŠ½å‡º
                top5 = ranking_df.head(5)
                bottom5 = ranking_df.tail(5)
                
                # è¿½åŠ æƒ…å ±ã®è¡¨ç¤º
                synergy_additional_info = html.Div([
                    html.H5("ã‚·ãƒŠã‚¸ãƒ¼å¹³å‡ãƒ©ãƒ³ã‚­ãƒ³ã‚°"),
                    html.Div([
                        html.Div([
                            html.H6("ç›¸æ€§ã®è‰¯ã„è·å“¡ TOP 5", style={'color': 'green'}),
                            dash_table.DataTable(
                                data=top5.to_dict('records'),
                                columns=[
                                    {'name': 'è·å“¡', 'id': 'è·å“¡'},
                                    {'name': 'å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼', 'id': 'å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼', 'type': 'numeric', 'format': {'specifier': '.3f'}},
                                    {'name': 'è·ç¨®', 'id': 'role'} if 'role' in top5.columns else {},
                                ],
                                style_cell={'textAlign': 'left'},
                                style_data_conditional=[
                                    {
                                        'if': {'column_id': 'å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼'},
                                        'color': 'green',
                                        'fontWeight': 'bold'
                                    }
                                ]
                            )
                        ], style={'width': '48%', 'display': 'inline-block', 'marginRight': '2%'}),
                        html.Div([
                            html.H6("ç›¸æ€§ã®æ‚ªã„è·å“¡ BOTTOM 5", style={'color': 'red'}),
                            dash_table.DataTable(
                                data=bottom5.to_dict('records'),
                                columns=[
                                    {'name': 'è·å“¡', 'id': 'è·å“¡'},
                                    {'name': 'å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼', 'id': 'å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼', 'type': 'numeric', 'format': {'specifier': '.3f'}},
                                    {'name': 'è·ç¨®', 'id': 'role'} if 'role' in bottom5.columns else {},
                                ],
                                style_cell={'textAlign': 'left'},
                                style_data_conditional=[
                                    {
                                        'if': {'column_id': 'å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼'},
                                        'color': 'red',
                                        'fontWeight': 'bold'
                                    }
                                ]
                            )
                        ], style={'width': '48%', 'display': 'inline-block'})
                    ], style={'marginTop': '20px'})
                ])
            else:
                synergy_additional_info = html.Div()
            
            # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼šå‡¦ç†å®Œäº†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªã‚¢
            if 'synergy_matrix_data' in locals() and synergy_matrix_data is not None:
                del synergy_matrix_data
            gc.collect()
        
        else:
            # ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
            log.error(f"[SYNERGY] ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼: {synergy_matrix_data.get('error', 'Unknown error') if synergy_matrix_data else 'ãƒ‡ãƒ¼ã‚¿ãªã—'}")
            synergy_fig = go.Figure()
            synergy_fig.add_annotation(
                text=f"ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {synergy_matrix_data.get('error', 'Unknown error') if synergy_matrix_data else 'ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ'}",
                x=0.5, y=0.5, xref="paper", yref="paper",
                showarrow=False, font=dict(size=16, color="red")
            )
            synergy_fig.update_layout(
                title="ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼",
                width=800, height=400
            )
            synergy_additional_info = html.Div()
            
    elif not synergy_df.empty:
        log.info(f"[SYNERGY] æœ€çµ‚çµæœ: {synergy_df.shape}")
        log.info(f"[SYNERGY] ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿: {synergy_df.head()}")
        
        # åˆ†æã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸè¡¨ç¤º
        if synergy_type == 'all_roles' and synergy_additional_data is not None:
            # å…¨è·ç¨®è©³ç´°åˆ†æã®å ´åˆ
            if 'role' in synergy_df.columns:
                # è·ç¨®åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦è¡¨ç¤º
                synergy_fig = px.bar(
                    synergy_df.head(15), x="ç›¸æ‰‹ã®è·å“¡", y="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢", 
                    color="role", title=f"{selected_staff}ã•ã‚“ã¨ã®ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆå…¨è·ç¨®è©³ç´°ï¼‰"
                )
                synergy_fig.update_layout(xaxis_title="ç›¸æ‰‹ã®è·å“¡", yaxis_title="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰")
            else:
                synergy_fig = px.bar(
                    synergy_df.head(10), x="ç›¸æ‰‹ã®è·å“¡", y="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢", color="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢",
                    color_continuous_scale='RdYlGn', title=f"{selected_staff}ã•ã‚“ã¨ã®ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆå…¨è·ç¨®è©³ç´°ï¼‰"
                )
        elif synergy_type == 'same_role':
            # åŒè·ç¨®é™å®šåˆ†æã®å ´åˆ
            color_col = "role" if "role" in synergy_df.columns else "ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢"
            synergy_fig = px.bar(
                synergy_df.head(10), x="ç›¸æ‰‹ã®è·å“¡", y="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢", color=color_col,
                title=f"{selected_staff}ã•ã‚“ã¨ã®ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆåŒè·ç¨®é™å®šï¼‰"
            )
            synergy_fig.update_layout(xaxis_title="ç›¸æ‰‹ã®è·å“¡", yaxis_title="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰")
        else:
            # åŸºæœ¬åˆ†æã®å ´åˆ
            synergy_df_top5 = synergy_df.head(5)
            if len(synergy_df) > 5:
                synergy_df_worst5 = synergy_df.tail(5).sort_values("ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢", ascending=True)
                synergy_display_df = pd.concat([synergy_df_top5, synergy_df_worst5])
            else:
                synergy_display_df = synergy_df_top5
            
            synergy_fig = px.bar(
                synergy_display_df, x="ç›¸æ‰‹ã®è·å“¡", y="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢", color="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢",
                color_continuous_scale='RdYlGn', title=f"{selected_staff}ã•ã‚“ã¨ã®ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆåŸºæœ¬åˆ†æï¼‰"
            )
            synergy_fig.update_layout(xaxis_title="ç›¸æ‰‹ã®è·å“¡", yaxis_title="ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ï¼ˆé«˜ã„ã»ã©è‰¯ã„ï¼‰")
    else:
        log.warning("[SYNERGY] å…¨ã¦ã®åˆ†æãŒå¤±æ•—ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ä¸è¶³")
        synergy_fig.add_annotation(
            text="ã‚·ãƒŠã‚¸ãƒ¼åˆ†æã®ãƒ‡ãƒ¼ã‚¿ãŒä¸ååˆ†ã§ã™",
            x=0.5, y=0.5, xref="paper", yref="paper",
            showarrow=False, font=dict(size=16)
        )
    
    # ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®è¿½åŠ æƒ…å ±ã‚’åˆæœŸåŒ–ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    # æ—¢ã«ä¸Šéƒ¨ã§åˆæœŸåŒ–æ¸ˆã¿ã®ãŸã‚ã€ã“ã“ã§ã¯ä¸è¦

    # --- 7 & 8. åƒãæ–¹ã®ã‚¯ã‚»åˆ†æ ---
    mannelido_score, rhythm_score = "è¨ˆç®—ä¸å¯", "è¨ˆç®—ä¸å¯"
    work_records_for_role = staff_df[staff_df.get('parsed_slots_count', 0) > 0]
    if not work_records_for_role.empty:
        role_per_day = work_records_for_role[['ds', 'role']].copy()
        role_per_day['date'] = role_per_day['ds'].dt.date
        role_counts = role_per_day.drop_duplicates(subset=['date', 'role'])['role'].value_counts(normalize=True)
        if not role_counts.empty:
            mannelido_score = f"{role_counts.max():.2f}"

        daily_starts = work_records_for_role.groupby(work_records_for_role['ds'].dt.date)['ds'].min()
        if len(daily_starts) > 1:
            start_hours = daily_starts.dt.hour + daily_starts.dt.minute / 60.0
            rhythm_score = f"{start_hours.std():.2f}"
        else:
            rhythm_score = "0.00"

    # --- ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®çµ„ã¿ç«‹ã¦ ---
    layout = html.Div([
        html.Div([
            html.Div([
                html.H4("ç–²åŠ´åº¦ãƒ»ä¸å…¬å¹³æ„Ÿãƒ»åƒãæ–¹ã®ã‚¯ã‚»"),
                create_metric_card("ç–²åŠ´ã‚¹ã‚³ã‚¢", fatigue_score, color="#ff7f0e"),
                create_metric_card("ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢", unfairness_score, color="#d62728"),
                create_metric_card("æ¥­å‹™ãƒãƒ³ãƒãƒªåº¦", mannelido_score, color="#9467bd"),
                create_metric_card("ç”Ÿæ´»ãƒªã‚ºãƒ ç ´å£Šåº¦", rhythm_score, color="#8c564b"),
            ], style={'width': '24%', 'display': 'inline-block', 'verticalAlign': 'top', 'paddingRight': '1%'}),
            html.Div([
                html.H5("ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢ã®å†…è¨³"),
                dash_table.DataTable(
                    data=score_details_df.to_dict('records'),
                    columns=[{'name': i, 'id': i} for i in score_details_df.columns],
                ) if not score_details_df.empty else html.P("è©³ç´°ãƒ‡ãƒ¼ã‚¿ãªã—")
            ], style={'width': '24%', 'display': 'inline-block', 'verticalAlign': 'top', 'paddingRight': '1%'}),
            html.Div([
                html.H5("å…±åƒãƒ©ãƒ³ã‚­ãƒ³ã‚° Top 5"),
                dash_table.DataTable(
                    data=coworker_ranking_df.to_dict('records'),
                    columns=[{'name': i, 'id': i} for i in coworker_ranking_df.columns],
                ) if not coworker_ranking_df.empty else html.P("å…±åƒãƒ‡ãƒ¼ã‚¿ãªã—"),
            ], style={'width': '24%', 'display': 'inline-block', 'verticalAlign': 'top', 'paddingRight': '1%'}),
            html.Div([
                html.H5("ä¸è¶³/éå‰°ã¸ã®è²¢çŒ®åº¦"),
                create_metric_card("ä¸è¶³æ™‚é–“å¸¯ã§ã®å‹¤å‹™ (h)", f"{shortage_contribution_h:.1f}", color="#c53d40"),
                create_metric_card("éå‰°æ™‚é–“å¸¯ã§ã®å‹¤å‹™ (h)", f"{excess_contribution_h:.1f}", color="#1f77b4"),
            ], style={'width': '24%', 'display': 'inline-block', 'verticalAlign': 'top'}),
        ], style={'marginBottom': '20px'}),
        html.Div([
            html.Div([dcc.Graph(figure=work_dist_fig)], style={'width': '49%', 'display': 'inline-block'}),
            html.Div([dcc.Graph(figure=leave_by_dow_fig)], style={'width': '49%', 'display': 'inline-block'}),
        ]),
        html.Div([
            html.H4("è·å“¡é–“ã®\u300cåŒ–å­¦åå¿œ\u300dåˆ†æ", style={'marginTop': '20px'}),
            html.P("ã‚·ãƒŠã‚¸ãƒ¼ã‚¹ã‚³ã‚¢ã¯ã€ãã®ãƒšã‚¢ãŒä¸€ç·’ã«å‹¤å‹™ã—ãŸéš›ã®\u300cäººå“¡ä¸è¶³ã®èµ·ã“ã‚Šã«ãã•\u300dã‚’ç¤ºã—ã¾ã™ã€‚ã‚¹ã‚³ã‚¢ãŒé«˜ã„ã»ã©ã€ä¸è¶³ãŒå°‘ãªããªã‚‹è‰¯ã„çµ„ã¿åˆã‚ã›ã§ã™ã€‚"),
            
            # åˆ†æã‚¿ã‚¤ãƒ—åˆ¥ã®è¿½åŠ æƒ…å ±
            html.Div([
                html.H5("åˆ†ææƒ…å ±"),
                html.Div([
                    html.P(f"åˆ†æã‚¿ã‚¤ãƒ—: {['åŸºæœ¬åˆ†æï¼ˆå…¨è·å“¡å¯¾è±¡ï¼‰' if synergy_type == 'basic' else 'åŒè·ç¨®é™å®šåˆ†æ' if synergy_type == 'same_role' else 'å…¨è·ç¨®è©³ç´°åˆ†æ' if synergy_type == 'all_roles' else 'ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ï¼ˆå…¨ä½“ï¼‰'][0]}", style={'fontWeight': 'bold'})
                ] + ([
                    html.P(f"å…¨ä½“å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼: {synergy_additional_data['overall_stats']['å…¨ä½“å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼']:.3f}"),
                    html.P(f"åˆ†æå¯¾è±¡è·å“¡æ•°: {synergy_additional_data['overall_stats']['åˆ†æå¯¾è±¡è·å“¡æ•°']}äºº"),
                    html.P(f"å¯¾è±¡è·ç¨®æ•°: {synergy_additional_data['overall_stats']['å¯¾è±¡è·ç¨®æ•°']}è·ç¨®"),
                ] if synergy_type == 'all_roles' and synergy_additional_data is not None and 'overall_stats' in synergy_additional_data else []) + ([
                    html.P(f"åˆ†æå¯¾è±¡è·å“¡æ•°: {synergy_matrix_data['summary']['è·å“¡æ•°']}äºº"),
                    html.P(f"å…¨ä½“å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼: {synergy_matrix_data['summary']['å…¨ä½“å¹³å‡ã‚·ãƒŠã‚¸ãƒ¼']:.3f}"),
                ] if synergy_type == 'correlation_matrix' and 'synergy_matrix_data' in locals() and synergy_matrix_data is not None and 'summary' in synergy_matrix_data else []))
            ], style={'marginBottom': '10px', 'padding': '10px', 'backgroundColor': '#f8f9fa', 'borderRadius': '5px'}) if synergy_type != 'basic' else html.Div(),
            
            dcc.Graph(figure=synergy_fig),
            
            # ç›¸é–¢ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã®å ´åˆã€è¿½åŠ æƒ…å ±ã‚’è¡¨ç¤º
            synergy_additional_info if synergy_type == 'correlation_matrix' and 'synergy_additional_info' in locals() and synergy_additional_info is not None else html.Div()
        ])
    ])

    return layout


@app.callback(
    Output('team-criteria-value-dropdown', 'options'),
    Input('team-criteria-key-dropdown', 'value')
)
@safe_callback
def update_team_value_options(selected_key):
    long_df = data_get('long_df', pd.DataFrame())
    if long_df.empty or not selected_key:
        return []
    options = sorted(long_df[selected_key].unique())
    return [{'label': opt, 'value': opt} for opt in options]


@app.callback(
    Output('team-analysis-content', 'children', allow_duplicate=True),
    Input('team-criteria-value-dropdown', 'value'),
    State('team-criteria-key-dropdown', 'value'),
    prevent_initial_call=True
)
@safe_callback
def update_team_analysis_graphs(selected_value, selected_key):
    if not selected_value or not selected_key:
        raise PreventUpdate

    long_df = data_get('long_df', pd.DataFrame())
    fatigue_df = data_get('fatigue_score', pd.DataFrame())
    fairness_df = data_get('fairness_after', pd.DataFrame())

    team_criteria = {selected_key: selected_value}
    team_df = analyze_team_dynamics(long_df, fatigue_df, fairness_df, team_criteria)

    if team_df.empty:
        return html.P("ã“ã®æ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

    fig_fatigue = px.line(
        team_df,
        y=['avg_fatigue', 'std_fatigue'],
        title=f"ãƒãƒ¼ãƒ ã€Œ{selected_value}ã€ã®ç–²åŠ´åº¦ã‚¹ã‚³ã‚¢æ¨ç§»"
    )
    fig_fairness = px.line(
        team_df,
        y=['avg_unfairness', 'std_unfairness'],
        title=f"ãƒãƒ¼ãƒ ã€Œ{selected_value}ã€ã®ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢æ¨ç§»"
    )

    return html.Div([
        html.H4(f"ãƒãƒ¼ãƒ ã€Œ{selected_value}ã€ã®åˆ†æçµæœ"),
        
        # ã‚°ãƒ©ãƒ•ã®èª­ã¿è§£ãæ–¹èª¬æ˜ã‚’è¿½åŠ 
        html.Div([
            html.H5("ğŸ“Š ã‚°ãƒ©ãƒ•ã®èª­ã¿è§£ãæ–¹"),
            html.Div([
                html.P("ğŸ” ç–²åŠ´åº¦ã‚¹ã‚³ã‚¢æ¨ç§»ã‚°ãƒ©ãƒ•ã®è¦‹æ–¹:", style={'fontWeight': 'bold', 'marginTop': '15px'}),
                html.Ul([
                    html.Li("å¹³å‡ç–²åŠ´åº¦ï¼ˆavg_fatigueï¼‰: ãƒãƒ¼ãƒ å…¨ä½“ã®ç–²åŠ´ãƒ¬ãƒ™ãƒ«ã€‚æ•°å€¤ãŒé«˜ã„ã»ã©ç–²åŠ´ãŒè“„ç©ã—ã¦ã„ã‚‹"),
                    html.Li("ç–²åŠ´åº¦ã®ã°ã‚‰ã¤ãï¼ˆstd_fatigueï¼‰: ãƒãƒ¼ãƒ å†…ã®ç–²åŠ´æ ¼å·®ã€‚æ•°å€¤ãŒé«˜ã„ã»ã©å€‹äººå·®ãŒå¤§ãã„"),
                    html.Li("âš ï¸ ä¸¡æ–¹ãŒé«˜ã„å ´åˆ: ãƒãƒ¼ãƒ å…¨ä½“ãŒç–²å¼Šã—ã€ã‹ã¤å€‹äººå·®ã‚‚å¤§ããä¸å®‰å®šãªçŠ¶æ…‹"),
                    html.Li("âœ… ç†æƒ³çš„ãªçŠ¶æ…‹: å¹³å‡ç–²åŠ´åº¦ãŒä½ãã€ã°ã‚‰ã¤ãã‚‚å°ã•ã„çŠ¶æ…‹")
                ]),
                
                html.P("ğŸ” ä¸å…¬å¹³æ„Ÿã‚¹ã‚³ã‚¢æ¨ç§»ã‚°ãƒ©ãƒ•ã®è¦‹æ–¹:", style={'fontWeight': 'bold', 'marginTop': '15px'}),
                html.Ul([
                    html.Li("å¹³å‡ä¸å…¬å¹³æ„Ÿï¼ˆavg_unfairnessï¼‰: ãƒãƒ¼ãƒ å…¨ä½“ã®ä¸å…¬å¹³æ„Ÿã€‚æ•°å€¤ãŒé«˜ã„ã»ã©ä¸æº€ãŒè“„ç©ã—ã¦ã„ã‚‹"),
                    html.Li("ä¸å…¬å¹³æ„Ÿã®ã°ã‚‰ã¤ãï¼ˆstd_unfairnessï¼‰: ãƒãƒ¼ãƒ å†…ã®ä¸å…¬å¹³æ„Ÿæ ¼å·®ã€‚æ•°å€¤ãŒé«˜ã„ã»ã©å€‹äººå·®ãŒå¤§ãã„"),
                    html.Li("âš ï¸ å¹³å‡ãŒé«˜ã„å ´åˆ: æ¥­å‹™é…åˆ†ã‚„å¾…é‡ã«å…¨ä½“çš„ãªä¸å…¬å¹³æ„ŸãŒã‚ã‚‹å¯èƒ½æ€§"),
                    html.Li("âš ï¸ ã°ã‚‰ã¤ããŒå¤§ãã„å ´åˆ: ä¸€éƒ¨ã®ãƒ¡ãƒ³ãƒãƒ¼ãŒç‰¹ã«ä¸å…¬å¹³æ„Ÿã‚’æ„Ÿã˜ã¦ã„ã‚‹å¯èƒ½æ€§"),
                    html.Li("âœ… ç†æƒ³çš„ãªçŠ¶æ…‹: å¹³å‡ä¸å…¬å¹³æ„ŸãŒä½ãã€ã°ã‚‰ã¤ãã‚‚å°ã•ã„çŠ¶æ…‹")
                ])
            ], style={
                'backgroundColor': '#f8f9fa',
                'padding': '15px',
                'borderRadius': '8px',
                'marginBottom': '20px',
                'border': '1px solid #dee2e6'
            })
        ]),
        
        dcc.Graph(figure=fig_fatigue),
        dcc.Graph(figure=fig_fairness),
        
        # æ”¹å–„ææ¡ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³
        html.Div([
            html.H5("ğŸ’¡ æ”¹å–„ææ¡ˆ"),
            html.P("åˆ†æçµæœã«åŸºã¥ãå…·ä½“çš„ãªæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:"),
            html.Ul([
                html.Li("ç–²åŠ´åº¦ãŒé«˜ã„å ´åˆ: å‹¤å‹™é–“éš”ã®èª¿æ•´ã€ä¼‘æš‡å–å¾—ã®ä¿ƒé€²ã€æ¥­å‹™é‡ã®è¦‹ç›´ã—"),
                html.Li("ç–²åŠ´åº¦ã®ã°ã‚‰ã¤ããŒå¤§ãã„å ´åˆ: æ¥­å‹™åˆ†æ‹…ã®å‡ç­‰åŒ–ã€ç‰¹å®šãƒ¡ãƒ³ãƒãƒ¼ã®è² è·è»½æ¸›"),
                html.Li("ä¸å…¬å¹³æ„ŸãŒé«˜ã„å ´åˆ: å‹¤å‹™æ¡ä»¶ã®é€æ˜åŒ–ã€å¸Œæœ›ä¼‘æ‰¿èªã®å…¬å¹³åŒ–"),
                html.Li("ä¸å…¬å¹³æ„Ÿã®ã°ã‚‰ã¤ããŒå¤§ãã„å ´åˆ: å€‹åˆ¥é¢è«‡ã®å®Ÿæ–½ã€ä¸æº€ã®èãå–ã‚Šèª¿æŸ»")
            ])
        ], style={
            'backgroundColor': '#e7f3ff',
            'padding': '15px',
            'borderRadius': '8px',
            'marginTop': '20px',
            'border': '1px solid #b3d9ff'
        })
    ])


@app.callback(
    Output('blueprint-results-store', 'data'),
    Output('tradeoff-scatter-plot', 'figure'),
    Output('rules-data-table', 'data'),
    Output('staff-selector-dropdown', 'options'),
    Output('facts-data-table', 'data', allow_duplicate=True),
    Output('facts-summary', 'children'),
    Output('integrated-analysis-content', 'children'),
    Input('generate-blueprint-button', 'n_clicks'),
    State('blueprint-analysis-type', 'value'),
    prevent_initial_call=True
)
@safe_callback
def update_blueprint_analysis_content(n_clicks, analysis_type):
    if not n_clicks:
        raise PreventUpdate

    try:
        long_df = data_get('long_df', pd.DataFrame())
        if long_df.empty:
            empty_fig = go.Figure()
            empty_fig.update_layout(
                title="ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                annotations=[
                    dict(
                        text="ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã‹ã‚‰åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚<br><br>" +
                             "ğŸ“‹ æ‰‹é †:<br>" +
                             "1. Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰<br>" +
                             "2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®å®Œäº†ã‚’ç¢ºèª<br>" +
                             "3. å†åº¦åˆ†æãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯",
                        xref="paper", yref="paper",
                        x=0.5, y=0.5, xanchor='center', yanchor='middle',
                        showarrow=False,
                        font=dict(size=14, color="#666"),
                        bgcolor="rgba(240, 240, 240, 0.8)",
                        bordercolor="#ccc",
                        borderwidth=1
                    )
                ]
            )
            helpful_message = html.Div([
                html.H4("ğŸ” ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã«ã¤ã„ã¦", style={'color': '#1976d2'}),
                html.P("ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã¯ã€ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥ã‚„åˆ¤æ–­ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã™ã‚‹é«˜åº¦ãªåˆ†ææ©Ÿèƒ½ã§ã™ã€‚"),
                html.H5("ğŸ’¡ åˆ†æã§ç™ºè¦‹ã§ãã‚‹ã“ã¨:"),
                html.Ul([
                    html.Li("ã‚¹ã‚¿ãƒƒãƒ•å€‹åˆ¥ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„åˆ¶ç´„"),
                    html.Li("ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çš„ãªãƒ«ãƒ¼ãƒ«"),
                    html.Li("è·ç¨®ãƒ»ãƒãƒ¼ãƒ é–“ã®ç›¸äº’é–¢ä¿‚"),
                    html.Li("åŠ¹ç‡çš„ãªã‚·ãƒ•ãƒˆçµ„ã¿åˆã‚ã›"),
                ]),
                html.P("åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ã¾ãšã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ãã ã•ã„ã€‚", 
                       style={'fontWeight': 'bold', 'color': '#d32f2f'})
            ], style={
                'padding': '20px', 'backgroundColor': '#f8f9fa', 'borderRadius': '8px',
                'border': '1px solid #dee2e6', 'marginTop': '20px'
            })
            return {}, empty_fig, [], [], [], helpful_message, helpful_message

        blueprint_data = create_blueprint_list(long_df)

        scatter_df = pd.DataFrame(blueprint_data.get('tradeoffs', {}).get('scatter_data', []))
        fig_scatter = px.scatter(scatter_df, x='fairness_score', y='cost_score', hover_data=['date']) if not scatter_df.empty else go.Figure()

        rules_df = blueprint_data.get('rules_df', pd.DataFrame())
        rules_table_data = []

        if not rules_df.empty:
            if 'è©³ç´°ãƒ‡ãƒ¼ã‚¿' in rules_df.columns:
                def safe_json_serialize(x):
                    if isinstance(x, dict):
                        try:
                            # NumPyå‹ã‚’æ¨™æº–Pythonå‹ã«å¤‰æ›
                            clean_dict = {}
                            for k, v in x.items():
                                if hasattr(v, 'item'):  # NumPy scalar
                                    clean_dict[k] = v.item()
                                elif isinstance(v, (list, tuple)):
                                    clean_dict[k] = [item.item() if hasattr(item, 'item') else item for item in v]
                                else:
                                    clean_dict[k] = v
                            return json.dumps(clean_dict, ensure_ascii=False, indent=2)
                        except (TypeError, ValueError):
                            return str(x)
                    else:
                        return str(x)
                
                rules_df['è©³ç´°ãƒ‡ãƒ¼ã‚¿'] = rules_df['è©³ç´°ãƒ‡ãƒ¼ã‚¿'].apply(safe_json_serialize)
            rules_table_data = rules_df.to_dict('records')

        staff_scores_df = blueprint_data.get('staff_level_scores', pd.DataFrame())
        dropdown_options = [{'label': s, 'value': s} for s in staff_scores_df.index] if not staff_scores_df.empty else []

        facts_df = blueprint_data.get('facts_df', pd.DataFrame())
        facts_table_data = []
        facts_summary = "äº‹å®Ÿãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"
        
        # ğŸ” æ‹¡å¼µãƒ«ãƒ¼ãƒ«åˆ†æã®çµæœè¡¨ç¤º
        rule_stats = blueprint_data.get('rule_statistics', {})
        total_rules = rule_stats.get('total_rules', 0)
        high_conf_rules = rule_stats.get('high_confidence_rules', 0)
        
        enhanced_summary = html.Div([
            html.Div([
                html.H4("ğŸ¯ ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥åˆ†æçµæœ", style={'margin': '0 0 15px 0', 'color': '#1976d2'}),
                html.Div([
                    html.Div([
                        html.H3(str(total_rules), style={'margin': '0', 'color': '#2e7d32', 'fontSize': '2rem'}),
                        html.P("ç™ºè¦‹ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«", style={'margin': '0', 'fontSize': '0.9rem', 'color': '#666'})
                    ], style={'textAlign': 'center', 'padding': '15px', 'backgroundColor': '#e8f5e8', 
                             'borderRadius': '8px', 'border': '2px solid #2e7d32', 'flex': '1'}),
                    html.Div([
                        html.H3(str(high_conf_rules), style={'margin': '0', 'color': '#ff9800', 'fontSize': '2rem'}),
                        html.P("é«˜ä¿¡é ¼åº¦ãƒ«ãƒ¼ãƒ«", style={'margin': '0', 'fontSize': '0.9rem', 'color': '#666'})
                    ], style={'textAlign': 'center', 'padding': '15px', 'backgroundColor': '#fff3e0', 
                             'borderRadius': '8px', 'border': '2px solid #ff9800', 'flex': '1'}),
                    html.Div([
                        html.H3(f"{(high_conf_rules/total_rules*100):.1f}%" if total_rules > 0 else "0%", 
                               style={'margin': '0', 'color': '#1976d2', 'fontSize': '2rem'}),
                        html.P("ä¿¡é ¼åº¦ç‡", style={'margin': '0', 'fontSize': '0.9rem', 'color': '#666'})
                    ], style={'textAlign': 'center', 'padding': '15px', 'backgroundColor': '#e3f2fd', 
                             'borderRadius': '8px', 'border': '2px solid #1976d2', 'flex': '1'}),
                ], style={'display': 'flex', 'gap': '15px'}),
            ], style={'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '12px', 
                     'boxShadow': '0 4px 8px rgba(0,0,0,0.1)', 'marginBottom': '20px'}),
        ]) if blueprint_data.get('enhanced_rules') else html.Div([
            html.Div([
                html.H4("âš ï¸ æ‹¡å¼µåˆ†æãƒ‡ãƒ¼ã‚¿", style={'color': '#ff9800'}),
                html.P("æ‹¡å¼µåˆ†æãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿é‡ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", 
                       style={'color': '#666', 'marginBottom': '10px'}),
                html.P("ğŸ’¡ æ”¹å–„æ–¹æ³•:", style={'fontWeight': 'bold', 'marginBottom': '5px'}),
                html.Ul([
                    html.Li("ã‚ˆã‚Šå¤šãã®ã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹"),
                    html.Li("ç•°ãªã‚‹æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹"),
                    html.Li("ã‚¹ã‚¿ãƒƒãƒ•æ•°ã‚’å¢—ã‚„ã™"),
                ])
            ], style={'padding': '15px', 'backgroundColor': '#fff3e0', 'borderRadius': '8px',
                     'border': '1px solid #ff9800'})
        ])
        
        # ğŸ” æ‹¡å¼µãƒ«ãƒ¼ãƒ«ã®è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        enhanced_table_data = []
        if blueprint_data.get('enhanced_rules'):
            for rule in blueprint_data.get('enhanced_rules', []):
                enhanced_table_data.append({
                    'ã‚¹ã‚¿ãƒƒãƒ•': rule.staff_name,
                    'ãƒ«ãƒ¼ãƒ«ã‚¿ã‚¤ãƒ—': rule.rule_type,
                    'ãƒ«ãƒ¼ãƒ«å†…å®¹': rule.rule_description,
                    'ä¿¡é ¼åº¦': f"{rule.confidence_score:.2f}",
                    'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ': rule.segment,
                    'çµ±è¨ˆçš„è¨¼æ‹ ': str(rule.statistical_evidence.get('sample_size', 'N/A'))
                })

        if not facts_df.empty:
            facts_df = facts_df.sort_values('ç¢ºä¿¡åº¦', ascending=False)
            facts_table_data = facts_df.to_dict('records')

            total_facts = len(facts_df)
            high_confidence_facts = len(facts_df[facts_df['ç¢ºä¿¡åº¦'] >= 0.8])
            unique_staff = facts_df['ã‚¹ã‚¿ãƒƒãƒ•'].nunique()

            facts_summary = html.Div([
                html.Div([
                    html.H4("ğŸ“Š äº‹å®Ÿåˆ†æã‚µãƒãƒªãƒ¼", style={'margin': '0 0 15px 0', 'color': '#1976d2'}),
                    html.Div([
                        html.Div([
                            html.H3(str(total_facts), style={'margin': '0', 'color': '#2e7d32', 'fontSize': '2rem'}),
                            html.P("ç™ºè¦‹ã•ã‚ŒãŸäº‹å®Ÿ", style={'margin': '0', 'fontSize': '0.9rem', 'color': '#666'})
                        ], style={'textAlign': 'center', 'padding': '15px', 'backgroundColor': '#e8f5e8', 
                                 'borderRadius': '8px', 'border': '2px solid #2e7d32'}),
                        html.Div([
                            html.H3(str(high_confidence_facts), style={'margin': '0', 'color': '#ff9800', 'fontSize': '2rem'}),
                            html.P("é«˜ç¢ºä¿¡åº¦(80%+)", style={'margin': '0', 'fontSize': '0.9rem', 'color': '#666'})
                        ], style={'textAlign': 'center', 'padding': '15px', 'backgroundColor': '#fff3e0', 
                                 'borderRadius': '8px', 'border': '2px solid #ff9800'}),
                        html.Div([
                            html.H3(str(unique_staff), style={'margin': '0', 'color': '#1976d2', 'fontSize': '2rem'}),
                            html.P("åˆ†æå¯¾è±¡ã‚¹ã‚¿ãƒƒãƒ•", style={'margin': '0', 'fontSize': '0.9rem', 'color': '#666'})
                        ], style={'textAlign': 'center', 'padding': '15px', 'backgroundColor': '#e3f2fd', 
                                 'borderRadius': '8px', 'border': '2px solid #1976d2'}),
                    ], style={'display': 'flex', 'gap': '15px', 'marginBottom': '20px'}),
                ], style={'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '12px', 
                         'boxShadow': '0 4px 8px rgba(0,0,0,0.1)', 'marginBottom': '20px'}),
                
                html.Div([
                    html.H5("ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªãƒ¼åˆ¥å†…è¨³", style={'marginBottom': '15px', 'color': '#1976d2'}),
                    html.Div([
                        html.Div([
                            html.Strong(f"{cat}: "),
                            html.Span(f"{len(df)}ä»¶", style={'color': '#2e7d32', 'fontWeight': 'bold'})
                        ], style={'padding': '8px 12px', 'backgroundColor': '#f5f5f5', 'borderRadius': '6px',
                                 'margin': '5px', 'display': 'inline-block', 'border': '1px solid #ddd'})
                        for cat, df in blueprint_data.get('facts_by_category', {}).items()
                        if not df.empty
                    ])
                ], style={'padding': '15px', 'backgroundColor': '#fafafa', 'borderRadius': '8px',
                         'border': '1px solid #e0e0e0'})
            ])

        # ğŸ” æ‹¡å¼µåˆ†æã¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æã®çµ±åˆè¡¨ç¤º
        segment_analysis = blueprint_data.get('segment_analysis', {})
        constraint_nature = blueprint_data.get('constraint_nature', {})
        advanced_constraints = blueprint_data.get('advanced_constraints', {})
        team_dynamics = blueprint_data.get('team_dynamics', {})
        
        integrated_content = html.Div([
            html.H5("ğŸ¯ ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥åˆ†æçµæœ"),
            enhanced_summary
        ])

        # Store data for other callbacks
        store_data = {
            'rules_df': rules_df.to_json(orient='split') if not rules_df.empty else None,
            'scored_df': blueprint_data.get('scored_df', pd.DataFrame()).to_json(orient='split') if blueprint_data.get('scored_df') is not None and not blueprint_data.get('scored_df').empty else None,
            'tradeoffs': blueprint_data.get('tradeoffs', {}),
            'staff_level_scores': blueprint_data.get('staff_level_scores', pd.DataFrame()).to_json(orient='split') if blueprint_data.get('staff_level_scores') is not None and not blueprint_data.get('staff_level_scores').empty else None,
            'facts_df': facts_df.to_json(orient='split') if not facts_df.empty else None,
            'facts_by_category': {k: v.to_json(orient='split') for k, v in blueprint_data.get('facts_by_category', {}).items()}
        }

        # ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        try:
            if CURRENT_SCENARIO_DIR and CURRENT_SCENARIO_DIR.exists():
                report_file = create_dashboard_analysis_report(CURRENT_SCENARIO_DIR, analysis_type="BLUEPRINT")
                if report_file:
                    log.info(f"[dash_app] ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file.name}")
                else:
                    log.warning("[dash_app] ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
        except Exception as e_report:
            log.error(f"[dash_app] ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e_report}")

        return store_data, fig_scatter, rules_table_data, dropdown_options, facts_table_data, facts_summary, integrated_content
    
    except Exception as e:
        log.error(f"ãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}", exc_info=True)
        empty_fig = go.Figure()
        error_msg = f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        return {}, empty_fig, [], [], [], error_msg, error_msg
@app.callback(
    Output('facts-data-table', 'data', allow_duplicate=True),
    Input('fact-category-filter', 'value'),
    State('blueprint-results-store', 'data'),
    prevent_initial_call=True
)
@safe_callback
def filter_facts_by_category(selected_category, stored_data):
    """ã‚«ãƒ†ã‚´ãƒªãƒ¼ã§äº‹å®Ÿã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
    if not stored_data or not stored_data.get('facts_df'):
        return []

    facts_df = pd.read_json(stored_data['facts_df'], orient='split')

    if selected_category == 'all':
        filtered_df = facts_df
    else:
        filtered_df = facts_df[facts_df['ã‚«ãƒ†ã‚´ãƒªãƒ¼'] == selected_category]

    filtered_df = filtered_df.sort_values('ç¢ºä¿¡åº¦', ascending=False)

    return filtered_df.to_dict('records')


def _extract_staff_from_rule(rule_text: str, staff_names: list[str]) -> str | None:
    """Return first staff name found in rule text."""
    for name in staff_names:
        if name in rule_text:
            return name
    return None


@app.callback(
    Output('staff-radar-chart', 'figure'),
    Output('staff-related-rules-list', 'children'),
    Input('staff-selector-dropdown', 'value'),
    Input('rules-data-table', 'selected_rows'),
    State('blueprint-results-store', 'data'),
    State('rules-data-table', 'data'),
    prevent_initial_call=True,
)
@safe_callback
def update_staff_view(selected_staff, selected_row_indices, stored_data, table_data):
    if not stored_data:
        raise PreventUpdate

    rules_json = stored_data.get('rules_df')
    staff_json = stored_data.get('staff_level_scores')
    if not rules_json or not staff_json:
        return go.Figure(), "ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

    rules_df = pd.read_json(rules_json, orient='split')
    staff_scores_df = pd.read_json(staff_json, orient='split')

    ctx = dash.callback_context
    trigger_id = ctx.triggered[0]['prop_id'].split('.')[0]

    target_staff = selected_staff
    if trigger_id == 'rules-data-table' and selected_row_indices:
        clicked_rule = table_data[selected_row_indices[0]]
        target_staff = _extract_staff_from_rule(clicked_rule.get('ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡', ''), list(staff_scores_df.index))

    if not target_staff or target_staff not in staff_scores_df.index:
        return go.Figure(), "ã‚¹ã‚¿ãƒƒãƒ•ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"

    row = staff_scores_df.loc[target_staff]
    score_cols = ['fairness_score', 'cost_score', 'risk_score', 'satisfaction_score']
    fig_radar = go.Figure()
    fig_radar.add_trace(
        go.Scatterpolar(
            r=row[score_cols].tolist(),
            theta=['å…¬å¹³æ€§', 'ã‚³ã‚¹ãƒˆ', 'ãƒªã‚¹ã‚¯', 'æº€è¶³åº¦'],
            fill='toself',
            name=target_staff,
        )
    )
    fig_radar.update_layout(polar=dict(radialaxis=dict(range=[0, 1])), showlegend=False)

    related_rules = rules_df[rules_df['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'].str.contains(target_staff)]
    rule_list_items = [html.P(r) for r in related_rules['ç™ºè¦‹ã•ã‚ŒãŸæ³•å‰‡'].tolist()] if not related_rules.empty else [html.P('é–¢é€£ãƒ«ãƒ¼ãƒ«ãªã—')]

    return fig_radar, rule_list_items


@app.callback(
    Output('sim-shortage-graph', 'figure'),
    Output('sim-cost-text', 'children'),
    Input('sim-work-pattern-dropdown', 'value'),
    Input('sim-hire-fte-slider', 'value'),
    State('kpi-data-store', 'data'),
)
@safe_callback
def update_hire_simulation(selected_pattern, added_fte, kpi_data):
    if not kpi_data or not selected_pattern:
        raise PreventUpdate

    from shift_suite.tasks.h2hire import (
        AVG_HOURLY_WAGE,
        RECRUIT_COST_PER_HIRE,
    )

    df_work_patterns = data_get('work_patterns', pd.DataFrame())
    df_shortage_role = data_get('shortage_role_summary', pd.DataFrame()).copy()

    pattern_info = df_work_patterns[df_work_patterns['code'] == selected_pattern]
    if pattern_info.empty:
        raise PreventUpdate
    slots_per_day = pattern_info['parsed_slots_count'].iloc[0]
    hours_per_day = slots_per_day * SLOT_HOURS
    reduction_hours = added_fte * hours_per_day * 20

    if not df_shortage_role.empty:
        most_lacking_role_index = df_shortage_role['lack_h'].idxmax()
        original_hours = df_shortage_role.loc[most_lacking_role_index, 'lack_h']
        df_shortage_role.loc[most_lacking_role_index, 'lack_h'] = max(0, original_hours - reduction_hours)

    fig = px.bar(
        df_shortage_role,
        x='role',
        y='lack_h',
        title=f'ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¾Œ: {selected_pattern}å‹¤å‹™è€…ã‚’{added_fte}äººè¿½åŠ æ¡ç”¨ã—ãŸå ´åˆã®æ®‹å­˜ä¸è¶³æ™‚é–“',
        labels={'lack_h': 'æ®‹å­˜ä¸è¶³æ™‚é–“(h)'},
    )

    # æ­£ã—ã„ç·ä¸è¶³æ™‚é–“ã®è¨ˆç®—
    new_total_lack_h = 0
    if 'lack_h' in df_shortage_role.columns:
        total_rows = df_shortage_role[df_shortage_role['role'].isin(['å…¨ä½“', 'åˆè¨ˆ', 'ç·è¨ˆ'])]
        if not total_rows.empty:
            new_total_lack_h = total_rows['lack_h'].iloc[0]
        else:
            # shortage_timeã‹ã‚‰ç›´æ¥è¨ˆç®—ã™ã‚‹å ´åˆã¯æŒ‰åˆ†æ–¹å¼ã¨æ•´åˆæ€§ã‚’ä¿ã¤
            shortage_time_df = data_get('shortage_time', pd.DataFrame())
            if not shortage_time_df.empty:
                try:
                    # æŒ‰åˆ†æ–¹å¼ã¨ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã€è·ç¨®åˆ¥åˆè¨ˆã‚’å„ªå…ˆ
                    if not df_shortage_role.empty:
                        role_only_df = df_shortage_role[
                            ~df_shortage_role['role'].isin(['å…¨ä½“', 'åˆè¨ˆ', 'ç·è¨ˆ']) &
                            ~df_shortage_role['role'].str.startswith('emp_', na=False)
                        ]
                        if not role_only_df.empty:
                            new_total_lack_h = role_only_df['lack_h'].sum()
                            log.info(f"ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: æŒ‰åˆ†æ–¹å¼è·ç¨®åˆ¥åˆè¨ˆã«ã‚ˆã‚‹ä¸è¶³æ™‚é–“: {new_total_lack_h:.2f}h")
                        else:
                            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: shortage_timeã‹ã‚‰è¨ˆç®—ï¼ˆæŒ‰åˆ†ä¿‚æ•°é©ç”¨ï¼‰
                            shortage_values = shortage_time_df.select_dtypes(include=[np.number]).values
                            raw_shortage_hours = float(np.nansum(shortage_values)) * SLOT_HOURS
                            new_total_lack_h = raw_shortage_hours  # æ­£ã—ã„ä¸è¶³æ™‚é–“ã‚’ä½¿ç”¨
                            log.info(f"ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: shortage_timeã‹ã‚‰æ­£å¸¸è¨ˆç®—: {new_total_lack_h:.2f}h")
                    else:
                        new_total_lack_h = df_shortage_role['lack_h'].sum()
                except Exception as e:
                    log.error(f"ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ shortage_timeè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    new_total_lack_h = df_shortage_role['lack_h'].sum()
            else:
                new_total_lack_h = df_shortage_role['lack_h'].sum()
    
    original_total_lack_h = kpi_data.get('total_lack_h', 0)

    cost_before = original_total_lack_h * WAGE_RATES["temporary_staff"]
    cost_after_temp = new_total_lack_h * WAGE_RATES["temporary_staff"]

    added_labor_cost = reduction_hours * WAGE_RATES["average_hourly_wage"]
    added_recruit_cost = added_fte * COST_PARAMETERS["recruit_cost_per_hire"]
    cost_after_hire = cost_after_temp + added_labor_cost + added_recruit_cost

    cost_text = f"""
    #### ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
    - **æ¡ç”¨ã‚³ã‚¹ãƒˆ:** {added_recruit_cost:,.0f} å†† (ä¸€æ™‚)
    - **è¿½åŠ äººä»¶è²»:** {added_labor_cost:,.0f} å†† (æœŸé–“ä¸­)
    - **ç·ã‚³ã‚¹ãƒˆ (æ¡ç”¨ã‚·ãƒŠãƒªã‚ª):** {cost_after_hire:,.0f} å††
    - **æ¯”è¼ƒ (å…¨ã¦æ´¾é£ã§è£œå¡«ã—ãŸå ´åˆ):** {cost_before:,.0f} å††
    """

    return fig, dcc.Markdown(cost_text)


@app.callback(
    Output('factor-output', 'children'),
    Input('factor-train-button', 'n_clicks')
)
@safe_callback
def run_factor_analysis(n_clicks):
    if not n_clicks:
        raise PreventUpdate

    heat_df = data_get('heat_ALL')
    short_df = data_get('shortage_time')
    leave_df = data_get('leave_analysis')

    if heat_df is None or heat_df.empty or short_df is None or short_df.empty:
        return html.Div('å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“')

    analyzer = ShortageFactorAnalyzer()
    feat_df = analyzer.generate_features(pd.DataFrame(), heat_df, short_df, leave_df, set())
    model, fi_df = analyzer.train_and_get_feature_importance(feat_df)
    DATA_CACHE.set('factor_features', feat_df)
    DATA_CACHE.set('factor_importance', fi_df)

    table = dash_table.DataTable(
        data=fi_df.head(5).to_dict('records'),
        columns=[{'name': c, 'id': c} for c in fi_df.columns]
    )
    return html.Div([html.H5('å½±éŸ¿åº¦ã®é«˜ã„è¦å›  ãƒˆãƒƒãƒ—5'), table])  # type: ignore


def generate_lightweight_tree_visualization(tree_model):
    """Generate a small decision tree visualisation."""
    if not tree_model or not hasattr(tree_model, 'tree_'):
        return html.P('æ±ºå®šæœ¨ãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚')

    try:
        buf = io.BytesIO()
        fig, ax = plt.subplots(figsize=(12, 6))
        plot_tree(
            tree_model,
            filled=True,
            feature_names=tree_model.feature_names_in_[:20],
            max_depth=2,
            fontsize=8,
            ax=ax,
            impurity=False,
            proportion=True,
        )
        fig.savefig(buf, format='png', dpi=72, bbox_inches='tight')
        plt.close(fig)
        buf.seek(0)
        encoded = base64.b64encode(buf.getvalue()).decode()
        return html.Img(
            src=f"data:image/png;base64,{encoded}",
            style={'width': '100%', 'maxWidth': '1000px'},
        )
    except Exception as exc:  # noqa: BLE001
        log.error(f'æ±ºå®šæœ¨å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {exc}')
        return html.P(f'æ±ºå®šæœ¨ã®å¯è¦–åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}')


def generate_results_display(full_results):
    """Create the final display for logic analysis results."""
    mind_results = full_results.get('mind_reading', {})

    if 'error' in mind_results:
        return html.Div(f"åˆ†æã‚¨ãƒ©ãƒ¼: {mind_results['error']}", style={'color': 'red'})

    importance_df = pd.DataFrame(mind_results.get('feature_importance', []))
    fig_bar = px.bar(
        importance_df.sort_values('importance', ascending=False).head(15),
        x='importance',
        y='feature',
        orientation='h',
        title='åˆ¤æ–­åŸºæº–ã®é‡è¦åº¦ï¼ˆTOP15ï¼‰',
    )

    tree_content = generate_lightweight_tree_visualization(
        mind_results.get('thinking_process_tree')
    )

    return html.Div([
        html.H4('åˆ†æå®Œäº†ï¼'),
        html.Hr(),
        html.H4('åˆ¤æ–­åŸºæº–ã®é‡è¦åº¦'),
        html.P('ä½œæˆè€…ãŒã©ã®è¦ç´ ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ã‚’æ•°å€¤åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚'),
        dcc.Graph(figure=fig_bar),
        html.H4('æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ', style={'marginTop': '30px'}),
        html.P('é…ç½®ã‚’æ±ºå®šã™ã‚‹éš›ã®æ€è€ƒã®åˆ†å²ã‚’æ¨¡å€£ã—ãŸã‚‚ã®ã§ã™ã€‚'),
        tree_content,
    ])


@app.callback(
    Output('save-log-msg', 'children'),
    Input('save-log-button', 'n_clicks'),
    State('over-shortage-table', 'data'),
    State('log-save-mode', 'value')
)
@safe_callback
def save_over_shortage_log(n_clicks, table_data, mode):
    if not n_clicks:
        raise PreventUpdate

    log_path = data_get('shortage_log_path')
    if not log_path:
        return 'ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'  # type: ignore

    df = pd.DataFrame(table_data)
    over_shortage_log.save_log(df, log_path, mode=mode)
    return 'ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ'


@app.callback(Output('log-viewer', 'value'), Input('log-interval', 'n_intervals'))
@safe_callback
def update_log_viewer(n):
    """ãƒ­ã‚°ãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’å®šæœŸçš„ã«æ›´æ–°"""
    log_stream.seek(0)
    return log_stream.read()


@app.callback(
    Output('creation-logic-results', 'children'),
    Output('full-analysis-store', 'data'),
    Input('analyze-creation-logic-button', 'n_clicks'),
    State('analysis-detail-level', 'value'),
    prevent_initial_call=True,
)
@safe_callback
def update_logic_analysis_immediate(n_clicks, detail_level):
    """Show basic results immediately and start deep analysis."""

    if not n_clicks:
        raise PreventUpdate

    long_df = data_get('long_df', pd.DataFrame())
    if long_df.empty:
        return html.Div('åˆ†æãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚', style={'color': 'red'}), None

    basic_stats = get_basic_shift_stats(long_df)
    quick_patterns = get_quick_patterns(long_df.head(500))

    immediate_results = html.Div([
        html.H4('âœ… åŸºæœ¬åˆ†æå®Œäº†ï¼ˆè©³ç´°åˆ†æå®Ÿè¡Œä¸­...ï¼‰', style={'color': 'green'}),
        html.Hr(),
        html.Div([
            html.H5('ğŸ“Š ã‚·ãƒ•ãƒˆã®åŸºæœ¬çµ±è¨ˆ'),
            create_stats_cards(basic_stats),
        ]),
        html.Div([
            html.H5('ğŸ” ç™ºè¦‹ã•ã‚ŒãŸä¸»è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆç°¡æ˜“ç‰ˆï¼‰'),
            create_pattern_list(quick_patterns),
        ], style={'marginTop': '20px'}),
        html.Div([
            html.H5('ğŸ§  AIã«ã‚ˆã‚‹æ·±å±¤åˆ†æ'),
            dcc.Loading(id='deep-analysis-loading', children=html.Div(id='deep-analysis-results'), type='circle'),
        ], style={'marginTop': '30px'}),
        # ğŸ¯ ä¿®æ­£: ç„¡é™ãƒ­ã‚°å•é¡Œå¯¾ç­– - 100msé–“éš”ã‚’5ç§’é–“éš”ã«å¤‰æ›´
        dcc.Interval(id='background-trigger', interval=5000, n_intervals=0, max_intervals=1),
    ])

    return immediate_results, {'status': 'pending', 'level': detail_level}


@app.callback(
    Output('deep-analysis-results', 'children'),
    Input('background-trigger', 'n_intervals'),
    State('analysis-detail-level', 'value'),
    prevent_initial_call=True,
)
@safe_callback
def run_deep_analysis_background(n_intervals, detail_level):
    """Run deeper analysis in the background."""
    
    # ğŸ¯ ä¿®æ­£: ç„¡é™ãƒ­ã‚°å¯¾ç­– - 1å›ã®ã¿å®Ÿè¡Œã§çµ‚äº†
    if n_intervals == 0 or n_intervals > 1:
        raise PreventUpdate

    long_df = data_get('long_df', pd.DataFrame())
    results = run_optimized_analysis(long_df, detail_level)

    return create_deep_analysis_display(results)


@app.callback(
    Output('progress-bar', 'figure'),
    Output('progress-message', 'children'),
    Input('logic-analysis-interval', 'n_intervals'),
    State('logic-analysis-progress', 'data'),
    prevent_initial_call=True,
)
@safe_callback
def update_progress_bar(n_intervals, progress_data):
    """Update the progress bar display."""
    if not progress_data:
        raise PreventUpdate

    progress = progress_data.get('progress', 0)
    stage = progress_data.get('stage', 'loading')

    messages = {
        'loading': 'ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...',
        'analyzing': 'ã‚·ãƒ•ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¦ã„ã¾ã™...',
        'visualizing': 'çµæœã‚’å¯è¦–åŒ–ã—ã¦ã„ã¾ã™...',
    }

    figure = {
        'data': [{
            'x': [progress],
            'y': ['Progress'],
            'type': 'bar',
            'orientation': 'h',
            'marker': {'color': '#1f77b4'},
        }],
        'layout': {
            'xaxis': {'range': [0, 100], 'title': 'é€²æ—ç‡ (%)'},
            'yaxis': {'visible': False},
            'height': 100,
            'margin': {'l': 0, 'r': 0, 't': 30, 'b': 30},
        },
    }

    return figure, messages.get(stage, 'å‡¦ç†ä¸­...')


# ğŸ§  AIåˆ†æã‚¿ãƒ–ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
@app.callback(
    Output('ai-analysis-content', 'children'),
    Input('ai-analysis-tab-container', 'style'),
    State('scenario-dropdown', 'value'),
    State('data-loaded', 'data'),
)
@safe_callback
def initialize_ai_analysis_content(style, selected_scenario, data_status):
    """AIåˆ†æã‚¿ãƒ–ã®å†…å®¹ã‚’åˆæœŸåŒ–"""
    if not selected_scenario or not data_status or style.get('display') == 'none':
        raise PreventUpdate
    try:
        return create_ai_analysis_tab()
    except Exception as e:
        log.error(f"AIåˆ†æã‚¿ãƒ–ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return html.Div(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", style={'color': 'red'})


def create_ai_analysis_tab() -> html.Div:
    """Mind Readeråˆ†æã‚¿ãƒ–ã‚’ä½œæˆï¼ˆapp.pyçµ±ä¸€ä»•æ§˜ï¼‰"""
    content = [
        html.H3("Mind Readeråˆ†æ", style={'marginBottom': '20px', 'color': '#2c3e50'}),
        
        # ã‚µãƒãƒªãƒ¼ãƒœãƒƒã‚¯ã‚¹
        html.Div(id='ai-analysis-summary', style={
            'padding': '20px',
            'backgroundColor': '#f8f9fa',
            'borderRadius': '10px',
            'marginBottom': '20px',
            'border': '2px solid #e9ecef',
            'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'
        }),
    ]
    
    # Mind Readeråˆ†æçµæœã‚’å–å¾—
    mind_results = data_get('mind_reader_analysis', {})
    advanced_results = data_get('advanced_analysis', {})
    
    if mind_results:
        content.extend(create_mind_reader_display(mind_results))
    else:
        # Mind Readeråˆ†æã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œ
        content.append(html.Div([
            html.H4("ğŸ”„ AIåˆ†æã‚’å®Ÿè¡Œä¸­..."),
            dcc.Loading(
                id="ai-analysis-loading",
                type="circle",
                children=[
                    html.Div(id='mind-reader-results'),
                    dcc.Interval(
                        id='ai-analysis-interval',
                        interval=2000,  # 2ç§’é–“éš”
                        n_intervals=0,
                        max_intervals=1
                    )
                ]
            )
        ], style={
            'padding': '20px',
            'backgroundColor': '#e3f2fd',
            'borderRadius': '8px',
            'textAlign': 'center'
        }))
    
    # é«˜åº¦åˆ†æçµæœè¡¨ç¤º
    if advanced_results:
        content.append(create_advanced_analysis_display(advanced_results))
    
    return html.Div(content)


def create_mind_reader_display(mind_results: dict) -> list:
    """Mind Readeråˆ†æçµæœã®è¡¨ç¤ºã‚’ä½œæˆ"""
    display_content = []
    
    # ğŸ¯ æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹åˆ†æ
    if 'decision_points' in mind_results:
        decision_points = mind_results['decision_points']
        
        display_content.append(html.Div([
            html.H4("ğŸ¯ AIæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹åˆ†æ", style={'color': '#e74c3c'}),
            html.P(f"æ¤œå‡ºã•ã‚ŒãŸæ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆ: {len(decision_points)}å€‹", 
                   style={'fontSize': '16px', 'fontWeight': 'bold'}),
        ], style={
            'padding': '15px',
            'backgroundColor': '#fff5f5',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #feb2b2'
        }))
        
        # æ±ºå®šè¦å› ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        if decision_points:
            feature_importance = []
            for dp in decision_points[:5]:  # ä¸Šä½5ã¤
                feature_importance.append({
                    'feature': dp.get('feature', 'Unknown'),
                    'importance': dp.get('importance', 0),
                    'reasoning': dp.get('reasoning', 'No explanation')
                })
            
            # é‡è¦åº¦ãƒãƒ£ãƒ¼ãƒˆ
            if feature_importance:
                fig_importance = go.Figure(data=[
                    go.Bar(
                        x=[f['importance'] for f in feature_importance],
                        y=[f['feature'] for f in feature_importance],
                        orientation='h',
                        marker_color='#e74c3c'
                    )
                ])
                fig_importance.update_layout(
                    title='ğŸ” æ±ºå®šè¦å› ã®é‡è¦åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°',
                    xaxis_title='é‡è¦åº¦ã‚¹ã‚³ã‚¢',
                    yaxis_title='è¦å› ',
                    height=400
                )
                display_content.append(dcc.Graph(figure=fig_importance))
    
    # ğŸ“Š ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜çµæœ
    if 'patterns' in mind_results:
        patterns = mind_results['patterns']
        display_content.append(html.Div([
            html.H4("ğŸ“Š ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³", style={'color': '#3498db'}),
            html.Ul([
                html.Li(f"{pattern.get('type', 'Unknown')}: {pattern.get('description', 'No description')}")
                for pattern in patterns[:10]  # ä¸Šä½10ãƒ‘ã‚¿ãƒ¼ãƒ³
            ])
        ], style={
            'padding': '15px',
            'backgroundColor': '#f0f8ff',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #b3d9ff'
        }))
    
    # ğŸ’¡ æ”¹å–„ææ¡ˆ
    if 'recommendations' in mind_results:
        recommendations = mind_results['recommendations']
        display_content.append(html.Div([
            html.H4("ğŸ’¡ AIæ”¹å–„ææ¡ˆ", style={'color': '#27ae60'}),
            html.Ol([
                html.Li([
                    html.Strong(rec.get('title', 'Recommendation')),
                    html.P(rec.get('description', 'No description'))
                ])
                for rec in recommendations[:5]  # ä¸Šä½5ææ¡ˆ
            ])
        ], style={
            'padding': '15px',
            'backgroundColor': '#f0fff0',
            'borderRadius': '8px',
            'marginBottom': '20px',
            'border': '1px solid #90ee90'
        }))
    
    return display_content


def create_advanced_analysis_display(advanced_results: dict) -> html.Div:
    """é«˜åº¦åˆ†æçµæœã®è¡¨ç¤ºã‚’ä½œæˆ"""
    return html.Div([
        html.H4("ğŸš€ é«˜åº¦åˆ†æã‚µãƒãƒªãƒ¼", style={'color': '#9b59b6'}),
        html.P(f"èª­ã¿è¾¼ã¿æ™‚åˆ»: {advanced_results.get('timestamp', 'Unknown')}"),
        html.P(f"ã‚½ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {advanced_results.get('source_dir', 'Unknown')}"),
        html.P(f"åˆ©ç”¨å¯èƒ½ãªåˆ†æé …ç›®: {len(advanced_results) - 3}å€‹"),
    ], style={
        'padding': '15px',
        'backgroundColor': '#faf0ff',
        'borderRadius': '8px',
        'marginTop': '20px',
        'border': '1px solid #d1a7d1'
    })


# Mind Readeråˆ†æã‚’å‹•çš„å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
@app.callback(
    Output('mind-reader-results', 'children'),
    Input('ai-analysis-interval', 'n_intervals'),
    prevent_initial_call=True
)
@safe_callback
def execute_mind_reader_analysis(n_intervals):
    """Mind Readeråˆ†æã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œ"""
    if n_intervals == 0:
        raise PreventUpdate
    
    try:
        # Mind Readeråˆ†æã‚’å®Ÿè¡Œ
        mind_results = data_get('mind_reader_analysis', {})
        
        if mind_results:
            return html.Div([
                html.H4("âœ… AIåˆ†æå®Œäº†", style={'color': '#27ae60'}),
                *create_mind_reader_display(mind_results)
            ])
        else:
            return html.Div([
                html.H4("âš ï¸ åˆ†æãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™", style={'color': '#f39c12'}),
                html.P("ã‚ˆã‚Šè©³ç´°ãªåˆ†æã‚’è¡Œã†ã«ã¯ã€ååˆ†ãªã‚·ãƒ•ãƒˆãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™ã€‚")
            ])
    
    except Exception as e:
        return html.Div([
            html.H4("âŒ åˆ†æã‚¨ãƒ©ãƒ¼", style={'color': '#e74c3c'}),
            html.P(f"Error: {str(e)}")
        ])


# --- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹• ---
if __name__ == '__main__':
    import os
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: ç’°å¢ƒå¤‰æ•°ã§ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’åˆ¶å¾¡ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç„¡åŠ¹ï¼‰
    debug_mode = os.environ.get('DASH_DEBUG', 'False').lower() == 'true'
    port = int(os.environ.get('DASH_PORT', '8080'))
    host = os.environ.get('DASH_HOST', '127.0.0.1')
    
    if debug_mode:
        log.warning("âš ï¸ ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ç„¡åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚")
    
    app.run(debug=debug_mode, port=port, host=host)
