"""
ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥ã‚’æ·±ãåˆ†æã™ã‚‹æ‹¡å¼µãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã¨åˆ¤æ–­åŸºæº–ã‚’é€†ç®—ãƒ»å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã€
ä»¥ä¸‹ã®ã‚ˆã†ãªã€Œä½œæˆãƒ«ãƒ¼ãƒ«ã€ã‚’çµ±è¨ˆçš„ãƒ»è«–ç†çš„ã«ç™ºè¦‹ãƒ»å¯è¦–åŒ–ã—ã¾ã™ï¼š

1. å€‹äººãƒ¬ãƒ™ãƒ«ã®ãƒ«ãƒ¼ãƒ«:
   - é€±â—‹æ—¥å‹¤å‹™åˆ¶é™ (ä¾‹: â—‹â—‹ã•ã‚“ã¯é€±3æ—¥ã¾ã§å‹¤å‹™)
   - æ›œæ—¥åˆ¶é™ (ä¾‹: â—‹â—‹ã•ã‚“ã¯ç«æ›œã€æ°´æ›œã€æ—¥æ›œã—ã‹å‹¤å‹™ã—ã¦ã„ãªã„)
   - å‹¤å‹™åŒºåˆ†åˆ¶é™ (ä¾‹: â—‹â—‹ã•ã‚“ã¯ã“ã®å‹¤å‹™åŒºåˆ†è¨˜å·ã—ã‹å¯¾å¿œã—ã¦ã„ãªã„)
   - æ™‚é–“å¸¯åˆ¶é™ (ä¾‹: â—‹â—‹ã•ã‚“ã¯åˆå‰ä¸­ã®ã¿ã€å¤œå‹¤ãªã—)

2. ãƒšã‚¢ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ«ãƒ¼ãƒ«:
   - çµ„ã¿åˆã‚ã›ç¦æ­¢ (ä¾‹: â—‹â—‹ã•ã‚“ã¨â–³ã•ã‚“ã¯çµ„ã¿åˆã‚ã›ã¦å‹¤å‹™ã—ã¦ã„ãªã„)
   - çµ„ã¿åˆã‚ã›å„ªé‡ (ä¾‹: â—‹â—‹ã•ã‚“ã¨â–³ã•ã‚“ã¯é »ç¹ã«çµ„ã¾ã‚Œã‚‹)

3. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ«ãƒ¼ãƒ«:
   - æ–½è¨­å…¨ä½“ã®ãƒ«ãƒ¼ãƒ«
   - å„è·ç¨®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå†…ã®ãƒ«ãƒ¼ãƒ«
   - å„å‹¤å‹™åŒºåˆ†ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå†…ã®ãƒ«ãƒ¼ãƒ«

4. çµ±è¨ˆçš„æš—é»™çŸ¥:
   - æœŸå¾…å€¤ã‹ã‚‰ã®æœ‰æ„ãªä¹–é›¢ãƒ‘ã‚¿ãƒ¼ãƒ³
   - æ™‚ç³»åˆ—ã§ã®è¡Œå‹•å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
   - ä¾‹å¤–çš„çŠ¶æ³ã§ã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«

Author: Claude Code Assistant
Created: 2025-01-14
"""

from __future__ import annotations

import logging
from collections import defaultdict, Counter
from itertools import combinations
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass

import numpy as np
import pandas as pd
from scipy import stats
from .constants import STATISTICAL_THRESHOLDS

log = logging.getLogger(__name__)

# çµ±è¨ˆçš„åˆ†æã®é–¾å€¤è¨­å®š
STATISTICAL_CONFIDENCE = STATISTICAL_THRESHOLDS["confidence_level"]  # 95%ä¿¡é ¼åŒºé–“
MIN_SAMPLE_SIZE = STATISTICAL_THRESHOLDS["min_sample_size"]          # æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
SIGNIFICANT_DEVIATION = STATISTICAL_THRESHOLDS["significant_deviation"]    # æ¨™æº–åå·®ã®ä½•å€ã§æœ‰æ„ã¨ã™ã‚‹ã‹


@dataclass
class ShiftRule:
    """ç™ºè¦‹ã•ã‚ŒãŸã‚·ãƒ•ãƒˆä½œæˆãƒ«ãƒ¼ãƒ«ã‚’è¡¨ã™ã‚¯ãƒ©ã‚¹"""
    staff_name: str
    rule_type: str
    rule_description: str
    confidence_score: float
    statistical_evidence: Dict[str, Any]
    segment: str = "å…¨ä½“"  # å…¨ä½“ã€è·ç¨®åˆ¥ã€å‹¤å‹™åŒºåˆ†åˆ¥
    

class EnhancedBlueprintAnalyzer:
    """ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥ã‚’æ·±ãåˆ†æã™ã‚‹æ‹¡å¼µãƒ–ãƒ«ãƒ¼ãƒ—ãƒªãƒ³ãƒˆåˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.weekday_names = ['æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        self.discovered_rules: List[ShiftRule] = []
        
    def analyze_shift_creation_blueprint(self, long_df: pd.DataFrame) -> Dict[str, Any]:
        """
        ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥ã‚’åŒ…æ‹¬çš„ã«åˆ†æ
        
        Args:
            long_df: å‹¤å‹™ãƒ‡ãƒ¼ã‚¿ (ds, staff, role, code, parsed_slots_countç­‰ã‚’å«ã‚€)
            
        Returns:
            ç™ºè¦‹ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã¨åˆ†æçµæœã®è¾æ›¸
        """
        if long_df.empty:
            return {"rules": [], "segments": {}, "statistical_summary": {}}
            
        log.info("ğŸ” ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")
        
        # 1. å€‹äººãƒ¬ãƒ™ãƒ«ã®ãƒ«ãƒ¼ãƒ«ç™ºè¦‹
        personal_rules = self._discover_personal_rules(long_df)
        
        # 2. ãƒšã‚¢ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ«ãƒ¼ãƒ«ç™ºè¦‹  
        pair_rules = self._discover_pair_rules(long_df)
        
        # 3. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ«ãƒ¼ãƒ«ç™ºè¦‹
        segment_rules = self._discover_segment_rules(long_df)
        
        # 4. çµ±è¨ˆçš„æš—é»™çŸ¥ã®æŠ½å‡º
        statistical_insights = self._extract_statistical_insights(long_df)
        
        # 5. çµæœã®çµ±åˆã¨ä¿¡é ¼åº¦è¨ˆç®—
        all_rules = personal_rules + pair_rules + segment_rules
        validated_rules = self._validate_and_score_rules(all_rules, long_df)
        
        return {
            "rules": validated_rules,
            "segments": self._analyze_by_segments(long_df),
            "statistical_summary": statistical_insights,
            "rule_count": len(validated_rules),
            "high_confidence_rules": [r for r in validated_rules if r.confidence_score >= STATISTICAL_THRESHOLDS["high_confidence_threshold"]]
        }
    
    def _discover_personal_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """å€‹äººãƒ¬ãƒ™ãƒ«ã®ãƒ«ãƒ¼ãƒ«ç™ºè¦‹"""
        personal_rules = []
        
        # é€±â—‹æ—¥å‹¤å‹™åˆ¶é™ã®ç™ºè¦‹
        personal_rules.extend(self._discover_weekly_limit_rules(long_df))
        
        # æ›œæ—¥åˆ¶é™ã®ç™ºè¦‹
        personal_rules.extend(self._discover_weekday_restriction_rules(long_df))
        
        # å‹¤å‹™åŒºåˆ†åˆ¶é™ã®ç™ºè¦‹
        personal_rules.extend(self._discover_code_restriction_rules(long_df))
        
        # æ™‚é–“å¸¯åˆ¶é™ã®ç™ºè¦‹  
        personal_rules.extend(self._discover_time_restriction_rules(long_df))
        
        return personal_rules
    
    def _discover_weekly_limit_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """é€±â—‹æ—¥å‹¤å‹™åˆ¶é™ãƒ«ãƒ¼ãƒ«ã®ç™ºè¦‹ï¼ˆé«˜åº¦ãªåˆ¶ç´„æ€§è³ªåˆ¤åˆ¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        rules = []
        
        # ğŸ” é«˜åº¦ãªåˆ¶ç´„æ€§è³ªåˆ¤åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
        try:
            from .constraint_nature_analyzer import analyze_constraint_nature
            constraint_results = analyze_constraint_nature(long_df)
            weekly_constraints = constraint_results.get('weekly_constraints', [])
            
            for analysis in weekly_constraints:
                if analysis.confidence_score >= STATISTICAL_THRESHOLDS["correlation_threshold"]:  # 70%ä»¥ä¸Šã®ä¿¡é ¼åº¦
                    # åˆ¶ç´„ã®æ€§è³ªã«å¿œã˜ãŸãƒ«ãƒ¼ãƒ«èª¬æ˜ã‚’ç”Ÿæˆ
                    nature_desc = self._generate_constraint_description(analysis)
                    
                    rule = ShiftRule(
                        staff_name=analysis.staff_name,
                        rule_type=f"é€±å‹¤å‹™æ—¥æ•°{analysis.constraint_type.value}",
                        rule_description=nature_desc,
                        confidence_score=analysis.confidence_score,
                        statistical_evidence={
                            "constraint_nature": analysis.constraint_type.value,
                            "threshold_value": analysis.threshold_value,
                            "detailed_evidence": analysis.evidence,
                            "recommendations": analysis.recommendations
                        },
                        segment="å…¨ä½“"
                    )
                    rules.append(rule)
                    
        except ImportError:
            log.warning("é«˜åº¦åˆ¶ç´„åˆ¤åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚åŸºæœ¬åˆ†æã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®ç°¡æ˜“åˆ†æ
            rules = self._discover_weekly_limit_rules_fallback(long_df)
        
        return rules
    
    def _generate_constraint_description(self, analysis) -> str:
        """åˆ¶ç´„åˆ†æçµæœã‹ã‚‰é©åˆ‡ãªèª¬æ˜æ–‡ã‚’ç”Ÿæˆ"""
        staff = analysis.staff_name
        threshold = int(analysis.threshold_value)
        constraint_type = analysis.constraint_type
        confidence = analysis.confidence_score
        
        descriptions = {
            "ä¸Šé™åˆ¶ç´„": f"é€±{threshold}æ—¥ãŒä¸Šé™åˆ¶ç´„ï¼ˆè¶…éä¸å¯ãƒ»ä¿¡é ¼åº¦{confidence:.0%}ï¼‰",
            "ä¸‹é™å¸Œæœ›": f"é€±{threshold}æ—¥ãŒä¸‹é™å¸Œæœ›ï¼ˆæœ€ä½ç¢ºä¿ãƒ»ä¿¡é ¼åº¦{confidence:.0%}ï¼‰", 
            "å›ºå®šå¸Œæœ›": f"é€±{threshold}æ—¥ãŒå›ºå®šå¸Œæœ›ï¼ˆæœ€é©æ—¥æ•°ãƒ»ä¿¡é ¼åº¦{confidence:.0%}ï¼‰",
            "æŸ”è»Ÿãƒ‘ã‚¿ãƒ¼ãƒ³": f"æŸ”è»Ÿãªå‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¹³å‡{threshold:.1f}æ—¥ãƒ»ä¿¡é ¼åº¦{confidence:.0%}ï¼‰",
            "å­£ç¯€åˆ¶ç´„": f"å­£ç¯€çš„åˆ¶ç´„ã‚ã‚Šï¼ˆå¹³å‡{threshold:.1f}æ—¥ãƒ»ä¿¡é ¼åº¦{confidence:.0%}ï¼‰"
        }
        
        return descriptions.get(constraint_type.value, f"é€±{threshold}æ—¥å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¿¡é ¼åº¦{confidence:.0%}ï¼‰")
    
    def _discover_weekly_limit_rules_fallback(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """å¾“æ¥ã®ç°¡æ˜“åˆ†æï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰"""
        rules = []
        
        # é€±ã”ã¨ã®å‹¤å‹™æ—¥æ•°ã‚’è¨ˆç®—
        long_df['week'] = long_df['ds'].dt.isocalendar().week
        long_df['year'] = long_df['ds'].dt.year
        
        working_df = long_df[long_df['parsed_slots_count'] > 0]
        
        for staff in working_df['staff'].unique():
            staff_df = working_df[working_df['staff'] == staff]
            
            # é€±ã”ã¨ã®å‹¤å‹™æ—¥æ•°é›†è¨ˆ
            weekly_days = staff_df.groupby(['year', 'week'])['ds'].nunique()
            
            if len(weekly_days) < MIN_SAMPLE_SIZE:
                continue
                
            # çµ±è¨ˆçš„åˆ†æ
            mean_days = weekly_days.mean()
            std_days = weekly_days.std()
            max_days = weekly_days.max()
            
            # ä¸Šé™å€¤ã®ç‰¹å®šï¼ˆå¹³å‡ + 1Ïƒä»¥ä¸‹ã§95%ãŒåã¾ã‚‹ï¼‰
            upper_limit = mean_days + std_days
            consistent_limit = weekly_days.quantile(STATISTICAL_THRESHOLDS["quantile_95"])
            
            # ãƒ«ãƒ¼ãƒ«ã¨ã—ã¦æœ‰åŠ¹ã‹åˆ¤å®š
            if max_days <= consistent_limit and std_days < 1.0:  # ä¸€è²«æ€§ãŒã‚ã‚‹
                confidence = 1.0 - (std_days / mean_days) if mean_days > 0 else 0.0
                
                if confidence >= STATISTICAL_THRESHOLDS["correlation_threshold"]:  # 70%ä»¥ä¸Šã®ä¿¡é ¼åº¦
                    rule = ShiftRule(
                        staff_name=staff,
                        rule_type="é€±å‹¤å‹™æ—¥æ•°åˆ¶é™ï¼ˆåŸºæœ¬åˆ†æï¼‰",
                        rule_description=f"é€±{int(consistent_limit)}æ—¥ä»¥ä¸‹ã®å‹¤å‹™ã«åˆ¶é™ï¼ˆè©³ç´°åˆ†ææ¨å¥¨ï¼‰",
                        confidence_score=confidence,
                        statistical_evidence={
                            "mean_weekly_days": mean_days,
                            "max_weekly_days": max_days,
                            "consistency_limit": consistent_limit,
                            "standard_deviation": std_days,
                            "sample_weeks": len(weekly_days),
                            "note": "é«˜åº¦åˆ†æã‚¨ãƒ³ã‚¸ãƒ³æœªä½¿ç”¨"
                        }
                    )
                    rules.append(rule)
        
        return rules
    
    def _discover_weekday_restriction_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """æ›œæ—¥åˆ¶é™ãƒ«ãƒ¼ãƒ«ã®ç™ºè¦‹"""
        rules = []
        
        working_df = long_df[long_df['parsed_slots_count'] > 0]
        
        for staff in working_df['staff'].unique():
            staff_df = working_df[working_df['staff'] == staff]
            
            if len(staff_df) < MIN_SAMPLE_SIZE:
                continue
                
            # æ›œæ—¥åˆ¥å‹¤å‹™å›æ•°
            weekday_counts = staff_df.groupby(staff_df['ds'].dt.dayofweek).size()
            total_possible_days = staff_df['ds'].dt.date.nunique()
            
            # å‹¤å‹™ã—ã¦ã„ã‚‹æ›œæ—¥ã‚’ç‰¹å®š
            working_weekdays = weekday_counts[weekday_counts > 0].index.tolist()
            
            # åˆ¶é™ãƒ«ãƒ¼ãƒ«ã®åˆ¤å®š
            if len(working_weekdays) < 7:  # å…¨æ›œæ—¥å‹¤å‹™ã—ã¦ã„ãªã„
                working_weekday_names = [self.weekday_names[d] for d in working_weekdays]
                
                # çµ±è¨ˆçš„æ¤œå®š: å‹¤å‹™æ›œæ—¥ãŒæœ‰æ„ã«åã£ã¦ã„ã‚‹ã‹
                expected_freq = total_possible_days / 7
                chi2_stat, p_value = stats.chisquare(weekday_counts.reindex(range(7), fill_value=0))
                
                if p_value < STATISTICAL_THRESHOLDS["significance_alpha"]:  # æœ‰æ„ãªåã‚Š
                    confidence = 1.0 - p_value
                    
                    rule = ShiftRule(
                        staff_name=staff,
                        rule_type="æ›œæ—¥åˆ¶é™å‹¤å‹™", 
                        rule_description=f"{', '.join(working_weekday_names)}ã®ã¿å‹¤å‹™",
                        confidence_score=confidence,
                        statistical_evidence={
                            "working_weekdays": working_weekdays,
                            "weekday_counts": weekday_counts.to_dict(),
                            "chi2_statistic": chi2_stat,
                            "p_value": p_value,
                            "total_days": total_possible_days
                        }
                    )
                    rules.append(rule)
        
        return rules
    
    def _discover_code_restriction_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """å‹¤å‹™åŒºåˆ†åˆ¶é™ãƒ«ãƒ¼ãƒ«ã®ç™ºè¦‹"""
        rules = []
        
        working_df = long_df[(long_df['parsed_slots_count'] > 0) & (long_df['code'] != '')]
        all_codes = sorted(working_df['code'].unique())
        
        for staff in working_df['staff'].unique():
            staff_df = working_df[working_df['staff'] == staff]
            
            if len(staff_df) < MIN_SAMPLE_SIZE:
                continue
                
            used_codes = sorted(staff_df['code'].unique())
            
            # åˆ¶é™ãƒ«ãƒ¼ãƒ«ã®åˆ¤å®šï¼ˆå…¨ã‚³ãƒ¼ãƒ‰ã®50%æœªæº€ã—ã‹ä½¿ç”¨ã—ã¦ã„ãªã„ï¼‰
            if len(used_codes) < len(all_codes) * STATISTICAL_THRESHOLDS["code_restriction_threshold"] and len(used_codes) >= 1:
                
                # çµ±è¨ˆçš„æ¤œå®š: ã‚³ãƒ¼ãƒ‰ä½¿ç”¨ãŒæœ‰æ„ã«åˆ¶é™ã•ã‚Œã¦ã„ã‚‹ã‹
                code_usage_ratio = len(used_codes) / len(all_codes)
                confidence = 1.0 - code_usage_ratio  # åˆ¶é™ãŒå¼·ã„ã»ã©ä¿¡é ¼åº¦ãŒé«˜ã„
                
                rule = ShiftRule(
                    staff_name=staff,
                    rule_type="å‹¤å‹™åŒºåˆ†åˆ¶é™",
                    rule_description=f"ä½¿ç”¨ã‚³ãƒ¼ãƒ‰: {', '.join(used_codes[:3])}{'...' if len(used_codes) > 3 else ''}ã®ã¿",
                    confidence_score=confidence,
                    statistical_evidence={
                        "used_codes": used_codes,
                        "total_available_codes": len(all_codes),
                        "usage_ratio": code_usage_ratio,
                        "total_shifts": len(staff_df)
                    }
                )
                rules.append(rule)
        
        return rules
    
    def _discover_time_restriction_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """æ™‚é–“å¸¯åˆ¶é™ãƒ«ãƒ¼ãƒ«ã®ç™ºè¦‹"""
        rules = []
        
        # TODO: æ™‚åˆ»æƒ…å ±ãŒã‚ã‚‹å ´åˆã®æ™‚é–“å¸¯åˆ¶é™åˆ†æ
        # ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã¯æ™‚åˆ»æƒ…å ±ãŒé™å®šçš„ãªãŸã‚ã€å°†æ¥ã®æ‹¡å¼µã¨ã—ã¦è¨­è¨ˆ
        
        return rules
    
    def _discover_pair_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """ãƒšã‚¢ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¬ãƒ™ãƒ«ã®ãƒ«ãƒ¼ãƒ«ç™ºè¦‹"""
        rules = []
        
        # åŒæ—¥åŒæ™‚å‹¤å‹™ã®åˆ†æ
        working_df = long_df[long_df['parsed_slots_count'] > 0]
        
        # æ—¥ä»˜ãƒ»å‹¤å‹™åŒºåˆ†ã”ã¨ã®ã‚¹ã‚¿ãƒƒãƒ•ã‚°ãƒ«ãƒ¼ãƒ—
        daily_groups = working_df.groupby(['ds', 'code'])['staff'].apply(list).reset_index()
        
        # ãƒšã‚¢ã®å…±èµ·å›æ•°ã‚’è¨ˆç®—
        pair_counts = defaultdict(int)
        total_shifts_per_staff = working_df.groupby('staff')['ds'].nunique()
        
        for _, row in daily_groups.iterrows():
            staff_list = row['staff']
            if len(staff_list) >= 2:
                for pair in combinations(sorted(set(staff_list)), 2):
                    pair_counts[pair] += 1
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®æ¤œå®š
        for (staff1, staff2), observed_count in pair_counts.items():
            if staff1 not in total_shifts_per_staff.index or staff2 not in total_shifts_per_staff.index:
                continue
                
            shifts1 = total_shifts_per_staff[staff1]
            shifts2 = total_shifts_per_staff[staff2]
            total_days = working_df['ds'].nunique()
            
            # æœŸå¾…å€¤è¨ˆç®—ï¼ˆç‹¬ç«‹æ€§ã‚’ä»®å®šï¼‰
            expected_count = (shifts1 * shifts2) / total_days
            
            if expected_count > 0:
                ratio = observed_count / expected_count
                
                # çµ±è¨ˆçš„æ¤œå®š
                if observed_count >= MIN_SAMPLE_SIZE:
                    # ãƒã‚¢ã‚½ãƒ³æ¤œå®šã§æœ‰æ„æ€§ã‚’ç¢ºèª
                    p_value = stats.poisson.sf(observed_count - 1, expected_count)
                    
                    if ratio >= STATISTICAL_THRESHOLDS["synergy_detection_high"] and p_value < STATISTICAL_THRESHOLDS["significance_alpha"]:  # æœŸå¾…å€¤ã®2å€ä»¥ä¸Šã§æœ‰æ„
                        rule = ShiftRule(
                            staff_name=f"{staff1} & {staff2}",
                            rule_type="çµ„ã¿åˆã‚ã›å„ªé‡",
                            rule_description=f"é »ç¹ã«ãƒšã‚¢å‹¤å‹™ï¼ˆæœŸå¾…å€¤ã®{ratio:.1f}å€ï¼‰",
                            confidence_score=1.0 - p_value,
                            statistical_evidence={
                                "observed_count": observed_count,
                                "expected_count": expected_count,
                                "ratio": ratio,
                                "p_value": p_value,
                                "staff1_shifts": shifts1,
                                "staff2_shifts": shifts2
                            }
                        )
                        rules.append(rule)
                    
                    elif ratio <= STATISTICAL_THRESHOLDS["synergy_detection_low"] and observed_count == 0:  # æœŸå¾…å€¤ã®åŠåˆ†ä»¥ä¸‹ã§å…±èµ·ãªã—
                        rule = ShiftRule(
                            staff_name=f"{staff1} & {staff2}",
                            rule_type="çµ„ã¿åˆã‚ã›å›é¿",
                            rule_description="ãƒšã‚¢å‹¤å‹™ã‚’é¿ã‘ã¦ã„ã‚‹ï¼ˆå…±èµ·ãªã—ï¼‰",
                            confidence_score=min(0.9, expected_count / 10),  # æœŸå¾…å€¤ãŒé«˜ã„ã»ã©ä¿¡é ¼åº¦UP
                            statistical_evidence={
                                "observed_count": 0,
                                "expected_count": expected_count,
                                "ratio": 0.0,
                                "staff1_shifts": shifts1,
                                "staff2_shifts": shifts2
                            }
                        )
                        rules.append(rule)
        
        return rules
    
    def _discover_segment_rules(self, long_df: pd.DataFrame) -> List[ShiftRule]:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ãƒ«ãƒ¼ãƒ«ç™ºè¦‹"""
        rules = []
        
        # è·ç¨®åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
        if 'role' in long_df.columns:
            for role in long_df['role'].unique():
                role_df = long_df[long_df['role'] == role]
                role_rules = self._discover_personal_rules(role_df)
                
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¿½åŠ 
                for rule in role_rules:
                    rule.segment = f"è·ç¨®:{role}"
                    rules.append(rule)
        
        # å‹¤å‹™åŒºåˆ†åˆ¥ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ†æ
        if 'code' in long_df.columns:
            for code in long_df[long_df['code'] != '']['code'].unique():
                code_df = long_df[long_df['code'] == code]
                code_rules = self._discover_personal_rules(code_df)
                
                # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæƒ…å ±ã‚’è¿½åŠ 
                for rule in code_rules:
                    rule.segment = f"å‹¤å‹™åŒºåˆ†:{code}"
                    rules.append(rule)
        
        return rules
    
    def _extract_statistical_insights(self, long_df: pd.DataFrame) -> Dict[str, Any]:
        """çµ±è¨ˆçš„æš—é»™çŸ¥ã®æŠ½å‡º"""
        insights = {}
        
        working_df = long_df[long_df['parsed_slots_count'] > 0]
        
        # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        insights['temporal_patterns'] = self._analyze_temporal_patterns(working_df)
        
        # å…¨ä½“çš„ãªå‚¾å‘åˆ†æ
        insights['overall_trends'] = self._analyze_overall_trends(working_df)
        
        return insights
    
    def _analyze_temporal_patterns(self, working_df: pd.DataFrame) -> Dict[str, Any]:
        """æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        patterns = {}
        
        # æœˆå†…æœŸé–“ã§ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³
        working_df['month_period'] = working_df['ds'].dt.day.apply(
            lambda d: 'æœˆåˆ' if d <= 10 else 'æœˆä¸­' if d <= 20 else 'æœˆæœ«'
        )
        
        period_counts = working_df.groupby(['staff', 'month_period']).size().unstack(fill_value=0)
        
        # æœŸé–“åé‡ã®ç™ºè¦‹
        for staff in period_counts.index:
            staff_counts = period_counts.loc[staff]
            total = staff_counts.sum()
            
            if total >= MIN_SAMPLE_SIZE:
                max_period = staff_counts.idxmax()
                max_ratio = staff_counts.max() / total
                
                if max_ratio >= 0.6:  # 60%ä»¥ä¸ŠãŒç‰¹å®šæœŸé–“ã«é›†ä¸­
                    patterns[staff] = {
                        'type': 'æœˆå†…æœŸé–“åé‡',
                        'pattern': f'{max_period}ã«é›†ä¸­ï¼ˆ{max_ratio:.1%}ï¼‰',
                        'confidence': max_ratio
                    }
        
        return patterns
    
    def _analyze_overall_trends(self, working_df: pd.DataFrame) -> Dict[str, Any]:
        """å…¨ä½“çš„ãªå‚¾å‘åˆ†æ"""
        trends = {}
        
        # ã‚¹ã‚¿ãƒƒãƒ•åˆ¥ã®å‹¤å‹™é »åº¦åˆ†æ
        staff_frequency = working_df['staff'].value_counts()
        
        trends['staff_distribution'] = {
            'most_frequent_staff': staff_frequency.index[0],
            'max_frequency': staff_frequency.iloc[0],
            'frequency_variance': staff_frequency.var(),
            'total_staff': len(staff_frequency)
        }
        
        # æ›œæ—¥åˆ¥ã®å…¨ä½“å‚¾å‘
        weekday_distribution = working_df.groupby(working_df['ds'].dt.dayofweek).size()
        weekday_names = [self.weekday_names[i] for i in weekday_distribution.index]
        
        trends['weekday_preferences'] = {
            'distribution': dict(zip(weekday_names, weekday_distribution.values)),
            'most_common_weekday': self.weekday_names[weekday_distribution.idxmax()],
            'least_common_weekday': self.weekday_names[weekday_distribution.idxmin()]
        }
        
        return trends
    
    def _analyze_by_segments(self, long_df: pd.DataFrame) -> Dict[str, Any]:
        """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåˆ¥ã®è©³ç´°åˆ†æ"""
        segments = {}
        
        # è·ç¨®åˆ¥åˆ†æ
        if 'role' in long_df.columns:
            segments['by_role'] = {}
            for role in long_df['role'].unique():
                role_df = long_df[long_df['role'] == role]
                working_role_df = role_df[role_df['parsed_slots_count'] > 0]
                
                segments['by_role'][role] = {
                    'staff_count': role_df['staff'].nunique(),
                    'total_shifts': len(working_role_df),
                    'avg_shifts_per_staff': len(working_role_df) / role_df['staff'].nunique() if role_df['staff'].nunique() > 0 else 0
                }
        
        # å‹¤å‹™åŒºåˆ†åˆ¥åˆ†æ
        if 'code' in long_df.columns:
            segments['by_code'] = {}
            for code in long_df[long_df['code'] != '']['code'].unique():
                code_df = long_df[long_df['code'] == code]
                
                segments['by_code'][code] = {
                    'staff_count': code_df['staff'].nunique(),
                    'total_shifts': len(code_df),
                    'avg_shifts_per_staff': len(code_df) / code_df['staff'].nunique() if code_df['staff'].nunique() > 0 else 0
                }
        
        return segments
    
    def _validate_and_score_rules(self, rules: List[ShiftRule], long_df: pd.DataFrame) -> List[ShiftRule]:
        """ãƒ«ãƒ¼ãƒ«ã®å¦¥å½“æ€§æ¤œè¨¼ã¨ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        validated_rules = []
        
        for rule in rules:
            # åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            if rule.confidence_score >= 0.5:  # 50%ä»¥ä¸Šã®ä¿¡é ¼åº¦
                # çµ±è¨ˆçš„è¨¼æ‹ ã®å­˜åœ¨ç¢ºèª
                if rule.statistical_evidence:
                    validated_rules.append(rule)
        
        # ä¿¡é ¼åº¦é †ã§ã‚½ãƒ¼ãƒˆ
        validated_rules.sort(key=lambda r: r.confidence_score, reverse=True)
        
        return validated_rules


def create_enhanced_blueprint_analysis(long_df: pd.DataFrame) -> Dict[str, Any]:
    """
    ã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™çŸ¥ã‚’æ·±ãåˆ†æã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    
    Args:
        long_df: å‹¤å‹™ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        ç™ºè¦‹ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã¨åˆ†æçµæœ
    """
    analyzer = EnhancedBlueprintAnalyzer()
    return analyzer.analyze_shift_creation_blueprint(long_df)