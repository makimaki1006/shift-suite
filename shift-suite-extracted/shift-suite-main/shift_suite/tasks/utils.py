# shift_suite/tasks/utils.py
# v1.2.1 ( _parse_as_date ç§»è¨­)
"""
shift_suite.tasks.utils  v1.2.0 â€“ UTF-8 write_meta ç‰ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
* å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
* 2025-05-06
    - write_meta(): Windows ã§ã‚‚æ–‡å­—åŒ–ã‘ã—ãªã„ã‚ˆã† `encoding="utf-8"` ã‚’æ˜ç¤º
      ï¼ˆæ—¢å®š CP932 ã ã¨ "â€“" ãªã©ã® Unicode æ–‡å­—åˆ—ã§ UnicodeEncodeErrorï¼‰
    - æ—§ APIãƒ»é–¢æ•°åã¯ä¸€åˆ‡å¤‰æ›´ãªã—
* 2025-05-16
    - build_stats.py ã‹ã‚‰ _parse_as_date ã‚’ç§»è¨­
* 2025-07-21
    - dash_app.py ã¨ app.py ã‹ã‚‰ _valid_df ã‚’çµ±åˆ
"""

from __future__ import annotations

import datetime as dt  #  dt ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚æ˜ç¤ºçš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ä»–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã®äº’æ›æ€§ã®ãŸã‚)
import json
import logging
import math
import re
import shutil
import tempfile
import zipfile
from datetime import (
    datetime,
    timedelta,
)  #  dt ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã§ã¯ãªã datetime, timedelta ã‚’ç›´æ¥ä½¿ç”¨
from pathlib import Path
from typing import Any, Dict, Sequence

import numpy as np
import pandas as pd
from pandas import DataFrame, Series

from ..logger_config import configure_logging

# è¿½åŠ ç®‡æ‰€: constants ã‹ã‚‰ SUMMARY5 ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ( _parse_as_date ã§ä½¿ç”¨)
from .constants import SUMMARY5

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. ãƒ­ã‚¬ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
configure_logging()
log = logging.getLogger(__name__)
analysis_logger = logging.getLogger('analysis')


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ä¼‘æš‡é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆçµ±åˆç‰ˆï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def apply_rest_exclusion_filter(df: pd.DataFrame, context: str = "unknown", for_display: bool = False, exclude_leave_records: bool = False) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã§ä½¿ç”¨ã™ã‚‹çµ±ä¸€çš„ãªä¼‘æš‡é™¤å¤–ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    
    Args:
        df: å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        context: é©ç”¨ç®‡æ‰€ã®èª¬æ˜ï¼ˆãƒ­ã‚°ç”¨ï¼‰
        for_display: True ã®å ´åˆã€è¡¨ç¤ºç”¨ã¨ã—ã¦å®Ÿç¸¾0ã®å‹¤å‹™æ—¥ã‚‚ä¿æŒ
        exclude_leave_records: True ã®å ´åˆã€holiday_type != 'é€šå¸¸å‹¤å‹™' ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å¤–
    
    Returns:
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    if df.empty:
        return df
    
    original_count = len(df)
    analysis_logger.info(f"[RestExclusion] {context}: ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼é–‹å§‹ ({original_count}ãƒ¬ã‚³ãƒ¼ãƒ‰)")
    
    # 1. ã‚¹ã‚¿ãƒƒãƒ•åã«ã‚ˆã‚‹é™¤å¤–ï¼ˆæœ€ã‚‚é‡è¦ï¼‰
    if 'staff' in df.columns:
        rest_patterns = [
            'Ã—', 'X', 'x',           # åŸºæœ¬çš„ãªä¼‘ã¿è¨˜å·
            'ä¼‘', 'ä¼‘ã¿', 'ä¼‘æš‡',      # æ—¥æœ¬èªã®ä¼‘ã¿
            'æ¬ ', 'æ¬ å‹¤',             # æ¬ å‹¤
            'OFF', 'off', 'Off',     # ã‚ªãƒ•
            '-', 'âˆ’', 'â€•',           # ãƒã‚¤ãƒ•ãƒ³é¡
            'nan', 'NaN', 'null',    # NULLå€¤ç³»
            'æœ‰', 'æœ‰ä¼‘',             # æœ‰çµ¦
            'ç‰¹', 'ç‰¹ä¼‘',             # ç‰¹ä¼‘
            'ä»£', 'ä»£ä¼‘',             # ä»£ä¼‘
            'æŒ¯', 'æŒ¯ä¼‘'              # æŒ¯æ›¿ä¼‘æ—¥
        ]
        
        excluded_by_pattern = {}
        for pattern in rest_patterns:
            if pattern.strip():  
                pattern_mask = (
                    (df['staff'].str.strip() == pattern) |
                    (df['staff'].str.contains(pattern, na=False, regex=False))
                )
                excluded_count = pattern_mask.sum()
                if excluded_count > 0:
                    excluded_by_pattern[pattern] = excluded_count
                    df = df[~pattern_mask]
        
        # ç©ºæ–‡å­—ãƒ»NaNé™¤å¤–
        empty_mask = (
            df['staff'].isna() |
            (df['staff'].str.strip() == '') |
            (df['staff'].str.strip() == ' ') |
            (df['staff'].str.strip() == 'ã€€')
        )
        excluded_count = empty_mask.sum()
        if excluded_count > 0:
            excluded_by_pattern['empty'] = excluded_count
            df = df[~empty_mask]
        
        if excluded_by_pattern:
            analysis_logger.info(f"[RestExclusion] {context}: ã‚¹ã‚¿ãƒƒãƒ•åã«ã‚ˆã‚‹é™¤å¤–: {excluded_by_pattern}")
    
    # 2. parsed_slots_count ã«ã‚ˆã‚‹é™¤å¤–
    if 'parsed_slots_count' in df.columns:
        zero_slots_mask = df['parsed_slots_count'] <= 0
        zero_slots_count = zero_slots_mask.sum()
        if zero_slots_count > 0:
            df = df[~zero_slots_mask]
            analysis_logger.info(f"[RestExclusion] {context}: 0ã‚¹ãƒ­ãƒƒãƒˆé™¤å¤–: {zero_slots_count}ä»¶")
    
    # 3. staff_count ã«ã‚ˆã‚‹é™¤å¤–ï¼ˆäº‹å‰é›†è¨ˆãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
    # ğŸ¯ è¡¨ç¤ºç”¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼åˆ†é›¢: è¡¨ç¤ºç”¨ã®å ´åˆã¯staff_count=0ã§ã‚‚ä¿æŒ
    if 'staff_count' in df.columns and not for_display:
        # æŒ‰åˆ†è¨ˆç®—ç”¨: å®Ÿç¸¾0ã‚’é™¤å¤–ï¼ˆç²¾åº¦å‘ä¸Šï¼‰
        if 'holiday_type' not in df.columns:
            # holiday_typeãŒãªã„å ´åˆã®ã¿ã€ä»¥å‰ã®å‹•ä½œã‚’ç¶­æŒ
            zero_staff_mask = df['staff_count'] <= 0
            zero_staff_count = zero_staff_mask.sum()
            if zero_staff_count > 0:
                df = df[~zero_staff_mask]
                analysis_logger.info(f"[RestExclusion] {context}: 0äººæ•°é™¤å¤–: {zero_staff_count}ä»¶")
    elif 'staff_count' in df.columns and for_display:
        # è¡¨ç¤ºç”¨: å®Ÿç¸¾0ã®å‹¤å‹™æ—¥ã‚‚ä¿æŒï¼ˆä¿¯ç°çš„è¦³å¯Ÿç”¨ï¼‰
        analysis_logger.info(f"[RestExclusion] {context}: è¡¨ç¤ºç”¨ã®ãŸã‚å®Ÿç¸¾0ã®å‹¤å‹™æ—¥ã‚‚ä¿æŒ")
    
    # 4. holiday_type ã«ã‚ˆã‚‹é™¤å¤–ï¼ˆæ˜ç¤ºçš„ã«è¦æ±‚ã•ã‚ŒãŸå ´åˆã®ã¿ï¼‰
    if 'holiday_type' in df.columns and exclude_leave_records:
        holiday_mask = df['holiday_type'] != 'é€šå¸¸å‹¤å‹™'
        holiday_count = holiday_mask.sum()
        if holiday_count > 0:
            df = df[~holiday_mask]
            analysis_logger.info(f"[RestExclusion] {context}: ä¼‘æš‡ã‚¿ã‚¤ãƒ—é™¤å¤–: {holiday_count}ä»¶")
    elif 'holiday_type' in df.columns:
        # ä¼‘æš‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¿æŒï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‹•ä½œï¼‰
        holiday_count = (df['holiday_type'] != 'é€šå¸¸å‹¤å‹™').sum()
        if holiday_count > 0:
            analysis_logger.info(f"[RestExclusion] {context}: ä¼‘æš‡ãƒ¬ã‚³ãƒ¼ãƒ‰ä¿æŒ: {holiday_count}ä»¶ (exclude_leave_records=False)")
    
    final_count = len(df)
    total_excluded = original_count - final_count
    exclusion_rate = total_excluded / original_count if original_count > 0 else 0
    
    analysis_logger.info(f"[RestExclusion] {context}: å®Œäº†: {original_count} -> {final_count} (é™¤å¤–: {total_excluded}ä»¶, {exclusion_rate:.1%})")
    
    return df


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. Excel æ—¥ä»˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def excel_date(excel_serial: Any) -> dt.date | None:
    """Excel 1900 ã‚·ãƒªã‚¢ãƒ« or pandas.Timestamp ç­‰ â†’ date"""  #  docstringå¤‰æ›´
    if excel_serial in (None, "", np.nan):
        return None
    if isinstance(excel_serial, (datetime, np.datetime64, pd.Timestamp)):
        # pd.Timestamp(...).to_pydatetime() ã¯ datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
        # ãã®æ—¥ä»˜éƒ¨åˆ†ã®ã¿ã‚’å–å¾—ã™ã‚‹ã«ã¯ .date() ã‚’å‘¼ã³å‡ºã™
        return pd.Timestamp(excel_serial).to_pydatetime().date()  #  .date() ã‚’è¿½åŠ 
    try:
        base = datetime(1899, 12, 30)  # Excel èµ·ç‚¹ (1900-01-00)
        return (base + timedelta(days=float(excel_serial))).date()
    except (ValueError, TypeError):
        return None


def to_hhmm(x: Any) -> str | None:
    """8.5 â†’ '08:30' / '23:45' â†’ '23:45' / Excel ã‚·ãƒªã‚¢ãƒ«ãªã©æŸ”è»Ÿå¤‰æ›"""
    if x in (None, "", np.nan):
        return None
    if isinstance(x, (int, float)) and not math.isnan(x):
        h = int(x)
        m = int(round((x - h) * 60))
        return f"{h:02d}:{m:02d}"
    x_str = str(x).strip()  #  å¤‰æ•°åã‚’ x_str ã«å¤‰æ›´
    m = re.match(r"^(\d{1,2}):(\d{1,2})$", x_str)
    if m:
        return f"{int(m.group(1)):02d}:{int(m.group(2)):02d}"
    #  è¿½åŠ : HH:MM:SS å½¢å¼ã‚‚è€ƒæ…® (ç§’ã¯åˆ‡ã‚Šæ¨ã¦)
    m_ss = re.match(r"^(\d{1,2}):(\d{1,2}):(\d{1,2})$", x_str)
    if m_ss:
        return f"{int(m_ss.group(1)):02d}:{int(m_ss.group(2)):02d}"
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. ãƒ©ãƒ™ãƒ«ï¼ãƒ•ã‚¡ã‚¤ãƒ«å â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def gen_labels(slot: int) -> list[str]:
    """00:00-23:59 ã‚’ slot åˆ†åˆ»ã¿ã§ HH:MM ãƒ©ãƒ™ãƒ«åŒ–"""
    t = datetime(2000, 1, 1)
    labels: list[str] = []
    while t.day == 1:  # 24h
        labels.append(t.strftime("%H:%M"))
        t += timedelta(minutes=slot)
    return labels


INVALID_CHARS = r"[\\/*?:\[\]]|\n|\r"


def safe_sheet(name: str, *, for_path: bool = False) -> str:
    """Excel ã‚·ãƒ¼ãƒˆï¼ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã«å±é™ºæ–‡å­—ã‚’ç½®æ›"""
    out = re.sub(INVALID_CHARS, "_", str(name))
    out = out.strip()[:31]  # Excel ã‚·ãƒ¼ãƒˆã¯ 31 æ–‡å­—ä¸Šé™
    if for_path:
        out = out.replace(" ", "_")
    return out or "sheet"


def safe_read_excel(fp: Path | str, **kwargs: Any) -> DataFrame:
    """Read an Excel file with basic error handling.

    Parameters
    ----------
    fp : Path | str
        Excel file path.
    **kwargs : Any
        Options forwarded to ``pd.read_excel``.

    Returns
    -------
    DataFrame
        Loaded DataFrame.

    Raises
    ------
    FileNotFoundError
        If the file does not exist.
    ValueError
        For empty files or other read errors.
    """
    path = Path(fp)
    if not path.exists():
        log.error("Excel file not found: %s", path)
        raise FileNotFoundError(path)
    try:
        return pd.read_excel(path, **kwargs)
    except pd.errors.EmptyDataError as e:
        log.error("Excel file '%s' is empty: %s", path, e)
        raise ValueError(f"Excel file '{path}' is empty") from e
    except Exception as e:  # noqa: BLE001
        log.error("Failed to read Excel file '%s': %s", path, e)
        raise ValueError(f"Failed to read Excel file '{path}': {e}") from e


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. DataFrame ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_df_xlsx(
    df: DataFrame,
    fp: Path | str,
    sheet_name: str | None = None,
    *,
    index: bool = True,
    engine: str = "openpyxl",
) -> Path:
    """
    æ±ç”¨ Excel ä¿å­˜ãƒ©ãƒƒãƒ‘ãƒ¼
    - é•·ã„ãƒ‘ã‚¹ã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«çµŒç”±ã§å®‰å…¨ã«ä¿å­˜
    - sheet_name=None â†’ ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å®‰å…¨åŒ–
    """
    fp_path = Path(fp)  #  å¤‰æ•°åã‚’ fp_path ã«å¤‰æ›´
    final_sheet_name = sheet_name or safe_sheet(
        fp_path.stem
    )  #  å¤‰æ•°åã‚’ final_sheet_name ã«å¤‰æ›´
    to_excel_kwargs = dict(index=index, sheet_name=final_sheet_name, engine=engine)

    with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as tmp:
        df.to_excel(tmp.name, **to_excel_kwargs)
        tmp.flush()  # Ensure all data is written to disk
        # tmp.close() # Close should happen before move on some OS, but pd might handle it.
        # Explicitly closing here might be safer before shutil.move.
        # However, pandas might need the file open if it's not fully flushed.
        # For safety, ensure pandas writer is closed if used, or rely on NamedTemporaryFile's delete=False behavior.
        # pd.ExcelWriter context manager is better if more control is needed.
    # Ensure the target directory exists
    fp_path.parent.mkdir(parents=True, exist_ok=True)
    shutil.move(tmp.name, fp_path)

    return fp_path


def save_df_parquet(
    df: DataFrame,
    fp: Path | str,
    *,
    index: bool = True,
) -> Path:
    """Save DataFrame to Parquet file."""
    fp_path = Path(fp)
    fp_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_parquet(fp_path, index=index)
    return fp_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. ãƒ¡ã‚¿ãƒ•ã‚¡ã‚¤ãƒ« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def write_meta(target: Path | str, /, **meta) -> Path:
    """
    JSON ãƒ¡ã‚¿æƒ…å ±ã‚’æ›¸ãå‡ºã—ã€‚UTF-8 å›ºå®šã§ UnicodeEncodeError ã‚’é˜²æ­¢ã€‚

    Parameters
    ----------
    target : Path | str
        * ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¸¡ã—ãŸå ´åˆ â†’ `<dir>/meta.json`
        * ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ¸¡ã—ãŸå ´åˆ â†’ ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    meta : dict
        æ›¸ãè¾¼ã‚€ã‚­ãƒ¼ï¼å€¤
    """
    target_path = Path(target)  #  å¤‰æ•°åã‚’ target_path ã«å¤‰æ›´
    meta_fp = target_path / "meta.json" if target_path.is_dir() else target_path
    meta_fp.parent.mkdir(parents=True, exist_ok=True)
    # ensure_ascii=False ã¨ indent=2 ã¯JSONã‚’è¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ã«è‰¯ã„ç¿’æ…£
    try:
        meta_fp.write_text(
            json.dumps(
                meta, ensure_ascii=False, indent=2, default=str
            ),  #  default=str ã‚’è¿½åŠ  (datetimeç­‰éã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå¯¾ç­–)
            encoding="utf-8",
        )
    except Exception as e:
        log.error(f"Failed to write meta file {meta_fp}: {e}")
        # Optionally, re-raise or handle as appropriate
    return meta_fp


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ZIP ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def safe_make_archive(src_dir: Path, dst_zip: Path) -> Path:
    """Windows é•·ãƒ‘ã‚¹å¯¾å¿œä»˜ã shutil.make_archive ç›¸å½“"""
    src_dir_path, dst_zip_path = Path(src_dir), Path(dst_zip)  #  å¤‰æ•°åå¤‰æ›´
    if dst_zip_path.exists():
        dst_zip_path.unlink()

    with zipfile.ZipFile(dst_zip_path, "w", compression=zipfile.ZIP_DEFLATED) as zf:
        for p_item in src_dir_path.rglob("*"):  #  å¤‰æ•°åå¤‰æ›´
            zf.write(p_item, p_item.relative_to(src_dir_path))
    return dst_zip_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ã‚¹ã‚¿ãƒƒãƒ•æ•°é–¾å€¤è¨ˆç®— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def derive_min_staff(heat: DataFrame | Series, method: str = "p25") -> Series:
    # (æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã¾ã¾ã¨ã—ã¾ã™)
    if isinstance(heat, Series):
        if method == "p25":
            return pd.Series([heat.quantile(0.25)]).round()
        if method == "mean-1s":
            return pd.Series([max(0, heat.mean() - heat.std())]).round()
        if method == "mode":
            # mode() can return an empty Series if all values are unique or NaN
            modes = heat.mode()
            return pd.Series([modes.iloc[0] if not modes.empty else np.nan]).round()
        raise ValueError(f"Unknown min_method: {method}")
    else:
        values = heat.select_dtypes(include=np.number)  #  include=np.number ã«å¤‰æ›´
        if values.empty:  # æ•°å€¤åˆ—ãŒãªã„å ´åˆ
            return pd.Series(dtype=float)  # ç©ºã®Seriesã‚’è¿”ã™
        if method == "p25":
            return values.quantile(0.25, axis=1).round()
        if method == "mean-1s":
            return (values.mean(axis=1) - values.std(axis=1)).clip(lower=0).round()
        if method == "mode":
            # DataFrame.mode() can also result in multiple rows if multiple modes exist for a row
            # Taking the first mode found for each row.
            modes_df = values.mode(axis=1)
            return (
                modes_df.iloc[:, 0].round()
                if not modes_df.empty
                else pd.Series(index=values.index, dtype=float).fillna(np.nan)
            )
        raise ValueError(f"Unknown min_method: {method}")  #  max_method -> min_method


def derive_max_staff(heat: DataFrame | Series, method: str = "mean+1s") -> Series:
    # (æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã¾ã¾ã¨ã—ã¾ã™)
    if isinstance(heat, Series):
        if method == "mean+1s":
            std_val = heat.std()
            if pd.isna(std_val):
                std_val = 0
            return pd.Series([heat.mean() + std_val]).round()
        if method == "p75":
            return pd.Series([heat.quantile(0.75)]).round()
        raise ValueError(f"Unknown max_method: {method}")
    else:
        values = heat.select_dtypes(include=np.number)  #  include=np.number ã«å¤‰æ›´
        if values.empty:  # æ•°å€¤åˆ—ãŒãªã„å ´åˆ
            return pd.Series(dtype=float)
        if method == "mean+1s":
            mu = values.mean(axis=1)
            sig = values.std(axis=1).fillna(0)  # NaNâ†’0
            return (mu + sig).round()
        if method == "p75":
            return values.quantile(0.75, axis=1).round()
        raise ValueError(f"Unknown max_method: {method}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. Jain æŒ‡æ•°è¨ˆç®— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def calculate_jain_index(values: pd.Series) -> float:
    """åˆ†å¸ƒã®å…¬å¹³æ€§ã‚’è©•ä¾¡ã™ã‚‹ Jain æŒ‡æ•° (0-1)ã€‚1 ãŒå®Œå…¨å…¬å¹³"""
    # Ensure the series contains numeric data and handle NaNs
    numeric_values = pd.to_numeric(values, errors="coerce").dropna()
    if (
        numeric_values.empty or len(numeric_values) == 0
    ):  #  len(numeric_values) == 0 ã‚‚ãƒã‚§ãƒƒã‚¯
        return 1.0  # Consider what to return for empty or all-NaN series
    if (
        numeric_values < 0
    ).any():  # Jain index is typically for non-negative resource allocation
        log.warning(
            "Jain index calculation: input series contains negative values. Results may be misleading."
        )

    # (sum of all values)^2 / (number of values * sum of squares of all values)
    sum_of_values_sq = (numeric_values.sum()) ** 2
    sum_of_squares = (numeric_values**2).sum()

    if sum_of_squares == 0:  # Avoid division by zero if all values are zero
        return (
            1.0 if numeric_values.nunique() <= 1 else 0.0
        )  # All zero is fair, multiple zeros and some non-zeros is not max fair.
        # If all are zero, it's perfectly fair in one sense.

    return round(sum_of_values_sq / (len(numeric_values) * sum_of_squares), 3)


# è¿½åŠ ç®‡æ‰€: _parse_as_date é–¢æ•°ã®å®šç¾© (build_stats.py ã‹ã‚‰ç§»è¨­)
def _parse_as_date(column_name: Any) -> dt.date | None:
    """åˆ—åã‚’æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãƒ‘ãƒ¼ã‚¹è©¦è¡Œã€‚å¤±æ•—æ™‚ã¯ None"""
    # ğŸ” ã€è¿½åŠ ã€‘ãƒ‘ãƒ¼ã‚¹éç¨‹ã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ï¼ˆå¿…è¦ã«å¿œã˜ã¦æœ‰åŠ¹åŒ–ï¼‰
    # log.debug(f"[DATE_PARSE] ãƒ‘ãƒ¼ã‚¹è©¦è¡Œ: '{column_name}' (å‹: {type(column_name)})")

    result: dt.date | None = None
    if isinstance(column_name, (dt.date, dt.datetime, pd.Timestamp)):
        if isinstance(column_name, pd.Timestamp):
            result = column_name.date()
        elif isinstance(column_name, dt.datetime):
            result = column_name.date()
        else:
            result = column_name
    elif isinstance(column_name, str):
        if column_name.lower() in [s.lower() for s in SUMMARY5]:
            result = None
        else:
            m = re.search(r"(\d{4}-\d{1,2}-\d{1,2})", column_name)
            if m:
                try:
                    result = pd.to_datetime(m.group(1), errors="raise").date()
                except (ValueError, TypeError, pd.errors.ParserError):
                    result = None
            if result is None:
                try:
                    result = pd.to_datetime(column_name.split(" ")[0], errors="raise").date()
                except (ValueError, TypeError, pd.errors.ParserError):
                    try:
                        if "." in column_name:
                            excel_serial = float(column_name)
                        else:
                            excel_serial = int(column_name)
                        if 0 < excel_serial < 200000:
                            result = (datetime(1899, 12, 30) + timedelta(days=excel_serial)).date()
                    except ValueError:
                        result = None
    elif isinstance(column_name, (int, float)):
        try:
            if column_name > 0 and column_name < 200000:
                result = (datetime(1899, 12, 30) + timedelta(days=int(column_name))).date()
        except (ValueError, OverflowError, TypeError):
            result = None
    if result and result.weekday() == 6:
        log.debug(f"[DATE_PARSE] æ—¥æ›œæ—¥ã‚’æ¤œå‡º: {column_name} â†’ {result}")
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 8. Date + Weekday Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def date_with_weekday(date_val: Any) -> str:
    """Return ``YYYY-MM-DD(æ›œæ—¥)`` for the given date string."""

    try:
        dt_val = pd.to_datetime(date_val, errors="raise")
    except Exception:  # noqa: BLE001 - parse failure
        return str(date_val)
    weekday_jp = "æœˆç«æ°´æœ¨é‡‘åœŸæ—¥"[dt_val.weekday()]
    return dt_val.strftime("%Y-%m-%d") + f"({weekday_jp})"


def validate_need_calculation(
    need_df: pd.DataFrame,
    actual_df: pd.DataFrame,
    tolerance_factor: float = 3.0,
) -> Dict[str, Any]:
    """Validate need calculation results."""

    validation_results = {
        "status": "PASS",
        "warnings": [],
        "errors": [],
        "statistics": {},
    }

    for column in need_df.columns:
        if column in actual_df.columns:
            need_total = need_df[column].sum()
            actual_total = actual_df[column].sum()

            if actual_total > 0 and need_total > actual_total * tolerance_factor:
                warning_msg = (
                    f"{column}: Need({need_total:.1f})ãŒå®Ÿç¸¾({actual_total:.1f})ã®{tolerance_factor}å€ã‚’è¶…é"
                )
                validation_results["warnings"].append(warning_msg)
                validation_results["status"] = "WARNING"
                log.warning(f"[VALIDATION_WARN] {warning_msg}")

            validation_results["statistics"][column] = {
                "need_total": need_total,
                "actual_total": actual_total,
                "ratio": need_total / actual_total if actual_total > 0 else float("inf"),
            }

    return validation_results


def log_need_calculation_summary(
    need_df: pd.DataFrame,
    actual_df: pd.DataFrame,
    method: str,
) -> None:
    """Log summary of need calculation."""

    log.info(f"[NEED_SUMMARY] ========== Needè¨ˆç®—ã‚µãƒãƒªãƒ¼ ({method}) ==========")

    dow_names = ["æœˆ", "ç«", "æ°´", "æœ¨", "é‡‘", "åœŸ", "æ—¥"]

    for column in need_df.columns:
        if column in actual_df.columns:
            date_obj = _parse_as_date(column)
            if date_obj:
                dow_name = dow_names[date_obj.weekday()]
                need_total = need_df[column].sum()
                actual_total = actual_df[column].sum()
                log.info(
                    f"[NEED_SUMMARY] {column}({dow_name}): Need={need_total:.1f}, å®Ÿç¸¾={actual_total:.1f}"
                )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 9. DataFrame Validation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _valid_df(df: pd.DataFrame) -> bool:
    """Return True if df is a non-empty pandas DataFrame."""
    return isinstance(df, pd.DataFrame) and not df.empty


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 10. Public Re-export â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
__all__: Sequence[str] = [
    "log",
    "excel_date",
    "to_hhmm",
    "gen_labels",
    "safe_sheet",
    "safe_read_excel",
    "save_df_xlsx",
    "save_df_parquet",
    "write_meta",
    "safe_make_archive",
    "derive_min_staff",
    "derive_max_staff",
    "calculate_jain_index",
    "_parse_as_date",  # è¿½åŠ 
    "_valid_df",       # çµ±åˆè¿½åŠ 
    "date_with_weekday",
    "validate_need_calculation",
    "log_need_calculation_summary",
]
