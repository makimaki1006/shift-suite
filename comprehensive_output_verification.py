#!/usr/bin/env python3
"""
åŒ…æ‹¬çš„ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆæ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ - å®Ÿéš›ã®analysis_results.zipã®è©³ç´°åˆ†æ
Windowsç’°å¢ƒã§å®Ÿè¡Œã—ã€å®Ÿéš›ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå“è³ªã‚’å¾¹åº•æ¤œè¨¼
"""

import os
import sys
import zipfile
import pandas as pd
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Tuple
import tempfile
import shutil

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class ComprehensiveOutputVerifier:
    """åŒ…æ‹¬çš„ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆæ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.verification_results = {
            "zip_file_analysis": {},
            "content_quality_assessment": {},
            "business_value_evaluation": {},
            "data_integrity_check": {},
            "user_experience_assessment": {},
            "performance_analysis": {}
        }
        
    def verify_output_quality(self) -> Dict[str, Any]:
        """ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå“è³ªã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        log.info("ğŸ” å®Ÿéš›ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå“è³ªæ¤œè¨¼ã‚’é–‹å§‹...")
        
        try:
            # 1. ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ
            self._analyze_zip_files()
            
            # 2. ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡
            self._assess_content_quality()
            
            # 3. ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤è©•ä¾¡
            self._evaluate_business_value()
            
            # 4. ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            self._check_data_integrity()
            
            # 5. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹è©•ä¾¡
            self._assess_user_experience()
            
            # 6. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
            self._analyze_performance()
            
            # æœ€çµ‚è©•ä¾¡ã‚µãƒãƒªãƒ¼
            summary = self._generate_final_assessment()
            
            log.info("âœ… ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå“è³ªæ¤œè¨¼å®Œäº†")
            return {
                "status": "completed",
                "summary": summary,
                "detailed_results": self.verification_results,
                "recommendations": self._generate_improvement_recommendations()
            }
            
        except Exception as e:
            log.error(f"âŒ æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return {
                "status": "error",
                "error": str(e),
                "partial_results": self.verification_results
            }
    
    def _analyze_zip_files(self):
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°åˆ†æ"""
        log.info("ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ...")
        
        zip_analysis = {}
        
        # åˆ©ç”¨å¯èƒ½ãªZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        zip_files = list(self.project_root.glob("analysis_results*.zip"))
        
        for zip_path in zip_files:
            log.info(f"åˆ†æä¸­: {zip_path.name}")
            
            try:
                file_stats = {
                    "file_size": zip_path.stat().st_size,
                    "file_size_mb": round(zip_path.stat().st_size / 1024 / 1024, 2),
                    "created": zip_path.stat().st_ctime,
                    "contents": []
                }
                
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®åˆ†æ
                with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                    files = zip_ref.namelist()
                    file_stats["total_files"] = len(files)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
                    file_types = {}
                    total_uncompressed_size = 0
                    
                    for file_name in files:
                        info = zip_ref.getinfo(file_name)
                        total_uncompressed_size += info.file_size
                        
                        # æ‹¡å¼µå­åˆ¥åˆ†é¡
                        ext = Path(file_name).suffix.lower()
                        if ext not in file_types:
                            file_types[ext] = {"count": 0, "size": 0}
                        file_types[ext]["count"] += 1
                        file_types[ext]["size"] += info.file_size
                        
                        # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°æƒ…å ±
                        if any(keyword in file_name.lower() for keyword in 
                               ['summary', 'stats', 'heat_all', 'forecast', 'hire_plan']):
                            file_stats["contents"].append({
                                "name": file_name,
                                "size": info.file_size,
                                "compressed_size": info.compress_size,
                                "compression_ratio": round(info.compress_size / info.file_size * 100, 1) if info.file_size > 0 else 0
                            })
                    
                    file_stats["total_uncompressed_size"] = total_uncompressed_size
                    file_stats["total_uncompressed_mb"] = round(total_uncompressed_size / 1024 / 1024, 2)
                    file_stats["compression_ratio"] = round(zip_path.stat().st_size / total_uncompressed_size * 100, 1) if total_uncompressed_size > 0 else 0
                    file_stats["file_types"] = file_types
                
                zip_analysis[zip_path.name] = file_stats
                
            except Exception as e:
                log.error(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ ({zip_path.name}): {e}")
                zip_analysis[zip_path.name] = {"error": str(e)}
        
        self.verification_results["zip_file_analysis"] = zip_analysis
        log.info(f"ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå®Œäº†: {len(zip_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    def _assess_content_quality(self):
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡"""
        log.info("ğŸ“Š ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡...")
        
        quality_assessment = {
            "data_completeness": {},
            "data_accuracy": {},
            "output_usefulness": {},
            "format_accessibility": {}
        }
        
        # extracted_resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆ†æ
        extracted_dir = self.project_root / "extracted_results"
        if extracted_dir.exists():
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®åˆ†æ
            formats = {}
            total_files = 0
            accessible_files = 0  # Excel, CSV, TXTå½¢å¼
            
            for file_path in extracted_dir.rglob("*"):
                if file_path.is_file():
                    total_files += 1
                    ext = file_path.suffix.lower()
                    
                    if ext not in formats:
                        formats[ext] = 0
                    formats[ext] += 1
                    
                    # ã‚¢ã‚¯ã‚»ã‚·ãƒ–ãƒ«ãªå½¢å¼ã‹ãƒã‚§ãƒƒã‚¯
                    if ext in ['.csv', '.xlsx', '.txt', '.json']:
                        accessible_files += 1
                    
                    # ä¸»è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ç¢ºèª
                    if file_path.name in ['stats_summary.txt', 'hire_plan.txt']:
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                                quality_assessment["data_completeness"][file_path.name] = {
                                    "size": len(content),
                                    "lines": len(content.split('\n')) if content else 0,
                                    "empty": len(content) == 0
                                }
                        except Exception as e:
                            quality_assessment["data_completeness"][file_path.name] = {"error": str(e)}
            
            quality_assessment["format_accessibility"] = {
                "total_files": total_files,
                "accessible_files": accessible_files,
                "accessibility_ratio": accessible_files / total_files if total_files > 0 else 0,
                "formats": formats
            }
        
        self.verification_results["content_quality_assessment"] = quality_assessment
        log.info("ğŸ“Š ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å“è³ªè©•ä¾¡å®Œäº†")
    
    def _evaluate_business_value(self):
        """ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤è©•ä¾¡"""
        log.info("ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤è©•ä¾¡...")
        
        business_value = {
            "actionable_insights": [],
            "redundant_outputs": [],
            "missing_critical_info": [],
            "value_density_analysis": {}
        }
        
        # extracted_resultsã®åˆ†æã‹ã‚‰å®Ÿç”¨çš„æƒ…å ±ã‚’æŠ½å‡º
        extracted_dir = self.project_root / "extracted_results"
        if extracted_dir.exists():
            total_data_size = 0
            actionable_data_size = 0
            
            # å„ã‚·ãƒŠãƒªã‚ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®åˆ†æ
            scenarios = ["out_mean_based", "out_median_based", "out_p25_based"]
            scenario_analysis = {}
            
            for scenario in scenarios:
                scenario_dir = extracted_dir / scenario
                if scenario_dir.exists():
                    scenario_files = list(scenario_dir.iterdir())
                    scenario_size = sum(f.stat().st_size for f in scenario_files if f.is_file())
                    total_data_size += scenario_size
                    
                    # stats_summary.txtã®åˆ†æ
                    stats_file = scenario_dir / "stats_summary.txt"
                    if stats_file.exists():
                        try:
                            with open(stats_file, 'r', encoding='utf-8') as f:
                                stats_content = f.read()
                                if "lack_hours_total" in stats_content and "excess_hours_total" in stats_content:
                                    actionable_data_size += stats_file.stat().st_size
                                    business_value["actionable_insights"].append({
                                        "type": "shortage_analysis",
                                        "file": str(stats_file),
                                        "value": "ç›´æ¥çš„ãªã‚·ãƒ•ãƒˆèª¿æ•´æŒ‡é‡",
                                        "content": stats_content.strip()
                                    })
                        except Exception as e:
                            log.warning(f"stats_summary.txtèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    
                    scenario_analysis[scenario] = {
                        "files": len(scenario_files),
                        "size_bytes": scenario_size,
                        "size_mb": round(scenario_size / 1024 / 1024, 2)
                    }
            
            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®æ¤œå‡º
            if len(scenario_analysis) > 1:
                business_value["redundant_outputs"].append({
                    "type": "duplicate_scenarios",
                    "count": len(scenario_analysis),
                    "scenarios": list(scenario_analysis.keys()),
                    "total_redundant_mb": round(sum(s["size_bytes"] for s in scenario_analysis.values()) * 0.67 / 1024 / 1024, 2)
                })
            
            # ä¾¡å€¤å¯†åº¦åˆ†æ
            business_value["value_density_analysis"] = {
                "total_output_size_mb": round(total_data_size / 1024 / 1024, 2),
                "actionable_size_bytes": actionable_data_size,
                "value_density_ratio": actionable_data_size / total_data_size if total_data_size > 0 else 0,
                "scenario_breakdown": scenario_analysis
            }
        
        self.verification_results["business_value_evaluation"] = business_value
        log.info("ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤è©•ä¾¡å®Œäº†")
    
    def _check_data_integrity(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯"""
        log.info("ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯...")
        
        integrity_check = {
            "consistency_across_scenarios": {},
            "data_validation_results": {},
            "calculation_verification": {}
        }
        
        # ã‚·ãƒŠãƒªã‚ªé–“ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        extracted_dir = self.project_root / "extracted_results"
        if extracted_dir.exists():
            scenarios = ["out_mean_based", "out_median_based", "out_p25_based"]
            stats_data = {}
            
            # å„ã‚·ãƒŠãƒªã‚ªã®stats_summary.txtã‚’æ¯”è¼ƒ
            for scenario in scenarios:
                stats_file = extracted_dir / scenario / "stats_summary.txt"
                if stats_file.exists():
                    try:
                        with open(stats_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            stats_data[scenario] = {}
                            for line in content.strip().split('\n'):
                                if ':' in line:
                                    key, value = line.split(':', 1)
                                    try:
                                        stats_data[scenario][key.strip()] = float(value.strip())
                                    except ValueError:
                                        stats_data[scenario][key.strip()] = value.strip()
                    except Exception as e:
                        log.warning(f"statsèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({scenario}): {e}")
            
            # ä¸€è²«æ€§åˆ†æ
            if len(stats_data) > 1:
                keys = set()
                for data in stats_data.values():
                    keys.update(data.keys())
                
                for key in keys:
                    values = []
                    for scenario, data in stats_data.items():
                        if key in data:
                            values.append((scenario, data[key]))
                    
                    if len(values) > 1:
                        numeric_values = [(s, v) for s, v in values if isinstance(v, (int, float))]
                        if numeric_values:
                            min_val = min(v for s, v in numeric_values)
                            max_val = max(v for s, v in numeric_values)
                            variance = max_val - min_val
                            
                            integrity_check["consistency_across_scenarios"][key] = {
                                "scenarios": dict(numeric_values),
                                "variance": variance,
                                "consistent": variance == 0
                            }
        
        self.verification_results["data_integrity_check"] = integrity_check
        log.info("ğŸ” ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†")
    
    def _assess_user_experience(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹è©•ä¾¡"""
        log.info("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹è©•ä¾¡...")
        
        ux_assessment = {
            "file_findability": {},
            "content_readability": {},
            "technical_barriers": {},
            "workflow_efficiency": {}
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹ã—ã‚„ã™ã•
        extracted_dir = self.project_root / "extracted_results"
        if extracted_dir.exists():
            all_files = list(extracted_dir.rglob("*"))
            file_count = len([f for f in all_files if f.is_file()])
            
            # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ç™ºè¦‹ã—ã‚„ã™ã•
            key_files = ['stats_summary.txt', 'hire_plan.txt', 'leave_analysis.csv']
            found_key_files = []
            
            for key_file in key_files:
                found = list(extracted_dir.rglob(key_file))
                if found:
                    found_key_files.extend(found)
            
            ux_assessment["file_findability"] = {
                "total_files": file_count,
                "key_files_found": len(found_key_files),
                "key_files_ratio": len(found_key_files) / (len(key_files) * 3),  # 3ã‚·ãƒŠãƒªã‚ªæƒ³å®š
                "directory_depth": max(len(f.parts) - len(extracted_dir.parts) for f in all_files if f.is_file()) if file_count > 0 else 0
            }
            
            # æŠ€è¡“çš„éšœå£ã®è©•ä¾¡
            parquet_files = len(list(extracted_dir.rglob("*.parquet")))
            excel_files = len(list(extracted_dir.rglob("*.xlsx")))
            csv_files = len(list(extracted_dir.rglob("*.csv")))
            txt_files = len(list(extracted_dir.rglob("*.txt")))
            
            ux_assessment["technical_barriers"] = {
                "parquet_files": parquet_files,
                "excel_files": excel_files,
                "csv_files": csv_files,
                "txt_files": txt_files,
                "user_friendly_ratio": (excel_files + csv_files + txt_files) / file_count if file_count > 0 else 0,
                "technical_barrier_ratio": parquet_files / file_count if file_count > 0 else 0
            }
        
        self.verification_results["user_experience_assessment"] = ux_assessment
        log.info("ğŸ‘¤ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹è©•ä¾¡å®Œäº†")
    
    def _analyze_performance(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        log.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ...")
        
        performance_analysis = {
            "storage_efficiency": {},
            "compression_efficiency": {},
            "processing_overhead": {}
        }
        
        # ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡åˆ†æ
        total_project_size = sum(f.stat().st_size for f in self.project_root.rglob("*") if f.is_file())
        
        zip_files = list(self.project_root.glob("analysis_results*.zip"))
        total_zip_size = sum(f.stat().st_size for f in zip_files)
        
        extracted_dir = self.project_root / "extracted_results"
        extracted_size = 0
        if extracted_dir.exists():
            extracted_size = sum(f.stat().st_size for f in extracted_dir.rglob("*") if f.is_file())
        
        performance_analysis["storage_efficiency"] = {
            "total_project_size_mb": round(total_project_size / 1024 / 1024, 2),
            "zip_output_size_mb": round(total_zip_size / 1024 / 1024, 2),
            "extracted_output_size_mb": round(extracted_size / 1024 / 1024, 2),
            "output_ratio": round(total_zip_size / total_project_size, 4) if total_project_size > 0 else 0
        }
        
        self.verification_results["performance_analysis"] = performance_analysis
        log.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº†")
    
    def _generate_final_assessment(self) -> Dict[str, Any]:
        """æœ€çµ‚è©•ä¾¡ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        results = self.verification_results
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        scores = {
            "data_completeness": 0,
            "business_value": 0,
            "user_experience": 0,
            "efficiency": 0
        }
        
        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§ã‚¹ã‚³ã‚¢
        if "content_quality_assessment" in results:
            accessibility = results["content_quality_assessment"].get("format_accessibility", {})
            if accessibility.get("accessibility_ratio", 0) > 0.7:
                scores["data_completeness"] = 8
            elif accessibility.get("accessibility_ratio", 0) > 0.5:
                scores["data_completeness"] = 6
            else:
                scores["data_completeness"] = 4
        
        # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚¹ã‚³ã‚¢
        if "business_value_evaluation" in results:
            actionable = len(results["business_value_evaluation"].get("actionable_insights", []))
            if actionable > 3:
                scores["business_value"] = 8
            elif actionable > 1:
                scores["business_value"] = 6
            else:
                scores["business_value"] = 3
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹ã‚¹ã‚³ã‚¢
        if "user_experience_assessment" in results:
            ux = results["user_experience_assessment"]
            friendly_ratio = ux.get("technical_barriers", {}).get("user_friendly_ratio", 0)
            if friendly_ratio > 0.8:
                scores["user_experience"] = 8
            elif friendly_ratio > 0.5:
                scores["user_experience"] = 6
            else:
                scores["user_experience"] = 3
        
        # åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢
        if "performance_analysis" in results:
            output_ratio = results["performance_analysis"].get("storage_efficiency", {}).get("output_ratio", 0)
            if output_ratio > 0.1:
                scores["efficiency"] = 2
            elif output_ratio > 0.01:
                scores["efficiency"] = 4
            else:
                scores["efficiency"] = 1
        
        overall_score = sum(scores.values()) / len(scores)
        
        return {
            "individual_scores": scores,
            "overall_score": round(overall_score, 1),
            "grade": self._get_grade(overall_score),
            "key_findings": self._extract_key_findings()
        }
    
    def _get_grade(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰è©•ä¾¡ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’å–å¾—"""
        if score >= 8:
            return "A"
        elif score >= 6:
            return "B"
        elif score >= 4:
            return "C"
        else:
            return "D"
    
    def _extract_key_findings(self) -> List[str]:
        """ä¸»è¦ãªç™ºè¦‹äº‹é …ã‚’æŠ½å‡º"""
        findings = []
        
        # ZIPãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‹ã‚‰
        if "zip_file_analysis" in self.verification_results:
            for zip_name, analysis in self.verification_results["zip_file_analysis"].items():
                if "total_files" in analysis:
                    findings.append(f"{zip_name}: {analysis['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«, {analysis.get('file_size_mb', 0)}MB")
        
        # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‹ã‚‰
        if "business_value_evaluation" in self.verification_results:
            insights = self.verification_results["business_value_evaluation"].get("actionable_insights", [])
            if insights:
                findings.append(f"å®Ÿç”¨çš„ãªåˆ†æçµæœ: {len(insights)}ä»¶")
        
        return findings
    
    def _generate_improvement_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ã‚¯ã‚¹ãƒšãƒªã‚¨ãƒ³ã‚¹æ”¹å–„
        if "user_experience_assessment" in self.verification_results:
            barriers = self.verification_results["user_experience_assessment"].get("technical_barriers", {})
            if barriers.get("technical_barrier_ratio", 0) > 0.3:
                recommendations.append("Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€èˆ¬çš„ãªExcel/CSVå½¢å¼ã«å¤‰æ›")
        
        # åŠ¹ç‡æ€§æ”¹å–„
        if "performance_analysis" in self.verification_results:
            efficiency = self.verification_results["performance_analysis"].get("storage_efficiency", {})
            if efficiency.get("output_ratio", 0) < 0.01:
                recommendations.append("å‡ºåŠ›ã‚µã‚¤ã‚ºå¯¾ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µã‚¤ã‚ºæ¯”ç‡ã®æ”¹å–„")
        
        # é‡è¤‡å‰Šæ¸›
        if "business_value_evaluation" in self.verification_results:
            redundant = self.verification_results["business_value_evaluation"].get("redundant_outputs", [])
            if redundant:
                recommendations.append("é‡è¤‡ã‚·ãƒŠãƒªã‚ªå‡ºåŠ›ã®çµ±åˆ")
        
        return recommendations

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    project_root = Path(__file__).parent
    verifier = ComprehensiveOutputVerifier(project_root)
    
    log.info("ğŸ” åŒ…æ‹¬çš„ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå“è³ªæ¤œè¨¼ã‚’é–‹å§‹...")
    results = verifier.verify_output_quality()
    
    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = project_root / "comprehensive_output_verification_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    # çµæœã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è¡¨ç¤º
    print("\n" + "="*80)
    print("ğŸ” åŒ…æ‹¬çš„ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå“è³ªæ¤œè¨¼çµæœ")
    print("="*80)
    
    if results['status'] == 'completed':
        summary = results['summary']
        print(f"ğŸ“Š ç·åˆè©•ä¾¡: {summary['grade']} ({summary['overall_score']}/10)")
        print(f"ğŸ¯ å€‹åˆ¥ã‚¹ã‚³ã‚¢:")
        for category, score in summary['individual_scores'].items():
            print(f"  â€¢ {category}: {score}/10")
        
        print(f"\nğŸ” ä¸»è¦ç™ºè¦‹äº‹é …:")
        for finding in summary['key_findings']:
            print(f"  â€¢ {finding}")
        
        print(f"\nğŸ“ æ”¹å–„æ¨å¥¨äº‹é …:")
        for rec in results['recommendations']:
            print(f"  â€¢ {rec}")
    else:
        print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {results['error']}")
    
    print(f"\nğŸ“„ è©³ç´°çµæœ: {output_file}")
    print("="*80)
    
    return results

if __name__ == "__main__":
    main()