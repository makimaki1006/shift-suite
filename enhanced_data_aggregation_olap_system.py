"""
å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPæ©Ÿèƒ½ã‚·ã‚¹ãƒ†ãƒ 
MECEæ¤œè¨¼ã§ç‰¹å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿é›†ç´„æ©Ÿèƒ½ã®æ‹¡å¼µã‚’å®Ÿç¾
"""

import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union, Callable
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime, timedelta
import itertools
import warnings
warnings.filterwarnings('ignore')

# Mock pandas implementation
class MockDataFrame:
    """Mock pandas DataFrame implementation"""
    
    def __init__(self, data=None):
        if data is None:
            self.data = {}
        elif isinstance(data, dict):
            self.data = data
        else:
            self.data = {'data': data}
        
        # Ensure all values are lists of same length
        if self.data:
            max_len = max(len(v) if isinstance(v, (list, tuple)) else 1 for v in self.data.values())
            for k, v in self.data.items():
                if not isinstance(v, (list, tuple)):
                    self.data[k] = [v] * max_len
                elif len(v) < max_len:
                    self.data[k] = list(v) + [v[-1]] * (max_len - len(v))
    
    def groupby(self, by):
        return MockGroupBy(self, by)
    
    def pivot_table(self, values=None, index=None, columns=None, aggfunc='mean'):
        # Simplified pivot table mock
        return MockDataFrame({'pivoted': [1, 2, 3, 4]})
    
    def sum(self):
        return {k: sum(v) if all(isinstance(x, (int, float)) for x in v) else len(v) 
                for k, v in self.data.items()}
    
    def mean(self):
        return {k: np.mean(v) if all(isinstance(x, (int, float)) for x in v) else 0 
                for k, v in self.data.items()}
    
    def count(self):
        return {k: len(v) for k, v in self.data.items()}
    
    def to_dict(self):
        return self.data
    
    def __len__(self):
        return len(next(iter(self.data.values()))) if self.data else 0

class MockGroupBy:
    """Mock pandas GroupBy implementation"""
    
    def __init__(self, df, by):
        self.df = df
        self.by = by
    
    def agg(self, func):
        if isinstance(func, dict):
            result = {}
            for col, f in func.items():
                if col in self.df.data:
                    if f == 'sum':
                        result[col] = sum(self.df.data[col])
                    elif f == 'mean':
                        result[col] = np.mean(self.df.data[col])
                    elif f == 'count':
                        result[col] = len(self.df.data[col])
                    else:
                        result[col] = self.df.data[col][0]
            return MockDataFrame(result)
        return MockDataFrame(self.df.data)
    
    def sum(self):
        return MockDataFrame({k: [sum(v)] for k, v in self.df.data.items()})
    
    def mean(self):
        return MockDataFrame({k: [np.mean(v)] for k, v in self.df.data.items()})
    
    def count(self):
        return MockDataFrame({k: [len(v)] for k, v in self.df.data.items()})

# Use mock DataFrame
pd = type('pd', (), {'DataFrame': MockDataFrame})


class AggregationType(Enum):
    """é›†ç´„ã‚¿ã‚¤ãƒ—"""
    SUM = "sum"
    MEAN = "mean"
    MEDIAN = "median"
    COUNT = "count"
    MIN = "min"
    MAX = "max"
    STD = "std"
    VAR = "var"
    PERCENTILE = "percentile"
    CUSTOM = "custom"


class DimensionType(Enum):
    """æ¬¡å…ƒã‚¿ã‚¤ãƒ—"""
    TIME = "time"
    CATEGORY = "category"
    HIERARCHY = "hierarchy"
    NUMERIC = "numeric"
    GEOGRAPHIC = "geographic"
    CUSTOM = "custom"


class DrillDirection(Enum):
    """ãƒ‰ãƒªãƒ«æ–¹å‘"""
    DOWN = "down"    # ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³
    UP = "up"        # ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—  
    ACROSS = "across"  # ãƒ‰ãƒªãƒ«ã‚¢ã‚¯ãƒ­ã‚¹


@dataclass
class Dimension:
    """æ¬¡å…ƒå®šç¾©"""
    name: str
    type: DimensionType
    hierarchy_levels: List[str]
    default_level: str
    description: str
    data_source: str
    format_function: Optional[Callable] = None


@dataclass
class Measure:
    """ãƒ¡ã‚¸ãƒ£ãƒ¼å®šç¾©"""
    name: str
    aggregation_type: AggregationType
    source_column: str
    description: str
    unit: str
    format_function: Optional[Callable] = None
    calculation_formula: Optional[str] = None


@dataclass
class CubeDefinition:
    """ã‚­ãƒ¥ãƒ¼ãƒ–å®šç¾©"""
    name: str
    dimensions: List[Dimension]
    measures: List[Measure]
    data_source: str
    refresh_frequency: str
    description: str


@dataclass
class OLAPQuery:
    """OLAPã‚¯ã‚¨ãƒª"""
    cube_name: str
    selected_dimensions: List[str]
    selected_measures: List[str]
    filters: Dict[str, Any]
    drill_path: List[Tuple[str, str]]  # (dimension, level)
    sort_by: Optional[str]
    limit: Optional[int]


@dataclass
class AggregationResult:
    """é›†ç´„çµæœ"""
    query: OLAPQuery
    data: Dict[str, Any]
    dimensions_used: List[str]
    measures_calculated: List[str]
    total_records: int
    execution_time_ms: float
    cache_hit: bool
    quality_score: float
    interpretation: str
    recommendations: List[str]


class EnhancedDataAggregationOLAPSystem:
    """å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.cubes = {}
        self.query_cache = {}
        self.aggregation_cache = {}
        
        # ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
        self.system_config = {
            'cache_enabled': True,
            'cache_ttl_minutes': 30,
            'max_cache_entries': 1000,
            'parallel_processing': True,
            'quality_threshold': 0.85,
            'performance_logging': True
        }
        
        # é›†ç´„é–¢æ•°ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.aggregation_functions = {
            AggregationType.SUM: np.sum,
            AggregationType.MEAN: np.mean,
            AggregationType.MEDIAN: np.median,
            AggregationType.COUNT: len,
            AggregationType.MIN: np.min,
            AggregationType.MAX: np.max,
            AggregationType.STD: np.std,
            AggregationType.VAR: np.var
        }
        
        # ã‚·ãƒ•ãƒˆåˆ†æç”¨ã®ã‚­ãƒ¥ãƒ¼ãƒ–ã‚’åˆæœŸåŒ–
        self._initialize_shift_analysis_cubes()
    
    def _initialize_shift_analysis_cubes(self):
        """ã‚·ãƒ•ãƒˆåˆ†æç”¨ã‚­ãƒ¥ãƒ¼ãƒ–ã®åˆæœŸåŒ–"""
        
        # æ™‚é–“æ¬¡å…ƒ
        time_dimension = Dimension(
            name="time",
            type=DimensionType.TIME,
            hierarchy_levels=["year", "quarter", "month", "week", "day", "hour"],
            default_level="day",
            description="æ™‚é–“éšå±¤",
            data_source="datetime_column"
        )
        
        # ã‚¹ã‚¿ãƒƒãƒ•æ¬¡å…ƒ
        staff_dimension = Dimension(
            name="staff",
            type=DimensionType.HIERARCHY,
            hierarchy_levels=["department", "team", "role", "individual"],
            default_level="role",
            description="ã‚¹ã‚¿ãƒƒãƒ•éšå±¤",
            data_source="staff_data"
        )
        
        # ã‚·ãƒ•ãƒˆæ¬¡å…ƒ
        shift_dimension = Dimension(
            name="shift",
            type=DimensionType.CATEGORY,
            hierarchy_levels=["shift_type", "shift_code"],
            default_level="shift_type",
            description="ã‚·ãƒ•ãƒˆåˆ†é¡",
            data_source="shift_data"
        )
        
        # æ–½è¨­æ¬¡å…ƒ
        facility_dimension = Dimension(
            name="facility",
            type=DimensionType.HIERARCHY,
            hierarchy_levels=["region", "facility_group", "facility"],
            default_level="facility",
            description="æ–½è¨­éšå±¤",
            data_source="facility_data"
        )
        
        # ãƒ¡ã‚¸ãƒ£ãƒ¼å®šç¾©
        measures = [
            Measure(
                name="total_hours",
                aggregation_type=AggregationType.SUM,
                source_column="work_hours",
                description="ç·åŠ´åƒæ™‚é–“",
                unit="æ™‚é–“"
            ),
            Measure(
                name="staff_count",
                aggregation_type=AggregationType.COUNT,
                source_column="staff_id",
                description="ã‚¹ã‚¿ãƒƒãƒ•æ•°",
                unit="äºº"
            ),
            Measure(
                name="avg_hours_per_staff",
                aggregation_type=AggregationType.MEAN,
                source_column="work_hours",
                description="ã‚¹ã‚¿ãƒƒãƒ•å¹³å‡åŠ´åƒæ™‚é–“",
                unit="æ™‚é–“/äºº"
            ),
            Measure(
                name="total_cost",
                aggregation_type=AggregationType.SUM,
                source_column="labor_cost",
                description="ç·äººä»¶è²»",
                unit="å††"
            ),
            Measure(
                name="efficiency_score",
                aggregation_type=AggregationType.MEAN,
                source_column="efficiency",
                description="åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢",
                unit="ãƒã‚¤ãƒ³ãƒˆ"
            )
        ]
        
        # ã‚·ãƒ•ãƒˆåˆ†æã‚­ãƒ¥ãƒ¼ãƒ–
        shift_cube = CubeDefinition(
            name="shift_analysis_cube",
            dimensions=[time_dimension, staff_dimension, shift_dimension, facility_dimension],
            measures=measures,
            data_source="shift_analysis_data",
            refresh_frequency="hourly",
            description="ã‚·ãƒ•ãƒˆåˆ†æç”¨å¤šæ¬¡å…ƒã‚­ãƒ¥ãƒ¼ãƒ–"
        )
        
        self.cubes["shift_analysis_cube"] = shift_cube
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚­ãƒ¥ãƒ¼ãƒ–
        performance_measures = [
            Measure(
                name="productivity_index",
                aggregation_type=AggregationType.MEAN,
                source_column="productivity",
                description="ç”Ÿç”£æ€§æŒ‡æ•°",
                unit="ãƒã‚¤ãƒ³ãƒˆ"
            ),
            Measure(
                name="quality_score",
                aggregation_type=AggregationType.MEAN,
                source_column="quality",
                description="å“è³ªã‚¹ã‚³ã‚¢",
                unit="ãƒã‚¤ãƒ³ãƒˆ"
            ),
            Measure(
                name="customer_satisfaction",
                aggregation_type=AggregationType.MEAN,
                source_column="satisfaction",
                description="é¡§å®¢æº€è¶³åº¦",
                unit="ãƒã‚¤ãƒ³ãƒˆ"
            )
        ]
        
        performance_cube = CubeDefinition(
            name="performance_analysis_cube",
            dimensions=[time_dimension, staff_dimension, facility_dimension],
            measures=performance_measures,
            data_source="performance_data",
            refresh_frequency="daily",
            description="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æç”¨å¤šæ¬¡å…ƒã‚­ãƒ¥ãƒ¼ãƒ–"
        )
        
        self.cubes["performance_analysis_cube"] = performance_cube
    
    def execute_olap_query(self, query: OLAPQuery) -> AggregationResult:
        """OLAPã‚¯ã‚¨ãƒªå®Ÿè¡Œ"""
        
        print(f"ğŸ¯ OLAPã‚¯ã‚¨ãƒªå®Ÿè¡Œ: {query.cube_name}")
        
        start_time = datetime.now()
        
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = self._generate_cache_key(query)
            if self.system_config['cache_enabled'] and cache_key in self.query_cache:
                cached_result = self.query_cache[cache_key]
                if self._is_cache_valid(cached_result['timestamp']):
                    print("  ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœå–å¾—")
                    return cached_result['result']
            
            # ã‚­ãƒ¥ãƒ¼ãƒ–å­˜åœ¨ç¢ºèª
            if query.cube_name not in self.cubes:
                raise ValueError(f"ã‚­ãƒ¥ãƒ¼ãƒ–ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {query.cube_name}")
            
            cube = self.cubes[query.cube_name]
            
            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ï¼‰
            raw_data = self._generate_mock_data(cube, query)
            
            # æ¬¡å…ƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_data = self._apply_filters(raw_data, query.filters)
            
            # é›†ç´„å®Ÿè¡Œ
            aggregated_data = self._perform_aggregation(filtered_data, cube, query)
            
            # ãƒ‰ãƒªãƒ«ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
            drilled_data = self._apply_drill_operations(aggregated_data, query.drill_path, cube)
            
            # ã‚½ãƒ¼ãƒˆé©ç”¨
            if query.sort_by:
                drilled_data = self._apply_sorting(drilled_data, query.sort_by)
            
            # åˆ¶é™é©ç”¨
            if query.limit:
                drilled_data = self._apply_limit(drilled_data, query.limit)
            
            # å®Ÿè¡Œæ™‚é–“è¨ˆç®—
            execution_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            quality_score = self._calculate_query_quality_score(query, drilled_data, cube)
            
            # è§£é‡ˆã¨æ¨å¥¨äº‹é …
            interpretation = self._interpret_aggregation_results(drilled_data, query, cube)
            recommendations = self._generate_aggregation_recommendations(drilled_data, query)
            
            # çµæœä½œæˆ
            result = AggregationResult(
                query=query,
                data=drilled_data,
                dimensions_used=query.selected_dimensions,
                measures_calculated=query.selected_measures,
                total_records=len(drilled_data.get('records', [])),
                execution_time_ms=execution_time,
                cache_hit=False,
                quality_score=quality_score,
                interpretation=interpretation,
                recommendations=recommendations
            )
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            if self.system_config['cache_enabled']:
                self.query_cache[cache_key] = {
                    'result': result,
                    'timestamp': datetime.now()
                }
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™
                if len(self.query_cache) > self.system_config['max_cache_entries']:
                    self._cleanup_cache()
            
            print(f"  âœ… ã‚¯ã‚¨ãƒªå®Ÿè¡Œå®Œäº†: {execution_time:.1f}ms, å“è³ª:{quality_score:.2f}")
            return result
            
        except Exception as e:
            print(f"  âŒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_aggregation_result(query, str(e))
    
    def create_pivot_table(self, data: Dict[str, Any], rows: List[str], columns: List[str], 
                          values: str, aggfunc: str = 'sum') -> Dict[str, Any]:
        """ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        
        print(f"ğŸ“Š ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ: {values} by {rows}Ã—{columns}")
        
        try:
            # Mock DataFrameä½œæˆ
            df = pd.DataFrame(data)
            
            # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            pivot_result = df.pivot_table(
                values=values,
                index=rows,
                columns=columns,
                aggfunc=aggfunc
            )
            
            # çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
            pivot_data = {
                'pivot_table': pivot_result.to_dict(),
                'rows': rows,
                'columns': columns,
                'values': values,
                'aggregation': aggfunc,
                'total_cells': len(rows) * len(columns) if rows and columns else 0
            }
            
            print(f"  âœ… ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆå®Œäº†")
            return pivot_data
            
        except Exception as e:
            print(f"  âŒ ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def perform_drill_down(self, current_query: OLAPQuery, dimension: str, 
                          target_level: str) -> AggregationResult:
        """ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³æ“ä½œ"""
        
        print(f"ğŸ” ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³: {dimension} â†’ {target_level}")
        
        try:
            # æ–°ã—ã„ã‚¯ã‚¨ãƒªä½œæˆ
            new_query = OLAPQuery(
                cube_name=current_query.cube_name,
                selected_dimensions=current_query.selected_dimensions,
                selected_measures=current_query.selected_measures,
                filters=current_query.filters,
                drill_path=current_query.drill_path + [(dimension, target_level)],
                sort_by=current_query.sort_by,
                limit=current_query.limit
            )
            
            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            result = self.execute_olap_query(new_query)
            
            print(f"  âœ… ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³å®Œäº†: {target_level}ãƒ¬ãƒ™ãƒ«")
            return result
            
        except Exception as e:
            print(f"  âŒ ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_aggregation_result(current_query, str(e))
    
    def perform_drill_up(self, current_query: OLAPQuery, dimension: str, 
                        target_level: str) -> AggregationResult:
        """ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—æ“ä½œ"""
        
        print(f"ğŸ” ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—: {dimension} â†’ {target_level}")
        
        try:
            # æ–°ã—ã„ã‚¯ã‚¨ãƒªä½œæˆï¼ˆãƒ‰ãƒªãƒ«ãƒ‘ã‚¹ã‹ã‚‰è©²å½“ãƒ¬ãƒ™ãƒ«ã‚’å‰Šé™¤ï¼‰
            new_drill_path = [
                (dim, level) for dim, level in current_query.drill_path 
                if not (dim == dimension and level != target_level)
            ]
            new_drill_path.append((dimension, target_level))
            
            new_query = OLAPQuery(
                cube_name=current_query.cube_name,
                selected_dimensions=current_query.selected_dimensions,
                selected_measures=current_query.selected_measures,
                filters=current_query.filters,
                drill_path=new_drill_path,
                sort_by=current_query.sort_by,
                limit=current_query.limit
            )
            
            # ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
            result = self.execute_olap_query(new_query)
            
            print(f"  âœ… ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—å®Œäº†: {target_level}ãƒ¬ãƒ™ãƒ«")
            return result
            
        except Exception as e:
            print(f"  âŒ ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_error_aggregation_result(current_query, str(e))
    
    def create_dynamic_aggregation(self, data: Dict[str, Any], group_by: List[str], 
                                 measures: Dict[str, str]) -> Dict[str, Any]:
        """å‹•çš„é›†ç´„å®Ÿè¡Œ"""
        
        print(f"ğŸ¯ å‹•çš„é›†ç´„å®Ÿè¡Œ: {group_by} â†’ {list(measures.keys())}")
        
        try:
            # Mock DataFrameä½œæˆ
            df = pd.DataFrame(data)
            
            if not group_by:
                # å…¨ä½“é›†ç´„
                aggregated = {}
                for measure, agg_func in measures.items():
                    if measure in data:
                        values = data[measure]
                        if agg_func == 'sum':
                            aggregated[measure] = sum(values)
                        elif agg_func == 'mean':
                            aggregated[measure] = np.mean(values)
                        elif agg_func == 'count':
                            aggregated[measure] = len(values)
                        else:
                            aggregated[measure] = values[0] if values else 0
            else:
                # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥é›†ç´„
                grouped = df.groupby(group_by)
                aggregated = grouped.agg(measures).to_dict()
            
            result = {
                'aggregated_data': aggregated,
                'group_by_columns': group_by,
                'measures': measures,
                'total_groups': len(aggregated) if isinstance(aggregated, dict) else 1,
                'execution_successful': True
            }
            
            print(f"  âœ… å‹•çš„é›†ç´„å®Œäº†: {len(aggregated)}ã‚°ãƒ«ãƒ¼ãƒ—")
            return result
            
        except Exception as e:
            print(f"  âŒ å‹•çš„é›†ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e), 'execution_successful': False}
    
    def create_multi_dimensional_view(self, cube_name: str, dimensions: List[str], 
                                    measures: List[str], filters: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼ä½œæˆ"""
        
        print(f"ğŸŒ å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼ä½œæˆ: {cube_name}")
        
        try:
            query = OLAPQuery(
                cube_name=cube_name,
                selected_dimensions=dimensions,
                selected_measures=measures,
                filters=filters or {},
                drill_path=[],
                sort_by=None,
                limit=None
            )
            
            result = self.execute_olap_query(query)
            
            # å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼ç”¨ã®æ§‹é€ åŒ–
            view = {
                'cube_name': cube_name,
                'dimensions': dimensions,
                'measures': measures,
                'data': result.data,
                'total_records': result.total_records,
                'quality_score': result.quality_score,
                'view_type': 'multi_dimensional',
                'created_at': datetime.now().isoformat()
            }
            
            print(f"  âœ… å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼ä½œæˆå®Œäº†")
            return view
            
        except Exception as e:
            print(f"  âŒ å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _generate_mock_data(self, cube: CubeDefinition, query: OLAPQuery) -> Dict[str, Any]:
        """ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        
        n_records = 1000
        
        mock_data = {}
        
        # æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        for dimension in cube.dimensions:
            if dimension.name == "time":
                dates = pd.DataFrame({
                    'dates': [datetime.now() - timedelta(days=i) for i in range(n_records)]
                })
                mock_data['time'] = [d.strftime('%Y-%m-%d') for d in dates.data['dates']]
            elif dimension.name == "staff":
                mock_data['staff'] = [f"Staff_{i%50}" for i in range(n_records)]
            elif dimension.name == "shift":
                shift_types = ['Morning', 'Afternoon', 'Night', 'Weekend']
                mock_data['shift'] = [shift_types[i % len(shift_types)] for i in range(n_records)]
            elif dimension.name == "facility":
                facilities = ['Facility_A', 'Facility_B', 'Facility_C']
                mock_data['facility'] = [facilities[i % len(facilities)] for i in range(n_records)]
        
        # ãƒ¡ã‚¸ãƒ£ãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        for measure in cube.measures:
            if measure.name == "total_hours":
                mock_data['work_hours'] = np.random.uniform(6, 10, n_records).tolist()
            elif measure.name == "staff_count":
                mock_data['staff_id'] = [f"ID_{i}" for i in range(n_records)]
            elif measure.name == "total_cost":
                mock_data['labor_cost'] = np.random.uniform(15000, 25000, n_records).tolist()
            elif measure.name == "efficiency_score":
                mock_data['efficiency'] = np.random.uniform(70, 95, n_records).tolist()
            elif measure.name == "productivity_index":
                mock_data['productivity'] = np.random.uniform(80, 120, n_records).tolist()
            elif measure.name == "quality_score":
                mock_data['quality'] = np.random.uniform(85, 98, n_records).tolist()
        
        return mock_data
    
    def _apply_filters(self, data: Dict[str, Any], filters: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚£ãƒ«ã‚¿é©ç”¨"""
        
        if not filters:
            return data
        
        # ç°¡å˜ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
        filtered_data = {}
        
        # ãƒ•ã‚£ãƒ«ã‚¿æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
        valid_indices = set(range(len(next(iter(data.values())))))
        
        for filter_column, filter_value in filters.items():
            if filter_column in data:
                column_data = data[filter_column]
                if isinstance(filter_value, list):
                    # INæ¡ä»¶
                    valid_indices &= {i for i, v in enumerate(column_data) if v in filter_value}
                else:
                    # ç­‰å€¤æ¡ä»¶
                    valid_indices &= {i for i, v in enumerate(column_data) if v == filter_value}
        
        # ãƒ•ã‚£ãƒ«ã‚¿å¾Œã®ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        for column, values in data.items():
            filtered_data[column] = [values[i] for i in sorted(valid_indices)]
        
        return filtered_data
    
    def _perform_aggregation(self, data: Dict[str, Any], cube: CubeDefinition, 
                           query: OLAPQuery) -> Dict[str, Any]:
        """é›†ç´„å®Ÿè¡Œ"""
        
        # Mocké›†ç´„å®Ÿè£…
        aggregated = {
            'records': [],
            'summary': {}
        }
        
        # æ¬¡å…ƒã¨ãƒ¡ã‚¸ãƒ£ãƒ¼ã®çµ„ã¿åˆã‚ã›ã§é›†ç´„
        for i in range(min(100, len(data.get(query.selected_dimensions[0], [])))):
            record = {}
            
            # æ¬¡å…ƒå€¤
            for dim in query.selected_dimensions:
                if dim in data:
                    record[dim] = data[dim][i % len(data[dim])]
            
            # ãƒ¡ã‚¸ãƒ£ãƒ¼å€¤
            for measure in query.selected_measures:
                # ã‚­ãƒ¥ãƒ¼ãƒ–å®šç¾©ã‹ã‚‰ã‚½ãƒ¼ã‚¹ã‚«ãƒ©ãƒ ç‰¹å®š
                source_col = None
                for m in cube.measures:
                    if m.name == measure:
                        source_col = m.source_column
                        break
                
                if source_col and source_col in data:
                    record[measure] = data[source_col][i % len(data[source_col])]
                else:
                    record[measure] = np.random.uniform(50, 100)
            
            aggregated['records'].append(record)
        
        # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
        for measure in query.selected_measures:
            values = [r.get(measure, 0) for r in aggregated['records']]
            aggregated['summary'][measure] = {
                'sum': sum(values),
                'mean': np.mean(values),
                'count': len(values),
                'min': min(values) if values else 0,
                'max': max(values) if values else 0
            }
        
        return aggregated
    
    def _apply_drill_operations(self, data: Dict[str, Any], drill_path: List[Tuple[str, str]], 
                              cube: CubeDefinition) -> Dict[str, Any]:
        """ãƒ‰ãƒªãƒ«ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨"""
        
        if not drill_path:
            return data
        
        # ãƒ‰ãƒªãƒ«ãƒ‘ã‚¹ã«åŸºã¥ããƒ‡ãƒ¼ã‚¿å¤‰æ›
        drilled_data = data.copy()
        
        for dimension, level in drill_path:
            # ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸãƒ‡ãƒ¼ã‚¿å¤‰æ›
            if dimension == "time":
                if level == "month":
                    # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æœˆæ¬¡ã«é›†ç´„
                    for record in drilled_data['records']:
                        if 'time' in record:
                            date_str = record['time']
                            try:
                                date_obj = datetime.strptime(date_str, '%Y-%m-%d')
                                record['time'] = date_obj.strftime('%Y-%m')
                            except:
                                pass
            elif dimension == "shift":
                if level == "shift_type":
                    # ã‚·ãƒ•ãƒˆã‚¿ã‚¤ãƒ—ãƒ¬ãƒ™ãƒ«ã§ã®é›†ç´„
                    pass
        
        return drilled_data
    
    def _apply_sorting(self, data: Dict[str, Any], sort_by: str) -> Dict[str, Any]:
        """ã‚½ãƒ¼ãƒˆé©ç”¨"""
        
        if 'records' not in data:
            return data
        
        try:
            data['records'].sort(key=lambda x: x.get(sort_by, 0), reverse=True)
        except:
            pass
        
        return data
    
    def _apply_limit(self, data: Dict[str, Any], limit: int) -> Dict[str, Any]:
        """åˆ¶é™é©ç”¨"""
        
        if 'records' in data:
            data['records'] = data['records'][:limit]
        
        return data
    
    def _generate_cache_key(self, query: OLAPQuery) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        return f"{query.cube_name}_{hash(str(query.__dict__))}"
    
    def _is_cache_valid(self, timestamp: datetime) -> bool:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯"""
        ttl_minutes = self.system_config['cache_ttl_minutes']
        return datetime.now() - timestamp < timedelta(minutes=ttl_minutes)
    
    def _cleanup_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        current_time = datetime.now()
        ttl_minutes = self.system_config['cache_ttl_minutes']
        
        expired_keys = [
            key for key, value in self.query_cache.items()
            if current_time - value['timestamp'] > timedelta(minutes=ttl_minutes)
        ]
        
        for key in expired_keys:
            del self.query_cache[key]
    
    def _calculate_query_quality_score(self, query: OLAPQuery, data: Dict[str, Any], 
                                     cube: CubeDefinition) -> float:
        """ã‚¯ã‚¨ãƒªå“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        quality_factors = []
        
        # ãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§
        data_completeness = 1.0 if data.get('records') else 0.5
        quality_factors.append(data_completeness * 0.4)
        
        # ã‚¯ã‚¨ãƒªè¤‡é›‘æ€§
        complexity_score = min(1.0, (len(query.selected_dimensions) + len(query.selected_measures)) / 10)
        quality_factors.append(complexity_score * 0.3)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        performance_score = 1.0  # Mockå®Ÿè£…ã§ã¯å¸¸ã«é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
        quality_factors.append(performance_score * 0.3)
        
        return sum(quality_factors)
    
    def _interpret_aggregation_results(self, data: Dict[str, Any], query: OLAPQuery, 
                                     cube: CubeDefinition) -> str:
        """é›†ç´„çµæœã®è§£é‡ˆ"""
        
        interpretations = []
        
        total_records = len(data.get('records', []))
        interpretations.append(f"{total_records}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒé›†ç´„ã•ã‚Œã¾ã—ãŸ")
        
        if data.get('summary'):
            for measure, stats in data['summary'].items():
                mean_val = stats.get('mean', 0)
                interpretations.append(f"{measure}ã®å¹³å‡å€¤ã¯{mean_val:.2f}ã§ã™")
        
        return "ã€‚".join(interpretations) + "ã€‚"
    
    def _generate_aggregation_recommendations(self, data: Dict[str, Any], 
                                            query: OLAPQuery) -> List[str]:
        """é›†ç´„çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …"""
        
        recommendations = []
        
        total_records = len(data.get('records', []))
        
        if total_records > 1000:
            recommendations.append("ãƒ‡ãƒ¼ã‚¿é‡ãŒå¤šã„ãŸã‚ã€ãƒ•ã‚£ãƒ«ã‚¿ã®è¿½åŠ ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        if len(query.selected_dimensions) > 5:
            recommendations.append("æ¬¡å…ƒæ•°ãŒå¤šã„ãŸã‚ã€é‡è¦ãªæ¬¡å…ƒã«çµã£ã¦åˆ†æã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™")
        
        if not query.drill_path:
            recommendations.append("ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã¦ã‚ˆã‚Šè©³ç´°ãªåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„")
        
        return recommendations
    
    def _create_error_aggregation_result(self, query: OLAPQuery, error_msg: str) -> AggregationResult:
        """ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ"""
        
        return AggregationResult(
            query=query,
            data={'error': error_msg},
            dimensions_used=[],
            measures_calculated=[],
            total_records=0,
            execution_time_ms=0,
            cache_hit=False,
            quality_score=0.0,
            interpretation=f"é›†ç´„å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}",
            recommendations=["ãƒ‡ãƒ¼ã‚¿ã¨ã‚¯ã‚¨ãƒªã®ç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"]
        )


def test_enhanced_data_aggregation_olap_system():
    """å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    
    print("ğŸ§ª å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹...")
    
    system = EnhancedDataAggregationOLAPSystem()
    
    test_results = {}
    
    try:
        print("\nğŸ¯ åŸºæœ¬OLAPã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ...")
        
        # åŸºæœ¬ã‚¯ã‚¨ãƒª
        basic_query = OLAPQuery(
            cube_name="shift_analysis_cube",
            selected_dimensions=["time", "staff"],
            selected_measures=["total_hours", "staff_count"],
            filters={"shift": ["Morning", "Afternoon"]},
            drill_path=[],
            sort_by="total_hours",
            limit=50
        )
        
        basic_result = system.execute_olap_query(basic_query)
        test_results['basic_olap'] = basic_result.quality_score > 0.8
        print(f"  âœ… åŸºæœ¬OLAP: å“è³ª{basic_result.quality_score:.2f}, ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°{basic_result.total_records}")
        
        print("\nğŸ” ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³ãƒ†ã‚¹ãƒˆ...")
        
        # ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³
        drill_result = system.perform_drill_down(basic_query, "time", "month")
        test_results['drill_down'] = drill_result.quality_score > 0.8
        print(f"  âœ… ãƒ‰ãƒªãƒ«ãƒ€ã‚¦ãƒ³: å“è³ª{drill_result.quality_score:.2f}")
        
        print("\nğŸ” ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ...")
        
        # ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—
        drill_up_result = system.perform_drill_up(drill_result.query, "time", "day")
        test_results['drill_up'] = drill_up_result.quality_score > 0.8
        print(f"  âœ… ãƒ‰ãƒªãƒ«ã‚¢ãƒƒãƒ—: å“è³ª{drill_up_result.quality_score:.2f}")
        
        print("\nğŸ“Š ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ãƒ†ã‚¹ãƒˆ...")
        
        # ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
        pivot_data = {
            'department': ['A', 'B', 'A', 'B', 'A', 'B'] * 20,
            'month': ['Jan', 'Jan', 'Feb', 'Feb', 'Mar', 'Mar'] * 20,
            'hours': np.random.uniform(100, 200, 120).tolist(),
            'cost': np.random.uniform(50000, 100000, 120).tolist()
        }
        
        pivot_result = system.create_pivot_table(
            pivot_data, 
            rows=['department'], 
            columns=['month'], 
            values='hours'
        )
        test_results['pivot_table'] = 'error' not in pivot_result
        print(f"  âœ… ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«: {'æˆåŠŸ' if test_results['pivot_table'] else 'å¤±æ•—'}")
        
        print("\nğŸ¯ å‹•çš„é›†ç´„ãƒ†ã‚¹ãƒˆ...")
        
        # å‹•çš„é›†ç´„
        agg_data = {
            'category': ['A', 'B', 'A', 'B'] * 25,
            'value1': np.random.uniform(50, 150, 100).tolist(),
            'value2': np.random.uniform(20, 80, 100).tolist()
        }
        
        dynamic_result = system.create_dynamic_aggregation(
            agg_data,
            group_by=['category'],
            measures={'value1': 'sum', 'value2': 'mean'}
        )
        test_results['dynamic_aggregation'] = dynamic_result.get('execution_successful', False)
        print(f"  âœ… å‹•çš„é›†ç´„: {'æˆåŠŸ' if test_results['dynamic_aggregation'] else 'å¤±æ•—'}")
        
        print("\nğŸŒ å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼ãƒ†ã‚¹ãƒˆ...")
        
        # å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼
        multi_view = system.create_multi_dimensional_view(
            cube_name="performance_analysis_cube",
            dimensions=["time", "staff"],
            measures=["productivity_index", "quality_score"],
            filters={"facility": ["Facility_A"]}
        )
        test_results['multi_dimensional_view'] = 'error' not in multi_view
        print(f"  âœ… å¤šæ¬¡å…ƒãƒ“ãƒ¥ãƒ¼: {'æˆåŠŸ' if test_results['multi_dimensional_view'] else 'å¤±æ•—'}")
        
        # çµæœåˆ†æ
        print("\n" + "="*60)
        print("ğŸ† å¼·åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆçµæœ")
        print("="*60)
        
        successful_tests = sum(test_results.values())
        total_tests = len(test_results)
        success_rate = (successful_tests / total_tests) * 100
        
        for test_name, success in test_results.items():
            status = "âœ…" if success else "âŒ"
            print(f"{status} {test_name}: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆæˆåŠŸç‡: {successful_tests}/{total_tests} ({success_rate:.1f}%)")
        
        # å…¨ä½“å“è³ªè©•ä¾¡
        overall_quality = (basic_result.quality_score + drill_result.quality_score) / 2
        print(f"ğŸ¯ å…¨ä½“å“è³ªã‚¹ã‚³ã‚¢: {overall_quality:.2f}")
        
        if success_rate >= 80 and overall_quality >= 0.80:
            print("\nğŸŒŸ ãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPæ©Ÿèƒ½ãŒç›®æ¨™å“è³ª80%+ã‚’é”æˆã—ã¾ã—ãŸï¼")
            return True
        else:
            print("\nâš ï¸ ãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPæ©Ÿèƒ½ã®å“è³ªå‘ä¸ŠãŒå¿…è¦ã§ã™")
            return False
            
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False


if __name__ == "__main__":
    success = test_enhanced_data_aggregation_olap_system()
    print(f"\nğŸ¯ ãƒ‡ãƒ¼ã‚¿é›†ç´„ãƒ»OLAPæ©Ÿèƒ½å¼·åŒ–: {'æˆåŠŸ' if success else 'è¦æ”¹å–„'}")