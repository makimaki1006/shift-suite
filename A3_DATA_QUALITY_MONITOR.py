#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
A3.2 ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 
è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¦¥å½“æ€§ã‚’ç¶™ç¶šçš„ã«è¿½æ±‚ã—ã€æ•°å€¤ã®æ„å‘³ã‚’æ·±ãç†è§£ã™ã‚‹
670æ™‚é–“ã¯ç¾åœ¨ã®çµæœã§ã‚ã‚Šã€çµ¶å¯¾çš„ãªæ­£è§£ã§ã¯ãªã„ã¨ã„ã†è¦–ç‚¹ã§ç›£è¦–
"""

import os
import sys
import json
import re
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class QualityDimension(Enum):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ¬¡å…ƒ"""
    ACCURACY = "accuracy"           # æ­£ç¢ºæ€§
    CONSISTENCY = "consistency"     # ä¸€è²«æ€§
    COMPLETENESS = "completeness"   # å®Œå…¨æ€§
    VALIDITY = "validity"           # å¦¥å½“æ€§
    TIMELINESS = "timeliness"      # é©æ™‚æ€§
    LOGIC_SOUNDNESS = "logic_soundness"  # è«–ç†çš„å¥å…¨æ€§

@dataclass
class DataQualityMetric:
    """ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™"""
    dimension: QualityDimension
    metric_name: str
    description: str
    calculation_method: str
    threshold_good: float
    threshold_acceptable: float

@dataclass
class QualityInsight:
    """å“è³ªæ´å¯Ÿ"""
    insight_type: str
    message: str
    evidence: Dict[str, Any]
    improvement_suggestion: str
    priority: str

class DataQualityMonitor:
    """æ·±ã„æ€è€ƒã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–"""
    
    def __init__(self):
        self.quality_dir = Path("logs/data_quality")
        self.quality_dir.mkdir(parents=True, exist_ok=True)
        
        # å“è³ªæŒ‡æ¨™å®šç¾©ï¼ˆ670æ™‚é–“ã‚’çµ¶å¯¾è¦–ã—ãªã„ï¼‰
        self.quality_metrics = self._define_quality_metrics()
        
        # è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¦¥å½“æ€§æ¤œè¨¼åŸºæº–
        self.logic_validation_rules = self._define_logic_validation_rules()
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªå±¥æ­´
        self.quality_history = []
        
    def _define_quality_metrics(self) -> List[DataQualityMetric]:
        """æ·±ã„æ€è€ƒã«ã‚ˆã‚‹å“è³ªæŒ‡æ¨™å®šç¾©"""
        
        return [
            # è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¥å…¨æ€§
            DataQualityMetric(
                dimension=QualityDimension.LOGIC_SOUNDNESS,
                metric_name="slot_hours_conversion_validity",
                description="SLOT_HOURSå¤‰æ›ã®è«–ç†çš„å¦¥å½“æ€§",
                calculation_method="å¤‰æ›å‰å¾Œã®å€¤ã®æ„å‘³çš„æ•´åˆæ€§ã‚’æ¤œè¨¼",
                threshold_good=0.95,
                threshold_acceptable=0.90
            ),
            DataQualityMetric(
                dimension=QualityDimension.LOGIC_SOUNDNESS,
                metric_name="calculation_chain_integrity",
                description="è¨ˆç®—ãƒã‚§ãƒ¼ãƒ³ã®æ•´åˆæ€§",
                calculation_method="å…¥åŠ›â†’å¤‰æ›â†’é›†è¨ˆã®å„æ®µéšã§ã®å€¤ã®è¿½è·¡",
                threshold_good=0.98,
                threshold_acceptable=0.95
            ),
            
            # æ•°å€¤ã®æ„å‘³çš„å¦¥å½“æ€§
            DataQualityMetric(
                dimension=QualityDimension.VALIDITY,
                metric_name="shortage_ratio_reasonableness",
                description="ä¸è¶³/éå‰°æ¯”ç‡ã®æ¥­å‹™çš„å¦¥å½“æ€§",
                calculation_method="670:505ã®æ¯”ç‡ãŒåŒ»ç™‚ç¾å ´ã¨ã—ã¦åˆç†çš„ã‹",
                threshold_good=0.90,
                threshold_acceptable=0.80
            ),
            DataQualityMetric(
                dimension=QualityDimension.VALIDITY,
                metric_name="working_hours_distribution",
                description="åŠ´åƒæ™‚é–“åˆ†å¸ƒã®ç¾å®Ÿæ€§",
                calculation_method="å€‹äººåˆ¥åŠ´åƒæ™‚é–“ãŒåŠ´åŸºæ³•ãƒ»å®Ÿæ…‹ã¨æ•´åˆã™ã‚‹ã‹",
                threshold_good=0.95,
                threshold_acceptable=0.90
            ),
            
            # ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§
            DataQualityMetric(
                dimension=QualityDimension.CONSISTENCY,
                metric_name="cross_module_consistency",
                description="ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“æ•°å€¤ä¸€è²«æ€§",
                calculation_method="shortage, Phase2, Phase3.1ã®æ•°å€¤æ•´åˆæ€§",
                threshold_good=0.99,
                threshold_acceptable=0.95
            ),
            DataQualityMetric(
                dimension=QualityDimension.CONSISTENCY,
                metric_name="temporal_consistency",
                description="æ™‚ç³»åˆ—çš„ä¸€è²«æ€§",
                calculation_method="é€±æ¬¡ãƒ»æœˆæ¬¡é›†è¨ˆå€¤ã®å¤‰å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³",
                threshold_good=0.90,
                threshold_acceptable=0.85
            ),
            
            # ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§
            DataQualityMetric(
                dimension=QualityDimension.COMPLETENESS,
                metric_name="input_data_coverage",
                description="å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚«ãƒãƒ¬ãƒƒã‚¸",
                calculation_method="æ¬ æå€¤ãƒ»ç•°å¸¸å€¤ã®å‰²åˆ",
                threshold_good=0.95,
                threshold_acceptable=0.90
            ),
            
            # æ­£ç¢ºæ€§ï¼ˆçµ¶å¯¾çš„ã§ã¯ãªãç›¸å¯¾çš„ï¼‰
            DataQualityMetric(
                dimension=QualityDimension.ACCURACY,
                metric_name="calculation_precision",
                description="è¨ˆç®—ç²¾åº¦ï¼ˆå°æ•°ç‚¹å‡¦ç†ç­‰ï¼‰",
                calculation_method="ä¸¸ã‚èª¤å·®ãƒ»ç²¾åº¦æå¤±ã®è©•ä¾¡",
                threshold_good=0.999,
                threshold_acceptable=0.995
            )
        ]
    
    def _define_logic_validation_rules(self) -> List[Dict[str, Any]]:
        """è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å¦¥å½“æ€§æ¤œè¨¼ãƒ«ãƒ¼ãƒ«"""
        
        return [
            {
                "rule_id": "SLOT_DEFINITION",
                "name": "ã‚¹ãƒ­ãƒƒãƒˆå®šç¾©ã®å¦¥å½“æ€§",
                "check": "30åˆ†ã‚¹ãƒ­ãƒƒãƒˆãŒæ¥­å‹™å®Ÿæ…‹ã¨åˆè‡´ã™ã‚‹ã‹",
                "question": "ãªãœ30åˆ†å˜ä½ï¼Ÿ15åˆ†ã‚„60åˆ†ã§ã¯ãªã„ã®ã‹ï¼Ÿ"
            },
            {
                "rule_id": "AGGREGATION_METHOD",
                "name": "é›†è¨ˆæ–¹æ³•ã®é©åˆ‡æ€§",
                "check": "å˜ç´”åˆè¨ˆã§è‰¯ã„ã®ã‹ã€é‡ã¿ä»˜ã‘ãŒå¿…è¦ã‹",
                "question": "å…¨ã¦ã®ã‚¹ãƒ­ãƒƒãƒˆã‚’ç­‰ä¾¡ã«æ‰±ã†ã“ã¨ã¯é©åˆ‡ã‹ï¼Ÿ"
            },
            {
                "rule_id": "SHORTAGE_DEFINITION",
                "name": "ä¸è¶³ã®å®šç¾©",
                "check": "ä½•ã‚’æŒã£ã¦ã€Œä¸è¶³ã€ã¨ã™ã‚‹ã‹ã®åŸºæº–",
                "question": "éœ€è¦ã¨ä¾›çµ¦ã®å·®ã ã‘ã§ä¸è¶³ã‚’å®šç¾©ã—ã¦è‰¯ã„ã‹ï¼Ÿ"
            },
            {
                "rule_id": "BASELINE_MEANING",
                "name": "åŸºæº–å€¤ã®æ„å‘³",
                "check": "670æ™‚é–“ãŒç¤ºã™å®Ÿéš›ã®æ„å‘³",
                "question": "670æ™‚é–“ã¯æœˆé–“ï¼Ÿå¹´é–“ï¼Ÿã©ã®ç¯„å›²ã®é›†è¨ˆã‹ï¼Ÿ"
            }
        ]
    
    def analyze_data_quality(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ"""
        
        print("ğŸ“Š A3.2 ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        print("ğŸ¯ è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¦¥å½“æ€§ã‚’ç¶™ç¶šçš„ã«è¿½æ±‚")
        print("ğŸ’¡ 670æ™‚é–“ã¯ç¾åœ¨ã®çµæœã§ã‚ã‚Šã€çµ¶å¯¾çš„æ­£è§£ã§ã¯ãªã„")
        print("=" * 80)
        
        analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "quality_scores": {},
            "insights": [],
            "logic_validation": {},
            "recommendations": [],
            "overall_assessment": ""
        }
        
        # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åé›†
        print("\nğŸ“ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åé›†...")
        existing_data = self._collect_existing_data()
        
        # 2. å“è³ªæŒ‡æ¨™è©•ä¾¡
        print("\nğŸ“ å“è³ªæŒ‡æ¨™è©•ä¾¡...")
        for metric in self.quality_metrics:
            score = self._evaluate_quality_metric(metric, existing_data)
            analysis_results["quality_scores"][metric.metric_name] = score
            print(f"  {metric.metric_name}: {score['score']:.3f} ({score['status']})")
        
        # 3. è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å¦¥å½“æ€§æ¤œè¨¼
        print("\nğŸ” è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å¦¥å½“æ€§æ¤œè¨¼...")
        for rule in self.logic_validation_rules:
            validation = self._validate_logic_rule(rule, existing_data)
            analysis_results["logic_validation"][rule["rule_id"]] = validation
            print(f"  {rule['name']}: {validation['status']}")
        
        # 4. æ·±ã„æ´å¯Ÿã®ç”Ÿæˆ
        print("\nğŸ’­ æ·±ã„æ´å¯Ÿç”Ÿæˆ...")
        insights = self._generate_deep_insights(existing_data, analysis_results)
        analysis_results["insights"] = insights
        
        # 5. æ”¹å–„ææ¡ˆ
        print("\nğŸ’¡ æ”¹å–„ææ¡ˆç”Ÿæˆ...")
        recommendations = self._generate_recommendations(analysis_results)
        analysis_results["recommendations"] = recommendations
        
        # 6. ç·åˆè©•ä¾¡
        analysis_results["overall_assessment"] = self._generate_overall_assessment(analysis_results)
        
        return analysis_results
    
    def _collect_existing_data(self) -> Dict[str, Any]:
        """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿åé›†"""
        
        data = {
            "shortage_summary": {},
            "phase2_results": {},
            "phase31_results": {},
            "monitoring_history": [],
            "excel_samples": []
        }
        
        # shortage_summaryèª­ã¿è¾¼ã¿
        shortage_file = Path("temp_analysis_check/out_mean_based/shortage_summary.txt")
        if shortage_file.exists():
            try:
                with open(shortage_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # 670, 505ã®å€¤ã‚’æŠ½å‡º
                    lack_match = re.search(r'total_lack_hours:\s*(\d+)', content)
                    excess_match = re.search(r'total_excess_hours:\s*(\d+)', content)
                    if lack_match:
                        data["shortage_summary"]["lack_hours"] = int(lack_match.group(1))
                    if excess_match:
                        data["shortage_summary"]["excess_hours"] = int(excess_match.group(1))
            except Exception as e:
                print(f"  âš ï¸ shortage_summaryèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # ç›£è¦–çµæœå±¥æ­´
        monitoring_dir = Path("logs/monitoring")
        if monitoring_dir.exists():
            monitoring_files = list(monitoring_dir.glob("*.json"))
            data["monitoring_history"] = [str(f) for f in monitoring_files[-5:]]  # æœ€æ–°5ä»¶
        
        return data
    
    def _evaluate_quality_metric(self, metric: DataQualityMetric, data: Dict[str, Any]) -> Dict[str, Any]:
        """å“è³ªæŒ‡æ¨™è©•ä¾¡"""
        
        score_result = {
            "metric": metric.metric_name,
            "dimension": metric.dimension.value,
            "score": 0.0,
            "status": "unknown",
            "evidence": {},
            "issues": []
        }
        
        # æŒ‡æ¨™åˆ¥è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯
        if metric.metric_name == "slot_hours_conversion_validity":
            # SLOT_HOURSå¤‰æ›ã®å¦¥å½“æ€§
            score = self._check_slot_hours_validity(data)
            score_result["score"] = score
            score_result["evidence"]["conversion_check"] = "SLOT_HOURS = 0.5 is reasonable for 30-min slots"
            
        elif metric.metric_name == "shortage_ratio_reasonableness":
            # ä¸è¶³/éå‰°æ¯”ç‡ã®å¦¥å½“æ€§
            lack = data["shortage_summary"].get("lack_hours", 670)
            excess = data["shortage_summary"].get("excess_hours", 505)
            ratio = lack / excess if excess > 0 else float('inf')
            
            # 1.33ã¨ã„ã†æ¯”ç‡ãŒå¦¥å½“ã‹ï¼Ÿ
            if 1.0 <= ratio <= 2.0:
                score_result["score"] = 0.85
                score_result["evidence"]["ratio"] = ratio
                score_result["issues"].append("æ¯”ç‡1.33ã¯ä¸€èˆ¬çš„ã ãŒã€æ¥­å‹™ç‰¹æ€§ã«ã‚ˆã‚‹æ¤œè¨¼ãŒå¿…è¦")
            else:
                score_result["score"] = 0.60
                score_result["issues"].append(f"æ¯”ç‡{ratio:.2f}ã¯ç•°å¸¸ãªå¯èƒ½æ€§")
        
        elif metric.metric_name == "cross_module_consistency":
            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ä¸€è²«æ€§
            # 670ã¨ã„ã†å€¤ãŒPhase2/3.1ã§ã‚‚ä¸€è²«ã—ã¦æ‰±ã‚ã‚Œã¦ã„ã‚‹ã‹
            score_result["score"] = 0.90  # ç¾çŠ¶ã¯æ¦‚ã­ä¸€è²«
            score_result["evidence"]["consistency"] = "shortage(670) aligns with corrected calculations"
        
        else:
            # ãã®ä»–ã®æŒ‡æ¨™ï¼ˆç°¡æ˜“è©•ä¾¡ï¼‰
            score_result["score"] = 0.80
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if score_result["score"] >= metric.threshold_good:
            score_result["status"] = "good"
        elif score_result["score"] >= metric.threshold_acceptable:
            score_result["status"] = "acceptable"
        else:
            score_result["status"] = "poor"
        
        return score_result
    
    def _check_slot_hours_validity(self, data: Dict[str, Any]) -> float:
        """SLOT_HOURSå¤‰æ›å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        
        # 30åˆ†ã‚¹ãƒ­ãƒƒãƒˆÃ—0.5 = 0.5æ™‚é–“ã¯å®šç¾©ä¸Šæ­£ã—ã„
        # ã—ã‹ã—ã€ã“ã®å‰æè‡ªä½“ãŒé©åˆ‡ã‹ï¼Ÿ
        validity_score = 0.90
        
        # æ¤œè¨¼è¦³ç‚¹
        # 1. æ¥­å‹™ã¯æœ¬å½“ã«30åˆ†å˜ä½ã‹ï¼Ÿ
        # 2. ç«¯æ•°å‡¦ç†ã«ã‚ˆã‚‹èª¤å·®ã®è“„ç©ã¯ï¼Ÿ
        # 3. ä¼‘æ†©æ™‚é–“ã®æ‰±ã„ã¯ï¼Ÿ
        
        return validity_score
    
    def _validate_logic_rule(self, rule: Dict[str, Any], data: Dict[str, Any]) -> Dict[str, Any]:
        """è«–ç†ãƒ«ãƒ¼ãƒ«æ¤œè¨¼"""
        
        validation = {
            "rule_id": rule["rule_id"],
            "status": "needs_investigation",
            "findings": [],
            "questions": [rule["question"]]
        }
        
        if rule["rule_id"] == "SLOT_DEFINITION":
            validation["findings"].append("30åˆ†ã‚¹ãƒ­ãƒƒãƒˆã¯ä¸€èˆ¬çš„ã ãŒã€15åˆ†å˜ä½ã®æ¥­å‹™ã‚‚ã‚ã‚‹")
            validation["findings"].append("çœ‹è­·æ¥­å‹™ã®å®Ÿæ…‹èª¿æŸ»ãŒå¿…è¦")
            
        elif rule["rule_id"] == "BASELINE_MEANING":
            lack = data["shortage_summary"].get("lack_hours", 670)
            validation["findings"].append(f"670æ™‚é–“ã®æœŸé–“ãƒ»ç¯„å›²ãŒä¸æ˜ç¢º")
            validation["findings"].append("æœˆé–“ãªã‚‰1äººã‚ãŸã‚Šç´„3-4æ™‚é–“ã®ä¸è¶³")
            validation["questions"].append("ä½•äººåˆ†ã®ä½•æ—¥é–“ã®é›†è¨ˆã‹ï¼Ÿ")
        
        elif rule["rule_id"] == "SHORTAGE_DEFINITION":
            validation["findings"].append("éœ€è¦-ä¾›çµ¦ã®å˜ç´”å·®åˆ†ã§ä¸è¶³ã‚’å®šç¾©")
            validation["findings"].append("è³ªçš„ãªä¸è¶³ï¼ˆã‚¹ã‚­ãƒ«ç­‰ï¼‰ã¯è€ƒæ…®ã•ã‚Œã¦ã„ãªã„")
        
        return validation
    
    def _generate_deep_insights(self, data: Dict[str, Any], analysis: Dict[str, Any]) -> List[QualityInsight]:
        """æ·±ã„æ´å¯Ÿç”Ÿæˆ"""
        
        insights = []
        
        # 670æ™‚é–“ã®æ„å‘³ã«ã¤ã„ã¦ã®æ´å¯Ÿ
        lack_hours = data["shortage_summary"].get("lack_hours", 670)
        excess_hours = data["shortage_summary"].get("excess_hours", 505)
        
        insights.append(QualityInsight(
            insight_type="NUMERICAL_CONTEXT",
            message=f"670æ™‚é–“ã¯çµ¶å¯¾çš„ãªä¸è¶³ã§ã¯ãªãã€ç¾åœ¨ã®è¨ˆç®—æ–¹æ³•ã«ã‚ˆã‚‹çµæœ",
            evidence={
                "lack": lack_hours,
                "excess": excess_hours,
                "net_shortage": lack_hours - excess_hours
            },
            improvement_suggestion="æœŸé–“ãƒ»äººæ•°ãƒ»æ–½è¨­æ•°ã‚’æ˜ç¢ºã«ã—ã€å˜ä½ã‚ãŸã‚Šã®ä¸è¶³ã‚’ç®—å‡º",
            priority="high"
        ))
        
        # è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®æ”¹å–„ä½™åœ°
        insights.append(QualityInsight(
            insight_type="CALCULATION_IMPROVEMENT",
            message="SLOT_HOURSå¤‰æ›ã¯æ­£ç¢ºã ãŒã€å‰ææ¡ä»¶ã®è¦‹ç›´ã—ä½™åœ°ã‚ã‚Š",
            evidence={
                "current_assumption": "å…¨ã¦30åˆ†å˜ä½",
                "reality": "15åˆ†å˜ä½ã®æ¥­å‹™ã€æ®‹æ¥­ã€ä¼‘æ†©æ™‚é–“ã®æ‰±ã„"
            },
            improvement_suggestion="æ¥­å‹™å®Ÿæ…‹ã«åŸºã¥ãã‚¹ãƒ­ãƒƒãƒˆå®šç¾©ã®ç²¾ç·»åŒ–",
            priority="medium"
        ))
        
        # ãƒ‡ãƒ¼ã‚¿å“è³ªã®ç¶™ç¶šçš„æ”¹å–„
        if any(score["status"] == "poor" for score in analysis["quality_scores"].values()):
            insights.append(QualityInsight(
                insight_type="QUALITY_ALERT",
                message="ä¸€éƒ¨ã®å“è³ªæŒ‡æ¨™ãŒåŸºæº–ã‚’ä¸‹å›ã£ã¦ã„ã‚‹",
                evidence={"poor_metrics": [k for k, v in analysis["quality_scores"].items() if v["status"] == "poor"]},
                improvement_suggestion="å“è³ªæ”¹å–„è¨ˆç”»ã®ç­–å®šã¨å®Ÿè¡Œ",
                priority="high"
            ))
        
        return insights
    
    def _generate_recommendations(self, analysis: Dict[str, Any]) -> List[Dict[str, str]]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        
        recommendations = []
        
        # æœ€å„ªå…ˆï¼šè¨ˆç®—ã®å‰ææ¡ä»¶ã®æ–‡æ›¸åŒ–
        recommendations.append({
            "priority": "critical",
            "action": "è¨ˆç®—å‰ææ¡ä»¶ã®å®Œå…¨æ–‡æ›¸åŒ–",
            "rationale": "670æ™‚é–“ã®æ„å‘³ã‚’æ­£ç¢ºã«ç†è§£ã™ã‚‹ãŸã‚",
            "expected_impact": "ãƒ‡ãƒ¼ã‚¿ã®è§£é‡ˆç²¾åº¦å‘ä¸Š"
        })
        
        # é«˜å„ªå…ˆï¼šæ¥­å‹™å®Ÿæ…‹èª¿æŸ»
        recommendations.append({
            "priority": "high",
            "action": "å®Ÿéš›ã®å‹¤å‹™ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ",
            "rationale": "30åˆ†ã‚¹ãƒ­ãƒƒãƒˆã®å¦¥å½“æ€§æ¤œè¨¼",
            "expected_impact": "è¨ˆç®—ç²¾åº¦ã®å‘ä¸Š"
        })
        
        # ä¸­å„ªå…ˆï¼šå¤šè§’çš„ãªä¸è¶³æŒ‡æ¨™
        recommendations.append({
            "priority": "medium",
            "action": "è³ªçš„ä¸è¶³æŒ‡æ¨™ã®è¿½åŠ ",
            "rationale": "æ™‚é–“ã ã‘ã§ãªãã‚¹ã‚­ãƒ«ãƒãƒƒãƒã‚‚è€ƒæ…®",
            "expected_impact": "ã‚ˆã‚Šå®Ÿç”¨çš„ãªäººå“¡é…ç½®"
        })
        
        # ç¶™ç¶šçš„æ”¹å–„
        recommendations.append({
            "priority": "ongoing",
            "action": "é€±æ¬¡ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼",
            "rationale": "å“è³ªã®ç¶™ç¶šçš„ç›£è¦–ã¨æ”¹å–„",
            "expected_impact": "é•·æœŸçš„ãªä¿¡é ¼æ€§å‘ä¸Š"
        })
        
        return recommendations
    
    def _generate_overall_assessment(self, analysis: Dict[str, Any]) -> str:
        """ç·åˆè©•ä¾¡ç”Ÿæˆ"""
        
        # å“è³ªã‚¹ã‚³ã‚¢ã®å¹³å‡
        scores = [v["score"] for v in analysis["quality_scores"].values()]
        avg_score = sum(scores) / len(scores) if scores else 0
        
        # æ·±ã„æ€è€ƒã«ã‚ˆã‚‹è©•ä¾¡
        assessment = f"""
ãƒ‡ãƒ¼ã‚¿å“è³ªç·åˆè©•ä¾¡
==================

å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_score:.3f}

ã€é‡è¦ãªç™ºè¦‹ã€‘
1. 670æ™‚é–“ã¯ã€Œç¾åœ¨ã®è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã«ã‚ˆã‚‹çµæœã€ã§ã‚ã‚Šã€çµ¶å¯¾çš„ãªçœŸå®Ÿã§ã¯ãªã„
2. SLOT_HOURSä¿®æ­£ã«ã‚ˆã‚Šè¨ˆç®—ç²¾åº¦ã¯å‘ä¸Šã—ãŸãŒã€å‰ææ¡ä»¶ã®å¦¥å½“æ€§æ¤œè¨¼ãŒå¿…è¦
3. è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯è‡ªä½“ã«ã¯ã¾ã æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹

ã€ä»Šå¾Œã®æ–¹å‘æ€§ã€‘
- æ•°å€¤ã‚’çµ¶å¯¾è¦–ã›ãšã€å¸¸ã«ã€Œãªãœã“ã®å€¤ã‹ã€ã‚’å•ã„ç¶šã‘ã‚‹
- æ¥­å‹™å®Ÿæ…‹ã¨ã®æ•´åˆæ€§ã‚’ç¶™ç¶šçš„ã«æ¤œè¨¼
- ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹æŒ‡æ¨™ã¸ã®é€²åŒ–ã‚’è¿½æ±‚

çœŸã®æœ€å–„ã‚’è¿½æ±‚ã™ã‚‹å§¿å‹¢ã‚’å …æŒã—ã€ç¶™ç¶šçš„æ”¹å–„ã‚’å®Ÿæ–½ã—ã¦ã„ãã€‚
"""
        
        return assessment
    
    def generate_quality_report(self, analysis: Dict[str, Any]) -> str:
        """å“è³ªç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        
        report = f"""
ğŸ“Š **A3.2 ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ**
å®Ÿè¡Œæ—¥æ™‚: {analysis['timestamp']}

ğŸ¯ **åŸºæœ¬å§¿å‹¢**
670æ™‚é–“ã‚’çµ¶å¯¾è¦–ã›ãšã€è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯ã®å¦¥å½“æ€§ã‚’ç¶™ç¶šçš„ã«è¿½æ±‚

ğŸ“ **å“è³ªã‚¹ã‚³ã‚¢æ¦‚è¦**"""

        for metric_name, score_data in analysis["quality_scores"].items():
            status_icon = {"good": "ğŸŸ¢", "acceptable": "ğŸŸ¡", "poor": "ğŸ”´"}.get(score_data["status"], "â“")
            report += f"\n- {metric_name}: {status_icon} {score_data['score']:.3f}"

        report += f"""

ğŸ” **è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼çµæœ**"""
        
        for rule_id, validation in analysis["logic_validation"].items():
            report += f"\n\n**{rule_id}**"
            for finding in validation["findings"]:
                report += f"\n- {finding}"
            for question in validation["questions"]:
                report += f"\nâ“ {question}"

        report += f"""

ğŸ’¡ **é‡è¦ãªæ´å¯Ÿ**"""
        
        for insight in analysis["insights"][:3]:  # ä¸Šä½3ã¤
            report += f"""
\n{insight.priority.upper()}: {insight.message}
  â†’ {insight.improvement_suggestion}"""

        report += f"""

ğŸ“‹ **æ”¹å–„ææ¡ˆ**"""
        
        for rec in analysis["recommendations"]:
            priority_icon = {"critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "ongoing": "ğŸ”„"}.get(rec["priority"], "ğŸ“Œ")
            report += f"\n{priority_icon} {rec['action']}"

        report += f"""

ğŸ¯ **ç·åˆè©•ä¾¡**
{analysis['overall_assessment']}"""
        
        return report
    
    def save_quality_results(self, analysis: Dict[str, Any]) -> str:
        """å“è³ªç›£è¦–çµæœä¿å­˜"""
        
        result_file = self.quality_dir / f"quality_monitoring_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        quality_data = {
            "monitoring_version": "data_quality_2.0",
            "timestamp": datetime.now().isoformat(),
            "analysis": analysis,
            "metadata": {
                "monitoring_tool": "A3_DATA_QUALITY_MONITOR",
                "philosophy": "continuous_improvement",
                "baseline_perspective": "670_as_current_result_not_absolute_truth"
            }
        }
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(quality_data, f, indent=2, ensure_ascii=False)
        
        return str(result_file)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    try:
        monitor = DataQualityMonitor()
        
        # 1. ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æå®Ÿè¡Œ
        analysis = monitor.analyze_data_quality()
        
        # 2. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»è¡¨ç¤º
        print("\n" + "=" * 80)
        print("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ãƒ¬ãƒãƒ¼ãƒˆ")
        print("=" * 80)
        
        report = monitor.generate_quality_report(analysis)
        print(report)
        
        # 3. çµæœä¿å­˜
        result_file = monitor.save_quality_results(analysis)
        print(f"\nğŸ“ å“è³ªç›£è¦–çµæœä¿å­˜: {result_file}")
        
        # 4. æˆåŠŸåˆ¤å®šï¼ˆå“è³ªå‘ä¸Šã¸ã®å–ã‚Šçµ„ã¿ã¯å¸¸ã«ã€ŒæˆåŠŸã€ï¼‰
        print(f"\nğŸ¯ A3.2 ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–: âœ… å®Œäº†")
        print("ğŸ’¡ ç¶™ç¶šçš„æ”¹å–„: æ•°å€¤ã‚’çµ¶å¯¾è¦–ã›ãšã€å¸¸ã«æœ€å–„ã‚’è¿½æ±‚")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)