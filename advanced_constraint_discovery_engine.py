#!/usr/bin/env python3
"""
é«˜åº¦åˆ¶ç´„ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³ - æ„å›³ç™ºè¦‹â†’åˆ¶ç´„æ˜‡è¯ã®2æ®µéšã‚·ã‚¹ãƒ†ãƒ 
ç›®çš„ï¼šã‚·ãƒ•ãƒˆä½œæˆè€…ã®æš—é»™æ„å›³ã‚’ç™ºè¦‹ã—ã€å¼·åˆ¶åŠ›ã®ã‚ã‚‹åˆ¶ç´„ã¨ã—ã¦æ˜‡è¯
"""

import sys
import json
import logging
import zipfile
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple, Set
from datetime import datetime
import re
import statistics
from collections import Counter, defaultdict
from dataclasses import dataclass
from enum import Enum

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
log = logging.getLogger(__name__)

class ConstraintType(Enum):
    """åˆ¶ç´„ã‚¿ã‚¤ãƒ—ã®åˆ†é¡"""
    STATIC_HARD = "static_hard"      # é™çš„å¼·åˆ¶åˆ¶ç´„ï¼ˆé•å=ã‚¨ãƒ©ãƒ¼ï¼‰
    STATIC_SOFT = "static_soft"      # é™çš„æ¨å¥¨åˆ¶ç´„ï¼ˆé•å=è­¦å‘Šï¼‰
    DYNAMIC_HARD = "dynamic_hard"    # å‹•çš„å¼·åˆ¶åˆ¶ç´„ï¼ˆçŠ¶æ³ä¾å­˜ï¼‰
    DYNAMIC_SOFT = "dynamic_soft"    # å‹•çš„æ¨å¥¨åˆ¶ç´„ï¼ˆçŠ¶æ³ä¾å­˜ï¼‰

class ConstraintAxis(Enum):
    """åˆ¶ç´„è»¸ã®åˆ†é¡"""
    STAFF_AXIS = "staff"         # ã‚¹ã‚¿ãƒƒãƒ•è»¸
    TIME_AXIS = "time"           # æ™‚é–“è»¸  
    TASK_AXIS = "task"           # æ¥­å‹™è»¸
    RELATIONSHIP_AXIS = "relationship"  # é–¢ä¿‚æ€§è»¸

@dataclass
class ConstraintRule:
    """åˆ¶ç´„ãƒ«ãƒ¼ãƒ«å®šç¾©"""
    rule_id: str
    constraint_type: ConstraintType
    axis: ConstraintAxis
    condition: str          # IFéƒ¨åˆ†
    action: str            # THENéƒ¨åˆ†
    confidence: float      # ç¢ºä¿¡åº¦ 0-1
    evidence: Dict[str, Any]  # æ ¹æ‹ ãƒ‡ãƒ¼ã‚¿
    measurement: float     # å®šè¦å€¤ 0-100
    violation_penalty: str # é•åæ™‚ã®å‡¦ç†

class AdvancedIntentionDiscovery:
    """é«˜åº¦æ„å›³ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.engine_name = "é«˜åº¦æ„å›³ç™ºè¦‹ã‚¨ãƒ³ã‚¸ãƒ³"
        self.version = "2.0.0"
        
    def discover_deep_patterns(self, shift_data: List[List[Any]]) -> Dict[str, Any]:
        """æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹"""
        print(f"\n=== æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æé–‹å§‹ ===")
        
        patterns = {
            "staff_specialization": {},    # ã‚¹ã‚¿ãƒƒãƒ•å°‚é–€æ€§
            "temporal_patterns": {},       # æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³
            "workload_distribution": {},   # è² è·åˆ†æ•£ãƒ‘ã‚¿ãƒ¼ãƒ³
            "relationship_patterns": {},   # é–¢ä¿‚æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
            "anomaly_patterns": {},        # ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³
            "sequence_patterns": {},       # é€£ç¶šæ€§ãƒ‘ã‚¿ãƒ¼ãƒ³
            "frequency_patterns": {}       # é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³
        }
        
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        staff_shifts = self._extract_staff_data(shift_data)
        
        # 1. ã‚¹ã‚¿ãƒƒãƒ•å°‚é–€æ€§åˆ†æï¼ˆå‘ä¸Šç‰ˆï¼‰
        patterns["staff_specialization"] = self._analyze_staff_specialization(staff_shifts)
        
        # 2. æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns["temporal_patterns"] = self._analyze_temporal_patterns(staff_shifts)
        
        # 3. è² è·åˆ†æ•£ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns["workload_distribution"] = self._analyze_workload_distribution(staff_shifts)
        
        # 4. é–¢ä¿‚æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns["relationship_patterns"] = self._analyze_relationship_patterns(staff_shifts)
        
        # 5. ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥
        patterns["anomaly_patterns"] = self._detect_anomaly_patterns(staff_shifts)
        
        # 6. é€£ç¶šæ€§ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns["sequence_patterns"] = self._analyze_sequence_patterns(staff_shifts)
        
        # 7. é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        patterns["frequency_patterns"] = self._analyze_frequency_patterns(staff_shifts)
        
        print(f"æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æå®Œäº†: {len([p for p in patterns.values() if p])}ã‚«ãƒ†ã‚´ãƒª")
        return patterns
    
    def _extract_staff_data(self, shift_data: List[List[Any]]) -> Dict[str, List[Dict]]:
        """ã‚¹ã‚¿ãƒƒãƒ•ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®å‘ä¸Šç‰ˆ"""
        staff_shifts = defaultdict(list)
        
        if len(shift_data) < 2:
            return {}
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã¨ã‚¹ã‚¿ãƒƒãƒ•åˆ—ã®ç‰¹å®šï¼ˆå‘ä¸Šç‰ˆï¼‰
        header_row_idx = self._find_header_row_advanced(shift_data)
        if header_row_idx is None:
            return {}
        
        headers = shift_data[header_row_idx]
        staff_col_idx = self._find_staff_column_advanced(headers)
        
        # ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        for row_idx in range(header_row_idx + 1, len(shift_data)):
            row = shift_data[row_idx]
            if not row or len(row) <= staff_col_idx or not row[staff_col_idx]:
                continue
            
            staff_name = str(row[staff_col_idx]).strip()
            
            # å„æ—¥ã®ã‚·ãƒ•ãƒˆæƒ…å ±ã‚’è©³ç´°è¨˜éŒ²
            for col_idx in range(staff_col_idx + 1, len(row)):
                if col_idx < len(headers) and row[col_idx]:
                    date_info = str(headers[col_idx]) if col_idx < len(headers) else f"Day{col_idx}"
                    shift_code = str(row[col_idx]).strip()
                    
                    if shift_code and shift_code not in ['', 'None', 'nan']:
                        staff_shifts[staff_name].append({
                            "date": date_info,
                            "shift": shift_code,
                            "col_index": col_idx,
                            "row_index": row_idx,
                            "sequence_position": len(staff_shifts[staff_name])
                        })
        
        return dict(staff_shifts)
    
    def _analyze_staff_specialization(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """ã‚¹ã‚¿ãƒƒãƒ•å°‚é–€æ€§åˆ†æï¼ˆé«˜åº¦ç‰ˆï¼‰"""
        specializations = {}
        
        for staff_name, shifts in staff_shifts.items():
            if not shifts:
                continue
            
            shift_counts = Counter(s["shift"] for s in shifts)
            total_shifts = len(shifts)
            
            # å°‚é–€æ€§æŒ‡æ¨™ã®è¨ˆç®—
            specialization_scores = {}
            for shift_type, count in shift_counts.items():
                ratio = count / total_shifts
                
                # å°‚é–€æ€§ã‚¹ã‚³ã‚¢ = æ¯”ç‡ Ã— ç¶™ç¶šæ€§ Ã— é›†ä¸­åº¦
                continuity = self._calculate_continuity(shifts, shift_type)
                concentration = self._calculate_concentration(shifts, shift_type)
                
                specialization_score = ratio * continuity * concentration
                
                if specialization_score > 0.3:  # 30%ä»¥ä¸Šã§å°‚é–€æ€§èªå®š
                    specialization_scores[shift_type] = {
                        "ratio": ratio,
                        "continuity": continuity,
                        "concentration": concentration,
                        "specialization_score": specialization_score,
                        "evidence_count": count,
                        "classification": self._classify_specialization(specialization_score)
                    }
            
            if specialization_scores:
                specializations[staff_name] = {
                    "primary_specializations": specialization_scores,
                    "versatility_index": len(shift_counts) / len(set(s["shift"] for s in shifts)),
                    "consistency_score": self._calculate_consistency(shifts)
                }
        
        return specializations
    
    def _analyze_temporal_patterns(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """æ™‚é–“çš„ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        temporal_patterns = {
            "daily_patterns": {},
            "weekly_patterns": {},
            "sequence_patterns": {},
            "rhythm_patterns": {}
        }
        
        for staff_name, shifts in staff_shifts.items():
            if len(shifts) < 3:
                continue
            
            # æ—¥æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            daily_pattern = self._analyze_daily_pattern(shifts)
            if daily_pattern:
                temporal_patterns["daily_patterns"][staff_name] = daily_pattern
            
            # é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            weekly_pattern = self._analyze_weekly_pattern(shifts)
            if weekly_pattern:
                temporal_patterns["weekly_patterns"][staff_name] = weekly_pattern
            
            # ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            rhythm_pattern = self._analyze_rhythm_pattern(shifts)
            if rhythm_pattern:
                temporal_patterns["rhythm_patterns"][staff_name] = rhythm_pattern
        
        return temporal_patterns
    
    def _analyze_workload_distribution(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """è² è·åˆ†æ•£ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        workload_patterns = {}
        
        # å…¨ä½“ã®è² è·åˆ†æ
        total_shifts = sum(len(shifts) for shifts in staff_shifts.values())
        staff_count = len(staff_shifts)
        average_load = total_shifts / staff_count if staff_count > 0 else 0
        
        for staff_name, shifts in staff_shifts.items():
            load_ratio = len(shifts) / average_load if average_load > 0 else 0
            
            workload_patterns[staff_name] = {
                "absolute_load": len(shifts),
                "relative_load": load_ratio,
                "load_classification": self._classify_workload(load_ratio),
                "load_consistency": self._calculate_load_consistency(shifts),
                "peak_periods": self._identify_peak_periods(shifts)
            }
        
        return workload_patterns
    
    def _detect_anomaly_patterns(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥"""
        anomalies = {
            "staff_anomalies": {},
            "shift_anomalies": {},
            "pattern_anomalies": {}
        }
        
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒ¬ãƒ™ãƒ«ã®ç•°å¸¸æ¤œçŸ¥
        for staff_name, shifts in staff_shifts.items():
            staff_anomalies = []
            
            # æ€¥æ¿€ãªå¤‰åŒ–ã®æ¤œçŸ¥
            if self._detect_sudden_change(shifts):
                staff_anomalies.append("sudden_pattern_change")
            
            # ç•°å¸¸ãªé›†ä¸­ã®æ¤œçŸ¥
            if self._detect_unusual_concentration(shifts):
                staff_anomalies.append("unusual_concentration")
            
            # ä¸è¦å‰‡æ€§ã®æ¤œçŸ¥
            if self._detect_irregularity(shifts):
                staff_anomalies.append("high_irregularity")
            
            if staff_anomalies:
                anomalies["staff_anomalies"][staff_name] = staff_anomalies
        
        return anomalies
    
    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ç¾¤
    def _find_header_row_advanced(self, data: List[List[Any]]) -> Optional[int]:
        """ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã®é«˜åº¦æ¤œå‡º"""
        keywords = ['æ°å', 'åå‰', 'ã‚¹ã‚¿ãƒƒãƒ•', 'è·å“¡', 'name', 'staff', 'æœˆ', 'ç«', 'æ°´', 'æœ¨', 'é‡‘', 'åœŸ', 'æ—¥']
        
        for i, row in enumerate(data[:10]):
            if not row:
                continue
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º
            row_text = ' '.join(str(cell).lower() for cell in row if cell)
            keyword_score = sum(1 for keyword in keywords if keyword in row_text)
            
            # åˆ—æ•°ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
            non_empty_cells = sum(1 for cell in row if cell)
            
            if keyword_score >= 2 and non_empty_cells >= 3:
                return i
        
        return 0 if data else None
    
    def _find_staff_column_advanced(self, headers: List[Any]) -> int:
        """ã‚¹ã‚¿ãƒƒãƒ•åˆ—ã®é«˜åº¦æ¤œå‡º"""
        staff_keywords = ['æ°å', 'åå‰', 'ã‚¹ã‚¿ãƒƒãƒ•', 'è·å“¡', 'name', 'staff', 'ç¤¾å“¡', 'å¾“æ¥­å“¡']
        
        for i, header in enumerate(headers):
            if not header:
                continue
            
            header_str = str(header).lower()
            for keyword in staff_keywords:
                if keyword in header_str:
                    return i
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®åˆ—
        return 0
    
    def _calculate_continuity(self, shifts: List[Dict], shift_type: str) -> float:
        """ç¶™ç¶šæ€§è¨ˆç®—"""
        if not shifts:
            return 0.0
        
        target_shifts = [s for s in shifts if s["shift"] == shift_type]
        if len(target_shifts) < 2:
            return 0.5
        
        # é€£ç¶šæ€§ã®è¨ˆç®—
        consecutive_count = 0
        max_consecutive = 0
        
        sorted_shifts = sorted(shifts, key=lambda x: x["sequence_position"])
        current_consecutive = 0
        
        for shift in sorted_shifts:
            if shift["shift"] == shift_type:
                current_consecutive += 1
                max_consecutive = max(max_consecutive, current_consecutive)
            else:
                current_consecutive = 0
        
        return min(1.0, max_consecutive / len(target_shifts))
    
    def _calculate_concentration(self, shifts: List[Dict], shift_type: str) -> float:
        """é›†ä¸­åº¦è¨ˆç®—"""
        if not shifts:
            return 0.0
        
        target_shifts = [s for s in shifts if s["shift"] == shift_type]
        if not target_shifts:
            return 0.0
        
        # æ™‚é–“çš„é›†ä¸­åº¦ã®è¨ˆç®—
        positions = [s["sequence_position"] for s in target_shifts]
        if len(positions) < 2:
            return 1.0
        
        # åˆ†æ•£ã®é€†æ•°ã¨ã—ã¦é›†ä¸­åº¦ã‚’è¨ˆç®—
        variance = statistics.variance(positions)
        max_possible_variance = (len(shifts) ** 2) / 12  # ä¸€æ§˜åˆ†å¸ƒã®åˆ†æ•£
        
        concentration = 1.0 - (variance / max_possible_variance) if max_possible_variance > 0 else 1.0
        return max(0.0, min(1.0, concentration))
    
    def _classify_specialization(self, score: float) -> str:
        """å°‚é–€æ€§åˆ†é¡"""
        if score >= 0.8:
            return "high_specialist"
        elif score >= 0.6:
            return "moderate_specialist"
        elif score >= 0.4:
            return "partial_specialist"
        else:
            return "generalist"
    
    def _calculate_consistency(self, shifts: List[Dict]) -> float:
        """ä¸€è²«æ€§è¨ˆç®—"""
        if len(shifts) < 2:
            return 1.0
        
        shift_types = [s["shift"] for s in shifts]
        unique_types = len(set(shift_types))
        total_shifts = len(shift_types)
        
        # ä¸€è²«æ€§ = 1 - (ç¨®é¡æ•° / ç·ã‚·ãƒ•ãƒˆæ•°)
        consistency = 1.0 - (unique_types / total_shifts)
        return max(0.0, consistency)
    
    def _analyze_daily_pattern(self, shifts: List[Dict]) -> Optional[Dict[str, Any]]:
        """æ—¥æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if len(shifts) < 7:  # æœ€ä½1é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            return None
        
        # æ›œæ—¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
        daily_distribution = defaultdict(list)
        for shift in shifts:
            # æ—¥ä»˜ã‹ã‚‰æ›œæ—¥ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            col_index = shift["col_index"]
            day_of_week = col_index % 7
            daily_distribution[day_of_week].append(shift["shift"])
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©•ä¾¡
        pattern_strength = 0.0
        dominant_patterns = {}
        
        for day, day_shifts in daily_distribution.items():
            if day_shifts:
                most_common = Counter(day_shifts).most_common(1)[0]
                ratio = most_common[1] / len(day_shifts)
                if ratio > 0.6:  # 60%ä»¥ä¸Šã§æ”¯é…çš„ãƒ‘ã‚¿ãƒ¼ãƒ³
                    dominant_patterns[day] = {
                        "shift": most_common[0],
                        "ratio": ratio
                    }
                    pattern_strength += ratio
        
        if dominant_patterns:
            return {
                "dominant_patterns": dominant_patterns,
                "pattern_strength": pattern_strength / 7,
                "regularity_index": len(dominant_patterns) / 7
            }
        
        return None
    
    def _analyze_weekly_pattern(self, shifts: List[Dict]) -> Optional[Dict[str, Any]]:
        """é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if len(shifts) < 14:  # æœ€ä½2é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            return None
        
        # é€±å˜ä½ã§ã®ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        weekly_patterns = []
        week_size = 7
        
        for i in range(0, len(shifts) - week_size + 1, week_size):
            week_shifts = shifts[i:i + week_size]
            week_pattern = [s["shift"] for s in week_shifts]
            weekly_patterns.append(week_pattern)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼æ€§åˆ†æ
        if len(weekly_patterns) >= 2:
            similarity_scores = []
            for i in range(len(weekly_patterns) - 1):
                similarity = self._calculate_pattern_similarity(
                    weekly_patterns[i], weekly_patterns[i + 1]
                )
                similarity_scores.append(similarity)
            
            avg_similarity = statistics.mean(similarity_scores)
            
            return {
                "weekly_consistency": avg_similarity,
                "pattern_variations": len(set(tuple(p) for p in weekly_patterns)),
                "dominant_weekly_pattern": self._find_dominant_weekly_pattern(weekly_patterns)
            }
        
        return None
    
    def _analyze_rhythm_pattern(self, shifts: List[Dict]) -> Optional[Dict[str, Any]]:
        """ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if len(shifts) < 5:
            return None
        
        # ã‚·ãƒ•ãƒˆå¤‰åŒ–ã®ãƒªã‚ºãƒ åˆ†æ
        change_intervals = []
        last_shift = shifts[0]["shift"]
        
        for i, shift in enumerate(shifts[1:], 1):
            if shift["shift"] != last_shift:
                change_intervals.append(i)
                last_shift = shift["shift"]
        
        if len(change_intervals) >= 3:
            # å¤‰åŒ–é–“éš”ã®è¦å‰‡æ€§ãƒã‚§ãƒƒã‚¯
            intervals = [change_intervals[i] - change_intervals[i-1] 
                        for i in range(1, len(change_intervals))]
            
            if intervals:
                avg_interval = statistics.mean(intervals)
                interval_variance = statistics.variance(intervals) if len(intervals) > 1 else 0
                
                return {
                    "change_frequency": len(change_intervals) / len(shifts),
                    "average_interval": avg_interval,
                    "rhythm_regularity": 1.0 / (1.0 + interval_variance),
                    "rhythm_type": self._classify_rhythm(avg_interval, interval_variance)
                }
        
        return None
    
    def _classify_workload(self, load_ratio: float) -> str:
        """è² è·åˆ†é¡"""
        if load_ratio >= 1.5:
            return "heavy_load"
        elif load_ratio >= 1.2:
            return "high_load"
        elif load_ratio >= 0.8:
            return "normal_load"
        elif load_ratio >= 0.5:
            return "light_load"
        else:
            return "minimal_load"
    
    def _calculate_load_consistency(self, shifts: List[Dict]) -> float:
        """è² è·ä¸€è²«æ€§è¨ˆç®—"""
        if len(shifts) < 7:
            return 1.0
        
        # é€±å˜ä½ã§ã®è² è·åˆ†æ
        weekly_loads = []
        week_size = 7
        
        for i in range(0, len(shifts), week_size):
            week_shifts = shifts[i:i + week_size]
            weekly_loads.append(len(week_shifts))
        
        if len(weekly_loads) > 1:
            variance = statistics.variance(weekly_loads)
            mean_load = statistics.mean(weekly_loads)
            cv = variance / mean_load if mean_load > 0 else 0
            return 1.0 / (1.0 + cv)
        
        return 1.0
    
    def _identify_peak_periods(self, shifts: List[Dict]) -> List[Dict[str, Any]]:
        """ãƒ”ãƒ¼ã‚¯æœŸé–“ç‰¹å®š"""
        if len(shifts) < 14:
            return []
        
        # ç§»å‹•å¹³å‡ã«ã‚ˆã‚‹è² è·åˆ†æ
        window_size = 7
        peak_periods = []
        
        for i in range(len(shifts) - window_size + 1):
            window_shifts = shifts[i:i + window_size]
            window_load = len(window_shifts)
            
            # ãƒ”ãƒ¼ã‚¯åˆ¤å®šï¼ˆå¹³å‡ã®1.5å€ä»¥ä¸Šï¼‰
            overall_avg = len(shifts) / (len(shifts) // window_size)
            if window_load >= overall_avg * 1.5:
                peak_periods.append({
                    "start_position": i,
                    "duration": window_size,
                    "intensity": window_load / overall_avg,
                    "period_type": "high_intensity"
                })
        
        return peak_periods
    
    def _detect_sudden_change(self, shifts: List[Dict]) -> bool:
        """æ€¥æ¿€ãªå¤‰åŒ–æ¤œçŸ¥"""
        if len(shifts) < 6:
            return False
        
        # å‰åŠã¨å¾ŒåŠã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
        mid_point = len(shifts) // 2
        first_half = shifts[:mid_point]
        second_half = shifts[mid_point:]
        
        first_pattern = Counter(s["shift"] for s in first_half)
        second_pattern = Counter(s["shift"] for s in second_half)
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é¡ä¼¼åº¦è¨ˆç®—
        similarity = self._calculate_counter_similarity(first_pattern, second_pattern)
        
        return similarity < 0.3  # 30%æœªæº€ã§æ€¥æ¿€ãªå¤‰åŒ–ã¨åˆ¤å®š
    
    def _detect_unusual_concentration(self, shifts: List[Dict]) -> bool:
        """ç•°å¸¸ãªé›†ä¸­æ¤œçŸ¥"""
        if len(shifts) < 5:
            return False
        
        shift_counts = Counter(s["shift"] for s in shifts)
        max_count = max(shift_counts.values())
        
        # 90%ä»¥ä¸Šã®é›†ä¸­ã¯ç•°å¸¸
        return max_count / len(shifts) > 0.9
    
    def _detect_irregularity(self, shifts: List[Dict]) -> bool:
        """ä¸è¦å‰‡æ€§æ¤œçŸ¥"""
        if len(shifts) < 10:
            return False
        
        # ã‚·ãƒ•ãƒˆå¤‰åŒ–ã®é »åº¦åˆ†æ
        changes = 0
        for i in range(1, len(shifts)):
            if shifts[i]["shift"] != shifts[i-1]["shift"]:
                changes += 1
        
        change_rate = changes / (len(shifts) - 1)
        
        # å¤‰åŒ–ç‡ãŒ70%ä»¥ä¸Šã§ä¸è¦å‰‡ã¨åˆ¤å®š
        return change_rate > 0.7
    
    def _calculate_pattern_similarity(self, pattern1: List[str], pattern2: List[str]) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é¡ä¼¼åº¦è¨ˆç®—"""
        if len(pattern1) != len(pattern2):
            return 0.0
        
        matches = sum(1 for a, b in zip(pattern1, pattern2) if a == b)
        return matches / len(pattern1)
    
    def _find_dominant_weekly_pattern(self, weekly_patterns: List[List[str]]) -> Optional[List[str]]:
        """æ”¯é…çš„é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹"""
        if not weekly_patterns:
            return None
        
        pattern_counts = Counter(tuple(p) for p in weekly_patterns)
        most_common = pattern_counts.most_common(1)[0]
        
        if most_common[1] >= len(weekly_patterns) * 0.5:  # 50%ä»¥ä¸Šã§æ”¯é…çš„
            return list(most_common[0])
        
        return None
    
    def _classify_rhythm(self, avg_interval: float, variance: float) -> str:
        """ãƒªã‚ºãƒ åˆ†é¡"""
        if variance < 0.5:
            if avg_interval <= 2:
                return "high_frequency_regular"
            elif avg_interval <= 5:
                return "medium_frequency_regular"
            else:
                return "low_frequency_regular"
        else:
            return "irregular"
    
    def _calculate_counter_similarity(self, counter1: Counter, counter2: Counter) -> float:
        """Counteré¡ä¼¼åº¦è¨ˆç®—"""
        all_keys = set(counter1.keys()) | set(counter2.keys())
        if not all_keys:
            return 1.0
        
        total_diff = 0
        total_count = 0
        
        for key in all_keys:
            count1 = counter1.get(key, 0)
            count2 = counter2.get(key, 0)
            total_diff += abs(count1 - count2)
            total_count += max(count1, count2)
        
        return 1.0 - (total_diff / total_count) if total_count > 0 else 1.0
    
    def _analyze_relationship_patterns(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """é–¢ä¿‚æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        relationship_patterns = {
            "shift_correlations": {},
            "staff_interactions": {},
            "coverage_patterns": {}
        }
        
        # ã‚·ãƒ•ãƒˆé–“ã®ç›¸é–¢åˆ†æ
        all_shifts = []
        for shifts in staff_shifts.values():
            all_shifts.extend(s["shift"] for s in shifts)
        
        shift_types = list(set(all_shifts))
        
        # ç°¡æ˜“ç›¸é–¢åˆ†æ
        for i, shift1 in enumerate(shift_types):
            for shift2 in shift_types[i+1:]:
                correlation = self._calculate_shift_correlation(staff_shifts, shift1, shift2)
                if abs(correlation) > 0.3:  # 30%ä»¥ä¸Šã®ç›¸é–¢
                    relationship_patterns["shift_correlations"][f"{shift1}-{shift2}"] = correlation
        
        return relationship_patterns
    
    def _analyze_sequence_patterns(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """é€£ç¶šæ€§ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        sequence_patterns = {}
        
        for staff_name, shifts in staff_shifts.items():
            if len(shifts) < 3:
                continue
            
            # é€£ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
            sequences = []
            for i in range(len(shifts) - 2):
                seq = [shifts[i]["shift"], shifts[i+1]["shift"], shifts[i+2]["shift"]]
                sequences.append(tuple(seq))
            
            if sequences:
                seq_counts = Counter(sequences)
                dominant_sequences = {seq: count for seq, count in seq_counts.items() if count >= 2}
                
                if dominant_sequences:
                    sequence_patterns[staff_name] = {
                        "dominant_sequences": dominant_sequences,
                        "sequence_diversity": len(seq_counts),
                        "repetition_rate": sum(dominant_sequences.values()) / len(sequences)
                    }
        
        return sequence_patterns
    
    def _analyze_frequency_patterns(self, staff_shifts: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """é »åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        frequency_patterns = {
            "shift_frequencies": {},
            "staff_frequencies": {},
            "temporal_frequencies": {}
        }
        
        # å…¨ä½“çš„ãªã‚·ãƒ•ãƒˆé »åº¦
        all_shifts = []
        for shifts in staff_shifts.values():
            all_shifts.extend(s["shift"] for s in shifts)
        
        shift_counts = Counter(all_shifts)
        total_shifts = len(all_shifts)
        
        for shift_type, count in shift_counts.items():
            frequency = count / total_shifts
            frequency_patterns["shift_frequencies"][shift_type] = {
                "count": count,
                "frequency": frequency,
                "rarity_level": self._classify_frequency(frequency)
            }
        
        return frequency_patterns
    
    def _calculate_shift_correlation(self, staff_shifts: Dict[str, List[Dict]], shift1: str, shift2: str) -> float:
        """ã‚·ãƒ•ãƒˆé–“ç›¸é–¢è¨ˆç®—"""
        correlations = []
        
        for shifts in staff_shifts.values():
            if len(shifts) < 2:
                continue
            
            shift_sequence = [s["shift"] for s in shifts]
            
            # shift1ã®å¾Œã«shift2ãŒæ¥ã‚‹é »åº¦
            transitions = 0
            shift1_occurrences = 0
            
            for i in range(len(shift_sequence) - 1):
                if shift_sequence[i] == shift1:
                    shift1_occurrences += 1
                    if shift_sequence[i + 1] == shift2:
                        transitions += 1
            
            if shift1_occurrences > 0:
                correlation = transitions / shift1_occurrences
                correlations.append(correlation)
        
        return statistics.mean(correlations) if correlations else 0.0
    
    def _classify_frequency(self, frequency: float) -> str:
        """é »åº¦åˆ†é¡"""
        if frequency >= 0.3:
            return "very_common"
        elif frequency >= 0.1:
            return "common"
        elif frequency >= 0.05:
            return "moderate"
        elif frequency >= 0.01:
            return "rare"
        else:
            return "very_rare"

class ConstraintElevationEngine:
    """åˆ¶ç´„æ˜‡è¯ã‚¨ãƒ³ã‚¸ãƒ³ - æ„å›³ã‚’åˆ¶ç´„ã«å¤‰æ›"""
    
    def __init__(self):
        self.engine_name = "åˆ¶ç´„æ˜‡è¯ã‚¨ãƒ³ã‚¸ãƒ³"
        self.version = "2.0.0"
        
    def elevate_to_constraints(self, deep_patterns: Dict[str, Any]) -> List[ConstraintRule]:
        """æ·±å±¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ¶ç´„ãƒ«ãƒ¼ãƒ«ã«æ˜‡è¯"""
        print(f"\n=== åˆ¶ç´„æ˜‡è¯å‡¦ç†é–‹å§‹ ===")
        
        constraint_rules = []
        
        # 1. ã‚¹ã‚¿ãƒƒãƒ•å°‚é–€æ€§åˆ¶ç´„ã®ç”Ÿæˆ
        staff_constraints = self._generate_staff_constraints(
            deep_patterns.get("staff_specialization", {})
        )
        constraint_rules.extend(staff_constraints)
        
        # 2. æ™‚é–“çš„åˆ¶ç´„ã®ç”Ÿæˆ
        temporal_constraints = self._generate_temporal_constraints(
            deep_patterns.get("temporal_patterns", {})
        )
        constraint_rules.extend(temporal_constraints)
        
        # 3. è² è·åˆ†æ•£åˆ¶ç´„ã®ç”Ÿæˆ
        workload_constraints = self._generate_workload_constraints(
            deep_patterns.get("workload_distribution", {})
        )
        constraint_rules.extend(workload_constraints)
        
        # 4. ç•°å¸¸å›é¿åˆ¶ç´„ã®ç”Ÿæˆ
        anomaly_constraints = self._generate_anomaly_constraints(
            deep_patterns.get("anomaly_patterns", {})
        )
        constraint_rules.extend(anomaly_constraints)
        
        print(f"åˆ¶ç´„æ˜‡è¯å®Œäº†: {len(constraint_rules)}å€‹ã®åˆ¶ç´„ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ")
        return constraint_rules
    
    def _generate_staff_constraints(self, specializations: Dict[str, Any]) -> List[ConstraintRule]:
        """ã‚¹ã‚¿ãƒƒãƒ•åˆ¶ç´„ç”Ÿæˆ"""
        constraints = []
        
        for staff_name, spec_data in specializations.items():
            for shift_type, spec_info in spec_data.get("primary_specializations", {}).items():
                
                # å°‚é–€æ€§ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸåˆ¶ç´„ã‚¿ã‚¤ãƒ—æ±ºå®š
                spec_score = spec_info["specialization_score"]
                classification = spec_info["classification"]
                
                if classification == "high_specialist":
                    constraint_type = ConstraintType.STATIC_HARD
                    violation_penalty = "ERROR: é«˜åº¦å°‚é–€æ€§é•å"
                elif classification == "moderate_specialist":
                    constraint_type = ConstraintType.STATIC_SOFT
                    violation_penalty = "WARNING: å°‚é–€æ€§é€¸è„±"
                else:
                    constraint_type = ConstraintType.DYNAMIC_SOFT
                    violation_penalty = "INFO: æ¨å¥¨é…ç½®ã¨ç•°ãªã‚‹"
                
                # åˆ¶ç´„ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
                rule = ConstraintRule(
                    rule_id=f"STAFF_SPEC_{staff_name}_{shift_type}",
                    constraint_type=constraint_type,
                    axis=ConstraintAxis.STAFF_AXIS,
                    condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}'",
                    action=f"ã‚·ãƒ•ãƒˆ == '{shift_type}' (å„ªå…ˆåº¦: {spec_score:.0%})",
                    confidence=spec_info["ratio"],
                    evidence={
                        "specialization_score": spec_score,
                        "evidence_count": spec_info["evidence_count"],
                        "continuity": spec_info["continuity"],
                        "concentration": spec_info["concentration"]
                    },
                    measurement=spec_score * 100,  # 0-100ã®å®šè¦å€¤
                    violation_penalty=violation_penalty
                )
                
                constraints.append(rule)
        
        return constraints
    
    def _generate_temporal_constraints(self, temporal_patterns: Dict[str, Any]) -> List[ConstraintRule]:
        """æ™‚é–“çš„åˆ¶ç´„ç”Ÿæˆ"""
        constraints = []
        
        # æ—¥æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„
        for staff_name, pattern_data in temporal_patterns.get("daily_patterns", {}).items():
            for day, day_pattern in pattern_data.get("dominant_patterns", {}).items():
                
                rule = ConstraintRule(
                    rule_id=f"TEMPORAL_DAILY_{staff_name}_{day}",
                    constraint_type=ConstraintType.DYNAMIC_SOFT,
                    axis=ConstraintAxis.TIME_AXIS,
                    condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}' AND æ›œæ—¥ == {day}",
                    action=f"æ¨å¥¨ã‚·ãƒ•ãƒˆ == '{day_pattern['shift']}'",
                    confidence=day_pattern["ratio"],
                    evidence={
                        "pattern_ratio": day_pattern["ratio"],
                        "pattern_strength": pattern_data["pattern_strength"]
                    },
                    measurement=day_pattern["ratio"] * 100,
                    violation_penalty="INFO: æ™‚é–“ãƒ‘ã‚¿ãƒ¼ãƒ³é€¸è„±"
                )
                
                constraints.append(rule)
        
        # é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¶ç´„
        for staff_name, weekly_data in temporal_patterns.get("weekly_patterns", {}).items():
            consistency = weekly_data["weekly_consistency"]
            
            if consistency > 0.7:  # 70%ä»¥ä¸Šã®ä¸€è²«æ€§
                rule = ConstraintRule(
                    rule_id=f"TEMPORAL_WEEKLY_{staff_name}",
                    constraint_type=ConstraintType.DYNAMIC_SOFT,
                    axis=ConstraintAxis.TIME_AXIS,
                    condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}' AND æœŸé–“ == 'é€±æ¬¡'",
                    action=f"é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ç¶­æŒ (ä¸€è²«æ€§: {consistency:.0%})",
                    confidence=consistency,
                    evidence={
                        "weekly_consistency": consistency,
                        "pattern_variations": weekly_data["pattern_variations"]
                    },
                    measurement=consistency * 100,
                    violation_penalty="WARNING: é€±æ¬¡ãƒ‘ã‚¿ãƒ¼ãƒ³ç ´ç¶»"
                )
                
                constraints.append(rule)
        
        return constraints
    
    def _generate_workload_constraints(self, workload_patterns: Dict[str, Any]) -> List[ConstraintRule]:
        """è² è·åˆ†æ•£åˆ¶ç´„ç”Ÿæˆ"""
        constraints = []
        
        for staff_name, load_data in workload_patterns.items():
            load_ratio = load_data["relative_load"]
            classification = load_data["load_classification"]
            
            # è² è·ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸåˆ¶ç´„ç”Ÿæˆ
            if classification == "heavy_load":
                constraint_type = ConstraintType.STATIC_HARD
                violation_penalty = "ERROR: éé‡è² è·é•å"
                action = f"è² è·å‰Šæ¸›å¿…é ˆ (ç¾åœ¨: {load_ratio:.1f}å€)"
            elif classification == "minimal_load":
                constraint_type = ConstraintType.DYNAMIC_SOFT
                violation_penalty = "INFO: è² è·ä¸è¶³"
                action = f"è² è·å¢—åŠ æ¨å¥¨ (ç¾åœ¨: {load_ratio:.1f}å€)"
            else:
                continue  # æ­£å¸¸è² è·ã¯ã‚¹ã‚­ãƒƒãƒ—
            
            rule = ConstraintRule(
                rule_id=f"WORKLOAD_{staff_name}",
                constraint_type=constraint_type,
                axis=ConstraintAxis.STAFF_AXIS,
                condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}'",
                action=action,
                confidence=abs(1.0 - load_ratio),  # æ­£å¸¸å€¤ã‹ã‚‰ã®ä¹–é›¢
                evidence={
                    "absolute_load": load_data["absolute_load"],
                    "relative_load": load_ratio,
                    "load_classification": classification
                },
                measurement=abs(load_ratio - 1.0) * 100,  # æ­£å¸¸å€¤ã‹ã‚‰ã®åå·®
                violation_penalty=violation_penalty
            )
            
            constraints.append(rule)
        
        return constraints
    
    def _generate_anomaly_constraints(self, anomaly_patterns: Dict[str, Any]) -> List[ConstraintRule]:
        """ç•°å¸¸å›é¿åˆ¶ç´„ç”Ÿæˆ"""
        constraints = []
        
        for staff_name, anomalies in anomaly_patterns.get("staff_anomalies", {}).items():
            for anomaly_type in anomalies:
                
                if anomaly_type == "sudden_pattern_change":
                    rule = ConstraintRule(
                        rule_id=f"ANOMALY_SUDDEN_{staff_name}",
                        constraint_type=ConstraintType.DYNAMIC_HARD,
                        axis=ConstraintAxis.STAFF_AXIS,
                        condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}' AND ãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–æ¤œå‡º",
                        action="æ€¥æ¿€ãªãƒ‘ã‚¿ãƒ¼ãƒ³å¤‰åŒ–ã‚’å›é¿",
                        confidence=0.8,
                        evidence={"anomaly_type": anomaly_type},
                        measurement=80.0,
                        violation_penalty="WARNING: ãƒ‘ã‚¿ãƒ¼ãƒ³æ€¥å¤‰ãƒªã‚¹ã‚¯"
                    )
                elif anomaly_type == "unusual_concentration":
                    rule = ConstraintRule(
                        rule_id=f"ANOMALY_CONC_{staff_name}",
                        constraint_type=ConstraintType.STATIC_SOFT,
                        axis=ConstraintAxis.STAFF_AXIS,
                        condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}'",
                        action="ã‚·ãƒ•ãƒˆå¤šæ§˜æ€§ç¢ºä¿",
                        confidence=0.7,
                        evidence={"anomaly_type": anomaly_type},
                        measurement=70.0,
                        violation_penalty="INFO: éåº¦ãªé›†ä¸­"
                    )
                elif anomaly_type == "high_irregularity":
                    rule = ConstraintRule(
                        rule_id=f"ANOMALY_IRREG_{staff_name}",
                        constraint_type=ConstraintType.DYNAMIC_SOFT,
                        axis=ConstraintAxis.TIME_AXIS,
                        condition=f"ã‚¹ã‚¿ãƒƒãƒ• == '{staff_name}'",
                        action="ãƒ‘ã‚¿ãƒ¼ãƒ³è¦å‰‡æ€§å‘ä¸Š",
                        confidence=0.6,
                        evidence={"anomaly_type": anomaly_type},
                        measurement=60.0,
                        violation_penalty="INFO: ä¸è¦å‰‡æ€§é«˜"
                    )
                
                constraints.append(rule)
        
        return constraints

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 80)
    print("é«˜åº¦åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ  - æ„å›³ç™ºè¦‹â†’åˆ¶ç´„æ˜‡è¯")
    print("=" * 80)
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    intention_engine = AdvancedIntentionDiscovery()
    constraint_engine = ConstraintElevationEngine()
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    excel_files = list(Path('.').glob('*.xlsx'))
    if not excel_files:
        print("Excelãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return 1
    
    print(f"ç™ºè¦‹ã•ã‚ŒãŸExcelãƒ•ã‚¡ã‚¤ãƒ«: {len(excel_files)}å€‹")
    
    # ãƒ‡ã‚¤é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
    target_file = None
    for f in excel_files:
        if 'ãƒ‡ã‚¤' in f.name and 'ãƒ†ã‚¹ãƒˆ' in f.name:
            target_file = f
            break
    
    if not target_file:
        target_file = excel_files[0]
    
    print(f"åˆ†æå¯¾è±¡: {target_file}")
    
    # Excelèª­ã¿è¾¼ã¿ï¼ˆdirect_excel_readerã‚’ä½¿ç”¨ï¼‰
    try:
        from direct_excel_reader import DirectExcelReader
        reader = DirectExcelReader()
        data = reader.read_xlsx_as_zip(str(target_file))
        
        if not data:
            print("ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¤±æ•—")
            return 1
            
    except ImportError:
        print("direct_excel_readerãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return 1
    
    # Phase 1: é«˜åº¦æ„å›³ç™ºè¦‹
    print(f"\n{'='*60}")
    print("Phase 1: é«˜åº¦æ„å›³ç™ºè¦‹")
    print(f"{'='*60}")
    
    deep_patterns = intention_engine.discover_deep_patterns(data)
    
    # Phase 2: åˆ¶ç´„æ˜‡è¯
    print(f"\n{'='*60}")
    print("Phase 2: åˆ¶ç´„æ˜‡è¯")
    print(f"{'='*60}")
    
    constraint_rules = constraint_engine.elevate_to_constraints(deep_patterns)
    
    # çµæœè¡¨ç¤º
    print(f"\n{'='*80}")
    print("ã€åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ çµæœã€‘")
    print(f"{'='*80}")
    
    print(f"\nâ—† ç”Ÿæˆã•ã‚ŒãŸåˆ¶ç´„ãƒ«ãƒ¼ãƒ«: {len(constraint_rules)}å€‹")
    
    # åˆ¶ç´„ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
    type_counts = Counter(rule.constraint_type.value for rule in constraint_rules)
    print(f"\nâ—† åˆ¶ç´„ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ:")
    for constraint_type, count in type_counts.items():
        print(f"  - {constraint_type}: {count}å€‹")
    
    # è»¸åˆ¥é›†è¨ˆ  
    axis_counts = Counter(rule.axis.value for rule in constraint_rules)
    print(f"\nâ—† åˆ¶ç´„è»¸åˆ†å¸ƒ:")
    for axis, count in axis_counts.items():
        print(f"  - {axis}: {count}å€‹")
    
    # ä¸Šä½åˆ¶ç´„è¡¨ç¤º
    sorted_rules = sorted(constraint_rules, key=lambda r: r.confidence, reverse=True)
    print(f"\nâ—† æœ€é«˜ç¢ºä¿¡åº¦ã®åˆ¶ç´„ãƒ«ãƒ¼ãƒ«ï¼ˆä¸Šä½5ä»¶ï¼‰:")
    
    for i, rule in enumerate(sorted_rules[:5], 1):
        print(f"\n{i}. {rule.rule_id}")
        print(f"   ã‚¿ã‚¤ãƒ—: {rule.constraint_type.value}")
        print(f"   è»¸: {rule.axis.value}")
        print(f"   æ¡ä»¶: {rule.condition}")
        print(f"   è¡Œå‹•: {rule.action}")
        print(f"   ç¢ºä¿¡åº¦: {rule.confidence:.0%}")
        print(f"   å®šè¦å€¤: {rule.measurement:.1f}/100")
        print(f"   é•åæ™‚: {rule.violation_penalty}")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report = {
        "analysis_metadata": {
            "timestamp": datetime.now().isoformat(),
            "file": str(target_file),
            "method": "advanced_constraint_discovery"
        },
        "discovery_summary": {
            "total_constraints": len(constraint_rules),
            "constraint_types": dict(type_counts),
            "constraint_axes": dict(axis_counts)
        },
        "constraint_rules": [
            {
                "rule_id": rule.rule_id,
                "constraint_type": rule.constraint_type.value,
                "axis": rule.axis.value,
                "condition": rule.condition,
                "action": rule.action,
                "confidence": rule.confidence,
                "measurement": rule.measurement,
                "violation_penalty": rule.violation_penalty,
                "evidence": rule.evidence
            }
            for rule in sorted_rules
        ],
        "deep_patterns": deep_patterns
    }
    
    try:
        with open("advanced_constraint_discovery_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"\n[OK] é«˜åº¦åˆ¶ç´„ç™ºè¦‹ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: advanced_constraint_discovery_report.json")
    except Exception as e:
        print(f"[WARNING] ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"\n[COMPLETE] é«˜åº¦åˆ¶ç´„ç™ºè¦‹ã‚·ã‚¹ãƒ†ãƒ å®Œäº†")
    print(f"ğŸ¯ æ„å›³ç™ºè¦‹â†’åˆ¶ç´„æ˜‡è¯ã«ã‚ˆã‚Š{len(constraint_rules)}å€‹ã®å¼·åˆ¶åˆ¶ç´„ã‚’ç”Ÿæˆ")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())