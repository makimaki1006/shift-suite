#!/usr/bin/env python3
"""
å®Ÿç”¨æ€§æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

ã€Œå®Ÿç”¨çš„ã§ã‚ã‚‹ã€ã¨ã„ã†ä¸»å¼µã®æ ¹æ‹ ã‚’å®¢è¦³çš„ã«æ¤œè¨¼
"""

import pandas as pd
import numpy as np
import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime
import re

logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)


class PracticalUsabilityVerifier:
    """å®Ÿç”¨æ€§æ¤œè¨¼ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.verification_criteria = {
            'technical_feasibility': 0.0,      # æŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§
            'operational_readiness': 0.0,      # é‹ç”¨æº–å‚™åº¦
            'user_accessibility': 0.0,         # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£
            'business_value': 0.0,             # ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤
            'maintenance_sustainability': 0.0,  # ä¿å®ˆæŒç¶šå¯èƒ½æ€§
            'scalability': 0.0,                # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
            'error_tolerance': 0.0,            # ã‚¨ãƒ©ãƒ¼è€æ€§
            'real_world_applicability': 0.0    # å®Ÿä¸–ç•Œé©ç”¨æ€§
        }
        
    def verify_practical_usability(self, system_data: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿç”¨æ€§ã®åŒ…æ‹¬çš„æ¤œè¨¼"""
        log.info("ğŸ” å®Ÿç”¨æ€§æ ¹æ‹ æ¤œè¨¼é–‹å§‹...")
        
        verification_result = {
            'overall_practicality_score': 0.0,
            'criterion_scores': {},
            'evidence_analysis': {},
            'gap_identification': {},
            'usability_barriers': {},
            'improvement_requirements': {},
            'practical_readiness_assessment': '',
            'objective_evidence': {},
            'subjective_assumptions': {},
            'verification_methodology': {}
        }
        
        # å„åŸºæº–ã®æ¤œè¨¼
        verification_result['criterion_scores'] = self._verify_all_criteria(system_data)
        
        # ç·åˆå®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
        verification_result['overall_practicality_score'] = np.mean(list(verification_result['criterion_scores'].values()))
        
        # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹åˆ†æ
        verification_result['evidence_analysis'] = self._analyze_evidence_quality(system_data)
        
        # ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š
        verification_result['gap_identification'] = self._identify_practical_gaps(verification_result['criterion_scores'])
        
        # å®Ÿç”¨æ€§é˜»å®³è¦å› 
        verification_result['usability_barriers'] = self._identify_usability_barriers(system_data)
        
        # å®¢è¦³çš„è¨¼æ‹  vs ä¸»è¦³çš„ä»®å®šã®åˆ†é›¢
        verification_result['objective_evidence'], verification_result['subjective_assumptions'] = self._separate_evidence_types(system_data)
        
        # æ”¹å–„è¦ä»¶
        verification_result['improvement_requirements'] = self._define_improvement_requirements(verification_result)
        
        # å®Ÿç”¨æº–å‚™åº¦è©•ä¾¡
        verification_result['practical_readiness_assessment'] = self._assess_practical_readiness(verification_result['overall_practicality_score'])
        
        # æ¤œè¨¼æ–¹æ³•è«–
        verification_result['verification_methodology'] = self._document_verification_methodology()
        
        return verification_result
    
    def _verify_all_criteria(self, system_data: Dict[str, Any]) -> Dict[str, float]:
        """å…¨åŸºæº–ã®æ¤œè¨¼"""
        log.info("ğŸ“Š å®Ÿç”¨æ€§åŸºæº–ã®å€‹åˆ¥æ¤œè¨¼...")
        
        scores = {}
        
        # 1. æŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§
        scores['technical_feasibility'] = self._verify_technical_feasibility(system_data)
        
        # 2. é‹ç”¨æº–å‚™åº¦  
        scores['operational_readiness'] = self._verify_operational_readiness(system_data)
        
        # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£
        scores['user_accessibility'] = self._verify_user_accessibility(system_data)
        
        # 4. ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤
        scores['business_value'] = self._verify_business_value(system_data)
        
        # 5. ä¿å®ˆæŒç¶šå¯èƒ½æ€§
        scores['maintenance_sustainability'] = self._verify_maintenance_sustainability(system_data)
        
        # 6. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
        scores['scalability'] = self._verify_scalability(system_data)
        
        # 7. ã‚¨ãƒ©ãƒ¼è€æ€§
        scores['error_tolerance'] = self._verify_error_tolerance(system_data)
        
        # 8. å®Ÿä¸–ç•Œé©ç”¨æ€§
        scores['real_world_applicability'] = self._verify_real_world_applicability(system_data)
        
        return scores
    
    def _verify_technical_feasibility(self, system_data: Dict[str, Any]) -> float:
        """æŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§ã®æ¤œè¨¼"""
        log.info("  ğŸ”§ æŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§æ¤œè¨¼ä¸­...")
        
        feasibility_factors = {
            'implementation_complexity': 0.0,
            'dependency_availability': 0.0,
            'performance_requirements': 0.0,
            'integration_capability': 0.0,
            'resource_requirements': 0.0
        }
        
        # å®Ÿè£…è¤‡é›‘æ€§ã®è©•ä¾¡ï¼ˆå®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰åˆ†æãŒå¿…è¦ï¼‰
        feasibility_factors['implementation_complexity'] = 0.6  # æ¨å®šå€¤ - è¦å®Ÿæ¸¬
        log.warning("    âš ï¸  å®Ÿè£…è¤‡é›‘æ€§: æ¨å®šå€¤ä½¿ç”¨ - å®Ÿéš›ã®ã‚³ãƒ¼ãƒ‰åˆ†æãŒå¿…è¦")
        
        # ä¾å­˜é–¢ä¿‚ã®å¯ç”¨æ€§ï¼ˆrequirements.txtã‹ã‚‰æ¨å®šï¼‰
        feasibility_factors['dependency_availability'] = 0.8  # Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ™ãƒ¼ã‚¹
        log.info("    âœ… ä¾å­˜é–¢ä¿‚: Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸­å¿ƒã§å®‰å®š")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ä»¶ï¼ˆæœªå®Ÿæ¸¬ï¼‰
        feasibility_factors['performance_requirements'] = 0.5  # æ¨å®šå€¤ - è¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        log.warning("    âš ï¸  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: æœªå®Ÿæ¸¬ - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆãŒå¿…è¦")
        
        # çµ±åˆèƒ½åŠ›ï¼ˆAPIãƒ¬ãƒ™ãƒ«ã§ã®è©•ä¾¡ï¼‰
        feasibility_factors['integration_capability'] = 0.7  # JSON/REST APIå¯¾å¿œ
        log.info("    âœ… çµ±åˆèƒ½åŠ›: JSON/REST APIå¯¾å¿œæ¸ˆã¿")
        
        # ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ï¼ˆæœªè©•ä¾¡ï¼‰
        feasibility_factors['resource_requirements'] = 0.4  # æ¨å®šå€¤ - è¦å®Ÿæ¸¬
        log.warning("    âš ï¸  ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶: æœªè©•ä¾¡ - ã‚·ã‚¹ãƒ†ãƒ è¦ä»¶åˆ†æãŒå¿…è¦")
        
        score = np.mean(list(feasibility_factors.values()))
        log.info(f"    ğŸ“Š æŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§ã‚¹ã‚³ã‚¢: {score:.1%}")
        return score
    
    def _verify_operational_readiness(self, system_data: Dict[str, Any]) -> float:
        """é‹ç”¨æº–å‚™åº¦ã®æ¤œè¨¼"""
        log.info("  ğŸ­ é‹ç”¨æº–å‚™åº¦æ¤œè¨¼ä¸­...")
        
        readiness_factors = {
            'deployment_procedures': 0.0,
            'monitoring_systems': 0.0,
            'backup_recovery': 0.0,
            'security_measures': 0.0,
            'documentation_completeness': 0.0,
            'staff_training': 0.0
        }
        
        # ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹é †ï¼ˆæœªæ•´å‚™ï¼‰
        readiness_factors['deployment_procedures'] = 0.2  # åŸºæœ¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ã¿
        log.warning("    âŒ ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæ‰‹é †: æœªæ•´å‚™ - æœ¬æ ¼çš„ãªCI/CDãŒå¿…è¦")
        
        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆéƒ¨åˆ†çš„å®Ÿè£…ï¼‰
        readiness_factors['monitoring_systems'] = 0.6  # ãƒ­ã‚°å‡ºåŠ›ã®ã¿
        log.warning("    âš ï¸  ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ : åŸºæœ¬çš„ãªãƒ­ã‚°ã®ã¿ - æœ¬æ ¼ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦")
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§ï¼ˆæœªå®Ÿè£…ï¼‰
        readiness_factors['backup_recovery'] = 0.1  # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã®ã¿
        log.error("    âŒ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§: æœªå®Ÿè£… - ãƒ‡ãƒ¼ã‚¿ä¿è­·æ©Ÿèƒ½ãŒå¿…è¦")
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ï¼ˆæœªå®Ÿè£…ï¼‰
        readiness_factors['security_measures'] = 0.2  # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«æ¨©é™ã®ã¿
        log.error("    âŒ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–: ä¸ååˆ† - èªè¨¼ãƒ»èªå¯ã‚·ã‚¹ãƒ†ãƒ ãŒå¿…è¦")
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå®Œæˆåº¦ï¼ˆéƒ¨åˆ†çš„ï¼‰
        readiness_factors['documentation_completeness'] = 0.7  # ã‚³ãƒ¡ãƒ³ãƒˆãƒ»READMEå­˜åœ¨
        log.info("    âœ… ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: åŸºæœ¬çš„ãªèª¬æ˜ã¯å­˜åœ¨")
        
        # ã‚¹ã‚¿ãƒƒãƒ•ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæœªå®Ÿæ–½ï¼‰
        readiness_factors['staff_training'] = 0.1  # ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãªã—
        log.error("    âŒ ã‚¹ã‚¿ãƒƒãƒ•ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°: æœªå®Ÿæ–½ - é‹ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãŒå¿…è¦")
        
        score = np.mean(list(readiness_factors.values()))
        log.warning(f"    ğŸ“Š é‹ç”¨æº–å‚™åº¦ã‚¹ã‚³ã‚¢: {score:.1%} - å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦")
        return score
    
    def _verify_user_accessibility(self, system_data: Dict[str, Any]) -> float:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã®æ¤œè¨¼"""
        log.info("  ğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼ä¸­...")
        
        accessibility_factors = {
            'interface_usability': 0.0,
            'learning_curve': 0.0,
            'error_handling': 0.0,
            'help_support': 0.0,
            'accessibility_compliance': 0.0
        }
        
        # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ä½¿ã„ã‚„ã™ã•ï¼ˆDashãƒ™ãƒ¼ã‚¹ï¼‰
        accessibility_factors['interface_usability'] = 0.6  # Dash UIã®åŸºæœ¬çš„ãªä½¿ã„ã‚„ã™ã•
        log.info("    âœ… ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹: Dashãƒ™ãƒ¼ã‚¹ã§åŸºæœ¬çš„ãªæ“ä½œæ€§ç¢ºä¿")
        
        # å­¦ç¿’ã‚³ã‚¹ãƒˆï¼ˆé«˜å°‚é–€æ€§ï¼‰
        accessibility_factors['learning_curve'] = 0.4  # å°‚é–€çŸ¥è­˜ãŒå¿…è¦
        log.warning("    âš ï¸  å­¦ç¿’ã‚³ã‚¹ãƒˆ: é«˜ - MECEæ¦‚å¿µã®ç†è§£ãŒå¿…è¦")
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆéƒ¨åˆ†çš„å®Ÿè£…ï¼‰
        accessibility_factors['error_handling'] = 0.5  # åŸºæœ¬çš„ãªtry-catch
        log.warning("    âš ï¸  ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: åŸºæœ¬çš„ãªã‚‚ã®ã®ã¿")
        
        # ãƒ˜ãƒ«ãƒ—ãƒ»ã‚µãƒãƒ¼ãƒˆï¼ˆæœªå®Ÿè£…ï¼‰
        accessibility_factors['help_support'] = 0.2  # READMEã®ã¿
        log.error("    âŒ ãƒ˜ãƒ«ãƒ—ãƒ»ã‚µãƒãƒ¼ãƒˆ: ä¸ååˆ† - ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ˜ãƒ«ãƒ—ãŒå¿…è¦")
        
        # ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æº–æ‹ ï¼ˆæœªè€ƒæ…®ï¼‰
        accessibility_factors['accessibility_compliance'] = 0.3  # åŸºæœ¬çš„ãªHTMLæ§‹é€ 
        log.warning("    âš ï¸  ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æº–æ‹ : æœªè€ƒæ…® - WCAGæº–æ‹ ãŒå¿…è¦")
        
        score = np.mean(list(accessibility_factors.values()))
        log.warning(f"    ğŸ“Š ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {score:.1%}")
        return score
    
    def _verify_business_value(self, system_data: Dict[str, Any]) -> float:
        """ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã®æ¤œè¨¼"""
        log.info("  ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤æ¤œè¨¼ä¸­...")
        
        value_factors = {
            'cost_reduction': 0.0,
            'efficiency_improvement': 0.0,
            'quality_enhancement': 0.0,
            'risk_mitigation': 0.0,
            'competitive_advantage': 0.0
        }
        
        # ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼ˆç†è«–çš„ï¼‰
        value_factors['cost_reduction'] = 0.7  # è‡ªå‹•åŒ–ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆå‰Šæ¸›æœŸå¾…
        log.info("    âœ… ã‚³ã‚¹ãƒˆå‰Šæ¸›: è‡ªå‹•åŒ–ã«ã‚ˆã‚‹äººä»¶è²»å‰Šæ¸›ãŒæœŸå¾…")
        
        # åŠ¹ç‡æ”¹å–„ï¼ˆç†è«–çš„ï¼‰
        value_factors['efficiency_improvement'] = 0.8  # ã‚·ãƒ•ãƒˆä½œæˆæ™‚é–“çŸ­ç¸®
        log.info("    âœ… åŠ¹ç‡æ”¹å–„: ã‚·ãƒ•ãƒˆä½œæˆæ™‚é–“ã®å¤§å¹…çŸ­ç¸®ãŒæœŸå¾…")
        
        # å“è³ªå‘ä¸Šï¼ˆæ¤œè¨¼æ¸ˆã¿ï¼‰
        value_factors['quality_enhancement'] = 0.9  # MECEå“è³ª88.1%é”æˆ
        log.info("    âœ… å“è³ªå‘ä¸Š: MECEå“è³ª88.1%ã§åˆ¶ç´„ã®è³ªãŒå‘ä¸Š")
        
        # ãƒªã‚¹ã‚¯è»½æ¸›ï¼ˆç†è«–çš„ï¼‰
        value_factors['risk_mitigation'] = 0.6  # åˆ¶ç´„é•åã®è‡ªå‹•æ¤œå‡º
        log.info("    âœ… ãƒªã‚¹ã‚¯è»½æ¸›: åˆ¶ç´„é•åã®è‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½")
        
        # ç«¶äº‰å„ªä½ï¼ˆç†è«–çš„ï¼‰
        value_factors['competitive_advantage'] = 0.5  # AIæ´»ç”¨ã®å…ˆé€²æ€§
        log.warning("    âš ï¸  ç«¶äº‰å„ªä½: ç†è«–çš„ - å¸‚å ´ã§ã®å®Ÿè¨¼ãŒå¿…è¦")
        
        score = np.mean(list(value_factors.values()))
        log.info(f"    ğŸ“Š ãƒ“ã‚¸ãƒã‚¹ä¾¡å€¤ã‚¹ã‚³ã‚¢: {score:.1%}")
        return score
    
    def _verify_maintenance_sustainability(self, system_data: Dict[str, Any]) -> float:
        """ä¿å®ˆæŒç¶šå¯èƒ½æ€§ã®æ¤œè¨¼"""
        log.info("  ğŸ”§ ä¿å®ˆæŒç¶šå¯èƒ½æ€§æ¤œè¨¼ä¸­...")
        
        maintenance_factors = {
            'code_maintainability': 0.0,
            'technical_debt': 0.0,
            'update_procedures': 0.0,
            'knowledge_documentation': 0.0,
            'team_expertise': 0.0
        }
        
        # ã‚³ãƒ¼ãƒ‰ä¿å®ˆæ€§ï¼ˆæœªè©•ä¾¡ï¼‰
        maintenance_factors['code_maintainability'] = 0.6  # æ¨å®š - è¦ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼
        log.warning("    âš ï¸  ã‚³ãƒ¼ãƒ‰ä¿å®ˆæ€§: æœªè©•ä¾¡ - æœ¬æ ¼çš„ãªã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒå¿…è¦")
        
        # æŠ€è¡“çš„è² å‚µï¼ˆè¦è©•ä¾¡ï¼‰
        maintenance_factors['technical_debt'] = 0.5  # æ¨å®š - è¦åˆ†æ
        log.warning("    âš ï¸  æŠ€è¡“çš„è² å‚µ: æœªåˆ†æ - è² å‚µãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡ãŒå¿…è¦")
        
        # æ›´æ–°æ‰‹é †ï¼ˆæœªæ•´å‚™ï¼‰
        maintenance_factors['update_procedures'] = 0.3  # åŸºæœ¬çš„ãªGitç®¡ç†ã®ã¿
        log.error("    âŒ æ›´æ–°æ‰‹é †: æœªæ•´å‚™ - ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ä½“åˆ¶ãŒå¿…è¦")
        
        # çŸ¥è­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆéƒ¨åˆ†çš„ï¼‰
        maintenance_factors['knowledge_documentation'] = 0.4  # ã‚³ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«
        log.warning("    âš ï¸  çŸ¥è­˜ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: ä¸ååˆ† - è©³ç´°è¨­è¨ˆæ›¸ãŒå¿…è¦")
        
        # ãƒãƒ¼ãƒ å°‚é–€æ€§ï¼ˆæœªè©•ä¾¡ï¼‰
        maintenance_factors['team_expertise'] = 0.4  # æ¨å®š - è¦ã‚¹ã‚­ãƒ«è©•ä¾¡
        log.warning("    âš ï¸  ãƒãƒ¼ãƒ å°‚é–€æ€§: æœªè©•ä¾¡ - ã‚¹ã‚­ãƒ«ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ãŒå¿…è¦")
        
        score = np.mean(list(maintenance_factors.values()))
        log.warning(f"    ğŸ“Š ä¿å®ˆæŒç¶šå¯èƒ½æ€§ã‚¹ã‚³ã‚¢: {score:.1%} - æ”¹å–„ãŒå¿…è¦")
        return score
    
    def _verify_scalability(self, system_data: Dict[str, Any]) -> float:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®æ¤œè¨¼"""
        log.info("  ğŸ“ˆ ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼ä¸­...")
        
        scalability_factors = {
            'data_volume_handling': 0.0,
            'user_concurrency': 0.0,
            'feature_extensibility': 0.0,
            'performance_scaling': 0.0,
            'resource_efficiency': 0.0
        }
        
        # ãƒ‡ãƒ¼ã‚¿é‡å‡¦ç†ï¼ˆæœªãƒ†ã‚¹ãƒˆï¼‰
        scalability_factors['data_volume_handling'] = 0.3  # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ†ã‚¹ãƒˆ
        log.error("    âŒ ãƒ‡ãƒ¼ã‚¿é‡å‡¦ç†: æœªãƒ†ã‚¹ãƒˆ - å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½è©•ä¾¡ãŒå¿…è¦")
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åŒæ™‚æ¥ç¶šï¼ˆæœªãƒ†ã‚¹ãƒˆï¼‰
        scalability_factors['user_concurrency'] = 0.2  # å˜ä¸€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã¿
        log.error("    âŒ ãƒ¦ãƒ¼ã‚¶ãƒ¼åŒæ™‚æ¥ç¶š: æœªãƒ†ã‚¹ãƒˆ - è² è·ãƒ†ã‚¹ãƒˆãŒå¿…è¦")
        
        # æ©Ÿèƒ½æ‹¡å¼µæ€§ï¼ˆè‰¯å¥½ï¼‰
        scalability_factors['feature_extensibility'] = 0.8  # ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆ
        log.info("    âœ… æ©Ÿèƒ½æ‹¡å¼µæ€§: ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆã§æ‹¡å¼µå®¹æ˜“")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆæœªè©•ä¾¡ï¼‰
        scalability_factors['performance_scaling'] = 0.3  # æœªè©•ä¾¡
        log.error("    âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°: æœªè©•ä¾¡ - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå¿…è¦")
        
        # ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ï¼ˆæœªè©•ä¾¡ï¼‰
        scalability_factors['resource_efficiency'] = 0.4  # æ¨å®š
        log.warning("    âš ï¸  ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡: æœªè©•ä¾¡ - ãƒ¡ãƒ¢ãƒªãƒ»CPUä½¿ç”¨é‡ã®æ¸¬å®šãŒå¿…è¦")
        
        score = np.mean(list(scalability_factors.values()))
        log.error(f"    ğŸ“Š ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢: {score:.1%} - å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦")
        return score
    
    def _verify_error_tolerance(self, system_data: Dict[str, Any]) -> float:
        """ã‚¨ãƒ©ãƒ¼è€æ€§ã®æ¤œè¨¼"""
        log.info("  ğŸ›¡ï¸ ã‚¨ãƒ©ãƒ¼è€æ€§æ¤œè¨¼ä¸­...")
        
        tolerance_factors = {
            'input_validation': 0.0,
            'graceful_degradation': 0.0,
            'error_recovery': 0.0,
            'fault_isolation': 0.0,
            'logging_monitoring': 0.0
        }
        
        # å…¥åŠ›æ¤œè¨¼ï¼ˆéƒ¨åˆ†çš„å®Ÿè£…ï¼‰
        tolerance_factors['input_validation'] = 0.5  # åŸºæœ¬çš„ãªvalidation
        log.warning("    âš ï¸  å…¥åŠ›æ¤œè¨¼: åŸºæœ¬çš„ãªã‚‚ã®ã®ã¿ - åŒ…æ‹¬çš„æ¤œè¨¼ãŒå¿…è¦")
        
        # æ®µéšçš„åŠ£åŒ–ï¼ˆæœªå®Ÿè£…ï¼‰
        tolerance_factors['graceful_degradation'] = 0.3  # éƒ¨åˆ†çš„ãªä¾‹å¤–å‡¦ç†
        log.warning("    âš ï¸  æ®µéšçš„åŠ£åŒ–: éƒ¨åˆ†çš„ - ã‚¨ãƒ©ãƒ¼æ™‚ã®ä»£æ›¿å‡¦ç†ãŒå¿…è¦")
        
        # ã‚¨ãƒ©ãƒ¼å›å¾©ï¼ˆåŸºæœ¬çš„ï¼‰
        tolerance_factors['error_recovery'] = 0.4  # try-catchãƒ¬ãƒ™ãƒ«
        log.warning("    âš ï¸  ã‚¨ãƒ©ãƒ¼å›å¾©: åŸºæœ¬çš„ - è‡ªå‹•å›å¾©æ©Ÿèƒ½ãŒå¿…è¦")
        
        # éšœå®³åˆ†é›¢ï¼ˆæœªå®Ÿè£…ï¼‰
        tolerance_factors['fault_isolation'] = 0.2  # æœªå®Ÿè£…
        log.error("    âŒ éšœå®³åˆ†é›¢: æœªå®Ÿè£… - éšœå®³æ™‚ã®å½±éŸ¿ç¯„å›²åˆ¶é™ãŒå¿…è¦")
        
        # ãƒ­ã‚°ãƒ»ç›£è¦–ï¼ˆåŸºæœ¬çš„ï¼‰
        tolerance_factors['logging_monitoring'] = 0.6  # ãƒ­ã‚°å‡ºåŠ›ã‚ã‚Š
        log.info("    âœ… ãƒ­ã‚°ãƒ»ç›£è¦–: åŸºæœ¬çš„ãªãƒ¬ãƒ™ãƒ«ã¯å®Ÿè£…æ¸ˆã¿")
        
        score = np.mean(list(tolerance_factors.values()))
        log.warning(f"    ğŸ“Š ã‚¨ãƒ©ãƒ¼è€æ€§ã‚¹ã‚³ã‚¢: {score:.1%}")
        return score
    
    def _verify_real_world_applicability(self, system_data: Dict[str, Any]) -> float:
        """å®Ÿä¸–ç•Œé©ç”¨æ€§ã®æ¤œè¨¼"""
        log.info("  ğŸŒ å®Ÿä¸–ç•Œé©ç”¨æ€§æ¤œè¨¼ä¸­...")
        
        applicability_factors = {
            'real_data_compatibility': 0.0,
            'workflow_integration': 0.0,
            'regulatory_compliance': 0.0,
            'organizational_fit': 0.0,
            'pilot_testing': 0.0
        }
        
        # å®Ÿãƒ‡ãƒ¼ã‚¿äº’æ›æ€§ï¼ˆæœªãƒ†ã‚¹ãƒˆï¼‰
        applicability_factors['real_data_compatibility'] = 0.4  # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ã¿
        log.error("    âŒ å®Ÿãƒ‡ãƒ¼ã‚¿äº’æ›æ€§: æœªãƒ†ã‚¹ãƒˆ - å®Ÿéš›ã®æ–½è¨­ãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼ãŒå¿…è¦")
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆï¼ˆæœªæ¤œè¨¼ï¼‰
        applicability_factors['workflow_integration'] = 0.3  # ç†è«–ãƒ¬ãƒ™ãƒ«
        log.error("    âŒ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çµ±åˆ: æœªæ¤œè¨¼ - å®Ÿéš›ã®æ¥­å‹™ãƒ•ãƒ­ãƒ¼ã¨ã®é©åˆæ€§ç¢ºèªãŒå¿…è¦")
        
        # è¦åˆ¶æº–æ‹ ï¼ˆæœªç¢ºèªï¼‰
        applicability_factors['regulatory_compliance'] = 0.2  # æœªç¢ºèª
        log.error("    âŒ è¦åˆ¶æº–æ‹ : æœªç¢ºèª - åŠ´åƒæ³•ãƒ»æ¥­ç•Œè¦åˆ¶ã¸ã®é©åˆç¢ºèªãŒå¿…è¦")
        
        # çµ„ç¹”é©åˆæ€§ï¼ˆæœªè©•ä¾¡ï¼‰
        applicability_factors['organizational_fit'] = 0.3  # ç†è«–ãƒ¬ãƒ™ãƒ«
        log.error("    âŒ çµ„ç¹”é©åˆæ€§: æœªè©•ä¾¡ - å®Ÿéš›ã®çµ„ç¹”ã§ã®å—å®¹æ€§ç¢ºèªãŒå¿…è¦")
        
        # ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ†ã‚¹ãƒˆï¼ˆæœªå®Ÿæ–½ï¼‰
        applicability_factors['pilot_testing'] = 0.1  # æœªå®Ÿæ–½
        log.error("    âŒ ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆãƒ†ã‚¹ãƒˆ: æœªå®Ÿæ–½ - å®Ÿç’°å¢ƒã§ã®è©¦é¨“é‹ç”¨ãŒå¿…è¦")
        
        score = np.mean(list(applicability_factors.values()))
        log.error(f"    ğŸ“Š å®Ÿä¸–ç•Œé©ç”¨æ€§ã‚¹ã‚³ã‚¢: {score:.1%} - é‡å¤§ãªæ”¹å–„ãŒå¿…è¦")
        return score
    
    def _analyze_evidence_quality(self, system_data: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹å“è³ªã®åˆ†æ"""
        return {
            'objective_evidence_ratio': 0.3,  # å®¢è¦³çš„è¨¼æ‹ 30%
            'subjective_assumptions_ratio': 0.7,  # ä¸»è¦³çš„ä»®å®š70%
            'empirical_validation_level': 'low',  # å®Ÿè¨¼çš„æ¤œè¨¼ãƒ¬ãƒ™ãƒ«ï¼šä½
            'theoretical_foundation': 'moderate',  # ç†è«–çš„åŸºç›¤ï¼šä¸­ç¨‹åº¦
            'data_quality': 'sample_based',  # ãƒ‡ãƒ¼ã‚¿å“è³ªï¼šã‚µãƒ³ãƒ—ãƒ«ãƒ™ãƒ¼ã‚¹
            'measurement_reliability': 'estimated'  # æ¸¬å®šä¿¡é ¼æ€§ï¼šæ¨å®š
        }
    
    def _identify_practical_gaps(self, criterion_scores: Dict[str, float]) -> Dict[str, Any]:
        """å®Ÿç”¨æ€§ã‚®ãƒ£ãƒƒãƒ—ã®ç‰¹å®š"""
        gaps = {}
        
        for criterion, score in criterion_scores.items():
            if score < 0.7:  # 70%æœªæº€ã‚’å®Ÿç”¨æ€§ã‚®ãƒ£ãƒƒãƒ—ã¨ã—ã¦ç‰¹å®š
                gaps[criterion] = {
                    'current_score': score,
                    'gap_to_practical': 0.7 - score,
                    'severity': 'critical' if score < 0.4 else 'high',
                    'impact_on_usability': self._assess_gap_impact(criterion, score)
                }
        
        return gaps
    
    def _identify_usability_barriers(self, system_data: Dict[str, Any]) -> Dict[str, List[str]]:
        """å®Ÿç”¨æ€§é˜»å®³è¦å› ã®ç‰¹å®š"""
        return {
            'missing_infrastructure': [
                'CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœªæ•´å‚™',
                'æœ¬æ ¼çš„ãªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ä¸åœ¨',
                'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ æœªå®Ÿè£…',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ä¸ååˆ†'
            ],
            'insufficient_testing': [
                'å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®æ¤œè¨¼æœªå®Ÿæ–½',
                'è² è·ãƒ†ã‚¹ãƒˆæœªå®Ÿæ–½',
                'ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆæœªå®Ÿæ–½',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆæœªå®Ÿæ–½'
            ],
            'documentation_gaps': [
                'é‹ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ä¸åœ¨',
                'ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰ä¸åœ¨',
                'APIä»•æ§˜æ›¸ä¸å®Œå…¨',
                'ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è³‡æ–™ä¸åœ¨'
            ],
            'compliance_unknowns': [
                'åŠ´åƒæ³•æº–æ‹ æœªç¢ºèª',
                'æ¥­ç•Œè¦åˆ¶é©åˆæœªç¢ºèª',
                'ãƒ‡ãƒ¼ã‚¿ä¿è­·æ³•å¯¾å¿œæœªç¢ºèª',
                'ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£åŸºæº–æœªæº–æ‹ '
            ]
        }
    
    def _separate_evidence_types(self, system_data: Dict[str, Any]) -> Tuple[Dict, Dict]:
        """å®¢è¦³çš„è¨¼æ‹ ã¨ä¸»è¦³çš„ä»®å®šã®åˆ†é›¢"""
        
        objective_evidence = {
            'code_exists': True,
            'basic_functionality_works': True,
            'test_data_processes': True,
            'json_output_generated': True,
            'dash_interface_renders': True,
            'quality_metrics_calculated': True
        }
        
        subjective_assumptions = {
            'performance_adequacy': 'æ¨å®šå€¤',
            'scalability_sufficiency': 'æœªæ¤œè¨¼ã®ä»®å®š',
            'user_satisfaction': 'æœªæ¸¬å®šã®æœŸå¾…',
            'business_value_realization': 'ç†è«–çš„äºˆæ¸¬',
            'maintenance_feasibility': 'çµŒé¨“å‰‡ã«ã‚ˆã‚‹æ¨å®š',
            'real_world_compatibility': 'ç†æƒ³çš„æ¡ä»¶ã§ã®ä»®å®š',
            'deployment_simplicity': 'é–‹ç™ºç’°å¢ƒã§ã®çµŒé¨“ã«ã‚ˆã‚‹æ¨å®š',
            'cost_effectiveness': 'æœªå®Ÿè¨¼ã®æœŸå¾…'
        }
        
        return objective_evidence, subjective_assumptions
    
    def _define_improvement_requirements(self, verification_result: Dict[str, Any]) -> Dict[str, Any]:
        """æ”¹å–„è¦ä»¶ã®å®šç¾©"""
        
        # å®Ÿç”¨æ€§é”æˆã«å¿…è¦ãªæœ€ä½è¦ä»¶
        minimum_requirements = {
            'critical_priority': [
                'å®Ÿç’°å¢ƒã§ã®è©¦é¨“é‹ç”¨å®Ÿæ–½',
                'å®Ÿãƒ‡ãƒ¼ã‚¿ã§ã®å‹•ä½œç¢ºèª',
                'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã®å®Ÿè£…',
                'ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰'
            ],
            'high_priority': [
                'è² è·ãƒ†ã‚¹ãƒˆã®å®Ÿæ–½',
                'é‹ç”¨ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®ä½œæˆ',
                'ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å¼·åŒ–',
                'ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–'
            ],
            'medium_priority': [
                'ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ',
                'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨æ”¹å–„',
                'ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŒ–',
                'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå……å®Ÿ'
            ]
        }
        
        # æ¨å®šæ”¹å–„æ™‚é–“ã¨ã‚³ã‚¹ãƒˆ
        improvement_estimates = {
            'time_to_minimum_viable': '2-3ãƒ¶æœˆ',
            'time_to_production_ready': '4-6ãƒ¶æœˆ',
            'estimated_effort': 'é«˜ï¼ˆå°‚é–€ãƒãƒ¼ãƒ å¿…è¦ï¼‰',
            'infrastructure_cost': 'ä¸­ç¨‹åº¦',
            'risk_level': 'ä¸­ï½é«˜ï¼ˆæœªæ¤œè¨¼è¦ç´ å¤šæ•°ï¼‰'
        }
        
        return {
            'minimum_requirements': minimum_requirements,
            'improvement_estimates': improvement_estimates
        }
    
    def _assess_practical_readiness(self, overall_score: float) -> str:
        """å®Ÿç”¨æº–å‚™åº¦ã®è©•ä¾¡"""
        if overall_score >= 0.8:
            return 'Production Ready - æœ¬æ ¼é‹ç”¨å¯èƒ½'
        elif overall_score >= 0.7:
            return 'Near Production - æœ€çµ‚èª¿æ•´å¾Œé‹ç”¨å¯èƒ½'
        elif overall_score >= 0.6:
            return 'Beta Quality - é™å®šé‹ç”¨å¯èƒ½'
        elif overall_score >= 0.5:
            return 'Alpha Quality - å†…éƒ¨ãƒ†ã‚¹ãƒˆæ®µéš'
        elif overall_score >= 0.4:
            return 'Development Phase - é–‹ç™ºç¶™ç¶šä¸­'
        else:
            return 'Proof of Concept - æ¦‚å¿µå®Ÿè¨¼æ®µéš'
    
    def _document_verification_methodology(self) -> Dict[str, Any]:
        """æ¤œè¨¼æ–¹æ³•è«–ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–"""
        return {
            'verification_approach': 'multi_criteria_assessment',
            'evaluation_basis': '8ã¤ã®å®Ÿç”¨æ€§åŸºæº–ã«ã‚ˆã‚‹åŒ…æ‹¬è©•ä¾¡',
            'scoring_method': 'å„åŸºæº–0-1.0ã®æ•°å€¤è©•ä¾¡',
            'evidence_classification': 'objective_vs_subjective separation',
            'limitation_acknowledgment': 'å¤šãã®è©•ä¾¡ãŒæ¨å®šå€¤ã«åŸºã¥ã',
            'validation_requirements': 'å®Ÿç’°å¢ƒã§ã®æ¤œè¨¼ãŒå¿…è¦',
            'methodology_confidence': 'medium - æ§‹é€ åŒ–ã•ã‚Œã¦ã„ã‚‹ãŒå®Ÿè¨¼ãƒ‡ãƒ¼ã‚¿ä¸è¶³'
        }
    
    def _assess_gap_impact(self, criterion: str, score: float) -> str:
        """ã‚®ãƒ£ãƒƒãƒ—ã®å®Ÿç”¨æ€§ã¸ã®å½±éŸ¿è©•ä¾¡"""
        impact_map = {
            'technical_feasibility': 'å®Ÿè£…ä¸å¯ãƒªã‚¹ã‚¯',
            'operational_readiness': 'é‹ç”¨å¤±æ•—ãƒªã‚¹ã‚¯',
            'user_accessibility': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼é›¢è„±ãƒªã‚¹ã‚¯',
            'business_value': 'ROIæœªé”ãƒªã‚¹ã‚¯',
            'maintenance_sustainability': 'é•·æœŸé‹ç”¨ä¸å¯ãƒªã‚¹ã‚¯',
            'scalability': 'æˆé•·å¯¾å¿œä¸å¯ãƒªã‚¹ã‚¯',
            'error_tolerance': 'ã‚·ã‚¹ãƒ†ãƒ åœæ­¢ãƒªã‚¹ã‚¯',
            'real_world_applicability': 'å®Ÿç”¨åŒ–å¤±æ•—ãƒªã‚¹ã‚¯'
        }
        
        return impact_map.get(criterion, 'å½±éŸ¿åº¦ä¸æ˜')


def run_practical_usability_verification():
    """å®Ÿç”¨æ€§æ¤œè¨¼ã®å®Ÿè¡Œ"""
    log.info("ğŸ¯ å®Ÿç”¨æ€§æ ¹æ‹ æ¤œè¨¼é–‹å§‹")
    log.info("=" * 80)
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆç¾åœ¨ã¯é™å®šçš„ãªã‚µãƒ³ãƒ—ãƒ«ï¼‰
    system_data = {
        'code_base': 'exists',
        'test_results': 'limited_sample_data',
        'performance_data': 'not_measured',
        'user_feedback': 'not_collected',
        'deployment_experience': 'development_only'
    }
    
    # æ¤œè¨¼å®Ÿè¡Œ
    verifier = PracticalUsabilityVerifier()
    verification_result = verifier.verify_practical_usability(system_data)
    
    # çµæœè¡¨ç¤º
    display_verification_results(verification_result)
    
    # çµæœä¿å­˜
    with open('practical_usability_verification.json', 'w', encoding='utf-8') as f:
        json.dump(verification_result, f, ensure_ascii=False, indent=2, default=str)
    
    return verification_result


def display_verification_results(result: Dict[str, Any]):
    """æ¤œè¨¼çµæœã®è¡¨ç¤º"""
    
    overall_score = result['overall_practicality_score']
    readiness = result['practical_readiness_assessment']
    
    log.info("\n" + "=" * 80)
    log.info("ğŸ“Š å®Ÿç”¨æ€§æ¤œè¨¼çµæœ")
    log.info("=" * 80)
    
    log.info(f"ğŸ¯ ç·åˆå®Ÿç”¨æ€§ã‚¹ã‚³ã‚¢: {overall_score:.1%}")
    log.info(f"ğŸ“‹ å®Ÿç”¨æº–å‚™åº¦: {readiness}")
    
    log.info("\nğŸ“ˆ å„åŸºæº–ã‚¹ã‚³ã‚¢:")
    for criterion, score in result['criterion_scores'].items():
        emoji = "ğŸŸ¢" if score >= 0.7 else "ğŸŸ¡" if score >= 0.5 else "ğŸ”´"
        log.info(f"  {emoji} {criterion}: {score:.1%}")
    
    log.info("\nğŸš§ å®Ÿç”¨æ€§ã‚®ãƒ£ãƒƒãƒ—:")
    for criterion, gap_info in result['gap_identification'].items():
        log.info(f"  â— {criterion}: {gap_info['current_score']:.1%} (ã‚®ãƒ£ãƒƒãƒ—: {gap_info['gap_to_practical']:.1%})")
        log.info(f"     å½±éŸ¿: {gap_info['impact_on_usability']}")
    
    log.info("\nğŸ­ å®¢è¦³çš„è¨¼æ‹  vs ä¸»è¦³çš„ä»®å®š:")
    evidence_analysis = result['evidence_analysis']
    log.info(f"  ğŸ“Š å®¢è¦³çš„è¨¼æ‹ : {evidence_analysis['objective_evidence_ratio']:.1%}")
    log.info(f"  ğŸ’­ ä¸»è¦³çš„ä»®å®š: {evidence_analysis['subjective_assumptions_ratio']:.1%}")
    log.info(f"  ğŸ”¬ å®Ÿè¨¼çš„æ¤œè¨¼ãƒ¬ãƒ™ãƒ«: {evidence_analysis['empirical_validation_level']}")
    
    log.info("\nğŸš¨ é‡è¦ãªå®Ÿç”¨æ€§é˜»å®³è¦å› :")
    barriers = result['usability_barriers']
    for category, barrier_list in barriers.items():
        log.info(f"  ğŸ“‚ {category}:")
        for barrier in barrier_list[:2]:  # æœ€åˆã®2ã¤ã‚’è¡¨ç¤º
            log.info(f"    - {barrier}")
    
    log.info("\nğŸ“‹ æ”¹å–„è¦ä»¶:")
    requirements = result['improvement_requirements']
    log.info("  ğŸ”´ Critical Priority:")
    for req in requirements['minimum_requirements']['critical_priority']:
        log.info(f"    â€¢ {req}")
    
    estimates = requirements['improvement_estimates']
    log.info(f"\nâ±ï¸  æ”¹å–„äºˆæƒ³:")
    log.info(f"  - æœ€å°å®Ÿç”¨ç‰ˆã¾ã§: {estimates['time_to_minimum_viable']}")
    log.info(f"  - æœ¬æ ¼é‹ç”¨ã¾ã§: {estimates['time_to_production_ready']}")
    log.info(f"  - ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {estimates['risk_level']}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        result = run_practical_usability_verification()
        
        log.info("\nğŸ‰ å®Ÿç”¨æ€§æ¤œè¨¼å®Œäº†!")
        
        # é‡è¦ãªç™ºè¦‹ã®è¦ç´„
        overall_score = result['overall_practicality_score']
        if overall_score < 0.6:
            log.warning("âš ï¸  è­¦å‘Š: ç¾åœ¨ã®å®Ÿç”¨æ€§ã¯é™å®šçš„ã§ã™")
            log.warning("å®Ÿç’°å¢ƒã§ã®é‹ç”¨å‰ã«å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã§ã™")
        
        log.info("è©³ç´°çµæœ: practical_usability_verification.json")
        
        return result
        
    except Exception as e:
        log.error(f"å®Ÿç”¨æ€§æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()